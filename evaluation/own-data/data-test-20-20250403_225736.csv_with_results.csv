Resource,Prompt,Rego intent,Difficulty,Reference output,Intent,LLM Output #0,LLM Plannable? #0,LLM Correct? #0,LLM Plan Phase Error #0,LLM OPA match phase Error #0,LLM Notes #0,LLM Output #1,LLM Plannable? #1,LLM Correct? #1,LLM Plan Phase Error #1,LLM OPA match phase Error #1,LLM Notes #1,LLM Output #2,LLM Plannable? #2,LLM Correct? #2,LLM Plan Phase Error #2,LLM OPA match phase Error #2,LLM Notes #2
"aws_api_gateway_integration, aws_api_gateway_integration, aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_dynamodb_table, aws_iam_role, aws_iam_role, aws_lambda_function, aws_lambda_function, aws_lambda_permission, aws_lambda_permission, aws_s3_bucket","An AWS service that holds a web server which allows you to upload cat pictures and provides random cat pictures on demand. Accomplish this using the following resources: AWS DynamoDB table, AWS S3 bucket, AWS Lambda function, AWS Lambda permission, AWS API Gateway rest API, AWS API Gateway resource, AWS API Gateway method. Add any necessary resources.","package caas_middle

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
	api.address in cat.expressions.parent_id.references
	api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
	method.expressions.http_method.constant_value == method_string
	resource.address in method.expressions.resource_id.references
	api.address in method.expressions.rest_api_id.references
}

table_valid(table) if {
	some attribute in table.expressions.attribute
	attribute.name
	attribute.type

	table.expressions.hash_key
}

lambda_valid(lambda, bucket) if {
	some env in lambda.expressions.environment
	bucket.address in env.variables.references
}

permission_valid(permission, lambda, api) if {
	permission.expressions.action.constant_value == ""lambda:InvokeFunction""
	lambda.address in permission.expressions.function_name.references
	permission.expressions.principal.constant_value == ""apigateway.amazonaws.com""
	api.address in permission.expressions.source_arn.references
}

integration_valid(integration, lambda, method, resource, api, integration_method) if {
	method.address in integration.expressions.http_method.references
	resource.address in integration.expressions.resource_id.references
	api.address in integration.expressions.rest_api_id.references
	integration.expressions.integration_http_method.constant_value == integration_method
	integration.expressions.type.constant_value == ""AWS_PROXY""
	lambda.address in integration.expressions.uri.references
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some api in resources
	api.type == ""aws_api_gateway_rest_api""

	some cat in resources
	cat.type == ""aws_api_gateway_resource""

	some method_get in resources
	method_get.type == ""aws_api_gateway_method""

	some method_put in resources
	method_put.type == ""aws_api_gateway_method""

	some table in resources
	table.type == ""aws_dynamodb_table""

	some lambda_get in resources
	lambda_get.type == ""aws_lambda_function""

	some lambda_put in resources
	lambda_put.type == ""aws_lambda_function""

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	some permission_get in resources
	permission_get.type == ""aws_lambda_permission""

	some permission_put in resources
	permission_put.type == ""aws_lambda_permission""

	some integration_get in resources
	integration_get.type == ""aws_api_gateway_integration""

	some integration_put in resources
	integration_put.type == ""aws_api_gateway_integration""

	api_valid(api)
	cat_valid(cat, api)
	method_valid(method_get, ""GET"", cat, api)
	method_valid(method_put, ""PUT"", cat, api)
	lambda_valid(lambda_get, bucket, archive_get)
	lambda_valid(lambda_put, bucket, archive_put)
	permission_valid(permission_get, lambda_get, api)
	permission_valid(permission_put, lambda_put, api)
	integration_valid(integration_get, lambda_get, method_get, cat, api, ""GET"")
	integration_valid(integration_put, lambda_put, method_put, cat, api, ""PUT"")
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_api resource id
    with ""resource_id"" linking to aws_api_gateway_resource resource
    with ""authorization"" argument
    with ""http_method"" argument with values ""GET"" and ""PUT""
have one ""aws_s3_bucket"" resource
have one ""aws_dynamodb_table"" resource
    with ""name"" argument
    with ""hash_key"" argument
    with ""attribute"" argument block
        with ""name"" argument
        with ""type"" argument
with two ""aws_lambda_function"" resources
    with ""function_name"" argument
    with ""role"" argument
    with ""environment"" argument block
        with ""variables"" argument block linking to the aws_s3_bucket
with two ""aws_lambda_permission"" resources
    with ""action"" argument with value ""lambda:InvokeFunction""
    with ""principal"" argument with value ""apigateway.amazonaws.com""
    with ""function_name"" argument linking to aws_lambda_function resource
    with ""source_arn"" argument linking to aws_api_gateway_rest_api (two different end points)
with two ""aws_api_gateway_integration"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource resource
    with ""http_method"" argument linking to aws_api_gateway_method resource
    with ""type"" argument with value ""AWS_PROXY""
    with ""integration_http_method"" argument with values ""GET"" and ""PUT""","I'll help you create a Terraform configuration for a cat picture service using the specified AWS resources. This solution will be similar to Example 1 but I'll ensure it meets your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# DynamoDB table to store cat picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures"" {
  name           = ""cat_pictures""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# S3 bucket to store cat pictures
resource ""aws_s3_bucket"" ""cat_storage"" {
  bucket_prefix = ""cat-pictures-storage""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_service_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""lambda.amazonaws.com""
      }
    }]
  })
}

# IAM policy for the Lambda role
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_service_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_storage.arn,
          ""${aws_s3_bucket.cat_storage.arn}/*""
        ]
      },
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = aws_dynamodb_table.cat_pictures.arn
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_service"" {
  filename         = ""lambda_function.zip""
  function_name    = ""cat_picture_service""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME = aws_s3_bucket.cat_storage.id
      TABLE_NAME  = aws_dynamodb_table.cat_pictures.name
    }
  }
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat_picture_api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway GET method
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# API Gateway POST method
resource ""aws_api_gateway_method"" ""upload_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""POST""
  ",,,,,,"I'll help you create a Terraform configuration for a cat picture service using the specified AWS resources. This solution will be similar to Example 1 but focused on the specific resources you mentioned.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# DynamoDB table to store cat picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures"" {
  name           = ""cat_pictures""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# S3 bucket to store cat pictures
resource ""aws_s3_bucket"" ""cat_storage"" {
  bucket_prefix = ""cat-pictures-storage""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_service_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""lambda.amazonaws.com""
      }
    }]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_service_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_storage.arn,
          ""${aws_s3_bucket.cat_storage.arn}/*""
        ]
      },
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = aws_dynamodb_table.cat_pictures.arn
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_service"" {
  filename      = ""lambda_function.zip""  # You'll need to provide this
  function_name = ""cat_picture_service""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME    = aws_s3_bucket.cat_storage.id
      DYNAMODB_TABLE = aws_dynamodb_table.cat_pictures.name
    }
  }
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat_picture_api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway methods
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""upload_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_",,,,,,"I'll help you create a Terraform configuration for a cat picture service using AWS services. This solution will be similar to Example 1 but I'll ensure it meets your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# DynamoDB table to store cat picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures"" {
  name           = ""cat_pictures""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# S3 bucket to store cat pictures
resource ""aws_s3_bucket"" ""cat_storage"" {
  bucket_prefix = ""cat-pictures-storage""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_service_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""lambda.amazonaws.com""
      }
    }]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_service_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_storage.arn,
          ""${aws_s3_bucket.cat_storage.arn}/*""
        ]
      },
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = aws_dynamodb_table.cat_pictures.arn
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_service"" {
  filename         = ""lambda_function.zip""
  function_name    = ""cat_picture_service""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      S3_BUCKET = aws_s3_bucket.cat_storage.id
      DYNAMODB_TABLE = aws_dynamodb_table.cat_pictures.name
    }
  }
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat_picture_api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway GET method
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# API Gateway POST method
resource ""aws_api_gateway_method"" ""upload_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_metho",,,,,
"aws_codebuild_project, aws_iam_role, aws_s3_bucket, aws_s3_bucket",create a AWS codebuild project resource with example iam role and secondary artifiacts,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for aws_iam_role resource
has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""codebuild.amazonaws.com"") 
} 

has_valid_codebuild {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_codebuild_project""
        resource.values.artifacts
    resource.values.environment
    resource.values.name
    resource.values.source[_].type
    resource.values.secondary_artifacts[_].type
    role := input.configuration.root_module.resources[i]
    role.expressions.service_role
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    
}


has_valid_resources {
        has_valid_iam_role(input.resource_changes)
    has_valid_codebuild
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""test_role8"" {
  name = ""test_role8""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Sid    = """"
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      },
    ]
  })
}

resource ""aws_s3_bucket"" ""apriltwentyninth"" {
  bucket = ""apriltwentyninth""
}

resource ""aws_s3_bucket"" ""apriltwentyninth2"" {
  bucket = ""apriltwentyninth2""
}


resource ""aws_codebuild_project"" ""example7"" {
  name          = ""test-project8""
  service_role  = aws_iam_role.test_role8.arn

  artifacts {
    location  = aws_s3_bucket.apriltwentyninth.bucket
    type      = ""S3""
    name     = ""results.zip""
    path      = ""/""
    packaging = ""ZIP""
  }

  secondary_artifacts {
    artifact_identifier =  ""SecondaryArtifact""
    type = ""S3""
    location  = aws_s3_bucket.apriltwentyninth2.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:7.0-24.10.29""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  source_version = ""master""

  secondary_sources {
    source_identifier = ""source2""
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  secondary_source_version {
    source_identifier = ""source2""
    source_version = ""master""
  }
}","Has one ""aws_iam_role"" resource, at least one ""aws_s3_bucket"" resource, and one ""aws_codebuild_project"" resource with a ""secondary_artifacts"" block","I'll help you create an AWS CodeBuild project with secondary artifacts and an IAM role. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM role for CodeBuild
data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""codebuild_role"" {
  name               = ""codebuild-secondary-artifacts-role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

# S3 bucket for artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild project
resource ""aws_codebuild_project"" ""project"" {
  name          = ""project-with-secondary-artifacts""
  service_role  = aws_iam_role.codebuild_role.arn
  
  artifacts {
    type = ""S3""
    location = aws_s3_bucket.artifact_bucket.bucket
    name = ""primary-artifact""
    packaging = ""ZIP""
  }

  secondary_artifacts {
    artifact_identifier = ""secondary_artifact1""
    type = ""S3""
    location = aws_s3_bucket.artifact_bucket.bucket
    name = ""secondary-artifact-1""
    packaging = ""ZIP""
    path = ""/secondary1""
  }

  secondary_artifacts {
    artifact_identifier = ""secondary_artifact2""
    type = ""S3""
    location = aws_s3_bucket.artifact_bucket.bucket
    name = ""secondary-artifact-2""
    packaging = ""ZIP""
    path = ""/secondary2""
  }

  environment {
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
  }

  cache {
    type = ""NO_CACHE""
  }
}

# Add necessary IAM policies for S3 access
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.artifact_bucket.arn}"",
          ""${aws_s3_bucket.artifact_bucket.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      }
    ]
  })
}
```

This configuration creates:
1. An IAM role for CodeBuild with necessary permissions
2. An S3 bucket to store artifacts
3. A CodeBuild project with:
   - Primary artifacts configuration
   - Two secondary artifacts configurations
   - Basic environment setup
   - GitHub source configuration
4. IAM policy for S3 access

The secondary artifacts are configured to store different outputs in different paths within the S3 bucket. Each secondary artifact has its own identifier and path configuration.

Make sure to:
1. Replace the S3 bucket name with a globally unique name
2. Update the GitHub repository URL to your actual repository
3. Adjust the IAM permissions",,,,,,"I'll help you create an AWS CodeBuild project with secondary artifacts and an IAM role. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create S3 buckets for artifacts
resource ""aws_s3_bucket"" ""primary_artifacts"" {
  bucket = ""primary-artifacts-bucket""
}

resource ""aws_s3_bucket"" ""secondary_artifacts"" {
  bucket = ""secondary-artifacts-bucket""
}

# Create IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-project-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# Create the CodeBuild project
resource ""aws_codebuild_project"" ""project"" {
  name          = ""project-with-secondary-artifacts""
  service_role  = aws_iam_role.codebuild_role.arn
  
  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
  }

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.primary_artifacts.bucket
  }

  secondary_artifacts {
    artifact_identifier = ""secondaryArtifact1""
    type               = ""S3""
    location           = aws_s3_bucket.secondary_artifacts.bucket
    path              = ""/secondary1""
    packaging         = ""ZIP""
    namespace_type    = ""BUILD_ID""
  }

  secondary_artifacts {
    artifact_identifier = ""secondaryArtifact2""
    type               = ""S3""
    location           = aws_s3_bucket.secondary_artifacts.bucket
    path              = ""/secondary2""
    packaging         = ""ZIP""
    namespace_type    = ""BUILD_ID""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:7.0-24.10.29""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }
}

# Add necessary IAM policies for S3 access
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
        Resource = [
          aws_s3_bucket.primary_artifacts.arn,
          ""${aws_s3_bucket.primary_artifacts.arn}/*"",
          aws_s3_bucket.secondary_artifacts.arn,
          ""${aws_s3_bucket.secondary_artifacts.arn}/*""
        ]
      }
    ]
  })
}
```

This configuration:
1. Creates two S3 buckets for storing primary and secondary artifacts
2. Sets up an IAM role for CodeBuild with necessary permissions
3. Creates a CodeBuild project with:
   - Primary artifacts configuration
   - Two secondary artifacts configurations
   - GitHub as the source provider
   - Standard environment configuration
4. Adds an IAM policy to allow CodeBuild to interact with the S3 buckets

Note: Make sure to replace",,,,,,"I'll help you create an AWS CodeBuild project with secondary artifacts configuration. Here's the Terraform code solution based on the provided examples and best practices:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM role for CodeBuild
data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""codebuild_role"" {
  name               = ""codebuild-secondary-artifacts-role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

# Create S3 buckets for artifacts
resource ""aws_s3_bucket"" ""primary_artifacts"" {
  bucket = ""primary-artifacts-bucket""
}

resource ""aws_s3_bucket"" ""secondary_artifacts"" {
  bucket = ""secondary-artifacts-bucket""
}

# Create CodeBuild project
resource ""aws_codebuild_project"" ""project"" {
  name          = ""project-with-secondary-artifacts""
  service_role  = aws_iam_role.codebuild_role.arn
  description   = ""CodeBuild project with primary and secondary artifacts""

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.primary_artifacts.bucket
  }

  secondary_artifacts {
    artifact_identifier = ""secondaryArtifact""
    type               = ""S3""
    location           = aws_s3_bucket.secondary_artifacts.bucket
    path              = ""/secondary""
    packaging         = ""ZIP""
    name              = ""secondary_output.zip""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:7.0-24.10.29""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
  }

  logs_config {
    cloudwatch_logs {
      status = ""ENABLED""
    }
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role for CodeBuild with assume role policy
3. Creates two S3 buckets - one for primary artifacts and one for secondary artifacts
4. Creates a CodeBuild project with:
   - Primary artifacts configuration pointing to the first S3 bucket
   - Secondary artifacts configuration pointing to the second S3 bucket
   - Standard environment configuration
   - GitHub source repository
   - Basic logs configuration

The secondary artifacts configuration allows you to output additional artifacts to a separate location during the build process. You can customize the artifact names, paths, and packaging according to your needs.

Make sure to:
- Replace the bucket names with unique values that match your naming requirements
- Update the GitHub repository location to your actual repository
- Add any additional IAM permissions needed for your specific use case
- Adjust the compute type and environment settings as needed for your build requirements",,,,,
"aws_db_instance, aws_db_subnet_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc","sets up an AWS environment with a main VPC, two subnets across different availability zones, and a security group to allow database access. It also configures a 20 GB 15.3 PostgresSQL RDS instance with public accessibility, linked to the defined subnets and security group, outputting the database endpoint for connectivity.","package terraform.validation

default is_valid_db_instance = false

default is_valid_vpc = false

default is_valid_subnet = false

default is_valid_security_group = false

default is_valid_db_subnet_group = false

# Validate aws_db_instance resource
is_valid_db_instance {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_instance""
        resource.expressions.allocated_storage.constant_value == 20
        resource.expressions.engine.constant_value == ""postgres""
        resource.expressions.engine_version.constant_value == ""15.3""
        resource.expressions.instance_class.constant_value != null
        resource.expressions.password.constant_value != null
        resource.expressions.username.constant_value != null
        resource.expressions.publicly_accessible.constant_value == true
        resource.expressions.db_subnet_group_name.references != null
        resource.expressions.vpc_security_group_ids.references != null
}

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# Validate aws_subnet resource (more than one)
is_valid_subnet {
        count(subnets) > 1
}

subnets[resource] {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
}

# Validate aws_security_group resource
is_valid_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""
        # Additional checks for specific ingress/egress rules can be added here
}

# Validate aws_db_subnet_group resource
is_valid_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}
",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type           = ""gp2""
  engine                 = ""postgres""
  engine_version         = ""17.2""
  instance_class         = ""db.t4g.micro""
  skip_final_snapshot    = true
  db_name                = ""mydbname""
  username               = ""username""
  password               = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name   = aws_db_subnet_group.main.id
  publicly_accessible    = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""
    with one publicly_accessible = true

has one aws_vpc

has more than one aws_subnet

has one security_group

has one db_subnet_group","I'll help you create a Terraform configuration for your AWS environment with a PostgreSQL RDS instance based on your requirements. This solution will be similar to Example 1 but with modifications for your specific needs.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type          = ""gp2""
  engine                = ""postgres""
  engine_version        = ""15.3""
  instance_class        = ""db.t3.micro""
  skip_final_snapshot   = true
  db_name               = ""mydbname""
  username              = ""username""
  password              = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name  = aws_db_subnet_group.main.id
  publicly_accessible   = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

output ""db_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with two private and two public subnets across different AZs
2. Creates a security group allowing PostgreSQL traffic (port 5432)
3. Provisions a 20GB PostgreSQL 15.3 RDS instance
4. Makes the database publicly accessible
5. Creates necessary subnet groups and security group associations
6. Outputs the database endpoint for connectivity

The main differences from the example are:
- Reduced storage to 20GB
- Updated PostgreSQL version to 15.3
- Added an output for the database endpoint
- Simplified provider configuration
- Using db.t3.micro instance class

Remember to replace the username and password values with secure credentials in a production environment, preferably using variables or secrets management.",,,,,,"I'll help you create a Terraform configuration for your AWS environment with a PostgreSQL RDS instance based on your requirements. This solution will be similar to Example 1 but with your specific parameters.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type          = ""gp2""
  engine                = ""postgres""
  engine_version        = ""15.3""
  instance_class        = ""db.t3.micro""
  skip_final_snapshot   = true
  db_name               = ""mydbname""
  username              = ""username""
  password              = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name  = aws_db_subnet_group.main.id
  publicly_accessible   = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

output ""database_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with two private and two public subnets
2. Creates a security group for PostgreSQL access (port 5432)
3. Provisions a 20GB PostgreSQL 15.3 RDS instance
4. Makes the database publicly accessible
5. Creates necessary subnet groups and security group rules
6. Outputs the database endpoint for connectivity

The main differences from the example are:
- Reduced storage to 20GB
- Changed PostgreSQL version to 15.3
- Added an output for the database endpoint
- Simplified provider configuration
- Using db.t3.micro as the instance class",,,,,,"I'll help you create a Terraform configuration for your AWS environment with a PostgreSQL RDS instance based on your requirements. This solution will be similar to Example 1 but with modifications for your specific needs.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type          = ""gp2""
  engine                = ""postgres""
  engine_version        = ""15.3""
  instance_class        = ""db.t3.micro""
  skip_final_snapshot   = true
  db_name               = ""mydbname""
  username              = ""username""
  password              = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name  = aws_db_subnet_group.main.id
  publicly_accessible   = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

output ""database_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with two private and two public subnets across different AZs
2. Creates a security group for PostgreSQL access (port 5432)
3. Configures a 20 GB PostgreSQL 15.3 RDS instance
4. Makes the RDS instance publicly accessible
5. Outputs the database endpoint
6. Uses gp2 storage type and db.t3.micro instance class for cost-effectiveness
7. Creates necessary subnet groups and security group rules

The database will be accessible via the endpoint provided in the output after applying this configuration.",,,,,
"aws_rds_cluster, aws_vpc","Create an mySQL cluster with 100 GB allocated to us-east-1a, us-east-1b, us-east-1c.","package main

import future.keywords.in

default allow = false

# Required AZs
required_azs := [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]

# Check for RDS cluster creation
rds_cluster_created(resources) {
some resource in resources
resource.type == ""aws_rds_cluster""
resource.change.actions[_] == ""create""
}

# Check if engine is MySQL
rds_engine_mysql(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.engine == ""mysql""
}

# Check if AZs are correct and storage is 100GB
azs_and_storage_valid(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.availability_zones == required_azs
resource.change.after.allocated_storage == 100
}

# Aggregate checks for RDS cluster
allow {
rds_cluster_created(input.resource_changes)
some resource in input.resource_changes
rds_engine_mysql(resource)
azs_and_storage_valid(resource)
}",3,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

# Configure the AWS Provider
provider ""aws"" {
region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
cidr_block = ""192.168.0.0/22""
}

resource ""aws_rds_cluster"" ""example"" {
cluster_identifier = ""example""
availability_zones = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
engine = ""mysql""
db_cluster_instance_class = ""db.r6gd.large""
storage_type = ""io1""
allocated_storage = 100
iops = 1000
master_username = ""test""
master_password = ""mustbeeightcharaters""
skip_final_snapshot = true
}
","Has one ""aws_rds_cluster"" resource
with one ""allocated_storage"" = 100
with one ""availability_zones"" = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
with one ""engine"" = ""mysql""","I'll help you create a MySQL cluster with 100 GB allocated across three availability zones in us-east-1. Based on the examples provided, I'll create a configuration that meets your requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Configure the AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

# Create a VPC for the RDS cluster
resource ""aws_vpc"" ""vpc"" {
  cidr_block = ""192.168.0.0/22""
}

# Create the RDS cluster
resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier      = ""mysql-cluster""
  engine                 = ""mysql""
  availability_zones     = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
  db_cluster_instance_class = ""db.r6gd.large""
  storage_type           = ""io1""
  allocated_storage      = 100
  iops                   = 1000
  
  # Database credentials
  master_username        = ""admin""
  master_password        = ""MySQLPassword123""  # Remember to use secure password management in production
  
  # Additional configuration
  skip_final_snapshot    = true
  
  # Enable deletion protection for production environments
  deletion_protection    = false
}
```

This configuration will:
1. Set up the AWS provider for us-east-1 region
2. Create a VPC with CIDR block 192.168.0.0/22
3. Create a MySQL cluster with:
   - 100 GB storage allocated
   - Deployment across three AZs (us-east-1a, us-east-1b, us-east-1c)
   - IO optimized storage with 1000 IOPS
   - Basic security settings (modify for production use)

Note: For production environments, you should:
- Use secure password management
- Enable deletion protection
- Configure proper backup retention
- Set up appropriate security groups and subnet groups
- Consider encrypting the database",,,,,,"I'll help you create a MySQL cluster with 100 GB allocated across three availability zones in us-east-1. Based on the examples provided, I'll generate a Terraform configuration that meets your requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Configure the AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

# Create VPC for the RDS cluster
resource ""aws_vpc"" ""vpc"" {
  cidr_block = ""192.168.0.0/22""
}

# Create the RDS cluster
resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier      = ""mysql-cluster""
  engine                 = ""mysql""
  availability_zones     = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
  db_cluster_instance_class = ""db.r6gd.large""
  storage_type           = ""io1""
  allocated_storage      = 100
  iops                   = 1000
  
  # Database credentials
  master_username        = ""admin""
  master_password        = ""mysqlpassword123""  # Replace with secure password
  
  # Additional configurations
  skip_final_snapshot    = true
  
  # Enable encryption
  storage_encrypted      = true
}
```

This configuration:
1. Sets up the AWS provider for us-east-1 region
2. Creates a VPC with CIDR block 192.168.0.0/22
3. Creates a MySQL cluster with:
   - 100 GB storage allocation
   - Deployment across three AZs (us-east-1a, us-east-1b, us-east-1c)
   - IO1 storage type for better performance
   - 1000 IOPS
   - Encryption enabled
   - Skip final snapshot for easier cleanup

Remember to:
- Replace the master password with a secure value
- Consider adding additional security groups and subnet configurations based on your network requirements
- Adjust the IOPS value based on your performance needs
- Consider adding backup retention and maintenance window configurations if needed",,,,,,"I'll help you create a MySQL cluster with 100 GB allocated across three availability zones in us-east-1. Based on the examples provided, I'll create a configuration that meets your requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Configure the AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

# Create a VPC for the RDS cluster
resource ""aws_vpc"" ""vpc"" {
  cidr_block = ""192.168.0.0/22""
}

# Create the RDS cluster
resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier      = ""mysql-cluster""
  engine                  = ""mysql""
  availability_zones      = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
  db_cluster_instance_class = ""db.r6gd.large""
  storage_type           = ""io1""
  allocated_storage      = 100
  iops                   = 1000
  
  # Database credentials
  master_username        = ""admin""
  master_password        = ""password123!"" # Replace with secure password in production
  
  # Additional configurations
  skip_final_snapshot    = true
  
  # Enable encryption
  storage_encrypted      = true
}
```

This configuration:
1. Sets up the AWS provider for us-east-1 region
2. Creates a VPC with the CIDR block 192.168.0.0/22
3. Creates a MySQL cluster with:
   - 100 GB storage allocation
   - Deployment across three AZs (us-east-1a, us-east-1b, us-east-1c)
   - IO1 storage type for better performance
   - 1000 IOPS
   - Basic security settings

Note: In a production environment, you should:
- Use a more secure password
- Configure additional security groups
- Set up proper backup retention
- Configure parameter groups as needed
- Add additional networking components like subnets",,,,,
"aws_cloudwatch_event_rule, aws_cloudwatch_event_target, aws_iam_role, aws_lambda_function, aws_lambda_permission, archive_file, aws_iam_policy_document",Create a Lambda function and invoke it every 15 minutes,"package main

import future.keywords.in

default allow = false

# Check if AWS Lambda function is being created
aws_lambda_function_created(resources) {
    some resource in resources
    resource.type == ""aws_lambda_function""
}

# Check if CloudWatch Event Rule is set to invoke Lambda every 15 minutes
cloudwatch_event_rule_for_lambda_valid(resources) {
    some resource in resources
    resource.type == ""aws_cloudwatch_event_rule""
    resource.change.after.schedule_expression == ""rate(15 minutes)""
}

# Aggregate all checks
allow {
    aws_lambda_function_created(input.resource_changes)
    cloudwatch_event_rule_for_lambda_valid(input.resource_changes)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_cloudwatch_event_rule"" ""lambda_schedule"" {
  name        = ""lambda-schedule-rule""
  description = ""Invoke Lambda function every 15 minutes""
  schedule_expression = ""rate(15 minutes)""
  role_arn = aws_iam_role.cron.arn
}

# Create a CloudWatch Events Target to associate with the Lambda function
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule = aws_cloudwatch_event_rule.lambda_schedule.name
  arn = aws_lambda_function.cron.arn
}

data ""archive_file"" ""lambda-func"" {
  type        = ""zip""
  source_file = ""./supplement/lambda_func.py""
  output_path = ""./supplement/lambda_func.zip""
}

resource ""aws_lambda_function"" ""cron"" {
  function_name = ""cron-lambda-function""
  role          = aws_iam_role.cron.arn
  filename      = data.archive_file.lambda-func.output_path
  source_code_hash = data.archive_file.lambda-func.output_base64sha256
  handler       = ""lambda_func.handler""
  runtime       = ""python3.12""
}

resource ""aws_lambda_permission"" ""cron"" {
  function_name = aws_lambda_function.cron.function_name
  action        = ""lambda:InvokeFunction""
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.lambda_schedule.arn
}

data ""aws_iam_policy_document"" ""cron_assume_role"" {
  statement {
    actions = [""sts:AssumeRole""]
    principals {
      type = ""Service""
      identifiers = [
        ""lambda.amazonaws.com"",
        ""events.amazonaws.com""
      ]
    }
  }
}

resource ""aws_iam_role"" ""cron"" {
  name               = ""cron_assume_role""
  assume_role_policy = data.aws_iam_policy_document.cron_assume_role.json
}

resource ""aws_iam_role_policy_attachment"" ""cron"" {
  role       = aws_iam_role.cron.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}
",Needs to have an aws_lambda_function defined. Create a aws_cloudwatch_event_rule to trigger every 15 minutes and aws_lambda_permission to have permission to run lambda function. Create a target to trigger the lambda function.,"Based on the provided examples and your requirement to create a Lambda function that runs every 15 minutes, I'll create a Terraform configuration that uses AWS EventBridge (CloudWatch Events) to trigger the Lambda function on a schedule. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# EventBridge rule to trigger every 15 minutes
resource ""aws_cloudwatch_event_rule"" ""fifteen_minute_trigger"" {
  name                = ""fifteen-minute-trigger""
  description         = ""Triggers Lambda function every 15 minutes""
  schedule_expression = ""rate(15 minutes)""
}

# Event target to connect the rule to the Lambda function
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule      = aws_cloudwatch_event_rule.fifteen_minute_trigger.name
  target_id = ""SendToLambda""
  arn       = aws_lambda_function.scheduled_lambda.arn
}

# Lambda function
resource ""aws_lambda_function"" ""scheduled_lambda"" {
  filename         = ""lambda_function.zip""
  function_name    = ""scheduled-task-lambda""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs18.x""
  source_code_hash = filebase64sha256(""lambda_function.zip"")
}

# Lambda permission to allow EventBridge to invoke the function
resource ""aws_lambda_permission"" ""allow_eventbridge"" {
  statement_id  = ""AllowEventBridgeInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.scheduled_lambda.function_name
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.fifteen_minute_trigger.arn
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""scheduled_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Attach basic Lambda execution role policy
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}
```

This configuration:

1. Creates an EventBridge (CloudWatch Events) rule that triggers every 15 minutes using the `rate(15 minutes)` expression
2. Creates a Lambda function with the necessary IAM role and permissions
3. Sets up the event target to connect the EventBridge rule to the Lambda function
4. Configures the required IAM roles and permissions for the Lambda function to execute and for EventBridge to invoke it

Note: You'll need to create and provide the `lambda_function.zip` file containing your Lambda function code in the same directory as your Terraform configuration.

The Lambda function will be triggered automatically every 15 minutes after deployment. The basic execution role allows the Lambda function to create CloudWatch logs for monitoring and troubleshooting.",,,,,,"I'll help you create a Terraform configuration that sets up a Lambda function and an EventBridge (CloudWatch Events) rule to invoke it every 15 minutes. This solution is similar to the examples provided but with a modified schedule expression.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# EventBridge rule that triggers every 15 minutes
resource ""aws_cloudwatch_event_rule"" ""fifteen_minute_trigger"" {
  name                = ""fifteen-minute-trigger""
  description         = ""Triggers Lambda function every 15 minutes""
  schedule_expression = ""rate(15 minutes)""
}

# EventBridge target to specify which Lambda to invoke
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule      = aws_cloudwatch_event_rule.fifteen_minute_trigger.name
  target_id = ""LambdaFunction""
  arn       = aws_lambda_function.scheduled_lambda.arn
}

# Lambda function
data ""archive_file"" ""lambda_zip"" {
  type        = ""zip""
  source_file = ""./lambda_function.py""  # Make sure this file exists
  output_path = ""./lambda_function.zip""
}

resource ""aws_lambda_function"" ""scheduled_lambda"" {
  filename         = data.archive_file.lambda_zip.output_path
  function_name    = ""fifteen-minute-function""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""lambda_function.handler""
  source_code_hash = data.archive_file.lambda_zip.output_base64sha256
  runtime         = ""python3.12""
}

# Lambda permission to allow EventBridge to invoke the function
resource ""aws_lambda_permission"" ""allow_eventbridge"" {
  statement_id  = ""AllowEventBridgeInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.scheduled_lambda.function_name
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.fifteen_minute_trigger.arn
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""fifteen_minute_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Attach basic Lambda execution role policy
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}
```

This configuration:
1. Creates an EventBridge rule that triggers every 15 minutes using `rate(15 minutes)` expression
2. Sets up a Lambda function with the necessary IAM roles and permissions
3. Creates an EventBridge target that connects the rule to the Lambda function
4. Includes the required permissions for EventBridge to invoke the Lambda function

Note: You'll need to create a `lambda_function.py` file in your working directory that contains your Lambda function code. The function should have a `handler` method that will be executed every 15 minutes.

A basic `lambda_function.py` might look like this:
```python
def handler(event, context):
    print(""Function executed successfully"")
    return {
        'statusCode': 200,
        'body': 'Function executed successfully'
    }
```",,,,,,"I'll help you create a Terraform configuration for a Lambda function that gets invoked every 15 minutes using EventBridge (CloudWatch Events). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# EventBridge Rule (runs every 15 minutes)
resource ""aws_cloudwatch_event_rule"" ""fifteen_minute_rule"" {
  name                = ""every-fifteen-minutes""
  description         = ""Fires every 15 minutes""
  schedule_expression = ""rate(15 minutes)""
}

# EventBridge Target
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule      = aws_cloudwatch_event_rule.fifteen_minute_rule.name
  target_id = ""SendToLambda""
  arn       = aws_lambda_function.scheduled_lambda.arn
}

# Lambda Function
resource ""aws_lambda_function"" ""scheduled_lambda"" {
  filename         = ""lambda_function.zip""  # Make sure this file exists
  function_name    = ""fifteen-minute-lambda""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs18.x""

  source_code_hash = filebase64sha256(""lambda_function.zip"")
}

# Lambda Permission for EventBridge
resource ""aws_lambda_permission"" ""allow_eventbridge"" {
  statement_id  = ""AllowEventBridgeInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.scheduled_lambda.function_name
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.fifteen_minute_rule.arn
}

# IAM Role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""fifteen_minute_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Attach basic Lambda execution role policy
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}
```

This configuration:

1. Creates an EventBridge rule that triggers every 15 minutes using `rate(15 minutes)` expression
2. Sets up a Lambda function with the necessary IAM role and permissions
3. Creates an EventBridge target that connects the rule to the Lambda function
4. Sets up the required IAM roles and policies for the Lambda function to execute

Key points to note:
- You'll need to provide your Lambda function code in a file named `lambda_function.zip`
- The Lambda runtime is set to Node.js 18.x, but you can change it to your preferred runtime
- The basic Lambda execution role is attached, which provides permissions for CloudWatch Logs
- The configuration includes the necessary permissions for EventBridge to invoke the Lambda function

Make sure to:
1. Replace the Lambda function's filename with your actual deployment package
2. Adjust the handler name according to your function's entry point
3. Modify the runtime if you're using a different programming language
4. Update the region in the provider block if needed",,,,,
"aws_dax_cluster, aws_iam_role",Create a cluster with dax.r4.large and replication factor = 1.,"package main

import future.keywords.in

default allow = false

# Check if the AWS DynamoDB table is being created in us-east-1
aws_dax_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_dax_cluster""
    resource.change.after.node_type == ""dax.r4.large""
    resource.change.after.replication_factor == 1
}

# Aggregate all checks
allow {
    aws_dax_cluster_valid(input.resource_changes)
}",3,"resource ""aws_iam_role"" ""example"" {
  name = ""redshift_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""redshift.amazonaws.com""
        },
      },
    ],
  })
}

resource ""aws_dax_cluster"" ""bar"" {
  cluster_name       = ""cluster-example""
  iam_role_arn       = aws_iam_role.example.arn
  node_type          = ""dax.r4.large""
  replication_factor = 1
}","require a cluster name, iam_role_arn, node_type, replication factor","I'll help you create a DAX cluster with the specified node type and replication factor. Based on the example provided, we'll need to create both the IAM role for DAX and the cluster itself.

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Create DAX cluster
resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an IAM role that DAX can assume
3. Creates a DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - Uses the created IAM role
   - Names the cluster ""dax-cluster""

The cluster will be created with the minimum required configuration while meeting your specific requirements for node type and replication factor.",,,,,,"I'll help you create a DAX cluster with the specified requirements using Terraform. Based on Example 1, I'll create a configuration that includes both the necessary IAM role and the DAX cluster with a dax.r4.large node type and replication factor of 1.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Create DAX cluster
resource ""aws_dax_cluster"" ""dax_cluster"" {
  cluster_name       = ""my-dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an IAM role that DAX can assume
3. Creates a DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - The IAM role created above

Note that you'll need to ensure:
- You have appropriate permissions to create these resources
- You're in a region where DAX is supported
- Your VPC settings are properly configured for DAX (you might want to add the subnet group configuration from Example 2 if needed)",,,,,,"I'll help you create a DAX cluster with the specified requirements using Terraform. Based on Example 1, I'll create a configuration that includes the necessary IAM role and the DAX cluster with a dax.r4.large node type and replication factor of 1.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax_cluster_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Create DAX cluster
resource ""aws_dax_cluster"" ""dax_cluster"" {
  cluster_name       = ""my-dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
}
```

This configuration will:
1. Set up the AWS provider
2. Create an IAM role that DAX can assume
3. Create a DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - The IAM role created above

The cluster will be created with minimal configuration while meeting your specific requirements for node type and replication factor.",,,,,
"aws_iam_role, aws_sagemaker_domain, aws_subnet, aws_vpc, aws_iam_policy_document",Create a SageMaker Domain resource.,"package main

import future.keywords.in

default allow = false


aws_sagemaker_domain_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_domain""
}

aws_iam_role_valid(resources) {
    some resource in resources
    resource.type == ""aws_iam_role""
}

aws_vpc_valid(resources) {
    some resource in resources
    resource.type == ""aws_vpc""
}

aws_subnet_valid(resources) {
    some resource in resources
    resource.type == ""aws_subnet""
}

# Aggregate all checks
allow {
    aws_sagemaker_domain_valid(input.resource_changes)
    aws_iam_role_valid(input.resource_changes)
    aws_vpc_valid(input.resource_changes)
    aws_subnet_valid(input.resource_changes)
}
",3,"resource ""aws_vpc"" ""example"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""example"" {
  vpc_id     = aws_vpc.example.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_sagemaker_domain"" ""example"" {
  domain_name = ""example""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.example.id
  subnet_ids  = [aws_subnet.example.id]

  default_user_settings {
    execution_role = aws_iam_role.example.arn
  }
}

resource ""aws_iam_role"" ""example"" {
  name               = ""example""
  path               = ""/""
  assume_role_policy = data.aws_iam_policy_document.example.json
}

data ""aws_iam_policy_document"" ""example"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""sagemaker.amazonaws.com""]
    }
  }
}",Has a sagemaker domain resources which requires a an iam role and a subnet,"I'll help you create a SageMaker Domain resource using Terraform. This example includes the necessary VPC, subnet, IAM role, and domain configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create VPC
resource ""aws_vpc"" ""sagemaker_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""sagemaker-vpc""
  }
}

# Create Subnet
resource ""aws_subnet"" ""sagemaker_subnet"" {
  vpc_id     = aws_vpc.sagemaker_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-subnet""
  }
}

# Create IAM Role
resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-domain-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Attach SageMaker full access policy to the role
resource ""aws_iam_role_policy_attachment"" ""sagemaker_full_access"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

# Create SageMaker Domain
resource ""aws_sagemaker_domain"" ""domain"" {
  domain_name = ""my-sagemaker-domain""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.sagemaker_vpc.id
  subnet_ids  = [aws_subnet.sagemaker_subnet.id]

  default_user_settings {
    execution_role = aws_iam_role.sagemaker_role.arn

    security_groups = []

    jupyter_server_app_settings {
      default_resource_spec {
        instance_type = ""ml.t3.medium""
      }
    }
  }

  tags = {
    Environment = ""Development""
    Name        = ""SageMaker-Domain""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC and subnet for the SageMaker domain
3. Creates an IAM role with the necessary permissions for SageMaker
4. Creates a SageMaker domain with:
   - IAM authentication
   - VPC and subnet configuration
   - Default user settings including execution role and Jupyter server settings
   - Basic tagging

The domain will be accessible via IAM authentication and will use the specified VPC and subnet for network isolation. The default instance type for Jupyter notebooks is set to ml.t3.medium, but you can adjust this based on your needs.

Remember to:
- Adjust the region in the provider block as needed
- Modify the CIDR blocks for VPC and subnet according to your network requirements
- Customize the tags and resource names to match your naming conventions
- Add additional security groups if required
- Modify the instance type based on your computational needs",,,,,,"I'll help you create a SageMaker Domain resource using Terraform. This will include the necessary VPC, subnet, and IAM role configurations as these are required for a SageMaker Domain.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create VPC
resource ""aws_vpc"" ""sagemaker_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""sagemaker-vpc""
  }
}

# Create Subnet
resource ""aws_subnet"" ""sagemaker_subnet"" {
  vpc_id     = aws_vpc.sagemaker_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-subnet""
  }
}

# Create IAM Role
resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-domain-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Attach SageMaker full access policy to the role
resource ""aws_iam_role_policy_attachment"" ""sagemaker_full_access"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

# Create SageMaker Domain
resource ""aws_sagemaker_domain"" ""domain"" {
  domain_name = ""my-sagemaker-domain""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.sagemaker_vpc.id
  subnet_ids  = [aws_subnet.sagemaker_subnet.id]

  default_user_settings {
    execution_role = aws_iam_role.sagemaker_role.arn

    security_groups = []

    jupyter_server_app_settings {
      default_resource_spec {
        instance_type = ""ml.t3.medium""
      }
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC and subnet for the SageMaker Domain
3. Creates an IAM role with the necessary permissions for SageMaker
4. Creates the SageMaker Domain with:
   - IAM authentication mode
   - VPC and subnet configuration
   - Default user settings including execution role and Jupyter server settings
   - Basic tagging

The domain will use IAM authentication and will be deployed in the specified VPC and subnet. The default instance type for Jupyter notebooks is set to ml.t3.medium, which you can adjust based on your needs.

Remember to:
- Adjust the CIDR blocks according to your network requirements
- Modify the tags as needed
- Consider adding additional security groups if required
- Adjust the instance type based on your workload requirements",,,,,,"I'll help you create a SageMaker Domain resource using Terraform. This will include the necessary components like VPC, subnet, and IAM role configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create VPC
resource ""aws_vpc"" ""sagemaker_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""sagemaker-vpc""
  }
}

# Create Subnet
resource ""aws_subnet"" ""sagemaker_subnet"" {
  vpc_id     = aws_vpc.sagemaker_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-subnet""
  }
}

# Create IAM Role
resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-domain-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Attach SageMaker full access policy to the role
resource ""aws_iam_role_policy_attachment"" ""sagemaker_full_access"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

# Create SageMaker Domain
resource ""aws_sagemaker_domain"" ""domain"" {
  domain_name = ""my-sagemaker-domain""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.sagemaker_vpc.id
  subnet_ids  = [aws_subnet.sagemaker_subnet.id]

  default_user_settings {
    execution_role = aws_iam_role.sagemaker_role.arn
    
    security_groups = []

    jupyter_server_app_settings {
      default_resource_spec {
        instance_type = ""system""
      }
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC and subnet for the SageMaker domain
3. Creates an IAM role with the necessary permissions for SageMaker
4. Creates a SageMaker domain with:
   - IAM authentication mode
   - VPC and subnet configuration
   - Default user settings including execution role and Jupyter server settings
   - Basic tagging

Make sure to:
- Adjust the CIDR blocks for VPC and subnet according to your network requirements
- Modify the domain name and tags as needed
- Review and adjust the IAM permissions based on your security requirements
- Customize the default user settings based on your needs",,,,,
"aws_eks_cluster, aws_iam_role, aws_iam_role_policy_attachment, aws_subnet, aws_subnet, aws_vpc","Generate a Terraform configuration to create an AWS IAM role called ""demo"" with attribute name set to ""eks-cluster-demo"", for an Amazon EKS cluster. The role should allow assuming roles for EKS services and have the AmazonEKSClusterPolicy attached. Additionally, create an AWS EKS cluster with the name ""demo"" and configure its VPC with specified subnet IDs from in the us-east-1 region. Ensure dependency handling for the IAM role's policy attachment during EKS cluster management.","package terraform.analysis

default has_aws_iam_role = false
default has_aws_iam_role_policy_attachment = false
default has_aws_eks_cluster = false

find_resource(rtype, rname) = resource {
    resource = input.configuration.root_module.resources[_]
    resource.type == rtype
    resource.name == rname
}

has_aws_iam_role {
    r := find_resource(""aws_iam_role"", ""demo"")
    r.expressions.name.constant_value == ""eks-cluster-demo""
    r.expressions.assume_role_policy
}

has_aws_iam_role_policy_attachment {
    r := input.configuration.root_module.resources[_]
    r.type == ""aws_iam_role_policy_attachment""
    r.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
    r.expressions.role.references[_] == ""aws_iam_role.demo.name""
}

has_aws_eks_cluster {
    r := find_resource(""aws_eks_cluster"", ""demo"")
    r.expressions.role_arn.references[_] == ""aws_iam_role.demo.arn""
    count(r.expressions.vpc_config[_].subnet_ids.references) >= 4
    count(r.depends_on) == 1
}

valid_config {
    has_aws_iam_role
    has_aws_iam_role_policy_attachment
    has_aws_eks_cluster
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_subnet"" ""public-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
}

resource ""aws_iam_role"" ""example"" {
  name = ""eks-cluster-demo""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""eks.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_eks_cluster"" ""demo"" {
  name     = ""cluster-name""
  role_arn = aws_iam_role.example.arn

  vpc_config {
    subnet_ids = [
      aws_subnet.private-us-east-1a.id,
      aws_subnet.public-us-east-1a.id,
    ]
  }

  depends_on = [aws_iam_role_policy_attachment.eks_cluster_policy,
                aws_iam_role_policy_attachment.eks_service_policy]
}","has one ""aws_iam_role""
with one ""name""
with one ""assume_role_policy"" 

has one ""aws_iam_role_policy_attachment"" 
with one ""policy_arn""
with one ""role"" referencing the ""aws_iam_role.demo.name""

has one ""variable"" block for ""cluster_name""
with one ""default""
with one ""type""
with one ""description""
with one ""nullable""

has one ""aws_eks_cluster"" 
with one ""name""
with one ""role_arn"" referencing the ""aws_iam_role.demo.arn""
with one ""vpc_config"" containing ""subnet_ids""
with dependencies on one IAM role policy attachment","Based on the provided examples and your requirements, I'll help you create a Terraform configuration for an EKS cluster with the specified IAM role setup. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_subnet"" ""public-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
}

resource ""aws_iam_role"" ""demo"" {
  name = ""eks-cluster-demo""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""eks.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.demo.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.demo.name
}

resource ""aws_eks_cluster"" ""demo"" {
  name     = ""demo""
  version  = ""1.27""
  role_arn = aws_iam_role.demo.arn

  vpc_config {
    endpoint_private_access = false
    endpoint_public_access  = true
    subnet_ids = [
      aws_subnet.private-us-east-1a.id,
      aws_subnet.public-us-east-1a.id
    ]
  }

  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy,
    aws_iam_role_policy_attachment.eks_service_policy
  ]
}
```

This configuration:
1. Creates an IAM role named ""eks-cluster-demo"" with the required assume role policy
2. Attaches the necessary EKS policies to the role
3. Sets up a VPC with public and private subnets
4. Creates an EKS cluster named ""demo"" in the us-east-1 region
5. Properly handles dependencies between the IAM role, policy attachments, and the EKS cluster
6. Configures the cluster with both public and private subnet access

The configuration includes all necessary resources and follows AWS best practices for EKS cluster setup.",,,,,,"Based on the examples provided and your requirements, I'll help you create a Terraform configuration for an EKS cluster with the specified IAM role setup. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_subnet"" ""public-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
}

resource ""aws_iam_role"" ""demo"" {
  name = ""eks-cluster-demo""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""eks.amazonaws.com""
        }
        Action = ""sts:AssumeRole""
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.demo.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.demo.name
}

resource ""aws_eks_cluster"" ""demo"" {
  name     = ""demo""
  version  = ""1.27""
  role_arn = aws_iam_role.demo.arn

  vpc_config {
    endpoint_private_access = false
    endpoint_public_access  = true
    subnet_ids = [
      aws_subnet.private-us-east-1a.id,
      aws_subnet.public-us-east-1a.id
    ]
  }

  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy,
    aws_iam_role_policy_attachment.eks_service_policy
  ]
}
```

This configuration:
1. Creates an IAM role named ""eks-cluster-demo"" with the specified assume role policy
2. Attaches the necessary EKS policies to the role
3. Creates an EKS cluster named ""demo"" in the us-east-1 region
4. Sets up the VPC configuration with both private and public subnets
5. Handles dependencies properly using the depends_on block
6. Includes the required provider configuration for AWS

The configuration follows AWS best practices and ensures proper integration between the IAM role and the EKS cluster.",,,,,,"Based on the examples provided and your requirements, I'll help you create a Terraform configuration for an EKS cluster with the specified IAM role and policies. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_subnet"" ""public-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
}

resource ""aws_iam_role"" ""demo"" {
  name = ""eks-cluster-demo""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""eks.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.demo.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.demo.name
}

resource ""aws_eks_cluster"" ""demo"" {
  name     = ""demo""
  role_arn = aws_iam_role.demo.arn
  version  = ""1.24""

  vpc_config {
    endpoint_private_access = false
    endpoint_public_access  = true
    subnet_ids = [
      aws_subnet.private-us-east-1a.id,
      aws_subnet.public-us-east-1a.id
    ]
  }

  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy,
    aws_iam_role_policy_attachment.eks_service_policy
  ]
}
```

This configuration:
1. Creates an IAM role named ""eks-cluster-demo""
2. Attaches the necessary EKS policies to the role
3. Sets up a VPC with public and private subnets
4. Creates an EKS cluster named ""demo""
5. Ensures proper dependency handling through the depends_on block
6. Configures the cluster in the us-east-1 region
7. Sets up the appropriate networking configuration with both public and private subnet access

The configuration follows AWS best practices and includes all the necessary components for a functional EKS cluster setup.",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer with ALB Target Group,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.values.subnet_mapping != null
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.ipv4_ipam_pool_id.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.ipv6_cider_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value != ""forward""
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    resources.values.target_type == ""alb""
    
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group""
        resource.expressions.port != null
        resource.expressions.protocol != null
        resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
        is_valid_lb
        is_valid_lb_listener
        is_valid_lb_target_group_attachment
        is_valid_lb_target_group
        is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.public_subnets[0]
  security_groups = [ aws_security_group.instance-sg.id ]
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.id
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id
    with target_type = ""alb""

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a target group. Based on the provided examples, I'll create a solution that includes the necessary resources and security groups.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Create VPC
module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""alb-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.available.names
  public_subnets  = [""10.0.1.0/24"", ""10.0.2.0/24""]
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = module.vpc.vpc_id

  tags = {
    Name = ""alb-sg""
  }
}

# ALB Security Group Rules
resource ""aws_vpc_security_group_ingress_rule"" ""alb_http"" {
  security_group_id = aws_security_group.alb_sg.id
  from_port         = 80
  to_port           = 80
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
}

resource ""aws_vpc_security_group_egress_rule"" ""alb_egress"" {
  security_group_id = aws_security_group.alb_sg.id
  from_port         = 0
  to_port           = 0
  ip_protocol       = ""-1""
  cidr_ipv4         = ""0.0.0.0/0""
}

# Create ALB
resource ""aws_lb"" ""application_lb"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = module.vpc.public_subnets

  tags = {
    Environment = ""production""
  }
}

# Create Target Group
resource ""aws_lb_target_group"" ""alb_tg"" {
  name     = ""my-alb-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.application_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.alb_tg.arn
  }
}
```

This configuration creates:
1. A VPC with public subnets in multiple availability zones
2. A security group for the ALB with appropriate ingress and egress rules
3. An Application Load Balancer (ALB)
4. A target group configured for HTTP traffic on port 80
5. A listener that forwards traffic to the target group

The ALB is set up to:
- Be internet-facing (public)
- Listen on port 80
- Use HTTP protocol
- Forward requests to the target group
- Perform health checks on the targets

You can attach targets",,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a target group. Based on the examples provided, I'll generate a complete solution that includes the necessary resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# VPC Configuration
module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""alb-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.available.names
  public_subnets  = [""10.0.1.0/24"", ""10.0.2.0/24""]
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Application Load Balancer
resource ""aws_lb"" ""alb"" {
  name               = ""application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = module.vpc.public_subnets

  enable_deletion_protection = false
}

# Target Group
resource ""aws_lb_target_group"" ""target_group"" {
  name     = ""alb-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    timeout             = 5
    path                = ""/""
    port                = ""traffic-port""
    protocol            = ""HTTP""
    matcher             = ""200""
    unhealthy_threshold = 2
  }
}

# Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.alb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}
```

This configuration creates:
1. A VPC with public subnets across multiple availability zones
2. A security group for the ALB allowing HTTP traffic
3. An Application Load Balancer (ALB)
4. A target group with health check configuration
5. A listener that forwards HTTP traffic to the target group

The ALB is set up as an internet-facing load balancer (internal = false) and will distribute traffic across the targets that you register with the target group. To add targets to this target group, you would need to create additional resources (like EC2 instances) and use aws_lb_target_group_attachment to attach them.",,,,,,"I'll help you generate a Terraform configuration for creating an Application Load Balancer (ALB) with a target group. Based on the provided examples, I'll create a complete solution that includes the necessary networking components and security groups.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# VPC Configuration
module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""alb-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.available.names
  public_subnets  = [""10.0.1.0/24"", ""10.0.2.0/24""]
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Application Load Balancer
resource ""aws_lb"" ""application_lb"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = module.vpc.public_subnets

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
  }
}

# ALB Target Group
resource ""aws_lb_target_group"" ""alb_tg"" {
  name     = ""my-alb-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.application_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.alb_tg.arn
  }
}
```

This configuration creates:
1. A VPC with public subnets across multiple availability zones
2. A security group for the ALB allowing HTTP traffic
3. An Application Load Balancer (ALB)
4. A target group for the ALB
5. A listener that forwards HTTP traffic to the target group

Key features:
- The ALB is internet-facing (public)
- HTTP traffic is allowed on port 80
- Health checks are configured for the target group
- The ALB is deployed across multiple availability zones for high availability
- All necessary security group rules are included

Note: You would need to separately add targets to the target group (like EC2 instances) using either `aws_lb_target_group_attachment` or auto-scaling groups depending on your specific needs.",,,,,
aws_elastic_beanstalk_application,Provision a resource to deploy and scale a web application that was developed with supported programming languages.,"package terraform.validation

default is_configuration_valid = false

default is_valid_eb_app = false


is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}


is_configuration_valid {
    is_valid_eb_app
}",1,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_elastic_beanstalk_application"" ""tftest"" {
  name        = ""tf-test-name""
  description = ""tf-test-desc""
}","Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""","I'll help you create a Terraform configuration to deploy a web application using AWS Elastic Beanstalk, which is an excellent service for deploying and scaling web applications. Based on the provided examples and best practices, I'll create a configuration that includes the necessary resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach necessary policies to the role
resource ""aws_iam_role_policy_attachment"" ""eb_web_tier"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Create an instance profile
resource ""aws_iam_instance_profile"" ""eb_instance_profile"" {
  name = ""elastic_beanstalk_instance_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Create Elastic Beanstalk application
resource ""aws_elastic_beanstalk_application"" ""web_app"" {
  name        = ""web-application""
  description = ""Web application deployment with Elastic Beanstalk""
}

# Create Elastic Beanstalk environment
resource ""aws_elastic_beanstalk_environment"" ""web_app_env"" {
  name                = ""web-app-production""
  application         = aws_elastic_beanstalk_application.web_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_instance_profile.name
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment:process:default""
    name      = ""HealthCheckPath""
    value     = ""/""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""ENVIRONMENT""
    value     = ""production""
  }
}

# Output the environment endpoint
output ""environment_endpoint"" {
  value = aws_elastic_beanstalk_environment.web_app_env.endpoint_url
}
```

This configuration:

1. Sets up the required AWS provider and version constraints
2. Creates an IAM role and instance profile for the Elastic Beanstalk EC2 instances
3. Creates an Elastic Beanstalk application
4. Sets up an Elastic Beanstalk environment with:
   - Auto-scaling configuration (min 1, max 4 instances)
   - Load balanced environment
   - Health check configuration
   - Environment variables
   - Python 3.9 runtime (can be changed to other supported languages)
5. Outputs the environment endpoint",,,,,,"I'll help you create a Terraform configuration to deploy and scale a web application using AWS Elastic Beanstalk, which is an ideal service for this requirement. Based on the examples provided, I'll create a solution that includes the necessary resources for a robust web application deployment.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""eb_web_tier"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Create instance profile for EC2 instances
resource ""aws_iam_instance_profile"" ""eb_instance_profile"" {
  name = ""eb_instance_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Create Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""web_app"" {
  name        = ""web-application""
  description = ""Web application deployment with Elastic Beanstalk""
}

# Create Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""web_app_env"" {
  name                = ""web-app-production""
  application         = aws_elastic_beanstalk_application.web_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_instance_profile.name
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""ENVIRONMENT""
    value     = ""production""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:command""
    name      = ""DeploymentPolicy""
    value     = ""Rolling""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""MeasureName""
    value     = ""CPUUtilization""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""Statistic""
    value     = ""Average""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""Unit""
    value     = ""Percent""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""UpperThreshold""
    value     = ""75""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""L",,,,,,"I'll help you create a Terraform configuration to deploy and scale a web application using AWS Elastic Beanstalk, which is ideal for deploying and scaling web applications. Based on the examples provided, I'll create a solution that includes the necessary resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policy for web tier
resource ""aws_iam_role_policy_attachment"" ""eb_web_tier"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Create instance profile
resource ""aws_iam_instance_profile"" ""eb_instance_profile"" {
  name = ""eb_instance_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Create Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""web_app"" {
  name        = ""web-application""
  description = ""Web application deployment with Elastic Beanstalk""
}

# Create Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""web_app_env"" {
  name                = ""web-app-production""
  application         = aws_elastic_beanstalk_application.web_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_instance_profile.name
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""ENVIRONMENT""
    value     = ""production""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:command""
    name      = ""DeploymentPolicy""
    value     = ""Rolling""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""MeasureName""
    value     = ""CPUUtilization""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""Statistic""
    value     = ""Average""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""Unit""
    value     = ""Percent""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""UpperThreshold""
    value     = ""75""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""LowerThreshold""
    value     = ""35",,,,,
"aws_iam_role, aws_kinesisanalyticsv2_application, aws_iam_policy_document",Generate a basic Amazon Kinesis V2 Analytics Application to manage a SQL application,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
	some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kinesisanalytics.amazonaws.com"") 
}


has_valid_kinesisanalyticsv2_application {
	some i
	resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesisanalyticsv2_application""
    resource.values.name
    resource.values.runtime_environment == ""SQL-1_0""
   	role := input.configuration.root_module.resources[i]
    role.expressions.service_execution_role
}

has_valid_resources {
	has_valid_iam_role(input.resource_changes)
    has_valid_kinesisanalyticsv2_application
}",3,"provider ""aws"" {
  region = ""us-west-2""
}

data ""aws_iam_policy_document"" ""KinesisAssume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}


resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_role""
  assume_role_policy = data.aws_iam_policy_document.KinesisAssume.json
}

resource ""aws_kinesisanalyticsv2_application"" ""flink_app"" {
  name                 = ""example-application1""
  runtime_environment  = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_role.arn
}","Has one ""aws_iam_role"" resource, and one ""aws_kinesisanalyticsv2_application"" with argument runtime_environment == ""SQL-1_0""","I'll help you generate a Terraform configuration for a basic Amazon Kinesis V2 Analytics Application specifically for SQL application processing. Based on the examples provided and adapting for SQL runtime, here's the solution:

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

# Create IAM assume role policy document for Kinesis Analytics
data ""aws_iam_policy_document"" ""kinesis_analytics_assume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}

# Create IAM role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name               = ""kinesis_analytics_sql_role""
  assume_role_policy = data.aws_iam_policy_document.kinesis_analytics_assume.json
}

# Create Kinesis Analytics V2 Application for SQL
resource ""aws_kinesisanalyticsv2_application"" ""sql_app"" {
  name                   = ""example-sql-application""
  runtime_environment    = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_analytics_role.arn

  application_configuration {
    sql_application_configuration {
      inputs {
        name_prefix = ""PREFIX_""
        
        schema_version = ""1""

        kinesis_streams_input {
          resource_arn = ""arn:aws:kinesis:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:stream/example-stream""
        }
      }
    }
  }
}

# Data sources for current region and account ID
data ""aws_region"" ""current"" {}
data ""aws_caller_identity"" ""current"" {}
```

This configuration:
1. Sets up the AWS provider
2. Creates an IAM assume role policy for Kinesis Analytics
3. Creates an IAM role that Kinesis Analytics can assume
4. Creates a Kinesis Analytics V2 application configured for SQL processing
5. Includes basic input configuration (you would need to adjust the stream ARN to match your actual Kinesis stream)

The main differences from the Flink example are:
- Uses `SQL-1_0` as the runtime environment instead of Flink
- Includes `sql_application_configuration` block with basic input configuration
- Includes additional data sources for current region and account ID for ARN construction

Note: You may need to add additional IAM permissions depending on your specific use case and what resources your SQL application needs to access.",,,,,,"I'll help you generate a Terraform configuration for a basic Amazon Kinesis V2 Analytics Application specifically for SQL application processing. Based on the examples provided, I'll adapt the configuration for SQL runtime environment.

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

# IAM assume role policy for Kinesis Analytics
data ""aws_iam_policy_document"" ""kinesis_assume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}

# IAM role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_sql_role"" {
  name               = ""kinesis_sql_role""
  assume_role_policy = data.aws_iam_policy_document.kinesis_assume.json
}

# Kinesis Analytics V2 Application for SQL
resource ""aws_kinesisanalyticsv2_application"" ""sql_app"" {
  name                   = ""example-sql-application""
  runtime_environment    = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_sql_role.arn

  application_configuration {
    sql_application_configuration {
      input {
        name_prefix = ""PREFIX_""
      }
    }
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an IAM assume role policy for Kinesis Analytics
3. Creates an IAM role that Kinesis Analytics can assume
4. Creates a Kinesis Analytics V2 application configured for SQL processing with:
   - SQL-1_0 runtime environment
   - Basic input configuration with a name prefix
   - Appropriate service execution role

The main differences from the Flink example are:
- Changed runtime environment to SQL-1_0
- Added application_configuration block with sql_application_configuration
- Added basic input configuration
- Updated resource names and descriptions to reflect SQL usage",,,,,,"I'll help you generate a Terraform configuration for a basic Amazon Kinesis V2 Analytics Application specifically for SQL processing. Based on the examples provided, I'll adapt the configuration for SQL runtime environment.

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

# IAM assume role policy for Kinesis Analytics
data ""aws_iam_policy_document"" ""kinesis_assume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}

# IAM role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_sql_role"" {
  name               = ""kinesis_sql_role""
  assume_role_policy = data.aws_iam_policy_document.kinesis_assume.json
}

# Kinesis Analytics V2 Application for SQL
resource ""aws_kinesisanalyticsv2_application"" ""sql_app"" {
  name                   = ""example-sql-application""
  runtime_environment    = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_sql_role.arn

  application_configuration {
    sql_application_configuration {
      input {
        name_prefix = ""PREFIX_""
      }
    }
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an IAM assume role policy for Kinesis Analytics
3. Creates an IAM role that Kinesis Analytics can assume
4. Creates a Kinesis Analytics V2 application configured for SQL processing with:
   - SQL-1_0 runtime environment
   - Basic input configuration with a name prefix
   - Appropriate service execution role

The main differences from the Flink example are:
- Different runtime environment (SQL-1_0 instead of FLINK-1_13)
- Addition of sql_application_configuration block
- Specific naming conventions for SQL application",,,,,
"aws_dax_cluster, aws_iam_role",Create a DAX cluster with node type = dax.r4.large and replication factor = 1.,"package main

import future.keywords.in

default allow = false

# Check if the AWS DynamoDB table is being created in us-east-1
aws_dax_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_dax_cluster""
    resource.change.after.node_type == ""dax.r4.large""
    resource.change.after.replication_factor == 1
}

# Aggregate all checks
allow {
    aws_dax_cluster_valid(input.resource_changes)
}",3,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""example"" {
  name = ""test""

  assume_role_policy = jsonencode({
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""eks.amazonaws.com""
      }
    }]
    Version = ""2012-10-17""
  })
}


resource ""aws_dax_cluster"" ""bar"" {
  cluster_name       = ""cluster-example""
  iam_role_arn       = aws_iam_role.example.arn
  node_type          = ""dax.r4.large""
  replication_factor = 1
}","require a cluster name, iam_role_arn, node_type, replication factor","I'll help you create a DAX cluster with the specified node type and replication factor using Terraform. Based on the examples provided, we'll need to create both an IAM role for DAX and the DAX cluster itself.

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Create DAX cluster
resource ""aws_dax_cluster"" ""dax_cluster"" {
  cluster_name       = ""my-dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an IAM role that DAX can assume
3. Creates a DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - The IAM role created above

The cluster will be named ""my-dax-cluster"" but you can modify this name as needed. The IAM role is a prerequisite for creating a DAX cluster, which is why it's included in the configuration.",,,,,,"I'll help you create a DAX cluster with the specified requirements. Based on the example provided, I'll generate the Terraform configuration for creating a DAX cluster with the specified node type and replication factor.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for DAX cluster
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax_cluster_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Create DAX cluster
resource ""aws_dax_cluster"" ""dax_cluster"" {
  cluster_name       = ""my-dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
}
```

This configuration will:
1. Set up the AWS provider
2. Create an IAM role that DAX can assume
3. Create a DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - The IAM role created above

The cluster will be created with the minimum required configuration. You can add additional optional parameters like maintenance_window, notification_topic_arn, or security_group_ids as needed.

Remember that you'll need to have appropriate permissions in AWS to create these resources, and you should also ensure that you have a VPC and subnet group configured if you want to deploy the DAX cluster in a specific VPC.",,,,,,"I'll help you create a DAX cluster with the specified node type and replication factor using Terraform. Based on the example provided, we'll need to create both the required IAM role and the DAX cluster itself.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Create DAX cluster
resource ""aws_dax_cluster"" ""dax_cluster"" {
  cluster_name       = ""my-dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
}
```

This configuration will:
1. Set up the AWS provider requirement
2. Create an IAM role that DAX can assume
3. Create a DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - The IAM role created above

The cluster will be named ""my-dax-cluster"" but you can modify the cluster_name value to match your needs.",,,,,
"aws_redshift_cluster, aws_redshift_usage_limit",Create a 2 node RedShift cluster and limit the concurrency scaling to 60 minutes,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 2
}

aws_redshift_usage_limit_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_usage_limit""
    resource.change.after.feature_type == ""concurrency-scaling""
    resource.change.after.limit_type == ""time""
    resource.change.after.amount == 60
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
    aws_redshift_usage_limit_valid(input.resource_changes)
}
",2,"resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type          = ""dc2.large""
  number_of_nodes    = 2

  database_name = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  skip_final_snapshot = true
}

resource ""aws_redshift_usage_limit"" ""example"" {
  cluster_identifier = aws_redshift_cluster.example.id
  feature_type       = ""concurrency-scaling""
  limit_type         = ""time""
  amount             = 60
}","Has an aws_redshift_cluster resource and check cluster_type is ""single_node"" or number of nodes is 2, check there is a aws_redshift_usage_limit resouce where the feature type is concurrency_scaling, limit_type is ""time"", and amount is 60","I'll help you create a Terraform configuration for a 2-node RedShift cluster with concurrency scaling limited to 60 minutes. This will be based on the provided examples while adding the specific concurrency scaling configuration.

```hcl
resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type         = ""dc2.large""
  cluster_type      = ""multi-node""
  number_of_nodes   = 2

  database_name   = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  # Concurrency scaling configuration
  concurrency_scaling = ""on""
  # 60 minutes = 3600 seconds
  max_concurrency_scaling_clusters = 1
  default_iam_role_arn = aws_iam_role.redshift_role.arn

  # Best practice to specify if this is not a production cluster
  skip_final_snapshot = true
}

# IAM role for Redshift
resource ""aws_iam_role"" ""redshift_role"" {
  name = ""redshift_concurrency_scaling_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""redshift.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for concurrency scaling
resource ""aws_iam_role_policy"" ""redshift_policy"" {
  name = ""redshift_concurrency_scaling_policy""
  role = aws_iam_role.redshift_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""redshift:EnableConcurrencyScaling"",
          ""redshift:DisableConcurrencyScaling""
        ]
        Resource = ""*""
      }
    ]
  })
}
```

This configuration:
1. Creates a 2-node RedShift cluster
2. Enables concurrency scaling
3. Sets up the necessary IAM role and policy for concurrency scaling
4. Limits scaling by setting `max_concurrency_scaling_clusters` to 1
5. Includes the required permissions through IAM roles

The cluster will be able to scale for concurrent workloads but will be limited to 60 minutes of usage through the AWS billing mechanisms. Note that concurrency scaling time is measured in terms of actual usage, so the cluster will only consume minutes when actually using the concurrency scaling feature.",,,,,,"I'll help you create a Terraform configuration for a 2-node RedShift cluster with concurrency scaling limited to 60 minutes. Here's the solution:

```hcl
resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type         = ""dc2.large""
  cluster_type      = ""multi-node""
  number_of_nodes   = 2

  database_name   = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  # Concurrency scaling configuration
  concurrency_scaling = ""on""
  # 60 minutes = 3600 seconds
  max_concurrency_scaling_clusters = 1
  aqua_configuration_status       = ""enabled""

  # Other recommended settings
  skip_final_snapshot    = true
  publicly_accessible    = false
  encrypted             = true
  enhanced_vpc_routing  = true

  # Add tags for better resource management
  tags = {
    Name        = ""redshift-cluster-1""
    Environment = ""production""
  }
}

resource ""aws_redshift_scheduling_policy"" ""example"" {
  name = ""concurrency-scaling-policy""

  # Set the maximum time for concurrency scaling to 60 minutes (3600 seconds)
  limits {
    concurrency_scaling_time_in_seconds = 3600
  }
}

resource ""aws_redshift_cluster_scheduling_policy_association"" ""example"" {
  cluster_identifier  = aws_redshift_cluster.example.id
  scheduling_policy_name = aws_redshift_scheduling_policy.example.name
}
```

This configuration:
1. Creates a 2-node RedShift cluster with concurrency scaling enabled
2. Creates a scheduling policy that limits concurrency scaling to 60 minutes
3. Associates the scheduling policy with the cluster
4. Includes recommended security settings like encryption and private access
5. Enables enhanced VPC routing for better network security
6. Uses tags for better resource management

The concurrency scaling is controlled through the scheduling policy, which limits the total time the cluster can use concurrency scaling to 60 minutes (3600 seconds).",,,,,,"I'll help you create a Terraform configuration for a 2-node RedShift cluster with concurrency scaling limited to 60 minutes. Here's the solution:

```hcl
resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type         = ""dc2.large""
  cluster_type      = ""multi-node""
  number_of_nodes   = 2

  database_name   = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  # Concurrency scaling configuration
  concurrency_scaling = ""on""
  # Set maximum concurrency scaling time to 60 minutes (3600 seconds)
  max_concurrency_scaling_minutes = 60

  # Best practice to specify if you don't want a final snapshot
  skip_final_snapshot = true

  # Additional recommended settings
  encrypted          = true
  publicly_accessible = false
}
```

This configuration:
1. Creates a 2-node RedShift cluster using dc2.large nodes
2. Enables concurrency scaling
3. Limits the concurrency scaling to 60 minutes
4. Includes basic security settings like encryption and private access
5. Sets up basic database credentials

The cluster will automatically scale up or down based on workload demands, but will not exceed 60 minutes of concurrent scaling time. This helps control costs while still providing performance benefits during peak usage periods.",,,,,
"aws_db_proxy, aws_db_subnet_group, aws_iam_role, aws_rds_cluster, aws_secretsmanager_secret, aws_secretsmanager_secret_version, aws_security_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc","Set up an AWS RDS Aurora MySQL cluster with a proxy for enhanced connection management, including VPC, subnets, security groups, IAM roles, and Secrets Manager for secure credential storage. The backup retention period for rds cluster should be 5, and the preffered backup window is 7 am to 9 am.","package terraform.validation

default is_valid_aws_db_proxy = false

default is_valid_aws_rds_cluster = false

default is_valid_aws_vpc = false

default is_valid_aws_subnet = false

default is_valid_aws_security_group = false

default is_valid_aws_db_subnet_group = false

default is_valid_aws_secretsmanager_secret = false

default is_valid_aws_iam_role = false

# Validate aws_db_proxy resource
is_valid_aws_db_proxy {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_proxy""
        resource.expressions.engine_family.constant_value == ""MYSQL""
        resource.expressions.require_tls.constant_value == true
        auth := resource.expressions.auth[_]
        auth.auth_scheme.constant_value == ""SECRETS""
}

# Validate aws_rds_cluster resource
is_valid_aws_rds_cluster {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_rds_cluster""
        resource.expressions.engine.constant_value == ""aurora-mysql""
        resource.expressions.master_username != null
        resource.expressions.master_password != null
    resource.expressions.preferred_backup_window.constant_value == ""07:00-09:00""
    resource.expressions.backup_retention_period.constant_value == 5
}

# Validate aws_vpc resource
is_valid_aws_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block != null
}

# Validate aws_subnet resource
is_valid_aws_subnet {
        count([x |
                resource := input.configuration.root_module.resources[x]
                resource.type == ""aws_subnet""
        ]) == 2 # Ensure there are exactly two subnet instances
}

# Validate aws_security_group resource
is_valid_aws_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""

        # Checks for at least one ingress and one egress rule, more specific validation can be added
        count(resource.expressions.ingress) > 0
        count(resource.expressions.egress) > 0
}

# Validate aws_db_subnet_group resource
is_valid_aws_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}

# Validate aws_secretsmanager_secret resource
is_valid_aws_secretsmanager_secret {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_secretsmanager_secret""
        resource.expressions.name != null
}

# Validate aws_iam_role resource
is_valid_aws_iam_role {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_iam_role""
        resource.expressions.assume_role_policy != null
}

",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port       = 3306
  to_port         = 3306
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port       = 3306
  to_port         = 3306
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

resource ""aws_rds_cluster"" ""example"" {
  cluster_identifier      = ""example-cluster""
  engine                  = ""aurora-mysql""
  engine_version          = ""8.0.mysql_aurora.3.08.0""
  master_username         = ""myusername""
  master_password         = ""password123""
  database_name           = ""exampledb""
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot     = true
  vpc_security_group_ids  = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name    = aws_db_subnet_group.main.name
}

resource ""aws_secretsmanager_secret"" ""db-credentials"" {
  name_prefix = ""db-credentials-""
}

resource ""aws_secretsmanager_secret_version"" ""db-credentials-version"" {
  secret_id     = aws_secretsmanager_secret.db-credentials.id
  secret_string = jsonencode({
    username = aws_rds_cluster.example.master_username
    password = aws_rds_cluster.example.master_password
  })
}

resource ""aws_iam_role"" ""rds-proxy-role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy"" ""rds-proxy-policy"" {
  name        = ""rds-proxy-policy""

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""secretsmanager:GetSecretValue"",
        ]
        Resource = [
          ""${aws_secretsmanager_secret_version.db-credentials-version.arn}"",
        ]
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""attach-proxy-policy"" {
  role       = aws_iam_role.rds-proxy-role.name
  policy_arn = aws_iam_policy.rds-proxy-policy.arn
}

resource ""aws_db_proxy"" ""example"" {
  name                   = ""example-proxy""
  debug_logging          = false
  engine_family          = ""MYSQL""
  idle_client_timeout    = 1800
  require_tls            = true
  role_arn               = aws_iam_role.rds-proxy-role.arn
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  vpc_subnet_ids         = module.vpc.private_subnets

  auth {
    auth_scheme  = ""SECRETS""
    secret_arn   = aws_secretsmanager_secret.db-credentials.arn
    iam_auth     = ""DISABLED""
  }
}

resource ""aws_db_proxy_default_target_group"" ""example"" {
  db_proxy_name = aws_db_proxy.example.name

  connection_pool_config {
    connection_borrow_timeout    = 120
    init_query                   = ""SET x=1, y=2""
    max_connections_percent      = 100
    max_idle_connections_percent = 50
    session_pinning_filters      = [""EXCLUDE_VARIABLE_SETS""]
  }
}

resource ""aws_db_proxy_target"" ""example"" {
  db_cluster_identifier = aws_rds_cluster.example.cluster_identifier
  db_proxy_name          = aws_db_proxy.example.name
  target_group_name      = aws_db_proxy_default_target_group.example.name
}
","has one ""aws_db_proxy"" resource
with ""engine_family"" = ""MYSQL""
with ""require_tls"" = true
with ""auth_scheme"" = ""SECRETS""

has one ""aws_rds_cluster"" resource
with ""engine"" = ""aurora-mysql""
with ""master_username"" and ""master_password"" from variables

has one ""aws_vpc"" resource
with a ""cidr_block""

has one ""aws_subnet"" resource
with two instances for different availability zones

has one ""aws_security_group"" resource
with ingress and egress rules for RDS and EC2 instances

has one ""aws_db_subnet_group"" resource
with subnet IDs

has one ""aws_secretsmanager_secret"" resource
with DB credentials

has one ""aws_iam_role"" resource
for the RDS proxy with an assume role policy","I'll help you create a Terraform configuration for an AWS RDS Aurora MySQL cluster with the specified backup requirements and all necessary components. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port         = 3306
  to_port          = 3306
  ip_protocol      = ""tcp""
  cidr_ipv4        = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port         = 3306
  to_port          = 3306
  ip_protocol      = ""tcp""
  cidr_ipv4        = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

resource ""aws_rds_cluster"" ""example"" {
  cluster_identifier      = ""example-cluster""
  engine                 = ""aurora-mysql""
  engine_version         = ""8.0.mysql_aurora.3.08.0""
  master_username        = ""myusername""
  master_password        = ""password123""
  database_name          = ""exampledb""
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot    = true
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name
}

resource ""aws_secretsmanager_secret"" ""db-credentials"" {
  name_prefix = ""db-credentials-""
}

resource ""aws_secretsmanager_secret_version"" ""db-credentials-version"" {
  secret_id     = aws_secretsmanager_secret.db-credentials.id
  secret_string = jsonencode({
    username = aws_rds_cluster.example.master_username
    password = aws_rds_cluster.example.master_password
  })
}

resource ""aws_iam_role"" ""rds-proxy-role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy"" ""rds-proxy-policy"" {
  name = ""rds-proxy-policy""

  policy = jsonencode({",,,,,,"I'll help you create a Terraform configuration for an AWS RDS Aurora MySQL cluster with the specified backup requirements and all necessary components. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port         = 3306
  to_port           = 3306
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port         = 3306
  to_port           = 3306
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

resource ""aws_rds_cluster"" ""example"" {
  cluster_identifier      = ""example-cluster""
  engine                  = ""aurora-mysql""
  engine_version          = ""8.0.mysql_aurora.3.08.0""
  master_username         = ""myusername""
  master_password         = ""password123""
  database_name          = ""exampledb""
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot     = true
  vpc_security_group_ids  = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name    = aws_db_subnet_group.main.name
}

resource ""aws_secretsmanager_secret"" ""db-credentials"" {
  name_prefix = ""db-credentials-""
}

resource ""aws_secretsmanager_secret_version"" ""db-credentials-version"" {
  secret_id     = aws_secretsmanager_secret.db-credentials.id
  secret_string = jsonencode({
    username = aws_rds_cluster.example.master_username
    password = aws_rds_cluster.example.master_password
  })
}

resource ""aws_iam_role"" ""rds-proxy-role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy"" ""rds-proxy-policy"" {
  name = ""rds-proxy-policy""

  policy = jsonen",,,,,,"I'll help you create a Terraform configuration for an AWS RDS Aurora MySQL cluster with the specified backup requirements and all necessary components. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  security_group_id = aws_security_group.rds-proxy-sg.id
  from_port        = 3306
  to_port          = 3306
  ip_protocol      = ""tcp""
  cidr_ipv4        = ""0.0.0.0/0""
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  security_group_id = aws_security_group.rds-proxy-sg.id
  from_port        = 3306
  to_port          = 3306
  ip_protocol      = ""tcp""
  cidr_ipv4        = ""0.0.0.0/0""
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

resource ""aws_rds_cluster"" ""example"" {
  cluster_identifier      = ""example-cluster""
  engine                  = ""aurora-mysql""
  engine_version          = ""8.0.mysql_aurora.3.08.0""
  master_username         = ""myusername""
  master_password         = ""password123""
  database_name           = ""exampledb""
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot     = true
  vpc_security_group_ids  = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name    = aws_db_subnet_group.main.name
}

resource ""aws_secretsmanager_secret"" ""db-credentials"" {
  name_prefix = ""db-credentials-""
}

resource ""aws_secretsmanager_secret_version"" ""db-credentials-version"" {
  secret_id     = aws_secretsmanager_secret.db-credentials.id
  secret_string = jsonencode({
    username = aws_rds_cluster.example.master_username
    password = aws_rds_cluster.example.master_password
  })
}

resource ""aws_iam_role"" ""rds-proxy-role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy"" ""rds-proxy-policy"" {
  name = ""rds-proxy-policy""

  policy = jsonencode",,,,,
"aws_dlm_lifecycle_policy, aws_glacier_vault",generate an S3 Glacier vault and use it with AWS Data Lifecycle Manager,"package terraform.validation

default is_valid_terraform_plan = false

# Rule to check for an AWS Glacier Vault resource with a 'name' attribute
is_valid_glacier_vault {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_glacier_vault""
        not is_null(resource.values.name)
}

# Rule to check for an AWS DLM Lifecycle Policy with required attributes
is_valid_dlm_lifecycle_policy {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_dlm_lifecycle_policy""
        not is_null(resource.values.description)
#         not is_null(resource.values.execution_role_arn)
        not is_null(resource.values.policy_details)
    
    execution_role_arn_configured(resource)
}

execution_role_arn_configured(_) {
    role_resource := input.configuration.root_module.resources[_]
    role_resource.type == ""aws_dlm_lifecycle_policy""
#     role_resource.name == ""dlm_lifecycle_role""
    not is_null(role_resource.expressions.execution_role_arn)
}

# Combined rule to validate the entire Terraform plan
is_valid_terraform_plan {
        is_valid_glacier_vault
        is_valid_dlm_lifecycle_policy
}",3,"resource ""aws_glacier_vault"" ""example"" {
  name = ""my-glacier-vault""
}

resource ""aws_dlm_lifecycle_policy"" ""example"" {
  description = ""Automated archiving policy""

  execution_role_arn = ""arn:aws:iam::123456789012:role/DLMServiceRole""  # Replace with your DLM execution role ARN

  policy_details {
    resource_types = [""VOLUME""]
    target_tags = {
      ""archive"" = ""true""
    }
    schedule {
      name = ""2 weeks of daily snapshots""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times         = [""23:45""]
      }

      retain_rule {
        count = 14
      }

      tags_to_add = {
        SnapshotCreator = ""DLM""
      }

      copy_tags = false
    }
  }
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute

has one ""aws_dlm_lifecycle_policy"" resource
    with ""description"" attribute
    with ""execution_role_arn""
    with ""policy_details""","I'll help you create a Terraform configuration that sets up an S3 Glacier vault and integrates it with AWS Data Lifecycle Manager. Here's the solution:

```hcl
# Configure AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""archive_vault"" {
  name = ""data-lifecycle-archive""
  
  tags = {
    Environment = ""Production""
    Purpose     = ""Long-term-archive""
  }
}

# Create IAM Role for DLM
resource ""aws_iam_role"" ""dlm_lifecycle_role"" {
  name = ""dlm-lifecycle-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dlm.amazonaws.com""
        }
      }
    ]
  })
}

# Create IAM Policy for DLM
resource ""aws_iam_role_policy"" ""dlm_lifecycle_policy"" {
  name = ""dlm-lifecycle-policy""
  role = aws_iam_role.dlm_lifecycle_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""glacier:InitiateJob"",
          ""glacier:AbortMultipartUpload"",
          ""glacier:CreateVault"",
          ""glacier:DeleteArchive"",
          ""glacier:UploadArchive"",
          ""glacier:InitiateMultipartUpload"",
          ""glacier:CompleteMultipartUpload"",
          ""glacier:UploadMultipartPart""
        ]
        Resource = [
          aws_glacier_vault.archive_vault.arn
        ]
      }
    ]
  })
}

# Create DLM Lifecycle Policy
resource ""aws_dlm_lifecycle_policy"" ""glacier_policy"" {
  description        = ""DLM lifecycle policy for Glacier archival""
  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn
  state             = ""ENABLED""

  policy_details {
    resource_types = [""VOLUME""]

    schedule {
      name = ""Archive to Glacier""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times        = [""23:45""]
      }

      retain_rule {
        count = 30
      }

      tags_to_add = {
        SnapshotCreator = ""DLM""
        ArchiveStatus   = ""ToGlacier""
      }

      copy_tags = true
    }

    target_tags = {
      Archive = ""true""
    }
  }

  tags = {
    Name = ""glacier-archive-policy""
  }
}

# Add vault access policy
data ""aws_iam_policy_document"" ""vault_policy"" {
  statement {
    actions = [
      ""glacier:UploadArchive"",
      ""glacier:InitiateMultipartUpload"",
      ""glacier:AbortMultipartUpload"",
      ""glacier:CompleteMultipartUpload""
    ]

    resources = [aws_glacier_vault.archive_vault.arn]

    principals {
      type        = ""AWS""
      identifiers = [aws_iam_role.dlm_lifecycle_role.arn]
    }
  }
}

resource ""aws_glacier_vault"" ""vault_access_policy"" {
  name          = aws_glacier_vault.archive_vault.name
  access_policy = data.aws_iam_policy_document.vault_policy.json
}
```

This configuration:

1. Creates an S3 Glacier vault for archival storage
2. Sets up an IAM role and policy for Data Lifecycle Manager (DLM)
3. Creates a DLM lifecycle policy that:
   - Manages volume snapshots
   - Creates snapshots every 24 hours
   - Retains snapshots for 30 ",,,,,,"I'll help you create a Terraform configuration that sets up an S3 Glacier vault and integrates it with AWS Data Lifecycle Manager. Here's the solution:

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

# Create the Glacier vault
resource ""aws_glacier_vault"" ""backup_vault"" {
  name = ""backup-archive-vault""
  
  tags = {
    Environment = ""Production""
    Purpose     = ""Long-term-backup""
  }
}

# Create IAM role for DLM
resource ""aws_iam_role"" ""dlm_lifecycle_role"" {
  name = ""dlm-lifecycle-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dlm.amazonaws.com""
        }
      }
    ]
  })
}

# Create IAM policy for DLM
resource ""aws_iam_role_policy"" ""dlm_lifecycle_policy"" {
  name = ""dlm-lifecycle-policy""
  role = aws_iam_role.dlm_lifecycle_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""glacier:InitiateJob"",
          ""glacier:CreateVault"",
          ""glacier:UploadArchive"",
          ""glacier:DeleteArchive"",
          ""glacier:ListVaults""
        ]
        Resource = [
          aws_glacier_vault.backup_vault.arn
        ]
      }
    ]
  })
}

# Create DLM lifecycle policy
resource ""aws_dlm_lifecycle_policy"" ""glacier_backup_policy"" {
  description        = ""DLM lifecycle policy for Glacier backups""
  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn
  state             = ""ENABLED""

  policy_details {
    resource_types = [""VOLUME""]
    
    schedule {
      name = ""Move to Glacier after 90 days""
      
      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times        = [""23:45""]
      }

      retain_rule {
        count = 90
      }

      copy_tags = true
    }

    target_tags = {
      Backup = ""true""
    }
  }

  tags = {
    Name = ""glacier-backup-lifecycle-policy""
  }
}

# Output the Glacier vault ARN
output ""glacier_vault_arn"" {
  value = aws_glacier_vault.backup_vault.arn
}

# Output the DLM role ARN
output ""dlm_role_arn"" {
  value = aws_iam_role.dlm_lifecycle_role.arn
}
```

This configuration:

1. Creates a Glacier vault for storing backups
2. Sets up an IAM role and policy for Data Lifecycle Manager (DLM) to interact with the Glacier vault
3. Creates a DLM lifecycle policy that:
   - Targets volumes tagged with ""Backup = true""
   - Creates backups every 24 hours at 23:45
   - Retains backups for 90 days
   - Copies all tags from source volumes to backups
4. Includes necessary permissions for DLM to manage archives in the Glacier vault
5. Outputs the ARNs of both the Glacier vault and DLM role

To use this configuration:
1. Tag your volumes with `Backup = ""true""` to include them in the backup policy
2. The backups will automatically be managed according to the lifecycle policy
3. After 90 days, the backups will be moved to Glacier for long-term storage

Note: Make sure to adjust the retention period, backup schedule, and tags according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration that sets up an S3 Glacier vault and integrates it with AWS Data Lifecycle Manager. Here's a solution:

```hcl
# Configure AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""archive_vault"" {
  name = ""data-lifecycle-archive""
  
  tags = {
    Environment = ""Production""
    Purpose     = ""Long-term-archive""
  }
}

# Create IAM Role for DLM
resource ""aws_iam_role"" ""dlm_lifecycle_role"" {
  name = ""dlm-lifecycle-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dlm.amazonaws.com""
        }
      }
    ]
  })
}

# Create IAM Policy for DLM
resource ""aws_iam_role_policy"" ""dlm_lifecycle_policy"" {
  name = ""dlm-lifecycle-policy""
  role = aws_iam_role.dlm_lifecycle_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""glacier:CreateVaultNotification"",
          ""glacier:DeleteVaultNotification"",
          ""glacier:GetVaultNotifications"",
          ""glacier:InitiateJob"",
          ""glacier:GetJobOutput"",
          ""glacier:UploadArchive"",
          ""glacier:DeleteArchive""
        ]
        Resource = [
          aws_glacier_vault.archive_vault.arn
        ]
      }
    ]
  })
}

# Create DLM Lifecycle Policy
resource ""aws_dlm_lifecycle_policy"" ""glacier_lifecycle_policy"" {
  description        = ""DLM lifecycle policy for Glacier archival""
  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn
  state             = ""ENABLED""

  policy_details {
    resource_types = [""VOLUME""]

    schedule {
      name = ""Archive to Glacier""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times        = [""23:45""]
      }

      retain_rule {
        count = 7
      }

      tags_to_add = {
        DestinationType = ""Glacier""
      }

      copy_tags = true
    }

    target_tags = {
      Archive = ""true""
    }
  }

  tags = {
    Name = ""glacier-archive-policy""
  }
}

# Add vault access policy
data ""aws_iam_policy_document"" ""vault_access_policy"" {
  statement {
    effect = ""Allow""
    
    principals {
      type        = ""AWS""
      identifiers = [aws_iam_role.dlm_lifecycle_role.arn]
    }

    actions = [
      ""glacier:UploadArchive"",
      ""glacier:InitiateJob"",
      ""glacier:GetJobOutput""
    ]

    resources = [aws_glacier_vault.archive_vault.arn]
  }
}

resource ""aws_glacier_vault"" ""vault_policy"" {
  name          = aws_glacier_vault.archive_vault.name
  access_policy = data.aws_iam_policy_document.vault_access_policy.json
}
```

This configuration:

1. Creates a Glacier vault for archive storage
2. Sets up an IAM role and policy for Data Lifecycle Manager (DLM)
3. Creates a DLM lifecycle policy that:
   - Runs daily at 23:45
   - Retains backups for 7 days
   - Targets volumes tagged with ""Archive = true""
4. Establishes a vault access policy that allows the DLM role to interact with the Glacier vault

Key features:
- The Glacier vault is configured for long-term archival storage
-",,,,,
"aws_connect_bot_association, aws_connect_instance, aws_lex_bot, aws_lex_intent, aws_region",create an aws_connect_bot_association resource with a SAML aws_connect_instance,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for multiple aws_lex_intent resources
has_valid_lex_intents {
    count([intent | intent := input.planned_values.root_module.resources[_]; intent.type == ""aws_lex_intent""; intent.values.name; intent.values.fulfillment_activity])
}

has_valid_connect_instance {
	count([instance | instance := input.planned_values.root_module.resources[_]; instance.type == ""aws_connect_instance""; instance.values.identity_management_type == ""SAML""; instance.values.inbound_calls_enabled; instance.values.outbound_calls_enabled])
}

# Rule for aws_lex_bot resource with specific arguments
has_valid_lex_bot_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lex_bot""
    resource.values.child_directed == true
    resource.values.clarification_prompt
    resource.values.abort_statement
    resource.values.name
    # is_boolean(resource.values.create_version)
    # resource.values.description
    # is_boolean(resource.values.detect_sentiment)
    # resource.values.idle_session_ttl_in_seconds
    # resource.values.process_behavior
    # resource.values.voice_id
    # resource.values.locale
    # is_boolean(resource.values.enable_model_improvements)
    # resource.values.nlu_intent_confidence_threshold
}

has_valid_connect_bot_association {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_connect_bot_association""
    resource.values.lex_bot[_].name
    instance := input.configuration.root_module.resources[i]
    instance.expressions.instance_id
    
    
}



# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lex_intents
    has_valid_lex_bot_instance
    has_valid_connect_instance
    has_valid_connect_bot_association
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_region"" ""current"" {}

resource ""aws_connect_instance"" ""example"" {
  identity_management_type = ""CONNECT_MANAGED""
  inbound_calls_enabled    = true
  instance_alias           = ""friendly-name-connect-13""
  outbound_calls_enabled   = true
}

resource ""aws_lex_intent"" ""example"" {
  create_version = true
  name           = ""connect_lex_intent""
  
  sample_utterances = [
    ""I would like to pick up flowers."",
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

resource ""aws_lex_bot"" ""example13"" {
  name             = ""connect_lex_bot""
  process_behavior = ""BUILD""
  child_directed   = false

  abort_statement {
    message {
      content      = ""Sorry, I am not able to assist at this time.""
      content_type = ""PlainText""
    }
  }
  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, what would you like to do?""
      content_type = ""PlainText""
    }
  }
  intent {
    intent_name    = aws_lex_intent.example.name
    intent_version = ""1""
  }  
}

resource ""aws_connect_bot_association"" ""example"" {
  instance_id = aws_connect_instance.example.id
  lex_bot {
    lex_region = data.aws_region.current.name
    name       = aws_lex_bot.example13.name
  }
}

","Has one resource ""aws_connect_instance"" with argument ""identity_management_type"" = SAML, at least one ""aws_lex_intent"" resource, one ""aws_lex_bot"" resource, and one ""aws_connnect_bot_association"" resource","I'll help you create an aws_connect_bot_association resource with a SAML-based AWS Connect instance. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_region"" ""current"" {}

resource ""aws_connect_instance"" ""example"" {
  identity_management_type = ""SAML""
  inbound_calls_enabled   = true
  instance_alias          = ""saml-connect-instance""
  outbound_calls_enabled  = true
}

resource ""aws_lex_intent"" ""example"" {
  create_version = true
  name          = ""connect_lex_intent""
  
  sample_utterances = [
    ""I would like to speak with an agent"",
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

resource ""aws_lex_bot"" ""example"" {
  name             = ""connect_saml_bot""
  process_behavior = ""BUILD""
  child_directed   = false

  abort_statement {
    message {
      content      = ""Sorry, I am not able to assist at this time.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, what would you like to do?""
      content_type = ""PlainText""
    }
  }

  intent {
    intent_name    = aws_lex_intent.example.name
    intent_version = ""1""
  }  
}

resource ""aws_connect_bot_association"" ""example"" {
  instance_id = aws_connect_instance.example.id
  lex_bot {
    lex_region = data.aws_region.current.name
    name       = aws_lex_bot.example.name
  }
}
```

This configuration:
1. Creates a SAML-based Connect instance by setting `identity_management_type = ""SAML""`
2. Creates a Lex intent and bot with basic configuration
3. Associates the Lex bot with the Connect instance using aws_connect_bot_association

Note that for a SAML-based Connect instance, you'll need to configure the SAML provider separately and ensure proper IAM roles and policies are in place for SAML authentication to work.",,,,,,"I'll help you create an AWS Connect bot association with a SAML-based Connect instance. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_region"" ""current"" {}

# Create a SAML-based Connect instance
resource ""aws_connect_instance"" ""saml_instance"" {
  identity_management_type = ""SAML""
  inbound_calls_enabled   = true
  instance_alias          = ""saml-connect-instance""
  outbound_calls_enabled  = true
}

# Create a Lex intent
resource ""aws_lex_intent"" ""example"" {
  create_version = true
  name          = ""connect_lex_intent""
  
  sample_utterances = [
    ""I need assistance"",
    ""Can you help me?""
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

# Create a Lex bot
resource ""aws_lex_bot"" ""example"" {
  name             = ""connect_lex_bot""
  process_behavior = ""BUILD""
  child_directed   = false

  abort_statement {
    message {
      content      = ""Sorry, I am not able to assist at this time.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, what would you like to do?""
      content_type = ""PlainText""
    }
  }

  intent {
    intent_name    = aws_lex_intent.example.name
    intent_version = ""1""
  }
}

# Create the bot association
resource ""aws_connect_bot_association"" ""saml_bot_association"" {
  instance_id = aws_connect_instance.saml_instance.id
  lex_bot {
    lex_region = data.aws_region.current.name
    name       = aws_lex_bot.example.name
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a SAML-based Connect instance by setting `identity_management_type = ""SAML""`
3. Creates a Lex intent and bot with basic configuration
4. Associates the Lex bot with the SAML-based Connect instance

The main difference from the example is the Connect instance configuration, which now uses SAML for identity management instead of CONNECT_MANAGED. The bot association remains similar but references the SAML-based instance.

Note that you'll need to configure SAML authentication separately in the AWS Console after the instance is created, as additional SAML configuration cannot be managed through Terraform.",,,,,,"I'll help you create an AWS Connect bot association with a SAML-based Connect instance. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_region"" ""current"" {}

resource ""aws_connect_instance"" ""saml_example"" {
  identity_management_type = ""SAML""
  inbound_calls_enabled   = true
  instance_alias          = ""saml-connect-instance""
  outbound_calls_enabled  = true
}

resource ""aws_lex_intent"" ""example"" {
  create_version = true
  name          = ""connect_lex_intent""
  
  sample_utterances = [
    ""I would like to speak with an agent"",
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

resource ""aws_lex_bot"" ""example"" {
  name             = ""connect_saml_bot""
  process_behavior = ""BUILD""
  child_directed   = false

  abort_statement {
    message {
      content      = ""Sorry, I am not able to assist at this time.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, what would you like to do?""
      content_type = ""PlainText""
    }
  }

  intent {
    intent_name    = aws_lex_intent.example.name
    intent_version = ""1""
  }
}

resource ""aws_connect_bot_association"" ""saml_example"" {
  instance_id = aws_connect_instance.saml_example.id
  lex_bot {
    lex_region = data.aws_region.current.name
    name       = aws_lex_bot.example.name
  }
}
```

This configuration:
1. Creates a SAML-based Connect instance
2. Creates a Lex intent and bot
3. Associates the Lex bot with the SAML Connect instance

The main difference from the reference examples is that the Connect instance is configured with `identity_management_type = ""SAML""` instead of ""CONNECT_MANAGED"". The bot association works the same way regardless of the identity management type of the Connect instance.",,,,,
aws_lightsail_database,"AWS Lightsail that creates a postgres database, which enable creating a final snapshot of your database on deletion","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.blueprint_id == ""postgres_12""
    resource.values.bundle_id
    resource.values.final_snapshot_name
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name     = ""test""
  availability_zone            = ""us-east-1a""
  master_database_name         = ""testdatabasename""
  master_password              = ""testdatabasepassword""
  master_username              = ""test""
  blueprint_id                 = ""postgres_12""
  bundle_id                    = ""micro_1_0""
  final_snapshot_name          = ""MyFinalSnapshot""
}","""have one """"aws_lightsail_database"""" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id = postgres_12
    with bundle_id
    with final_snapshot_name","I'll help you create a Terraform configuration for an AWS Lightsail Postgres database that creates a final snapshot when the database is deleted. Here's the solution:

```hcl
resource ""aws_lightsail_database"" ""postgres_db"" {
  relational_database_name = ""postgres-db""
  availability_zone        = ""us-east-1a""
  master_database_name     = ""postgresdb""
  master_password         = ""your-secure-password""
  master_username         = ""dbadmin""
  blueprint_id            = ""postgres_12""
  bundle_id               = ""micro_1_0""
  
  # Enable final snapshot creation on deletion
  skip_final_snapshot     = false
  final_snapshot_name     = ""final-snapshot-postgres-db""
}
```

This configuration:
1. Creates a Lightsail Postgres database
2. Sets `skip_final_snapshot` to `false` to ensure a final snapshot is created when the database is deleted
3. Specifies the name for the final snapshot using `final_snapshot_name`

Key features:
- Uses Postgres 12 as the database engine
- Creates the database in the us-east-1a availability zone
- Uses the micro_1_0 bundle (smallest instance size)
- Will create a final snapshot named ""final-snapshot-postgres-db"" when the database is deleted

Remember to replace ""your-secure-password"" with a strong password that meets AWS Lightsail's password requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail PostgreSQL database that creates a final snapshot when the database is deleted. Here's the solution:

```hcl
resource ""aws_lightsail_database"" ""postgres_db"" {
  relational_database_name = ""postgres-database""
  availability_zone        = ""us-east-1a""
  master_database_name     = ""postgresdb""
  master_password         = ""DatabasePassword123!""  # Replace with secure password
  master_username         = ""dbadmin""
  blueprint_id            = ""postgres_12""
  bundle_id              = ""micro_1_0""
  
  # Enable final snapshot creation on deletion
  skip_final_snapshot     = false
  final_snapshot_name     = ""final-snapshot-postgres-db""
}
```

This configuration:
- Creates a PostgreSQL database in Lightsail
- Sets `skip_final_snapshot` to `false` to ensure a final snapshot is created when the database is deleted
- Specifies a name for the final snapshot using `final_snapshot_name`
- Uses PostgreSQL 12 as the database engine
- Deploys in the micro_1_0 bundle size
- Sets up basic database credentials (remember to use secure password management in production)

The final snapshot will be automatically created when you destroy this resource using `terraform destroy`, helping you preserve your data even after deletion.",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail PostgreSQL database that creates a final snapshot when the database is deleted. Based on the examples provided, I'll modify the configuration to include the final snapshot feature.

```hcl
resource ""aws_lightsail_database"" ""postgres_db"" {
  relational_database_name = ""postgres-db""
  availability_zone        = ""us-east-1a""
  master_database_name     = ""postgresdb""
  master_password         = ""your-secure-password""
  master_username         = ""dbadmin""
  blueprint_id            = ""postgres_12""
  bundle_id               = ""micro_1_0""
  
  # Enable final snapshot creation when the database is deleted
  skip_final_snapshot     = false
  final_snapshot_name     = ""postgres-final-snapshot""
}
```

This configuration:
1. Creates a PostgreSQL database in AWS Lightsail
2. Sets `skip_final_snapshot` to `false` to ensure a final snapshot is created when the database is deleted
3. Specifies a name for the final snapshot using `final_snapshot_name`
4. Uses PostgreSQL 12 as the database engine
5. Uses the micro_1_0 bundle for the database instance size

The database will create a final snapshot with the specified name when you destroy the resource using `terraform destroy`, helping you preserve your data even after deletion.",,,,,
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket and an example payment configuration resource for the S3 bucket.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""

        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""Requester""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name","I'll help you create a Terraform configuration for an S3 bucket with a payment configuration resource. Based on the provided examples, here's the solution:

```hcl
resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-payment-config-bucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""BucketOwner""
}
```

This configuration:
1. Creates an S3 bucket named ""my-payment-config-bucket""
2. Sets up a request payment configuration where the bucket owner pays for all request fees
3. Uses the bucket reference (aws_s3_bucket.example.id) to associate the payment configuration with the correct bucket

The payment configuration is set to ""BucketOwner"" which means the owner of the bucket will pay for all requests, including those made by others to the bucket.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a payment configuration resource. Based on the examples provided, I'll create a solution that follows AWS best practices.

```hcl
# Create the S3 bucket
resource ""aws_s3_bucket"" ""main"" {
  bucket = ""my-example-bucket""
}

# Configure the request payment configuration for the bucket
resource ""aws_s3_bucket_request_payment_configuration"" ""main"" {
  bucket = aws_s3_bucket.main.id
  payer  = ""BucketOwner""  # Can be either ""BucketOwner"" or ""Requester""
}
```

This configuration:
1. Creates an S3 bucket named ""my-example-bucket""
2. Sets up a request payment configuration where the bucket owner pays for all request fees
3. Uses resource references to ensure the payment configuration is associated with the correct bucket

Remember to:
- Replace ""my-example-bucket"" with your desired bucket name
- Ensure the bucket name is globally unique across all AWS accounts
- Consider adding additional S3 bucket configurations like versioning or encryption if needed",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a payment configuration resource. Based on the provided example, I'll create a solution that follows AWS best practices.

```hcl
# Create the S3 bucket
resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-example-bucket""
}

# Configure the request payment configuration
resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""BucketOwner""
}
```

This configuration:
1. Creates an S3 bucket named ""my-example-bucket""
2. Sets up a request payment configuration where the bucket owner pays for all request fees
3. Uses resource references to ensure the payment configuration is associated with the correct bucket

The `payer` parameter can be set to either ""BucketOwner"" (default) or ""Requester"". In this case, it's set to ""BucketOwner"" meaning the bucket owner will pay for all request fees.",,,,,
"aws_internet_gateway, aws_vpc","creates a VPC with a CIDR block, and an internet gateway in the AWS us-east-2 region, both tagged with names ""vpc"" and ""ig"" respectively.","package terraform.validation

# Set default validation states
default is_valid_vpc = false

default is_valid_internet_gateway = false

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
        resource.expressions.enable_dns_hostnames.constant_value == true
        resource.expressions.tags != null
}

# Validate aws_internet_gateway resource
is_valid_internet_gateway {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_internet_gateway""
        resource.expressions.vpc_id.references[0] == ""aws_vpc._.id""
        resource.expressions.tags != null
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""_"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  tags = merge({
    ""Name"" = ""vpc""
  })
}

resource ""aws_internet_gateway"" ""_"" {
  vpc_id = aws_vpc._.id
  tags = merge({
    ""Name"" = ""ig""
  })
}","Has one ""aws_vpc"" resource:
with a specified ""cidr_block""
with ""enable_dns_hostnames"" set to true

Has one ""aws_internet_gateway"" resource:
with ""vpc_id"" referencing the ""aws_vpc"" resource by its ID
with a ""tags"" map that is not null, including at least a ""Name"" key with a value","Based on the provided examples and the current question, I'll help you create a Terraform configuration that creates a VPC with a CIDR block and an internet gateway in AWS us-east-2 region with appropriate tags.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

resource ""aws_vpc"" ""_"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  tags = merge({
    ""Name"" = ""vpc""
  })
}

resource ""aws_internet_gateway"" ""_"" {
  vpc_id = aws_vpc._.id
  tags = merge({
    ""Name"" = ""ig""
  })
}
```

This configuration:
1. Sets up the required AWS provider configuration
2. Creates a VPC with a CIDR block of 10.0.0.0/16 and enables DNS hostnames
3. Creates an Internet Gateway and attaches it to the VPC
4. Tags both resources with their respective names (""vpc"" and ""ig"")

The resources are properly linked using the VPC ID reference, and all the required configurations are included to ensure proper functionality.",,,,,,"Based on the examples provided, I'll help you create a Terraform configuration for a VPC with a CIDR block and an internet gateway in AWS us-east-2 region. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

resource ""aws_vpc"" ""_"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  
  tags = merge({
    ""Name"" = ""vpc""
  })
}

resource ""aws_internet_gateway"" ""_"" {
  vpc_id = aws_vpc._.id
  
  tags = merge({
    ""Name"" = ""ig""
  })
}
```

This configuration:
1. Sets up the required AWS provider and Terraform version
2. Creates a VPC with CIDR block 10.0.0.0/16 and enables DNS hostnames
3. Creates an Internet Gateway and attaches it to the VPC
4. Tags both resources with their respective names (""vpc"" and ""ig"")
5. Uses the us-east-2 region as specified

The VPC and Internet Gateway are properly linked using the `vpc_id` reference, and both resources are tagged appropriately.",,,,,,"Based on the provided examples and the current question, I'll help you create a Terraform configuration that creates a VPC with a CIDR block and an internet gateway in AWS us-east-2 region with appropriate tags.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

resource ""aws_vpc"" ""_"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  tags = merge({
    ""Name"" = ""vpc""
  })
}

resource ""aws_internet_gateway"" ""_"" {
  vpc_id = aws_vpc._.id
  tags = merge({
    ""Name"" = ""ig""
  })
}
```

This configuration:
1. Sets up the required AWS provider and Terraform version
2. Creates a VPC with CIDR block 10.0.0.0/16 and enables DNS hostnames
3. Creates an Internet Gateway and attaches it to the VPC
4. Tags both resources with their respective names (""vpc"" and ""ig"")

The resources are properly linked using the VPC ID reference, and all necessary configurations are included to ensure proper functionality.",,,,,
"aws_route53_health_check, aws_route53_health_check, aws_route53_record, aws_route53_record, aws_route53_zone","Configure Active-Active Failover policies using Route 53 resources. Name the health check resources ""primary_health_check"" and ""secondary_health_check"" and use a fqdn for both. Call the zone ""main""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record_primary = false

default is_valid_r53_record_secondary = false

default is_valid_r53_health_check_primary = false

default is_valid_r53_health_check_secondary = false



# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""

}

# Validate aws_route53_record
is_valid_r53_record_primary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.failover_routing_policy[0].type.constant_value == ""PRIMARY""
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.set_identifier
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.main.zone_id""
    resource.expressions.health_check_id.references[0] == ""aws_route53_health_check.primary_health_check.id""
}

is_valid_r53_record_secondary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.failover_routing_policy[0].type.constant_value == ""SECONDARY""
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.set_identifier
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.main.zone_id""
    resource.expressions.health_check_id.references[0] == ""aws_route53_health_check.secondary_health_check.id""

}

# Validate aws_route53_health_check
is_valid_r53_health_check_primary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_health_check""
    resource.name == ""primary_health_check""
    resource.expressions.fqdn
    resource.expressions.request_interval
    resource.expressions.type
}

is_valid_r53_health_check_secondary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_health_check""
    resource.name == ""secondary_health_check""
    resource.expressions.fqdn
    resource.expressions.request_interval
    resource.expressions.type
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_health_check_primary
    is_valid_r53_health_check_secondary
    is_valid_r53_zone
    is_valid_r53_record_primary
    is_valid_r53_record_secondary
}




",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
}

resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example53.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
}

resource ""aws_route53_record"" ""primary_record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""60""
  records = [""192.0.2.101""]
  set_identifier = ""primary-endpoint""

  failover_routing_policy {
    type = ""PRIMARY""
  }

  health_check_id = aws_route53_health_check.primary_health_check.id
}

resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example53.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
}

resource ""aws_route53_record"" ""secondary_record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""60""
  records = [""192.0.2.102""] 
  set_identifier = ""secondary-endpoint""
  
  failover_routing_policy {
    type = ""SECONDARY""
  }

  health_check_id = aws_route53_health_check.secondary_health_check.id
}","Has one ""aws_route53_zone"" resource
    with ""name""

Has two ""aws_route53_record"" resources
    The first ""aws_route53_record"" 
        with ""name""
        with ""type""
        with ""ttl""
        with ""records""
        with ""zone_id"" referencing the ""aws_route53_zone"" resource
        with ""health_check_id"" referencing one of the ""aws_health_check"" resource
        with ""set_identifier""
        with ""failover_routing_policy"" block that contains ""type"" = ""PRIMARY""
    The second ""aws_route53_record""
        with ""name""
        with ""type""
        with ""ttl""
        with ""records""
        with ""zone_id"" referencing the ""aws_route53_zone"" resource
        with ""health_check_id"" referencing the other ""aws_health_check"" resource
        with ""set_identifier""
        with ""failover_routing_policy"" block that contains ""type"" = ""SECONDARY""

Has two ""aws_health_check"" resources
    with ""fqdn"" 
    with ""request_interval""
    with ""type""

","I'll help you create a Terraform configuration for Active-Active Failover using Route 53 with two health checks. Based on the examples provided, I'll adapt the configuration for your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Health check for the first endpoint
resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
  request_interval  = 30
}

# Health check for the second endpoint
resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
  request_interval  = 30
}

# First A record with weighted routing policy
resource ""aws_route53_record"" ""primary"" {
  zone_id         = aws_route53_zone.main.zone_id
  name            = ""active.example.com""
  type            = ""A""
  ttl             = ""60""
  records         = [""192.0.2.10""]
  set_identifier  = ""primary""
  health_check_id = aws_route53_health_check.primary_health_check.id

  weighted_routing_policy {
    weight = 50
  }
}

# Second A record with weighted routing policy
resource ""aws_route53_record"" ""secondary"" {
  zone_id         = aws_route53_zone.main.zone_id
  name            = ""active.example.com""
  type            = ""A""
  ttl             = ""60""
  records         = [""192.0.2.20""]
  set_identifier  = ""secondary""
  health_check_id = aws_route53_health_check.secondary_health_check.id

  weighted_routing_policy {
    weight = 50
  }
}
```

This configuration creates:
1. A Route 53 hosted zone named ""main""
2. Two health checks (primary_health_check and secondary_health_check) monitoring different FQDNs
3. Two A records with weighted routing policies, each with:
   - 50% weight distribution for active-active configuration
   - Associated health checks
   - Different IP addresses
   - The same domain name but different set identifiers

The weighted routing policy with equal weights (50/50) ensures traffic is distributed evenly between both endpoints when they are healthy. If one endpoint fails its health check, Route 53 will automatically route all traffic to the healthy endpoint.",,,,,,"I'll help you create a Terraform configuration for Active-Active Failover using Route 53 with two health checks. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Health check for the first endpoint
resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
  request_interval  = 30

  tags = {
    Name = ""Primary-Endpoint-Health-Check""
  }
}

# Health check for the second endpoint
resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
  request_interval  = 30

  tags = {
    Name = ""Secondary-Endpoint-Health-Check""
  }
}

# First weighted record with health check
resource ""aws_route53_record"" ""primary"" {
  zone_id        = aws_route53_zone.main.zone_id
  name           = ""example.com""
  type           = ""A""
  ttl            = ""60""
  records        = [""192.0.2.10""]
  set_identifier = ""primary""
  health_check_id = aws_route53_health_check.primary_health_check.id

  weighted_routing_policy {
    weight = 50
  }
}

# Second weighted record with health check
resource ""aws_route53_record"" ""secondary"" {
  zone_id        = aws_route53_zone.main.zone_id
  name           = ""example.com""
  type           = ""A""
  ttl            = ""60""
  records        = [""192.0.2.20""]
  set_identifier = ""secondary""
  health_check_id = aws_route53_health_check.secondary_health_check.id

  weighted_routing_policy {
    weight = 50
  }
}
```

This configuration:
1. Creates a Route 53 zone named ""main""
2. Sets up two health checks (primary_health_check and secondary_health_check) using FQDNs
3. Creates two A records with weighted routing policy (50-50 split for active-active)
4. Associates each record with its corresponding health check
5. Uses weighted routing policy instead of failover routing policy to enable active-active configuration

Both endpoints will receive traffic under normal conditions, and if one endpoint fails its health check, Route 53 will route all traffic to the healthy endpoint.",,,,,,"I'll help you create a Terraform configuration for Active-Active Failover using Route 53 with two health checks. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Health check for the first endpoint
resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
  request_interval  = 30
}

# Health check for the second endpoint
resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
  request_interval  = 30
}

# First record with weighted routing policy
resource ""aws_route53_record"" ""primary"" {
  zone_id         = aws_route53_zone.main.zone_id
  name            = ""www.example.com""
  type            = ""A""
  ttl             = ""60""
  records         = [""192.0.2.10""]
  set_identifier  = ""primary""
  health_check_id = aws_route53_health_check.primary_health_check.id

  weighted_routing_policy {
    weight = 50
  }
}

# Second record with weighted routing policy
resource ""aws_route53_record"" ""secondary"" {
  zone_id         = aws_route53_zone.main.zone_id
  name            = ""www.example.com""
  type            = ""A""
  ttl             = ""60""
  records         = [""192.0.2.20""]
  set_identifier  = ""secondary""
  health_check_id = aws_route53_health_check.secondary_health_check.id

  weighted_routing_policy {
    weight = 50
  }
}
```

This configuration creates:
1. A Route 53 hosted zone named ""main""
2. Two health checks monitoring different FQDNs
3. Two A records with weighted routing policies set to equal weights (50/50 split)
4. Each record is associated with its respective health check

The setup enables active-active failover where traffic is distributed equally between both endpoints when they're healthy. If one endpoint fails its health check, Route 53 will automatically route all traffic to the healthy endpoint.",,,,,
"aws_iam_role, aws_iam_role_policy, aws_kendra_index",Generate a basic Kendra index resource With user group resolution configuration,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
    some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kendra.amazonaws.com"") 
}

has_valid_kendra_index {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_index""
    resource.values.name
    resource.values.user_group_resolution_configuration
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
}


has_valid_resources {
    has_valid_iam_role(input.resource_changes)
    has_valid_kendra_index
}
",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""role"" {
  name = ""Kendra-Role""
  assume_role_policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": ""sts:AssumeRole"",
      ""Principal"": {
        ""Service"": ""kendra.amazonaws.com""
      },
      ""Effect"": ""Allow"",
      ""Sid"": """"
    }
  ]
}
EOF
}

resource ""aws_iam_role_policy"" ""policy"" {
  name = ""Kendra-Policy""
  role = aws_iam_role.role.id

  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:GetObject"",
        ""s3:ListBucket""
      ],
      ""Resource"": ""*""
    }
  ]
}
EOF
}

resource ""aws_kendra_index"" ""example"" {
  name     = ""example""
  role_arn = aws_iam_role.role.arn

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""
  }
}","Has one ""aws_iam_role"" resource and one ""aws_kendra_index"" resource with the ""user_group_resolution_configuration"" block","I'll help you create a Terraform configuration for an AWS Kendra index resource with user group resolution configuration. Based on the provided examples and best practices, here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra-index-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Kendra role
resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra-index-policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kendra:*"",
          ""iam:GetGroup"",
          ""iam:GetUser"",
          ""iam:ListGroupsForUser"",
          ""iam:ListUsers""
        ]
        Resource = ""*""
      }
    ]
  })
}

# Kendra index resource
resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra index with user group resolution""
  role_arn    = aws_iam_role.kendra_role.arn
  edition     = ""ENTERPRISE_EDITION""

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""
  }

  tags = {
    Environment = ""production""
    Purpose     = ""search""
  }
}
```

This configuration includes:

1. The required AWS provider configuration
2. An IAM role for Kendra with the necessary assume role policy
3. An IAM policy attached to the role with required permissions
4. A Kendra index resource with user group resolution configuration set to use AWS SSO

The index is configured with:
- Enterprise Edition (required for user group resolution)
- User group resolution using AWS SSO mode
- Basic tags for resource management
- Appropriate IAM permissions to access user and group information

Note that this is a basic configuration and you might need to adjust the IAM permissions or add additional settings based on your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra index resource with user group resolution configuration. Based on the examples provided, I'll generate a complete solution that includes the necessary IAM role and the Kendra index with user group resolution settings.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

# IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra-index-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Kendra role
resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra-index-policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""cloudwatch:PutMetricData"",
          ""iam:ListGroupsForUser"",
          ""iam:ListUsers"",
          ""iam:GetGroup"",
          ""iam:GetUser""
        ]
        Resource = ""*""
      }
    ]
  })
}

# Kendra index resource
resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra index with user group resolution""
  role_arn    = aws_iam_role.kendra_role.arn
  edition     = ""ENTERPRISE_EDITION""

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_IAM""
  }

  tags = {
    Environment = ""production""
    Purpose     = ""search""
  }
}
```

This configuration includes:

1. The required AWS provider configuration
2. An IAM role that Kendra can assume
3. An IAM policy attached to the role with necessary permissions
4. A Kendra index resource with user group resolution configuration set to use AWS IAM

The Kendra index is configured with:
- Enterprise Edition (required for user group resolution)
- User group resolution using AWS IAM
- Basic tagging for resource management
- Necessary IAM permissions to access user and group information

Make sure to adjust the names, descriptions, and tags according to your specific needs. Also, you might need to add additional IAM permissions depending on your specific use case.",,,,,,"I'll help you generate a Terraform configuration for an AWS Kendra index resource with user group resolution configuration. Based on the examples provided, I'll create a solution that includes the necessary IAM role and the Kendra index with user group resolution settings.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

# IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra-index-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Kendra role
resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra-index-policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""cloudwatch:PutMetricData"",
          ""iam:GetRole"",
          ""iam:GetRolePolicy"",
          ""logs:DescribeLogGroups"",
          ""logs:CreateLogGroup""
        ]
        Resource = ""*""
      }
    ]
  })
}

# Kendra index resource
resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra index with user group resolution""
  role_arn    = aws_iam_role.kendra_role.arn
  edition     = ""DEVELOPER_EDITION""

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""
  }

  tags = {
    Environment = ""production""
    Purpose     = ""search""
  }
}
```

This configuration includes:

1. The required AWS provider configuration
2. An IAM role that Kendra can assume
3. An IAM policy attached to the role with necessary permissions
4. A Kendra index resource with user group resolution configuration set to use AWS SSO

The Kendra index is configured with:
- A descriptive name and description
- The DEVELOPER_EDITION edition type
- User group resolution configuration using AWS SSO mode
- Basic tags for resource management
- The necessary IAM role association

This configuration provides a basic but complete setup for a Kendra index with user group resolution capabilities. You can customize the name, description, tags, and other parameters according to your specific needs.",,,,,
aws_elasticache_cluster,Provide a Memcached Cluster,"package main

import future.keywords.in

default allow = false

aws_elasticache_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_cluster""
    resource.change.after.engine == ""memcached""
}

# Aggregate all checks
allow {
    aws_elasticache_cluster_valid(input.resource_changes)
}",2,"provider ""aws"" {
  region=""us-east-1""
}
resource ""aws_elasticache_cluster"" ""example"" {
  cluster_id           = ""cluster-example""
  engine               = ""memcached""
  node_type            = ""cache.m4.large""
  num_cache_nodes      = 2
  parameter_group_name = ""default.memcached1.4""
  port                 = 11211
}","requires cluster id, engine=memcached, node_type, num_cache_nodes, parameter_group_name","I'll help you create a Terraform configuration for an AWS ElastiCache Memcached cluster. This configuration will set up a basic Memcached cluster with proper networking and security settings.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC for the ElastiCache cluster
resource ""aws_vpc"" ""cache_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""cache-vpc""
  }
}

# Subnets for the ElastiCache cluster
resource ""aws_subnet"" ""cache_subnet_1"" {
  vpc_id            = aws_vpc.cache_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-east-1a""

  tags = {
    Name = ""cache-subnet-1""
  }
}

resource ""aws_subnet"" ""cache_subnet_2"" {
  vpc_id            = aws_vpc.cache_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-east-1b""

  tags = {
    Name = ""cache-subnet-2""
  }
}

# Subnet group for ElastiCache
resource ""aws_elasticache_subnet_group"" ""cache_subnet_group"" {
  name       = ""cache-subnet-group""
  subnet_ids = [aws_subnet.cache_subnet_1.id, aws_subnet.cache_subnet_2.id]
}

# Security group for ElastiCache
resource ""aws_security_group"" ""cache_security_group"" {
  name        = ""cache-security-group""
  description = ""Security group for Memcached cluster""
  vpc_id      = aws_vpc.cache_vpc.id

  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}

# Parameter group for Memcached
resource ""aws_elasticache_parameter_group"" ""cache_parameter_group"" {
  family = ""memcached1.6""
  name   = ""cache-params""

  parameter {
    name  = ""max_item_size""
    value = ""10485760""
  }
}

# ElastiCache Memcached cluster
resource ""aws_elasticache_cluster"" ""memcached_cluster"" {
  cluster_id           = ""memcached-cluster""
  engine              = ""memcached""
  node_type           = ""cache.t3.micro""
  num_cache_nodes     = 2
  port                = 11211
  
  parameter_group_name = aws_elasticache_parameter_group.cache_parameter_group.name
  subnet_group_name    = aws_elasticache_subnet_group.cache_subnet_group.name
  security_group_ids   = [aws_security_group.cache_security_group.id]

  az_mode             = ""cross-az""
  preferred_availability_zones = [""us-east-1a"", ""us-east-1b""]

  tags = {
    Environment = ""production""
  }
}
```

This configuration creates:
1. A VPC with two subnets in different availability zones
2. A subnet group for the ElastiCache cluster
3. A security group allowing inbound traffic on port 11211 (default Memcached port)
4. A parameter group with some basic Memcached settings
5. A Memcached cluster with:
   - 2 nodes spread across different availability zones
   - t3.micro instance type (can be changed based on needs)
   - Cross-AZ deployment for high availability
   - Basic security and networking settings

Key features:
- High availability with cross-AZ deployment
-",,,,,,"I'll help you create a Terraform configuration for an AWS ElastiCache Memcached cluster. Here's a solution that sets up a Memcached cluster with proper networking and security configurations:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC for the ElastiCache cluster
resource ""aws_vpc"" ""cache_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""elasticache-vpc""
  }
}

# Subnets for the ElastiCache cluster
resource ""aws_subnet"" ""cache_subnet_1"" {
  vpc_id            = aws_vpc.cache_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-east-1a""

  tags = {
    Name = ""elasticache-subnet-1""
  }
}

resource ""aws_subnet"" ""cache_subnet_2"" {
  vpc_id            = aws_vpc.cache_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-east-1b""

  tags = {
    Name = ""elasticache-subnet-2""
  }
}

# Subnet group for ElastiCache
resource ""aws_elasticache_subnet_group"" ""cache_subnet_group"" {
  name       = ""cache-subnet-group""
  subnet_ids = [aws_subnet.cache_subnet_1.id, aws_subnet.cache_subnet_2.id]
}

# Security group for ElastiCache
resource ""aws_security_group"" ""cache_security_group"" {
  name        = ""elasticache-security-group""
  description = ""Security group for ElastiCache Memcached cluster""
  vpc_id      = aws_vpc.cache_vpc.id

  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}

# Parameter group for Memcached
resource ""aws_elasticache_parameter_group"" ""cache_parameter_group"" {
  family = ""memcached1.6""
  name   = ""cache-params""

  parameter {
    name  = ""max_item_size""
    value = ""10485760""
  }
}

# ElastiCache Memcached cluster
resource ""aws_elasticache_cluster"" ""memcached_cluster"" {
  cluster_id           = ""memcached-cluster""
  engine              = ""memcached""
  node_type           = ""cache.t3.micro""
  num_cache_nodes     = 2
  port                = 11211
  
  parameter_group_name = aws_elasticache_parameter_group.cache_parameter_group.name
  subnet_group_name    = aws_elasticache_subnet_group.cache_subnet_group.name
  security_group_ids   = [aws_security_group.cache_security_group.id]

  az_mode             = ""cross-az""
  preferred_availability_zones = [""us-east-1a"", ""us-east-1b""]

  tags = {
    Environment = ""production""
  }
}
```

This configuration:
1. Creates a VPC with two subnets in different availability zones
2. Sets up a subnet group for the ElastiCache cluster
3. Creates a security group allowing Memcached traffic (port 11211)
4. Defines a parameter group with custom settings
5. Creates a Memcached cluster with:
   - 2 nodes distributed across availability zones
   - t3.micro instance type (can be adjusted based on needs)
   - Cross-AZ deployment for high availability
   - Custom parameter group and security settings

The cluster will be accessible within the",,,,,,"I'll help you create a Terraform configuration for an AWS ElastiCache Memcached cluster. Here's a solution that creates a Memcached cluster with proper networking and security configurations:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC for the ElastiCache cluster
resource ""aws_vpc"" ""cache_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""cache-vpc""
  }
}

# Subnets for the ElastiCache cluster
resource ""aws_subnet"" ""cache_subnet_1"" {
  vpc_id            = aws_vpc.cache_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-east-1a""

  tags = {
    Name = ""cache-subnet-1""
  }
}

resource ""aws_subnet"" ""cache_subnet_2"" {
  vpc_id            = aws_vpc.cache_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-east-1b""

  tags = {
    Name = ""cache-subnet-2""
  }
}

# Subnet group for ElastiCache
resource ""aws_elasticache_subnet_group"" ""cache_subnet_group"" {
  name       = ""cache-subnet-group""
  subnet_ids = [aws_subnet.cache_subnet_1.id, aws_subnet.cache_subnet_2.id]
}

# Security group for ElastiCache
resource ""aws_security_group"" ""cache_security_group"" {
  name        = ""cache-security-group""
  description = ""Security group for Memcached cluster""
  vpc_id      = aws_vpc.cache_vpc.id

  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}

# Parameter group for Memcached
resource ""aws_elasticache_parameter_group"" ""cache_parameter_group"" {
  family = ""memcached1.6""
  name   = ""cache-params""

  parameter {
    name  = ""max_item_size""
    value = ""10485760""
  }
}

# ElastiCache Memcached cluster
resource ""aws_elasticache_cluster"" ""memcached_cluster"" {
  cluster_id           = ""memcached-cluster""
  engine              = ""memcached""
  node_type           = ""cache.t3.micro""
  num_cache_nodes     = 2
  port                = 11211
  
  subnet_group_name    = aws_elasticache_subnet_group.cache_subnet_group.name
  security_group_ids   = [aws_security_group.cache_security_group.id]
  parameter_group_name = aws_elasticache_parameter_group.cache_parameter_group.name

  az_mode             = ""cross-az""
  preferred_availability_zones = [""us-east-1a"", ""us-east-1b""]

  tags = {
    Environment = ""production""
  }
}
```

This configuration:
1. Creates a VPC with two subnets in different availability zones
2. Sets up a subnet group for the ElastiCache cluster
3. Creates a security group allowing inbound traffic on port 11211 (default Memcached port)
4. Defines a parameter group with custom settings
5. Creates a Memcached cluster with:
   - 2 nodes distributed across availability zones
   - t3.micro instance type (can be adjusted based on needs)
   - Cross-AZ deployment for high availability
   - Custom parameter group and security settings

The cluster will be accessible within the VPC on port 11211",,,,,
aws_dax_parameter_group,Create a DAX parameter group.,"package main

import future.keywords.in

default allow = false

# Check for DAX parameter group creation
dax_parameter_group_created(resources) {
some resource in resources
resource.type == ""aws_dax_parameter_group""
resource.change.actions[_] == ""create""
}

# Allow DAX parameter group creation with specific parameters
allow {
dax_parameter_group_created(input.resource_changes)
}",2,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

resource ""aws_dax_parameter_group"" ""example"" {
name = ""example""

parameters {
name = ""query-ttl-millis""
value = ""100000""
}

parameters {
name = ""record-ttl-millis""
value = ""100000""
}
}","Has one ""aws_dax_parameter_group""","I'll help you create a DAX parameter group using Terraform. Based on the provided example and best practices, here's a solution for creating a DAX parameter group:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

resource ""aws_dax_parameter_group"" ""example"" {
  name        = ""example-dax-parameter-group""
  description = ""Example DAX parameter group""

  parameters {
    name  = ""query-ttl-millis""
    value = ""300000""  # 5 minutes in milliseconds
  }

  parameters {
    name  = ""record-ttl-millis""
    value = ""600000""  # 10 minutes in milliseconds
  }
}
```

This configuration:
1. Declares the required AWS provider
2. Creates a DAX parameter group with:
   - A descriptive name
   - Two common parameters for controlling TTL (Time To Live) settings:
     - query-ttl-millis: Controls how long DAX caches query results
     - record-ttl-millis: Controls how long DAX caches individual items

You can customize the parameter values according to your specific needs. The values shown are examples and can be adjusted based on your caching requirements.",,,,,,"I'll help you create a DAX parameter group using Terraform. Based on the provided examples, I'll create a parameter group with common DAX settings.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

resource ""aws_dax_parameter_group"" ""example"" {
  name        = ""my-dax-parameter-group""
  description = ""Custom DAX parameter group""

  parameters {
    name  = ""query-ttl-millis""
    value = ""300000""  # 5 minutes
  }

  parameters {
    name  = ""record-ttl-millis""
    value = ""600000""  # 10 minutes
  }
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates a DAX parameter group named ""my-dax-parameter-group""
3. Sets two common parameters:
   - query-ttl-millis: Time-to-live for query results cache (set to 5 minutes)
   - record-ttl-millis: Time-to-live for item cache (set to 10 minutes)

You can adjust the parameter values based on your specific needs. The parameter group can then be associated with a DAX cluster when you create one.",,,,,,"I'll help you create a DAX parameter group using Terraform. Based on the provided examples, I'll create a configuration that sets up a basic DAX parameter group with some common parameters.

```hcl
resource ""aws_dax_parameter_group"" ""example"" {
  name        = ""my-dax-parameter-group""
  description = ""Custom DAX parameter group""

  parameters {
    name  = ""query-ttl-millis""
    value = ""300000""  # 5 minutes in milliseconds
  }

  parameters {
    name  = ""record-ttl-millis""
    value = ""600000""  # 10 minutes in milliseconds
  }
}
```

This configuration:
1. Creates a DAX parameter group named ""my-dax-parameter-group""
2. Sets two common parameters:
   - query-ttl-millis: Sets the TTL for query results cache
   - record-ttl-millis: Sets the TTL for item cache

You can modify the parameter values according to your specific requirements. Remember that the parameter group needs to be associated with a DAX cluster to take effect.

To use this in your broader infrastructure, make sure you have configured the AWS provider in your Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}
```",,,,,
"aws_s3_bucket, aws_s3_bucket_notification, aws_sns_topic, aws_iam_policy_document",Create an IAM policy document that allows the 's3.amazonaws.com' service to publish to an SNS topic for S3 event notifications. Specify the actions as 'SNS:Publish' and set the resource to the ARN of the SNS topic 's3-event-notification-topic'. Apply a condition to allow this action only if the source ARN is similar to the ARN of the specified S3 bucket. Create an SNS topic named 's3-event-notification-topic' with the generated IAM policy. Set up an S3 bucket named 'your-bucket-name' and configure it to send notifications to the SNS topic for 's3:ObjectCreated:*' events with a filter for '.log' suffix.,"package terraform.validation

default has_aws_iam_policy_document_topic = false
default has_aws_sns_topic = false
default has_aws_s3_bucket = false
default has_aws_s3_bucket_notification = false

has_aws_iam_policy_document_topic {
    policy := input.planned_values.root_module.resources[_]
    policy.type == ""aws_iam_policy_document""
}

has_aws_sns_topic {
    topic := input.planned_values.root_module.resources[_]
    topic.type == ""aws_sns_topic""
    topic.name == ""topic""
    topic.values.name == ""s3-event-notification-topic""
    topic.values.policy == input.planned_values.root_module.resources[_].values.json  # Validate policy is correctly used from data source
}

has_aws_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""bucket""
    bucket.values.bucket == ""your-bucket-name""
}

has_aws_s3_bucket_notification {
    notification := input.planned_values.root_module.resources[_]
    notification.type == ""aws_s3_bucket_notification""
    notification.name == ""bucket_notification""
    notification.values.bucket == input.planned_values.root_module.resources[_].values.id  # Correct bucket ID reference
    notification.values.topic.topic_arn == input.planned_values.root_module.resources[_].values.arn  # Correct topic ARN reference
    notification.values.topic.events[0] == ""s3:ObjectCreated:*""
    notification.values.topic.filter_suffix == "".log""
}

valid_configuration {
    has_aws_iam_policy_document_topic
    has_aws_sns_topic
    has_aws_s3_bucket
    has_aws_s3_bucket_notification
}
",3,"provider ""aws"" {
    region = ""us-west-1""
}

data ""aws_iam_policy_document"" ""topic"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""s3.amazonaws.com""]
    }

    actions   = [""SNS:Publish""]
    resources = [""arn:aws:sns:*:*:s3-event-notification-topic""]

    condition {
      test     = ""ArnLike""
      variable = ""aws:SourceArn""
      values   = [aws_s3_bucket.bucket.arn]
    }
  }
}
resource ""aws_sns_topic"" ""topic"" {
  name   = ""s3-event-notification-topic""
  policy = data.aws_iam_policy_document.topic.json
}

resource ""aws_s3_bucket"" ""bucket"" {
  bucket = ""your-bucket-name""
}

resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.bucket.id

  topic {
    topic_arn     = aws_sns_topic.topic.arn
    events        = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }
}","Data ""aws_iam_policy_document""
has one ""effect""
has one ""principals"" 
has one ""actions""
has one ""resources""

Resource ""aws_sns_topic"" 
has one ""name""
uses the JSON output

Resource ""aws_s3_bucket"" 
has one ""bucket""

Resource ""aws_s3_bucket_notification"" 
has one ""bucket""
configures a ""topic"" notification with:
""topic_arn""
""events""
""filter_suffix""","I'll help you create a Terraform configuration that sets up an S3 bucket with SNS notifications and the required IAM policy. Here's the solution:

```hcl
# Create SNS Topic
resource ""aws_sns_topic"" ""topic"" {
  name = ""s3-event-notification-topic""
}

# Create IAM Policy Document
data ""aws_iam_policy_document"" ""sns_topic_policy"" {
  statement {
    effect = ""Allow""
    
    principals {
      type = ""Service""
      identifiers = [""s3.amazonaws.com""]
    }

    actions = [""SNS:Publish""]
    
    resources = [aws_sns_topic.topic.arn]

    condition {
      test     = ""ArnLike""
      variable = ""aws:SourceArn""
      values   = [""arn:aws:s3:*:*:your-bucket-name""]
    }
  }
}

# Attach Policy to SNS Topic
resource ""aws_sns_topic_policy"" ""default"" {
  arn    = aws_sns_topic.topic.arn
  policy = data.aws_iam_policy_document.sns_topic_policy.json
}

# Create S3 Bucket
resource ""aws_s3_bucket"" ""bucket"" {
  bucket = ""your-bucket-name""
}

# Configure S3 Bucket Notification
resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.bucket.id

  topic {
    topic_arn     = aws_sns_topic.topic.arn
    events        = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }

  depends_on = [aws_sns_topic_policy.default]
}
```

This configuration:
1. Creates an SNS topic named ""s3-event-notification-topic""
2. Creates an IAM policy document that allows S3 to publish to the SNS topic
3. Attaches the policy to the SNS topic
4. Creates an S3 bucket
5. Sets up the bucket notification to send events to the SNS topic when .log files are created

The `depends_on` attribute ensures that the bucket notification is created after the SNS topic policy is in place. The condition in the IAM policy ensures that only the specified S3 bucket can publish to the topic.",,,,,,"I'll help you create a complete Terraform configuration that sets up the IAM policy, SNS topic, S3 bucket, and notification configuration. Here's the solution:

```hcl
# Define the IAM policy document
data ""aws_iam_policy_document"" ""sns_topic_policy"" {
  statement {
    effect = ""Allow""
    
    principals {
      type = ""Service""
      identifiers = [""s3.amazonaws.com""]
    }

    actions = [""SNS:Publish""]
    
    resources = [""arn:aws:sns:*:*:s3-event-notification-topic""]
    
    condition {
      test     = ""ArnLike""
      variable = ""aws:SourceArn""
      values   = [""arn:aws:s3:*:*:your-bucket-name""]
    }
  }
}

# Create the SNS topic with the policy
resource ""aws_sns_topic"" ""topic"" {
  name   = ""s3-event-notification-topic""
  policy = data.aws_iam_policy_document.sns_topic_policy.json
}

# Create the S3 bucket
resource ""aws_s3_bucket"" ""bucket"" {
  bucket = ""your-bucket-name""
}

# Configure S3 bucket notification
resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.bucket.id

  topic {
    topic_arn     = aws_sns_topic.topic.arn
    events        = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }
}
```

This configuration:
1. Creates an IAM policy document that allows S3 to publish to the SNS topic
2. Creates an SNS topic with the defined policy
3. Creates an S3 bucket
4. Sets up the bucket notification to send events to the SNS topic

The policy includes the necessary permission for S3 to publish to the SNS topic, with a condition that restricts it to the specified bucket's ARN. The notification is configured to trigger on object creation events for files ending in "".log"".",,,,,,"I'll help you create a Terraform configuration that sets up an S3 bucket with SNS notifications, including the necessary IAM policy. Here's the solution:

```hcl
# Create SNS Topic
resource ""aws_sns_topic"" ""topic"" {
  name = ""s3-event-notification-topic""
}

# Create IAM Policy Document
data ""aws_iam_policy_document"" ""topic_policy"" {
  statement {
    effect = ""Allow""
    
    principals {
      type = ""Service""
      identifiers = [""s3.amazonaws.com""]
    }

    actions = [""SNS:Publish""]
    
    resources = [aws_sns_topic.topic.arn]

    condition {
      test     = ""ArnLike""
      variable = ""aws:SourceArn""
      values   = [""arn:aws:s3:*:*:your-bucket-name""]
    }
  }
}

# Attach Policy to SNS Topic
resource ""aws_sns_topic_policy"" ""default"" {
  arn    = aws_sns_topic.topic.arn
  policy = data.aws_iam_policy_document.topic_policy.json
}

# Create S3 Bucket
resource ""aws_s3_bucket"" ""bucket"" {
  bucket = ""your-bucket-name""
}

# Configure S3 Bucket Notification
resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.bucket.id

  topic {
    topic_arn     = aws_sns_topic.topic.arn
    events        = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }

  depends_on = [aws_sns_topic_policy.default]
}
```

This configuration:
1. Creates an SNS topic named ""s3-event-notification-topic""
2. Creates an IAM policy document that allows S3 to publish to the SNS topic
3. Attaches the policy to the SNS topic
4. Creates an S3 bucket
5. Sets up the bucket notification to send events to the SNS topic when .log files are created

The `depends_on` attribute ensures that the topic policy is created before the bucket notification is configured. The condition in the IAM policy ensures that only the specified S3 bucket can publish to the SNS topic.",,,,,
aws_db_instance,Create a PostgresQL instance.,"package main

import future.keywords.in

default allow = false

# Check for DB instance creation
db_instance_created(resources) {
some resource in resources
resource.type == ""aws_db_instance""
resource.change.actions[_] == ""create""
}

# Check if engine is MySQL
db_engine_mysql(resource) {
resource.type == ""aws_db_instance""
resource.change.after.engine == ""postgres""
}

# Aggregate checks for DB instance
allow {
db_instance_created(input.resource_changes)
some resource in input.resource_changes
db_engine_mysql(resource)
}",3,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

# Configure the AWS Provider
provider ""aws"" {
region = ""us-east-1""
}

resource ""aws_db_instance"" ""default"" {
allocated_storage = 10
db_name = ""mydb""
engine = ""postgres""
instance_class = ""db.t3.micro""
username = ""USERNAME""
password = ""PASSWORD""
skip_final_snapshot = true
}","Has one ""aws_db_instance"" resource
with one ""engine"" = ""postgres""","I'll help you create a PostgreSQL instance using Terraform. Based on the examples provided, I'll create a basic configuration that includes the necessary resources for a PostgreSQL RDS instance with standard best practices.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create VPC for the database
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  enable_dns_hostnames = true
  enable_dns_support   = true
  
  tags = {
    Name = ""postgresql-vpc""
  }
}

# Create subnet group for the database
resource ""aws_db_subnet_group"" ""postgresql"" {
  name       = ""postgresql-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""PostgreSQL subnet group""
  }
}

# Create subnets in different AZs
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-east-1a""

  tags = {
    Name = ""postgresql-subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-east-1b""

  tags = {
    Name = ""postgresql-subnet-2""
  }
}

# Create PostgreSQL RDS instance
resource ""aws_db_instance"" ""postgresql"" {
  identifier           = ""postgresql-instance""
  engine              = ""postgres""
  engine_version      = ""14.7""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""

  db_name  = ""mydb""
  username = ""postgres""
  password = ""your-secure-password""

  db_subnet_group_name   = aws_db_subnet_group.postgresql.name
  
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  multi_az               = false
  publicly_accessible    = false
  skip_final_snapshot    = true

  enabled_cloudwatch_logs_exports = [""postgresql""]

  tags = {
    Name = ""postgresql-instance""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC with two subnets in different availability zones
3. Creates a DB subnet group
4. Creates a PostgreSQL RDS instance with:
   - 20GB of GP2 storage
   - Basic backup and maintenance windows
   - CloudWatch logs export enabled
   - PostgreSQL 14.7 engine
   - db.t3.micro instance class (suitable for development/testing)

Note: You should:
1. Replace ""your-secure-password"" with a strong password
2. Adjust the storage size, instance class, and other parameters based on your specific needs
3. Consider adding security groups and proper network access controls in a production environment
4. Review and adjust the backup and maintenance windows according to your requirements",,,,,,"I'll help you create a PostgreSQL instance using Terraform. Based on the provided examples, I'll create a basic configuration that includes the necessary resources for a PostgreSQL RDS instance with common best practices.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create VPC resources
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  enable_dns_hostnames = true
  enable_dns_support   = true
  
  tags = {
    Name = ""postgresql-vpc""
  }
}

# Create subnet group for RDS
resource ""aws_db_subnet_group"" ""postgresql"" {
  name       = ""postgresql-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""PostgreSQL subnet group""
  }
}

# Create subnets in different AZs
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-east-1a""

  tags = {
    Name = ""postgresql-subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-east-1b""

  tags = {
    Name = ""postgresql-subnet-2""
  }
}

# Create PostgreSQL RDS instance
resource ""aws_db_instance"" ""postgresql"" {
  identifier           = ""postgresql-instance""
  engine              = ""postgres""
  engine_version      = ""14.7""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""yourpassword123""  # Remember to use sensitive variables in production
  
  db_subnet_group_name = aws_db_subnet_group.postgresql.name
  
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""
  
  multi_az               = false
  publicly_accessible    = false
  skip_final_snapshot    = true

  enabled_cloudwatch_logs_exports = [""postgresql""]

  tags = {
    Name = ""postgresql-instance""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC with two subnets in different availability zones
3. Creates a DB subnet group
4. Creates a PostgreSQL RDS instance with:
   - 20GB storage
   - PostgreSQL 14.7
   - t3.micro instance type
   - Basic backup and maintenance windows
   - CloudWatch logs export enabled
   - Private access (not publicly accessible)

Important notes:
- In a production environment, you should store sensitive information like passwords in variables or secrets management
- Consider adding security groups and proper network ACLs
- Adjust the instance size, storage, and other parameters based on your specific needs
- The configuration uses basic networking setup; you might want to add internet gateways, route tables, etc., for a complete networking stack",,,,,,"Based on the provided examples and the current question to create a PostgreSQL instance, I'll create a Terraform configuration that sets up a basic PostgreSQL RDS instance with recommended settings. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
  cidr_block = ""10.0.0.0/16""
  
  enable_dns_hostnames = true
  enable_dns_support   = true
}

resource ""aws_subnet"" ""private_subnet_1"" {
  vpc_id            = aws_vpc.vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-east-1a""
}

resource ""aws_subnet"" ""private_subnet_2"" {
  vpc_id            = aws_vpc.vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-east-1b""
}

resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""postgresql-subnet-group""
  subnet_ids = [aws_subnet.private_subnet_1.id, aws_subnet.private_subnet_2.id]
}

resource ""aws_db_instance"" ""postgresql"" {
  identifier           = ""postgresql-instance""
  engine              = ""postgres""
  engine_version      = ""15.5""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username           = ""dbadmin""
  password           = ""your-secure-password""
  
  db_subnet_group_name = aws_db_subnet_group.db_subnet_group.name
  
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""
  
  multi_az               = false
  publicly_accessible    = false
  skip_final_snapshot    = true
  
  enabled_cloudwatch_logs_exports = [""postgresql""]
  
  performance_insights_enabled = false
  
  timeouts {
    create = ""40m""
    delete = ""40m""
    update = ""40m""
  }
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with two private subnets for high availability
3. Creates a DB subnet group
4. Creates a PostgreSQL RDS instance with:
   - 20GB of GP2 storage
   - PostgreSQL 15.5 engine
   - db.t3.micro instance class (suitable for development/testing)
   - Basic backup and maintenance windows
   - CloudWatch logs export enabled
   - Security best practices like private subnet placement

Remember to:
1. Replace ""your-secure-password"" with a strong password
2. Adjust the instance class and storage based on your needs
3. Consider adding security groups if needed
4. Modify the backup and maintenance windows according to your requirements

The instance will be created in a private subnet for better security. You may need to set up additional networking components (like NAT Gateway, Internet Gateway) depending on your specific requirements.",,,,,
"aws_glacier_vault, aws_sns_topic, aws_iam_policy_document",generate an S3 Glacier vault with notification service,"package terraform.validation

default is_valid_terraform_plan = false

# Rule to check for an AWS Glacier Vault resource with 'name' and 'notification' attributes
is_valid_glacier_vault {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_glacier_vault""
	not is_null(resource.name)
	not is_null(resource.expressions)
	not is_null(resource.expressions.notification[0].sns_topic)
	not is_null(resource.expressions.notification[0].events)
}


# Rule to check for the existence of an AWS SNS Topic resource
is_valid_sns_topic {
	sns_topic_resource := input.planned_values.root_module.resources[_]
	sns_topic_resource.type == ""aws_sns_topic""
}

# Combined rule to validate the entire Terraform plan
is_valid_terraform_plan {
	is_valid_glacier_vault
	is_valid_sns_topic
}",3,"resource ""aws_sns_topic"" ""aws_sns_topic"" {
  name = ""glacier-sns-topic""
}

data ""aws_iam_policy_document"" ""my_archive"" {
  statement {
    sid    = ""add-read-only-perm""
    effect = ""Allow""

    principals {
      type        = ""*""
      identifiers = [""*""]
    }

    actions = [
      ""glacier:InitiateJob"",
      ""glacier:GetJobOutput"",
    ]

    resources = [""arn:aws:glacier:eu-west-1:432981146916:vaults/MyArchive""]
  }
}

resource ""aws_glacier_vault"" ""my_archive"" {
  name = ""MyArchive""

  notification {
    sns_topic = aws_sns_topic.aws_sns_topic.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  access_policy = data.aws_iam_policy_document.my_archive.json

  tags = {
    Test = ""MyArchive""
  }
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute
     with ""notification"" attribute
          with ""event"" attribute
          with ""sns_topic"" attribute

with ""aws_sns_topic"" resource","I'll help you create an S3 Glacier vault with notification service using Terraform. Based on the examples provided, I'll create a solution that includes both the Glacier vault and an SNS topic for notifications.

```hcl
# Create an SNS topic for Glacier notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-notifications""
}

# Create the Glacier vault with notification configuration
resource ""aws_glacier_vault"" ""archive_vault"" {
  name = ""archive-vault""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  tags = {
    Environment = ""Production""
    Purpose     = ""Archive Storage""
  }
}

# Create an IAM policy document for the vault
data ""aws_iam_policy_document"" ""vault_policy"" {
  statement {
    sid    = ""glacier-access-policy""
    effect = ""Allow""

    principals {
      type        = ""*""
      identifiers = [""*""]
    }

    actions = [
      ""glacier:InitiateJob"",
      ""glacier:GetJobOutput""
    ]

    resources = [aws_glacier_vault.archive_vault.arn]
  }
}

# Attach the policy to the vault
resource ""aws_glacier_vault"" ""archive_vault_policy"" {
  name          = aws_glacier_vault.archive_vault.name
  access_policy = data.aws_iam_policy_document.vault_policy.json
}
```

This configuration:
1. Creates an SNS topic for notifications
2. Creates a Glacier vault with notification configuration
3. Sets up notifications for archive and inventory retrieval completion events
4. Includes a basic IAM policy for vault access
5. Includes relevant tags for resource management

The vault will send notifications to the SNS topic when archives are retrieved or when inventory retrieval is completed. You can modify the `events` list in the notification block to include or exclude specific event types as needed.",,,,,,"I'll help you create an S3 Glacier vault with notification service using Terraform. Based on the examples provided, I'll create a solution that includes both the Glacier vault and an SNS topic for notifications.

```hcl
# Create an SNS topic for Glacier notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-notifications-topic""
}

# Create the Glacier vault with notification configuration
resource ""aws_glacier_vault"" ""vault"" {
  name = ""my-glacier-vault""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  tags = {
    Environment = ""Production""
    Purpose     = ""LongTermArchival""
  }
}

# Create an IAM policy document for the vault
data ""aws_iam_policy_document"" ""vault_policy"" {
  statement {
    sid    = ""glacier-access""
    effect = ""Allow""

    principals {
      type        = ""*""
      identifiers = [""*""]
    }

    actions = [
      ""glacier:InitiateJob"",
      ""glacier:GetJobOutput""
    ]

    resources = [aws_glacier_vault.vault.arn]
  }
}

# Attach the policy to the vault
resource ""aws_glacier_vault"" ""vault_policy"" {
  name          = aws_glacier_vault.vault.name
  access_policy = data.aws_iam_policy_document.vault_policy.json
}
```

This configuration:
1. Creates an SNS topic that will be used for notifications
2. Creates a Glacier vault with notification settings configured to notify on archive and inventory retrieval completion
3. Sets up an IAM policy document that defines access permissions for the vault
4. Attaches the access policy to the vault

The vault will send notifications to the SNS topic when specified events occur, and the IAM policy allows basic read operations on the vault.",,,,,,"I'll help you create an S3 Glacier vault with notification service using Terraform. Based on the examples provided, I'll create a solution that includes both the Glacier vault and an SNS topic for notifications.

```hcl
# Create an SNS topic for Glacier notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-notifications""
}

# Create the Glacier vault with notifications
resource ""aws_glacier_vault"" ""archive_vault"" {
  name = ""archive-vault""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  tags = {
    Environment = ""Production""
    Purpose     = ""Archive Storage""
  }
}

# Create an IAM policy document for the vault
data ""aws_iam_policy_document"" ""vault_policy"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""*""
      identifiers = [""*""]
    }

    actions = [
      ""glacier:InitiateJob"",
      ""glacier:GetJobOutput""
    ]

    resources = [aws_glacier_vault.archive_vault.arn]
  }
}

# Attach the policy to the vault
resource ""aws_glacier_vault"" ""archive_vault_policy"" {
  name          = aws_glacier_vault.archive_vault.name
  access_policy = data.aws_iam_policy_document.vault_policy.json
}
```

This configuration:
1. Creates an SNS topic for notifications
2. Creates a Glacier vault with notification configuration
3. Sets up notifications for archive and inventory retrieval completion
4. Includes a basic access policy for the vault
5. Adds tags for better resource management

The vault will send notifications to the SNS topic when archives are retrieved or inventory retrieval is completed. You can further customize the events and policies based on your specific requirements.",,,,,
"aws_route53_record, aws_route53_zone","Set up a non-alias aws_route_53 record that is linked to an aws_route53_zone resource named ""example53.com""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
	some i
	resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}

# Validate aws_route53_record
is_valid_r53_record {
	some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.example53.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""example53_A"" {
  zone_id = aws_route53_zone.example53.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""300""
  records = [""192.0.2.1""]  
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
","I'll help you create a Terraform configuration for setting up a Route 53 record linked to a Route 53 zone for ""example53.com"". Based on the examples provided, I'll create a non-alias record configuration.

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""example53_record"" {
  zone_id = aws_route53_zone.example53.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""300""
  records = [""10.0.0.1""]  # Replace with your desired IP address
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Route 53 zone for ""example53.com""
3. Creates a non-alias A record in the zone that:
   - Links to the created zone using its zone_id
   - Sets a TTL of 300 seconds
   - Points to an IP address (replace 10.0.0.1 with your desired IP)

The record is directly linked to the zone through the `zone_id` reference, and it's configured as a non-alias record by using the `records` attribute instead of `alias` configurations.",,,,,,"I'll help you create a Terraform configuration for setting up a non-alias Route 53 record linked to a Route 53 zone. Based on the examples provided, here's the solution:

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""300""
  records = [""10.0.0.1""]  # Replace with your actual IP address
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Route 53 hosted zone named ""example53.com""
3. Creates a non-alias A record in the hosted zone
4. Uses standard record properties including:
   - TTL of 300 seconds
   - Points to an IPv4 address (can be modified as needed)
   - Links to the zone using the zone_id reference

Note: You should replace the IP address in the `records` array with your actual desired IP address.",,,,,,"I'll help you create a Terraform configuration for setting up a non-alias Route 53 record linked to a Route 53 zone. Based on the examples provided, here's the solution:

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""example53_record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""300""
  records = [""10.0.0.1""]  # Replace with your desired IP address
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Route 53 hosted zone named ""example53.com""
3. Creates a non-alias A record in the hosted zone with:
   - A TTL of 300 seconds
   - Points to the IP address 10.0.0.1 (which you should replace with your actual IP address)
   - Links to the hosted zone using the zone_id reference

The record is non-alias as specified, using the standard records array rather than an alias configuration block.",,,,,
"aws_vpc, aws_vpc_dhcp_options, aws_vpc_dhcp_options_association","sets up a basic AWS network infrastructure consisting of a Virtual Private Cloud (VPC) with a custom DHCP options set. The VPC is configured with a CIDR block of 192.168.0.0/16. The DHCP options include a domain name (windomain.local), a combination of custom and Google's DNS servers (192.168.56.102 and 8.8.8.8), and a NetBIOS name server. These DHCP options are then associated with the created VPC, customizing its DHCP settings.






","package terraform.validation

# Set default validation states
default is_valid_vpc = false

default is_valid_dhcp_options = false

default is_valid_dhcp_options_association = false

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value == ""192.168.0.0/16""
}

# Validate aws_vpc_dhcp_options resource
is_valid_dhcp_options {
        some i
        resource := input.planned_values.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options""
        resource.values.domain_name == ""windomain.local""
    resource.values.domain_name_servers == [""192.168.56.102"", ""8.8.8.8""]
        resource.values.netbios_name_servers != null
}

# Validate aws_vpc_dhcp_options_association resource
is_valid_dhcp_options_association {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options_association""
        resource.expressions.dhcp_options_id.references[0] == ""aws_vpc_dhcp_options.default.id""
        resource.expressions.vpc_id.references[0] == ""aws_vpc.default.id""
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""default"" {
  cidr_block = ""192.168.0.0/16""
}

resource ""aws_vpc_dhcp_options"" ""default"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = concat([""192.168.56.102""], [""8.8.8.8""])
  netbios_name_servers = [""192.168.56.102""]
}

resource ""aws_vpc_dhcp_options_association"" ""default"" {
  vpc_id          = aws_vpc.default.id
  dhcp_options_id = aws_vpc_dhcp_options.default.id
}","Has one ""aws_vpc"" resource:
Configured with a ""cidr_block"" of ""192.168.0.0/16""
Includes associated custom DHCP options

Has one ""aws_vpc_dhcp_options"" resource:
Specifies a ""domain_name"" of ""windomain.local""
Includes DNS servers with a combination of a custom DNS server (""192.168.56.102"") and Google's DNS server (""8.8.8.8"")
Configures a ""NetBIOS"" name server

Has one ""aws_vpc_dhcp_options_association"" resource:
Associates the ""aws_vpc_dhcp_options"" with the created ""aws_vpc""
Customizes DHCP settings specifically for the created VPC environment","Based on your requirements, I'll help you create a Terraform configuration for setting up a VPC with custom DHCP options. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_support   = true
  enable_dns_hostnames = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""main_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""main-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""main_dhcp_association"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.main_dhcp_options.id
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and Google's DNS (8.8.8.8)
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The configuration includes proper tagging for better resource management and enables DNS support in the VPC. All resources are properly referenced using Terraform's resource addressing syntax.",,,,,,"Based on your requirements, I'll help you create a Terraform configuration for setting up a VPC with custom DHCP options. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""main_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""main-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""main_dhcp_assoc"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.main_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider configuration
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and Google's DNS (8.8.8.8)
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The configuration includes proper tagging for better resource management and enables DNS support in the VPC. You can customize the region and tags according to your needs.",,,,,,"Based on your requirements, I'll help you create a Terraform configuration for setting up a VPC with custom DHCP options. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block = ""192.168.0.0/16""
  
  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""vpc_dhcp_assoc"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp.id
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and Google's DNS (8.8.8.8)
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

I've also added appropriate tags to help with resource identification. The configuration follows AWS best practices and includes all the required elements from your specification.",,,,,
"aws_elastic_beanstalk_application, aws_elastic_beanstalk_application_version, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment, aws_s3_bucket, aws_s3_object, aws_sqs_queue","Create an elastic beanstalk worker environment for running batch processing jobs. Name the iam_role ""eb_ec2_role"", the instance profile ""eb_ec2_profile"", the elastic beanstalk application ""batch_job_app"", the bucket ""sampleapril26426"", the object ""examplebucket_object"", the sqs queue ""batch_job_queue"", and the application version ""version"".","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_s3_bucket = false

default is_valid_s3_object = false

default is_valid_eb_app = false

default is_valid_eb_env = false

is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0] == ""aws_iam_role.eb_ec2_role.name""
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0] == ""aws_iam_role.eb_ec2_role.name""
}

is_valid_s3_bucket {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    resource.name
    resource.expressions.bucket.constant_value == ""sampleapril26426""
}

is_valid_s3_object {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_s3_object""
    resource.expressions.bucket.references[0] == ""aws_s3_bucket.sampleapril26426.id""
    resource.expressions.key
    resource.expressions.source
    
}

is_valid_sqs_queue {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_sqs_queue""
    resource.expressions.name
}

# Validate aws_eb_app
is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}

is_valid_eb_app_version {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application_version""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.batch_job_app.name""
    resource.expressions.bucket.references[0] == ""aws_s3_object.examplebucket_object.bucket""
    resource.expressions.key.references[0] == ""aws_s3_object.examplebucket_object.key""
}
# Validate aws_eb_env
is_valid_eb_env {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.batch_job_app.name""
    resource.expressions.solution_stack_name
    resource.expressions.tier.constant_value == ""Worker""
    resource.expressions.version_label.references[0] == ""aws_elastic_beanstalk_application_version.version.name""
            some a, b
    resource.expressions.setting[a].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource.expressions.setting[b].value.references[0] == ""aws_sqs_queue.batch_job_queue.id""
}


# Combine all checks into a final rule
is_configuration_valid {
        is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
    is_valid_s3_bucket
    is_valid_s3_object 
    is_valid_sqs_queue
    is_valid_eb_app_version
    is_valid_eb_app
    is_valid_eb_env
}
",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region = ""us-east-1"" 
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}


resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}


# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}


resource ""aws_s3_bucket"" ""sample_bucket"" {
  bucket_prefix = ""sample-""
}

resource ""aws_s3_object"" ""examplebucket_object"" {
  key    = ""randofile""
  bucket = aws_s3_bucket.sample_bucket.id
  source = ""./supplement/app.zip""
}

# Define the Elastic Beanstalk application
resource ""aws_elastic_beanstalk_application"" ""batch_job_app"" {
  name        = ""batch-job-application""
}

# Define the Elastic Beanstalk application version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""batch-job-app-version""
  application = aws_elastic_beanstalk_application.batch_job_app.name
  bucket = aws_s3_object.examplebucket_object.bucket
  key    = aws_s3_object.examplebucket_object.key
}

# Create an Elastic Beanstalk environment in the worker tier
resource ""aws_elastic_beanstalk_environment"" ""batch_job_env"" {
  name                = ""batch-job-environment""
  application         = aws_elastic_beanstalk_application.batch_job_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  tier                = ""Worker""
  version_label       = aws_elastic_beanstalk_application_version.version.name

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""SingleInstance"" 
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""WorkerQueueURL""
    value     = aws_sqs_queue.batch_job_queue.id 
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""HttpPath""
    value     = ""/""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""MimeType""
    value     = ""application/json"" 
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Define the SQS queue
resource ""aws_sqs_queue"" ""batch_job_queue"" {
  name = ""batch-job-queue""
}
","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has one ""aws_elastic_beanstalk_application"" resource
    with ""name"" 

Has one ""aws_s3_bucket"" resource
    with ""bucket""

Has one ""aws_s3_object"" resource
    with ""key""
    with ""source""
    with ""bucket"" referencing the ""aws_s3_bucket"" resource

Has one ""aws_elastic_beanstalk_application_version"" resource
    with ""name
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""bucket"" referencing the ""bucket"" from the ""aws_s3_object"" resource
    with ""key"" referencing the ""key"" from the ""aws_s3_object"" resource

Has one ""aws_sqs_queue"" resource
    with ""name""

Has one ""aws_elastic_beanstalk_environment"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with ""tier"" set to ""Worker""
    with ""version_label"" referencing the ""aws_elastic_beanstalk_application_version"" resource
    with several ""setting"" blocks that have ""namespaces"" set to either ""aws:elasticbeanstalk:environment"" or ""aws:elasticbeanstalk:sqsd"", 
    with one ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource
","I'll help you create a Terraform configuration for an Elastic Beanstalk worker environment with SQS integration for batch processing. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_worker_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policies for Elastic Beanstalk worker tier
resource ""aws_iam_role_policy_attachment"" ""eb_worker_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWorkerTier""
}

resource ""aws_iam_role_policy_attachment"" ""eb_sqs_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Create an instance profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_worker_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Create SQS queue for batch jobs
resource ""aws_sqs_queue"" ""batch_job_queue"" {
  name = ""batch-job-queue""
}

# S3 bucket for application version
resource ""aws_s3_bucket"" ""sampleapril26426"" {
  bucket = ""sampleapril26426""
}

# S3 bucket object for application code
resource ""aws_s3_object"" ""examplebucket_object"" {
  bucket = aws_s3_bucket.sampleapril26426.id
  key    = ""worker-app.zip""
  source = ""./supplement/app.zip""
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""batch_job_app"" {
  name = ""batch-processing-application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""worker-app-version""
  application = aws_elastic_beanstalk_application.batch_job_app.name
  bucket      = aws_s3_bucket.sampleapril26426.id
  key         = aws_s3_object.examplebucket_object.key
}

# Elastic Beanstalk Worker Environment
resource ""aws_elastic_beanstalk_environment"" ""worker_environment"" {
  name                = ""batch-worker-environment""
  application         = aws_elastic_beanstalk_application.batch_job_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  tier                = ""Worker""
  version_label      = aws_elastic_beanstalk_application_version.version.name

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""WorkerQueueURL""
    value     =",,,,,,"Based on the provided examples and the current question, I'll help create a Terraform configuration for an Elastic Beanstalk worker environment with SQS integration for batch processing. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
}

# IAM role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_worker_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policies for Elastic Beanstalk worker tier
resource ""aws_iam_role_policy_attachment"" ""eb_worker_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWorkerTier""
}

resource ""aws_iam_role_policy_attachment"" ""eb_sqs_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSQSFullAccess""
}

# Create an instance profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_worker_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Create SQS queue for batch jobs
resource ""aws_sqs_queue"" ""batch_job_queue"" {
  name = ""batch-processing-queue""
}

# S3 bucket for application version
resource ""aws_s3_bucket"" ""sampleapril26426"" {
  bucket = ""sampleapril26426""
}

# S3 object for application code
resource ""aws_s3_object"" ""examplebucket_object"" {
  key    = ""worker-app.zip""
  bucket = aws_s3_bucket.sampleapril26426.id
  source = ""./supplement/worker-app.zip""
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""batch_job_app"" {
  name = ""batch-processing-application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""worker-app-version""
  application = aws_elastic_beanstalk_application.batch_job_app.name
  bucket      = aws_s3_bucket.sampleapril26426.id
  key         = aws_s3_object.examplebucket_object.key
}

# Elastic Beanstalk Worker Environment
resource ""aws_elastic_beanstalk_environment"" ""worker_environment"" {
  name                = ""batch-worker-environment""
  application         = aws_elastic_beanstalk_application.batch_job_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  tier                = ""Worker""
  version_label       = aws_elastic_beanstalk_application_version.version.name

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""WorkerQ",,,,,,"I'll help you create a Terraform configuration for an Elastic Beanstalk worker environment with SQS queue integration for batch processing jobs. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM role for Elastic Beanstalk worker instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_worker_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policies for Elastic Beanstalk worker tier
resource ""aws_iam_role_policy_attachment"" ""eb_worker_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWorkerTier""
}

resource ""aws_iam_role_policy_attachment"" ""eb_sqs_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSQSFullAccess""
}

# Create an instance profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_worker_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Create SQS queue for batch jobs
resource ""aws_sqs_queue"" ""batch_job_queue"" {
  name = ""batch-processing-queue""
}

# S3 bucket for application version
resource ""aws_s3_bucket"" ""sampleapril26426"" {
  bucket = ""sampleapril26426""
}

# S3 bucket object for application code
resource ""aws_s3_object"" ""examplebucket_object"" {
  key    = ""worker-app.zip""
  bucket = aws_s3_bucket.sampleapril26426.id
  source = ""./supplement/worker-app.zip""
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""batch_job_app"" {
  name = ""batch-processing-application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""worker-app-version""
  application = aws_elastic_beanstalk_application.batch_job_app.name
  bucket      = aws_s3_object.examplebucket_object.bucket
  key         = aws_s3_object.examplebucket_object.key
}

# Elastic Beanstalk Worker Environment
resource ""aws_elastic_beanstalk_environment"" ""worker_env"" {
  name                = ""batch-worker-environment""
  application         = aws_elastic_beanstalk_application.batch_job_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  tier                = ""Worker""
  version_label       = aws_elastic_beanstalk_application_version.version.name

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""WorkerQueueURL""
    value     = aws",,,,,
"aws_s3_bucket, aws_s3_bucket_metric","Create a S3 bucket with bucket name = ""mybucket"" and a bucket metric resource that adds metrics configuration for the entire bucket.","package s3_bucket_metric
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some metrics in input.configuration.root_module.resources
    metrics.type == ""aws_s3_bucket_metric""
    bucket.address in metrics.expressions.bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some metrics in input.configuration.root_module.resources
    metrics.type == ""aws_s3_bucket_metric""
    metrics.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example""
}

resource ""aws_s3_bucket_metric"" ""example-entire-bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""
}","Has an aws_s3_bucket resource

Has an aws_s3_bucket_metric
with bucket = bucked id OR name","I'll help you create a Terraform configuration for an S3 bucket with a bucket metrics configuration for the entire bucket. Based on the examples provided, here's the solution:

```hcl
resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_metric"" ""entire-bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""
}
```

This configuration:
1. Creates an S3 bucket named ""mybucket""
2. Adds a metric configuration for the entire bucket using the `aws_s3_bucket_metric` resource
3. References the bucket using its ID rather than the name directly
4. Uses ""EntireBucket"" as the name for the metric configuration

The metric configuration will enable CloudWatch metrics for the entire bucket, allowing you to monitor bucket usage and performance.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a metric configuration for the entire bucket. Based on the provided examples and best practices, here's the solution:

```hcl
# Create the S3 bucket
resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

# Add metrics configuration for the entire bucket
resource ""aws_s3_bucket_metric"" ""entire-bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""
}
```

This configuration:
1. Creates an S3 bucket named ""mybucket""
2. Adds a metric configuration that covers the entire bucket
3. Uses the bucket reference (bucket.id) instead of the bucket name directly
4. Names the metric configuration ""EntireBucket""

The metric configuration will allow you to monitor the bucket's usage and performance through CloudWatch metrics.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a bucket metrics configuration for the entire bucket. Based on the provided examples and best practices, here's the solution:

```hcl
# Create the S3 bucket
resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

# Add metrics configuration for the entire bucket
resource ""aws_s3_bucket_metric"" ""entire-bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""
}
```

This configuration:
1. Creates an S3 bucket named ""mybucket""
2. Adds a metric configuration that monitors the entire bucket
3. Uses the bucket reference (bucket.id) instead of the bucket name directly
4. Names the metric configuration ""EntireBucket""

The metric configuration will allow you to track metrics for the entire bucket in CloudWatch.",,,,,
"aws_glacier_vault, aws_iam_policy_document",generage an S3 Glacier vault with a access policy,"package terraform.validation

default is_valid_terraform_plan = false

# Rule to check for an AWS Glacier Vault resource with 'name' and 'access_policy' attributes
is_valid_glacier_vault {
	resource := input.planned_values.root_module.resources[_]
	resource.type == ""aws_glacier_vault""
	not is_null(resource.values.name)
	not is_null(resource.values.access_policy)
}

# Combined rule to validate the entire Terraform plan
is_valid_terraform_plan {
	is_valid_glacier_vault
}",3,"provider ""aws"" {
  region     = ""us-east-1""
}

data ""aws_iam_policy_document"" ""my_archive"" {
  statement {
    sid    = ""add-read-only-perm""
    effect = ""Allow""

    principals {
      type        = ""*""
      identifiers = [""*""]
    }

    actions = [
      ""glacier:InitiateJob"",
      ""glacier:GetJobOutput"",
    ]

    resources = [""arn:aws:glacier:eu-west-1:432981146916:vaults/MyArchive""]
  }
}

resource ""aws_glacier_vault"" ""my_archive"" {
  name = ""MyArchive""
  access_policy = data.aws_iam_policy_document.my_archive.json

  tags = {
    Test = ""MyArchive""
  }
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute
     with ""access_policy"" attribute","I'll help you create a Terraform configuration for an S3 Glacier vault with an access policy. Based on the examples provided, I'll generate the appropriate HCL code:

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

# Create the Glacier vault
resource ""aws_glacier_vault"" ""example"" {
  name = ""example-vault""

  # Access policy directly attached to the vault
  access_policy = data.aws_iam_policy_document.vault_policy.json
}

# Define the access policy using aws_iam_policy_document
data ""aws_iam_policy_document"" ""vault_policy"" {
  statement {
    actions = [
      ""glacier:InitiateJob"",
      ""glacier:GetJobOutput"",
      ""glacier:UploadArchive"",
      ""glacier:DescribeVault""
    ]

    effect = ""Allow""

    principals {
      type = ""AWS""
      identifiers = [""*""]
    }

    resources = [
      ""arn:aws:glacier:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:vaults/${aws_glacier_vault.example.name}""
    ]
  }
}

# Get current region
data ""aws_region"" ""current"" {}

# Get current account ID
data ""aws_caller_identity"" ""current"" {}
```

This configuration:
1. Creates a Glacier vault named ""example-vault""
2. Defines an access policy using `aws_iam_policy_document`
3. Attaches the policy directly to the vault using the `access_policy` argument
4. Includes basic permissions for vault operations
5. Uses data sources to get the current region and account ID for the ARN

The access policy allows basic operations like initiating jobs, getting job output, uploading archives, and describing the vault. You can modify the permissions in the policy document according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault with an access policy. Based on the examples provided, I'll generate a solution that includes the necessary resources but without the vault lock component since it's not required in your question.

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

# Create the Glacier vault
resource ""aws_glacier_vault"" ""example"" {
  name = ""example-vault""
  
  # Access policy directly attached to the vault
  access_policy = data.aws_iam_policy_document.vault_policy.json
}

# Define the access policy using aws_iam_policy_document
data ""aws_iam_policy_document"" ""vault_policy"" {
  statement {
    effect = ""Allow""
    
    principals {
      type = ""AWS""
      identifiers = [""*""]
    }
    
    actions = [
      ""glacier:ListVaults"",
      ""glacier:DescribeVault"",
      ""glacier:UploadArchive"",
      ""glacier:InitiateMultipartUpload"",
      ""glacier:ListParts"",
      ""glacier:ListMultipartUploads""
    ]
    
    resources = [
      ""arn:aws:glacier:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:vaults/${aws_glacier_vault.example.name}""
    ]
  }
}

# Get current region
data ""aws_region"" ""current"" {}

# Get current account ID
data ""aws_caller_identity"" ""current"" {}
```

This configuration:
1. Creates a Glacier vault named ""example-vault""
2. Defines an access policy using `aws_iam_policy_document`
3. Attaches the policy directly to the vault using the `access_policy` attribute
4. Includes common Glacier permissions for basic vault operations
5. Uses data sources to get the current region and account ID for proper ARN construction

The policy allows basic vault operations while maintaining security best practices. You can modify the permissions in the policy document according to your specific needs by adjusting the actions, principals, and conditions as required.",,,,,,"I'll help you generate a Terraform configuration for an S3 Glacier vault with an access policy. Based on the examples provided, I'll create a solution that implements the required configuration.

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

# Create the Glacier vault
resource ""aws_glacier_vault"" ""vault"" {
  name = ""example-vault""
}

# Define the access policy using aws_iam_policy_document
data ""aws_iam_policy_document"" ""vault_policy"" {
  statement {
    actions = [
      ""glacier:UploadArchive"",
      ""glacier:InitiateMultipartUpload"",
      ""glacier:AbortMultipartUpload"",
      ""glacier:CompleteMultipartUpload""
    ]
    effect = ""Allow""
    principals {
      type = ""AWS""
      identifiers = [""*""]
    }
    resources = [aws_glacier_vault.vault.arn]
    
    condition {
      test     = ""StringLike""
      variable = ""aws:PrincipalArn""
      values   = [""arn:aws:iam::*:role/allowed-glacier-role""]
    }
  }

  statement {
    actions = [
      ""glacier:DeleteArchive""
    ]
    effect = ""Deny""
    principals {
      type = ""AWS""
      identifiers = [""*""]
    }
    resources = [aws_glacier_vault.vault.arn]
  }
}

# Attach the policy to the Glacier vault
resource ""aws_glacier_vault_policy"" ""vault_policy"" {
  vault_name = aws_glacier_vault.vault.name
  policy     = data.aws_iam_policy_document.vault_policy.json
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Glacier vault named ""example-vault""
3. Defines an IAM policy document that:
   - Allows specific Glacier operations (upload and multipart upload operations) for specified roles
   - Denies deletion operations to all principals
4. Attaches the policy to the Glacier vault using aws_glacier_vault_policy

The policy implements basic security best practices by:
- Explicitly denying deletion operations
- Allowing only specific upload operations
- Using condition blocks to restrict access to specific roles
- Following the principle of least privilege

You can customize the policy statements, actions, and conditions according to your specific security requirements.",,,,,
"aws_chime_voice_connector, aws_chime_voice_connector, aws_chime_voice_connector_group","Create two AWS Chime Voice Connectors, both with encryption. Then create an aws_chime_voice_connector_group resource with both of these connectors. One connector should have priority=1 and the other should have priority=3","package aws_chime_voice_connector
import future.keywords.in

default valid := false
default connectors_valid := false

valid {
    connectors_valid
}

connectors_valid {
    some vc1 in input.configuration.root_module.resources
    vc1.type == ""aws_chime_voice_connector""
    vc1.expressions.require_encryption.constant_value == true

    some vc2 in input.configuration.root_module.resources
    vc2.type == ""aws_chime_voice_connector""
    vc2.expressions.require_encryption.constant_value == true

    not vc1 == vc2
    
    some vcgroup in input.configuration.root_module.resources
    vcgroup.type == ""aws_chime_voice_connector_group""

    some connector1 in vcgroup.expressions.connector
    vc1.address in connector1.voice_connector_id.references
    some connector2 in vcgroup.expressions.connector
    vc2.address in connector2.voice_connector_id.references
    
    not connector1 == connector2
    
    connector1.priority.constant_value == 1
    connector2.priority.constant_value == 3
}

connectors_valid {
        some vc1 in input.configuration.root_module.resources
    vc1.type == ""aws_chime_voice_connector""
    vc1.expressions.require_encryption.constant_value == true

    some vc2 in input.configuration.root_module.resources
    vc2.type == ""aws_chime_voice_connector""
    vc2.expressions.require_encryption.constant_value == true

    not vc1 == vc2
    
    some vcgroup in input.configuration.root_module.resources
    vcgroup.type == ""aws_chime_voice_connector_group""

    some connector1 in vcgroup.expressions.connector
    vc1.address in connector1.voice_connector_id.references
    some connector2 in vcgroup.expressions.connector
    vc2.address in connector2.voice_connector_id.references
    
    not connector1 == connector2
    
    connector1.priority.constant_value == 3
    connector2.priority.constant_value == 1
}",3,"resource ""aws_chime_voice_connector"" ""vc1"" {
  name               = ""connector-test-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""vc2"" {
  name               = ""connector-test-2""
  require_encryption = true
  aws_region         = ""us-west-2""
}

resource ""aws_chime_voice_connector_group"" ""group"" {
  name = ""test-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.vc1.id
    priority           = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.vc2.id
    priority           = 3
  }
}","Has one resource ""aws_chime_voice_connector""
with require_encryption=""true""

Has another resource ""aws_chime_voice_connector""
with require_encryption=""true""

Has one resource ""aws_chime_voice_connector_group""
with a connector for each voice connector with the correct id
one connector should have priority=1 and the other priority=3","Based on your request, I'll help you create two AWS Chime Voice Connectors with encryption and group them together with different priorities. Here's the Terraform configuration:

```hcl
resource ""aws_chime_voice_connector"" ""vc1"" {
  name               = ""connector-test-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""vc2"" {
  name               = ""connector-test-2""
  require_encryption = true
  aws_region         = ""us-west-2""
}

resource ""aws_chime_voice_connector_group"" ""group"" {
  name = ""test-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.vc1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.vc2.id
    priority          = 3
  }
}
```

This configuration:
1. Creates two voice connectors (vc1 and vc2) with encryption enabled
2. Creates a voice connector group that includes both connectors
3. Sets vc1 with priority 1 (higher priority)
4. Sets vc2 with priority 3 (lower priority)
5. Places the connectors in different AWS regions for redundancy

The voice connector with priority 1 will be used first, and the one with priority 3 will serve as a backup.",,,,,,"I'll help you create two AWS Chime Voice Connectors with encryption and group them together with different priorities. Based on the examples provided, here's the Terraform configuration:

```hcl
resource ""aws_chime_voice_connector"" ""vc1"" {
  name               = ""connector-test-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""vc2"" {
  name               = ""connector-test-2""
  require_encryption = true
  aws_region         = ""us-west-2""
}

resource ""aws_chime_voice_connector_group"" ""group"" {
  name = ""test-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.vc1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.vc2.id
    priority          = 3
  }
}
```

This configuration:
1. Creates the first voice connector ""vc1"" in us-east-1 with encryption enabled
2. Creates the second voice connector ""vc2"" in us-west-2 with encryption enabled
3. Creates a voice connector group that includes both connectors
4. Sets vc1 with priority 1 (higher priority)
5. Sets vc2 with priority 3 (lower priority)",,,,,,"Based on the provided examples and your specific requirements, I'll help you create two AWS Chime Voice Connectors with encryption and group them together with different priorities. Here's the solution:

```hcl
resource ""aws_chime_voice_connector"" ""vc1"" {
  name               = ""connector-test-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""vc2"" {
  name               = ""connector-test-2""
  require_encryption = true
  aws_region         = ""us-west-2""
}

resource ""aws_chime_voice_connector_group"" ""group"" {
  name = ""test-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.vc1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.vc2.id
    priority          = 3
  }
}
```

This configuration:
1. Creates two voice connectors (vc1 and vc2) with encryption enabled
2. Creates a voice connector group that includes both connectors
3. Sets vc1 with priority 1 and vc2 with priority 3, as requested
4. Places the connectors in different AWS regions for redundancy

The higher priority (lower number) connector will be used first, with the lower priority connector serving as a backup.",,,,,
"aws_efs_file_system, aws_efs_file_system_policy, aws_iam_policy_document",create a AWS EFS with a system policy,"package terraform.validation

default is_valid_efs_setup = false

# Rule to check if a valid aws_efs_file_system and aws_efs_file_system_policy exists
is_valid_efs_setup {
    has_valid_efs_file_system
    has_valid_efs_file_system_policy
}

# Helper rule to check if a valid aws_efs_file_system exists
has_valid_efs_file_system {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_file_system""
}

# Helper rule to check if a valid aws_efs_file_system_policy exists with required arguments
has_valid_efs_file_system_policy {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_file_system_policy""
    resource.expressions.file_system_id
    resource.expressions.policy
}
",3,"resource ""aws_efs_file_system"" ""fs"" {
  creation_token = ""my-product""
}

data ""aws_iam_policy_document"" ""policy"" {
  statement {
    sid    = ""ExampleStatement01""
    effect = ""Allow""

    principals {
      type        = ""AWS""
      identifiers = [""*""]
    }

    actions = [
      ""elasticfilesystem:ClientMount"",
      ""elasticfilesystem:ClientWrite"",
    ]

    resources = [aws_efs_file_system.fs.arn]

    condition {
      test     = ""Bool""
      variable = ""aws:SecureTransport""
      values   = [""true""]
    }
  }
}

resource ""aws_efs_file_system_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.fs.id
  policy         = data.aws_iam_policy_document.policy.json
}","Have one ""aws_efs_file_system"" resource

Have one ""aws_efs_file_system_policy"" resource
with one ""file_system_id""
with one ""policy""","I'll help you create an AWS EFS with a system policy using Terraform. Based on the best practices and examples provided, here's a solution that implements a file system policy for EFS:

```hcl
# Create the EFS File System
resource ""aws_efs_file_system"" ""efs"" {
  creation_token = ""my-efs""
  encrypted      = true
  
  tags = {
    Name = ""MyEFS""
  }
}

# Create the EFS File System Policy
resource ""aws_efs_file_system_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.efs.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""AllowRootAccess""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""elasticfilesystem:ClientMount"",
          ""elasticfilesystem:ClientWrite""
        ]
        Resource = aws_efs_file_system.efs.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"": ""true""
          }
        }
      }
    ]
  })
}
```

This configuration:
1. Creates an EFS file system with encryption enabled
2. Attaches a file system policy that:
   - Allows client mount and write operations
   - Requires secure transport (SSL)
   - Can be customized further based on your specific security requirements

You can modify the policy statement according to your specific access control needs by adjusting the Principal, Action, and Condition elements.",,,,,,"I'll help you create an AWS EFS with a system policy using Terraform. Based on the provided examples and best practices, here's a solution:

```hcl
# Create the EFS File System
resource ""aws_efs_file_system"" ""efs"" {
  creation_token = ""my-efs""
  encrypted      = true

  tags = {
    Name = ""MyEFS""
  }
}

# Create the EFS System Policy
resource ""aws_efs_file_system_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.efs.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""AllowRootAccess""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""elasticfilesystem:ClientMount"",
          ""elasticfilesystem:ClientWrite""
        ]
        Resource = aws_efs_file_system.efs.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"" = ""true""
          }
        }
      }
    ]
  })
}
```

This configuration:
1. Creates an EFS file system with encryption enabled
2. Attaches a system policy that:
   - Allows client mount and write operations
   - Requires secure transport (HTTPS)
   - Can be customized further based on your specific access requirements

You can modify the policy statement according to your specific security requirements and access patterns.",,,,,,"I'll help you create an AWS EFS with a system policy using Terraform. Based on the examples provided, I'll create a configuration that implements a file system policy for EFS.

Here's the Terraform configuration:

```hcl
# Create the EFS File System
resource ""aws_efs_file_system"" ""efs"" {
  creation_token = ""my-efs""
  encrypted      = true

  tags = {
    Name = ""MyEFS""
  }
}

# Create the EFS File System Policy
resource ""aws_efs_file_system_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.efs.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""AllowRootAccess""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""elasticfilesystem:ClientMount"",
          ""elasticfilesystem:ClientWrite""
        ]
        Resource = aws_efs_file_system.efs.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"" = ""true""
          }
        }
      }
    ]
  })
}
```

This configuration:
1. Creates an EFS file system with encryption enabled
2. Attaches a file system policy that:
   - Allows client mount and write operations
   - Requires secure transport (TLS)
   - Can be customized further based on your specific access requirements

The policy can be modified to be more restrictive by adjusting the Principal, Actions, and Conditions as needed for your use case.",,,,,
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration","create a bucket 'pike-680235478471'. Set up request payment configuration for an AWS S3 bucket named 'pike-680235478471', specifying the payer as 'Requester'.","package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_request_payment_configuration = false

default valid_configuration = false

has_s3_bucket {
    some i
    bucket := input.planned_values.root_module.resources[i]
    bucket.type == ""aws_s3_bucket""
    bucket.values.bucket == ""pike-680235478471""
}

has_s3_bucket_request_payment_configuration {
    some i
    payment_config := input.planned_values.root_module.resources[i]
    payment_config.type == ""aws_s3_bucket_request_payment_configuration""
    payment_config.values.bucket == ""pike-680235478471""
    payment_config.values.payer == ""Requester""
}

valid_configuration {
    has_s3_bucket
    has_s3_bucket_request_payment_configuration
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""a"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""pike"" {
  bucket = ""pike-680235478471""
  payer  = ""Requester""
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_request_payment_configuration resource
with bucket
with payer (BucketOwner or Requester)","I'll help you create a Terraform configuration for an S3 bucket with a request payment configuration where the requester pays for the fees.

```hcl
# Create the S3 bucket
resource ""aws_s3_bucket"" ""pike_bucket"" {
  bucket = ""pike-680235478471""
}

# Configure request payment settings
resource ""aws_s3_bucket_request_payment_configuration"" ""pike_payment_config"" {
  bucket = aws_s3_bucket.pike_bucket.id
  payer  = ""Requester""
}
```

This configuration:
1. Creates an S3 bucket named ""pike-680235478471""
2. Sets up a request payment configuration where the requester pays for the transfer fees instead of the bucket owner
3. Uses the reference to the bucket ID to ensure the payment configuration is associated with the correct bucket

The `payer` is set to ""Requester"" as specified in your requirements, meaning that the person requesting the data from the bucket will pay for the data transfer costs.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a request payment configuration where the requester pays for the fees.

```hcl
# Create the S3 bucket
resource ""aws_s3_bucket"" ""pike_bucket"" {
  bucket = ""pike-680235478471""
}

# Configure request payment configuration
resource ""aws_s3_bucket_request_payment_configuration"" ""pike_payment_config"" {
  bucket = aws_s3_bucket.pike_bucket.id
  payer  = ""Requester""
}
```

This configuration:
1. Creates an S3 bucket named ""pike-680235478471""
2. Sets up a request payment configuration where the requester pays for the fees (transfer costs, request costs, etc.) instead of the bucket owner
3. Uses proper referencing between resources with `aws_s3_bucket.pike_bucket.id`

The configuration follows AWS best practices and ensures that anyone who requests data from the bucket will be responsible for the associated costs.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with request payment configuration where the requester pays for the fees.

```hcl
# Create the S3 bucket
resource ""aws_s3_bucket"" ""pike_bucket"" {
  bucket = ""pike-680235478471""
}

# Configure request payment configuration
resource ""aws_s3_bucket_request_payment_configuration"" ""pike_payment_config"" {
  bucket = aws_s3_bucket.pike_bucket.id
  payer  = ""Requester""
}
```

This configuration will:
1. Create an S3 bucket named ""pike-680235478471""
2. Set up a request payment configuration where the requester pays for the data transfer fees and request fees instead of the bucket owner.

The configuration uses the `aws_s3_bucket_request_payment_configuration` resource to specify that the ""Requester"" will pay for the fees, rather than the bucket owner. The bucket reference is linked to the created S3 bucket using the bucket's id attribute.",,,,,
"aws_vpc, aws_vpc, aws_vpc_peering_connection","establishes a VPC peering connection between two AWS Virtual Private Clouds (VPCs). The first VPC, named ""peer,"" is configured with a CIDR block of 10.0.0.0/24, and the second VPC, named ""base,"" has a CIDR block of 10.1.0.0/24. The peering connection, named ""pike,"" connects these two VPCs, allowing them to communicate with each other as if they were in the same network, and is tagged with pike = ""permissions"" to indicate its purpose or management policy.","package terraform.validation

# Set default validation states
default is_valid_vpcs = false
default is_valid_vpc_peering_connection = false

# Validate aws_vpc resources
is_valid_vpcs {
    # Validate the first VPC named ""peer""
    peer_vpc := input.configuration.root_module.resources[_]
    peer_vpc.type == ""aws_vpc""
    peer_vpc.name == ""peer""
    peer_vpc.expressions.cidr_block.constant_value == ""10.0.0.0/24""

    # Validate the second VPC named ""base""
    base_vpc := input.configuration.root_module.resources[_]
    base_vpc.type == ""aws_vpc""
    base_vpc.name == ""base""
    base_vpc.expressions.cidr_block.constant_value == ""10.1.0.0/24""

    # Ensure different VPCs
    peer_vpc != base_vpc
}

# Validate aws_vpc_peering_connection resource
is_valid_vpc_peering_connection {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_vpc_peering_connection""
    resource.name == ""pike""
    # Ensure connection between ""peer"" and ""base"" VPCs
    resource.expressions.peer_vpc_id.references[0] == ""aws_vpc.peer.id""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.base.id""
    # Check for the specific tag indicating its purpose
    resource.expressions.tags.constant_value[""pike""] == ""permissions""
}",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id

  tags = {
    pike = ""permissions""
  }
}

resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
}

resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
}","Has two ""aws_vpc"" resources:
The first VPC is named ""peer"" and is configured with a ""cidr_block"" of ""10.0.0.0/24"".
The second VPC is named ""base"" and is configured with a ""cidr_block"" of ""10.1.0.0/24"".

Has one ""aws_vpc_peering_connection"" resource:
Named ""pike"" to connect the two specified VPCs (""peer"" and ""base"").
Configured to allow the VPCs to communicate with each other as if they were in the same network.
Tagged with ""pike"" set to ""permissions"", indicating its purpose or management policy within the infrastructure.","Based on the examples and the current question, I'll help you create a Terraform configuration that establishes a VPC peering connection between two VPCs with the specified requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create the first VPC named ""peer""
resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
  
  tags = {
    Name = ""peer""
  }
}

# Create the second VPC named ""base""
resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
  
  tags = {
    Name = ""base""
  }
}

# Create the VPC peering connection named ""pike""
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    Name = ""pike""
    pike = ""permissions""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates two VPCs with their specified CIDR blocks and names
3. Establishes a VPC peering connection between them with the required tag
4. Uses auto_accept for simplicity (assuming both VPCs are in the same account)

The VPCs will be able to communicate with each other once you also configure the appropriate route table entries (not included in this basic configuration).",,,,,,"Based on the examples provided and the current question, I'll help you create a Terraform configuration that establishes a VPC peering connection between two VPCs with the specified requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create the ""peer"" VPC
resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
  tags = {
    Name = ""peer""
  }
}

# Create the ""base"" VPC
resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
  tags = {
    Name = ""base""
  }
}

# Create the VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    Name = ""pike""
    pike = ""permissions""
  }
}
```

This configuration:
1. Sets up the required AWS provider configuration
2. Creates two VPCs (""peer"" and ""base"") with their specified CIDR blocks
3. Establishes a VPC peering connection named ""pike"" between the two VPCs
4. Tags the peering connection with ""pike = permissions""

The VPCs are created with their respective CIDR blocks (10.0.0.0/24 for peer and 10.1.0.0/24 for base) and are properly tagged. The peering connection is automatically accepted using the `auto_accept = true` parameter and includes the required tag.",,,,,,"Based on the provided examples and your requirements, I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs with the specified CIDR blocks and tags.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

# Create the first VPC named ""peer""
resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
  
  tags = {
    Name = ""peer""
  }
}

# Create the second VPC named ""base""
resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
  
  tags = {
    Name = ""base""
  }
}

# Establish VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    Name = ""pike""
    pike = ""permissions""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates two VPCs with specified CIDR blocks and names
3. Establishes a VPC peering connection between them with the required tag
4. Includes auto_accept for automatic peering connection acceptance
5. Properly tags all resources for identification and management

The configuration matches your requirements exactly while following AWS best practices for VPC peering connections.",,,,,
"aws_iam_role, aws_iam_role_policy, aws_kendra_index",Generate a basic Kendra index resource with the default Document Metadata Configuration Updates,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
    some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kendra.amazonaws.com"") 
}

has_valid_kendra_index {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_index""
    resource.values.name
    resource.values.document_metadata_configuration_updates
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
}


has_valid_resources {
    has_valid_iam_role(input.resource_changes)
    has_valid_kendra_index
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""role"" {
  name = ""Kendra-Role""
  assume_role_policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": ""sts:AssumeRole"",
      ""Principal"": {
        ""Service"": ""kendra.amazonaws.com""
      },
      ""Effect"": ""Allow"",
      ""Sid"": """"
    }
  ]
}
EOF
}

resource ""aws_iam_role_policy"" ""policy"" {
  name = ""Kendra-Policy""
  role = aws_iam_role.role.id

  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:GetObject"",
        ""s3:ListBucket""
      ],
      ""Resource"": ""*""
    }
  ]
}
EOF
}

resource ""aws_kendra_index"" ""example"" {
  name     = ""example""
  role_arn = aws_iam_role.role.arn

  document_metadata_configuration_updates {
    name = ""_authors""
    type = ""STRING_LIST_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = false
    }
    relevance {
      importance = 1
    }
  }

  document_metadata_configuration_updates {
    name = ""_category""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_created_at""
    type = ""DATE_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      freshness  = false
      importance = 1
      duration   = ""25920000s""
      rank_order = ""ASCENDING""
    }
  }

  document_metadata_configuration_updates {
    name = ""_data_source_id""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_document_title""
    type = ""STRING_VALUE""
    search {
      displayable = true
      facetable   = false
      searchable  = true
      sortable    = true
    }
    relevance {
      importance            = 2
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_excerpt_page_number""
    type = ""LONG_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = false
    }
    relevance {
      importance = 2
      rank_order = ""ASCENDING""
    }
  }

  document_metadata_configuration_updates {
    name = ""_faq_id""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_file_type""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_language_code""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_last_updated_at""
    type = ""DATE_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      freshness  = false
      importance = 1
      duration   = ""25920000s""
      rank_order = ""ASCENDING""
    }
  }

  document_metadata_configuration_updates {
    name = ""_source_uri""
    type = ""STRING_VALUE""
    search {
      displayable = true
      facetable   = false
      searchable  = false
      sortable    = false
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_tenant_id""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_version""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_view_count""
    type = ""LONG_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance = 1
      rank_order = ""ASCENDING""
    }
  }
}","Has one ""aws_iam_role"" resource and one ""aws_kendra_index"" resource with all default ""document_metadata_configuration_updates"" set","I'll help you create a Terraform configuration for an AWS Kendra index resource with default Document Metadata Configuration Updates. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra_index_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary permissions to the IAM role
resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra_index_policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""cloudwatch:PutMetricData"",
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = ""*""
      }
    ]
  })
}

# Create Kendra index with document metadata configuration
resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra index with default document metadata configuration""
  role_arn    = aws_iam_role.kendra_role.arn
  edition     = ""DEVELOPER_EDITION""

  document_metadata_configuration_updates {
    name = ""_created_at""
    type = ""DATE_VALUE""
    relevance {
      freshness = true
    }
    search {
      displayable = true
      facetable   = true
      searchable  = true
      sortable    = true
    }
  }

  document_metadata_configuration_updates {
    name = ""_last_updated_at""
    type = ""DATE_VALUE""
    relevance {
      freshness = true
    }
    search {
      displayable = true
      facetable   = true
      searchable  = true
      sortable    = true
    }
  }

  document_metadata_configuration_updates {
    name = ""_source_uri""
    type = ""STRING_VALUE""
    search {
      displayable = true
      facetable   = true
      searchable  = true
      sortable    = true
    }
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role with necessary permissions for Kendra
3. Creates a Kendra index with default document metadata configuration updates for:
   - `_created_at`: Document creation date
   - `_last_updated_at`: Document last update date
   - `_source_uri`: Document source URI

The index is set up with the DEVELOPER_EDITION and includes common search attributes (displayable, facetable, searchable, and sortable) for each metadata field. The configuration also includes relevance settings for date-based fields to enable freshness ranking.",,,,,,"I'll help you create a Terraform configuration for a basic AWS Kendra index resource with default Document Metadata Configuration Updates. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra_index_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary permissions to the IAM role
resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra_index_policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""cloudwatch:PutMetricData""
        ],
        Resource = ""*""
        Condition = {
          StringEquals = {
            ""cloudwatch:namespace"" = ""AWS/Kendra""
          }
        }
      }
    ]
  })
}

# Create Kendra index with document metadata configuration
resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra index with default document metadata configuration""
  role_arn    = aws_iam_role.kendra_role.arn
  edition     = ""DEVELOPER_EDITION""

  document_metadata_configuration_updates {
    name = ""_created_at""
    type = ""DATE_VALUE""
    relevance {
      freshness = true
    }
    search {
      displayable = true
      facetable   = true
      searchable  = true
      sortable    = true
    }
  }

  document_metadata_configuration_updates {
    name = ""_last_updated_at""
    type = ""DATE_VALUE""
    relevance {
      freshness = true
    }
    search {
      displayable = true
      facetable   = true
      searchable  = true
      sortable    = true
    }
  }
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates an IAM role that Kendra can assume
3. Attaches necessary permissions to the IAM role
4. Creates a Kendra index with:
   - Basic index configuration (name, description, edition)
   - Default document metadata configuration for '_created_at' and '_last_updated_at' fields
   - Search and relevance settings for the metadata fields

The document metadata configuration allows you to:
- Track document creation and update dates
- Use these dates for relevance scoring (freshness)
- Make these fields searchable, sortable, facetable, and displayable in search results

Note: You might want to adjust the IAM role permissions based on your specific needs, and you can add more document metadata configurations as required for your use case.",,,,,,"I'll help you create a basic AWS Kendra index resource with default document metadata configuration updates. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra-index-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Kendra role
resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra-index-policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""cloudwatch:PutMetricData""
        ]
        Resource = ""*""
        Condition = {
          StringEquals = {
            ""cloudwatch:namespace"" = ""AWS/Kendra""
          }
        }
      }
    ]
  })
}

# Kendra index resource with document metadata configuration
resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra index with default document metadata configuration""
  role_arn    = aws_iam_role.kendra_role.arn
  edition     = ""DEVELOPER_EDITION""

  document_metadata_configuration_updates {
    name = ""_created_at""
    type = ""DATE_VALUE""
    relevance {
      importance = ""HIGH""
    }
    search {
      displayable = true
      facetable   = true
      searchable  = true
      sortable    = true
    }
  }

  document_metadata_configuration_updates {
    name = ""_last_updated_at""
    type = ""DATE_VALUE""
    relevance {
      importance = ""HIGH""
    }
    search {
      displayable = true
      facetable   = true
      searchable  = true
      sortable    = true
    }
  }

  document_metadata_configuration_updates {
    name = ""_source_uri""
    type = ""STRING_VALUE""
    relevance {
      importance = ""HIGH""
    }
    search {
      displayable = true
      facetable   = true
      searchable  = true
      sortable    = true
    }
  }
}
```

This configuration creates:
1. A Terraform AWS provider configuration
2. An IAM role and policy for Kendra to use
3. A Kendra index with default document metadata configuration updates for:
   - `_created_at`: Document creation date
   - `_last_updated_at`: Document last update date
   - `_source_uri`: Document source URI

The metadata configurations are set with:
- HIGH importance for relevance
- Enabled search capabilities (displayable, facetable, searchable, and sortable)
- Appropriate data types (DATE_VALUE for dates, STRING_VALUE for URI)

This provides a foundation for a Kendra index with common metadata fields that can be used for document search and organization.",,,,,
"aws_kms_key, aws_s3_bucket, aws_s3_bucket_server_side_encryption_configuration",Create an AWS S3 bucket and apply KMS server side encryption that uses a defined KMS key resource.,"package aws_s3_bucket_sse
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some kms_key in input.configuration.root_module.resources
        kms_key.type == ""aws_kms_key""

        some sse in input.configuration.root_module.resources
        sse.type = ""aws_s3_bucket_server_side_encryption_configuration""
        bucket.address in sse.expressions.bucket.references
        some rule in sse.expressions.rule
        some rule_args in rule.apply_server_side_encryption_by_default
        kms_key.address in rule_args.kms_master_key_id.references
    rule_args.sse_algorithm.constant_value == ""aws:kms""
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some kms_key in input.configuration.root_module.resources
        kms_key.type == ""aws_kms_key""

        some sse in input.configuration.root_module.resources
        sse.type = ""aws_s3_bucket_server_side_encryption_configuration""
        bucket.expressions.bucket.constant_value == sse.expressions.bucket.constant_value
        some rule in sse.expressions.rule
        some rule_args in rule.apply_server_side_encryption_by_default
        kms_key.address in rule_args.kms_master_key_id.references
    rule_args.sse_algorithm.constant_value == ""aws:kms""
    
}",2,"resource ""aws_kms_key"" ""mykey"" {
  description             = ""This key is used to encrypt bucket objects""
  deletion_window_in_days = 10
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_server_side_encryption_configuration"" ""example"" {
  bucket = aws_s3_bucket.mybucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.mykey.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}","Has one resource ""aws_s3_bucket""

Has one resource ""aws_kms_key""

Has one resource ""aws_s3_bucket_server_side_encryption""
with bucket = bucket id OR name
with rule
    with apply_server_side_encryption_by_default
        with kms_master_key_id = kms key id
        and sse_algorithm = ""aws:kms""","I'll help you create a Terraform configuration for an S3 bucket with KMS server-side encryption using a defined KMS key resource. Based on the examples provided, here's a secure configuration:

```hcl
provider ""aws"" {
  region = ""us-west-1""
}

# Create KMS key for S3 bucket encryption
resource ""aws_kms_key"" ""s3_encryption_key"" {
  description             = ""KMS key for S3 bucket encryption""
  deletion_window_in_days = 10
  enable_key_rotation    = true
}

# Create KMS key alias for better identification
resource ""aws_kms_alias"" ""s3_encryption_key_alias"" {
  name          = ""alias/s3-encryption-key""
  target_key_id = aws_kms_key.s3_encryption_key.key_id
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""encrypted_bucket"" {
  bucket_prefix = ""encrypted-bucket-""
}

# Configure server-side encryption using the KMS key
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""bucket_encryption"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.s3_encryption_key.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}

# Add bucket ownership controls
resource ""aws_s3_bucket_ownership_controls"" ""bucket_ownership"" {
  bucket = aws_s3_bucket.encrypted_bucket.id
  
  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

# Configure bucket ACL
resource ""aws_s3_bucket_acl"" ""bucket_acl"" {
  depends_on = [aws_s3_bucket_ownership_controls.bucket_ownership]
  bucket     = aws_s3_bucket.encrypted_bucket.id
  acl        = ""private""
}

# Block public access
resource ""aws_s3_bucket_public_access_block"" ""bucket_public_access_block"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
```

This configuration:
1. Creates a KMS key specifically for S3 encryption with a 10-day deletion window and key rotation enabled
2. Creates an alias for the KMS key for easier reference
3. Creates an S3 bucket with a prefix
4. Configures server-side encryption using the created KMS key
5. Sets appropriate bucket ownership controls
6. Configures private ACL
7. Blocks all public access to the bucket

The configuration follows AWS security best practices by ensuring the bucket is private and encrypted using KMS. The KMS key is properly configured with rotation enabled and a suitable deletion window.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with KMS server-side encryption using a defined KMS key resource. Based on the provided examples, here's a secure implementation:

```hcl
provider ""aws"" {
  region = ""us-west-1""
}

# Create KMS key for S3 bucket encryption
resource ""aws_kms_key"" ""s3_encryption_key"" {
  description             = ""KMS key for S3 bucket encryption""
  deletion_window_in_days = 10
  enable_key_rotation    = true
}

# Create KMS key alias for better identification
resource ""aws_kms_alias"" ""s3_encryption_key_alias"" {
  name          = ""alias/s3-encryption-key""
  target_key_id = aws_kms_key.s3_encryption_key.key_id
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""encrypted_bucket"" {
  bucket_prefix = ""encrypted-bucket-""
}

# Configure server-side encryption using KMS
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""bucket_encryption"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.s3_encryption_key.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}

# Add bucket ownership controls
resource ""aws_s3_bucket_ownership_controls"" ""bucket_ownership"" {
  bucket = aws_s3_bucket.encrypted_bucket.id
  
  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

# Set bucket ACL to private
resource ""aws_s3_bucket_acl"" ""bucket_acl"" {
  depends_on = [aws_s3_bucket_ownership_controls.bucket_ownership]
  
  bucket = aws_s3_bucket.encrypted_bucket.id
  acl    = ""private""
}

# Block public access
resource ""aws_s3_bucket_public_access_block"" ""bucket_public_access_block"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
```

This configuration:
1. Creates a KMS key with a 10-day deletion window and enabled key rotation
2. Creates a friendly alias for the KMS key
3. Creates an S3 bucket with a prefix
4. Configures server-side encryption using the created KMS key
5. Sets appropriate bucket ownership controls
6. Sets the bucket ACL to private
7. Blocks all public access to the bucket

The configuration follows AWS security best practices by ensuring:
- The bucket is private by default
- All public access is blocked
- Server-side encryption is enabled using KMS
- Proper key management with rotation enabled
- Clear resource naming and identification",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with KMS server-side encryption using a defined KMS key resource. Based on the examples provided, here's a secure configuration:

```hcl
provider ""aws"" {
  region = ""us-west-1""
}

# Create KMS key for S3 bucket encryption
resource ""aws_kms_key"" ""s3_encryption_key"" {
  description             = ""KMS key for S3 bucket encryption""
  deletion_window_in_days = 10
  enable_key_rotation    = true
}

# Create KMS key alias for easier reference
resource ""aws_kms_alias"" ""s3_encryption_key_alias"" {
  name          = ""alias/s3-encryption-key""
  target_key_id = aws_kms_key.s3_encryption_key.key_id
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""encrypted_bucket"" {
  bucket_prefix = ""encrypted-bucket-""
}

# Configure server-side encryption for the S3 bucket using KMS
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""bucket_encryption"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.s3_encryption_key.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}

# Add bucket ownership controls
resource ""aws_s3_bucket_ownership_controls"" ""bucket_ownership"" {
  bucket = aws_s3_bucket.encrypted_bucket.id
  
  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

# Configure bucket ACL
resource ""aws_s3_bucket_acl"" ""bucket_acl"" {
  depends_on = [aws_s3_bucket_ownership_controls.bucket_ownership]
  bucket     = aws_s3_bucket.encrypted_bucket.id
  acl        = ""private""
}

# Block public access
resource ""aws_s3_bucket_public_access_block"" ""bucket_public_access_block"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
```

This configuration:
1. Creates a KMS key with a 10-day deletion window and enables key rotation
2. Creates a friendly alias for the KMS key
3. Creates an S3 bucket with a prefix
4. Configures server-side encryption using the KMS key
5. Sets up bucket ownership controls
6. Configures private ACL
7. Blocks all public access to the bucket

The configuration includes security best practices such as:
- Using KMS encryption instead of AES256
- Blocking public access
- Setting private ACL
- Enabling key rotation for the KMS key
- Using bucket prefix for unique naming",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_route53_record, aws_route53_zone, aws_s3_bucket, aws_security_group, aws_subnet, aws_subnet, aws_vpc, aws_ami","The infrastructure for a video streaming site that has multiple servers to balance load. Accomplish this using the following resources: AWS Load Balancing, AWS Load Balancing target group, AWS Route53 zone, AWS Route53 record, and AWS VPC module, AWS VPC subnet, AWS VPC security group, AWS EC2 instance. Add any necessary resources.","package load_balancer_middle

import rego.v1

default valid := false

instance_valid(instance, ami) if {
	instance.expressions.instance_type
	ami.address in instance.expressions.ami.references
}

lb_valid(lb, security_group, subnets) if {
	every subnet in subnets {
		subnet.address in lb.expressions.subnets.references
	}
	security_group.address in lb.expressions.security_groups.references
}

lb_listener_valid(lb_listener, lb, target_group) if {
	some default_action in lb_listener.expressions.default_action
	target_group.address in default_action.target_group_arn.references
	default_action.type.constant_value == ""forward""
	lb.address in lb_listener.expressions.load_balancer_arn.references
}

lb_target_group_valid(target_group, vpc) if {
	vpc.address in target_group.expressions.vpc_id.references
	target_group.expressions.port
	target_group.expressions.protocol
}

lb_target_group_attachment_valid(target_group_attachment, target_group, instance) if {
	target_group.address in target_group_attachment.expressions.target_group_arn.references
	instance.address in target_group_attachment.expressions.target_id.references
}

lb_resources_valid(lb, lb_listener, target_group, target_group_attachment, instance, vpc, subnets, security_group) if {
	lb_valid(lb, security_group, subnets)
	lb_listener_valid(lb_listener, lb, target_group)
	lb_target_group_valid(target_group, vpc)
	lb_target_group_attachment_valid(target_group_attachment, target_group, instance)
}

route53_record_valid(record, zone, lb) if {
	zone.address in record.expressions.zone_id.references
	record.expressions.name
	record.expressions.type
	some alias in record.expressions.alias
	lb.address in alias.zone_id.references
	lb.address in alias.name.references
	alias.evaluate_target_health
}

route53_records_valid(records, zone, lb) if {
	some record_ipv4 in records
	route53_record_valid(record_ipv4, zone, lb)
}

route53_zone_valid(zone) if {
	zone.expressions.name
}

security_group_valid(security_group, vpc) if {
	vpc.address in security_group.expressions.vpc_id.references
}

subnet_valid(subnet, vpc) if {
	vpc.address in subnet.expressions.vpc_id.references
	subnet.expressions.cidr_block
}

subnets_valid(subnets, vpc) if {
	every subnet in subnets {
		subnet_valid(subnet, vpc)
	}
}

vpc_valid(vpc) if {
	vpc.expressions.cidr_block
}

valid if {
	resources := input.configuration.root_module.resources

	# ec2
	some instance in resources
	instance.type == ""aws_instance""
	some ami in resources
	ami.type == ""aws_ami""

	some vpc in resources
	vpc.type == ""aws_vpc""

	subnets := [subnet | subnet := resources[_]; subnet.type == ""aws_subnet""]
	count(subnets) > 1
	some security_group in resources
	security_group.type == ""aws_security_group""

	# lb resources
	some lb in resources
	lb.type == ""aws_lb""
	some lb_listener in resources
	lb_listener.type == ""aws_lb_listener""
	some target_group in resources
	target_group.type == ""aws_lb_target_group""
	some target_group_attachment in resources
	target_group_attachment.type == ""aws_lb_target_group_attachment""

	# route53
	records := [record | record := resources[_]; record.type == ""aws_route53_record""]
	some zone in resources
	zone.type == ""aws_route53_zone""

	# s3
	some s3_bucket in resources
	s3_bucket.type == ""aws_s3_bucket""

	instance_valid(instance, ami)
	lb_resources_valid(lb, lb_listener, target_group, target_group_attachment, instance, vpc, subnets, security_group)
	route53_records_valid(records, zone, lb)
	route53_zone_valid(zone)

	security_group_valid(security_group, vpc)

	subnets_valid(subnets, vpc)
	vpc_valid(vpc)
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_launch_configuration"" ""launch-config"" {
  name_prefix     = ""aws-asg-launch-config-""
  image_id        = data.aws_ami.ubuntu.id
  instance_type   = ""t2.micro""
  # user_data       = file(""user-data.sh"")  # load your script if needed
  security_groups = [aws_security_group.instance-sg.id]

  lifecycle {
    create_before_destroy = true
  }
}

resource ""aws_autoscaling_group"" ""asg"" {
  name                 = ""asg""
  min_size             = 1
  max_size             = 3
  desired_capacity     = 1
  launch_configuration = aws_launch_configuration.launch-config.name
  vpc_zone_identifier  = module.vpc.public_subnets

  lifecycle { 
    ignore_changes = [desired_capacity, target_group_arns]
  }

  health_check_type    = ""ELB""
}

resource ""aws_autoscaling_policy"" ""scale-down"" {
  name                   = ""scale-down""
  autoscaling_group_name = aws_autoscaling_group.asg.name
  adjustment_type        = ""ChangeInCapacity""
  scaling_adjustment     = -1
  cooldown               = 120
}

resource ""aws_cloudwatch_metric_alarm"" ""scale-down"" {
  alarm_description   = ""Monitors CPU utilization for Terramino ASG""
  alarm_actions       = [aws_autoscaling_policy.scale-down.arn]
  alarm_name          = ""scale-down""
  comparison_operator = ""LessThanOrEqualToThreshold""
  namespace           = ""AWS/EC2""
  metric_name         = ""CPUUtilization""
  threshold           = ""10""
  evaluation_periods  = ""2""
  period              = ""120""
  statistic           = ""Average""

  dimensions = {
    AutoScalingGroupName = aws_autoscaling_group.asg.name
  }
}

resource ""aws_autoscaling_policy"" ""scale-up"" {
  name                   = ""scale-up""
  autoscaling_group_name = aws_autoscaling_group.asg.name
  adjustment_type        = ""ChangeInCapacity""
  scaling_adjustment     = 1
  cooldown               = 120
}

resource ""aws_cloudwatch_metric_alarm"" ""scale-up"" {
  alarm_description   = ""Monitors CPU utilization for Terramino ASG""
  alarm_actions       = [aws_autoscaling_policy.scale-up.arn]
  alarm_name          = ""scale-up""
  comparison_operator = ""GreaterThanOrEqualToThreshold""
  namespace           = ""AWS/EC2""
  metric_name         = ""CPUUtilization""
  threshold           = ""80""
  evaluation_periods  = ""2""
  period              = ""120""
  statistic           = ""Average""

  dimensions = {
    AutoScalingGroupName = aws_autoscaling_group.asg.name
  }
}

resource ""aws_lb"" ""lb"" {
  name               = ""my-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
  subnets            = module.vpc.public_subnets
}

resource ""aws_lb_listener"" ""lb-listener"" {
  load_balancer_arn = aws_lb.lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target-group.arn
  }
}

resource ""aws_lb_target_group"" ""target-group"" {
  name     = ""my-lb-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_autoscaling_attachment"" ""as-attachment"" {
  autoscaling_group_name = aws_autoscaling_group.asg.id
  lb_target_group_arn   = aws_lb_target_group.target-group.arn
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_route53_zone"" ""test-video-stream"" {
  name = ""test-video-stream.com""
}

resource ""aws_route53_record"" ""lb_ipv4"" {
  type    = ""A""
  name    = ""lb""
  zone_id = aws_route53_zone.test-video-stream.zone_id

  alias {
    name                   = aws_lb.lb.dns_name
    zone_id                = aws_lb.lb.zone_id
    evaluate_target_health = true
  }
}

output ""lb_endpoint"" {
  value = ""http://${aws_lb.lb.dns_name}""
}

output ""application_endpoint"" {
  value = ""http://${aws_lb.lb.dns_name}/index.php""
}

output ""asg_name"" {
  value = aws_autoscaling_group.asg.name
}
","have one ""aws_s3_bucket"" resource
have one ""aws_lb"" resource
  with ""subnets"" argument linking to aws_subnet resources
  with ""security_groups"" argument linking aws_security_group resource
have one ""aws_lb_listener"" resource
  with ""load_balancer_arn"" argument linking to the aws_lb resource
  with ""default_action"" argument block
    with ""type"" argument with value ""forward""
    with ""target_group_arn"" argument linking to aws_lb_target_group resource
with one ""aws_lb_target_group"" resource
  with ""vpc_id"" argument linking to the aws_vpc resource
with one ""aws_lb_target_group_attachment"" resource
  with ""target_group_arn"" argument linking to the aws_lb_target_group resource
  with ""target_id"" argument linking to the aws_instance resource
have one ""aws_ami"" data resource
have one ""aws_instance"" resource
  with ""instance_type"" argument
  with ""ami"" argument linking to the aws_ami data resource
have one ""aws_vpc"" resource
  with ""cidr_block"" argument
have two ""aws_subnet"" resources
  with ""vpc_id"" argument linking to the aws_vpc resource
  with ""cidr_block"" argument
have one ""aws_security_group"" resource
  with ""vpc_id"" argument linking to the aws_vpc resource
have one ""aws_route53_zone"" resource
  with ""name"" argument
have one ""aws_route53_record"" resources
  with ""type"" argument
  with ""name"" argument
  with ""zone_id"" argument linking to the aws_route53_zone resource
  with ""alias"" argument block
    with ""name"" argument linking to the aws_lb dns_name
    with ""zone_id"" argument linking to aws_lb zone_id
    with ""evaluate_target_health"" argument","Based on the provided examples and requirements, I'll help create a Terraform configuration for a video streaming site with load balancing. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# VPC Module
module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""streaming-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.available.names
  public_subnets  = [""10.0.1.0/24"", ""10.0.2.0/24""]
  private_subnets = [""10.0.3.0/24"", ""10.0.4.0/24""]

  enable_nat_gateway = true
}

# EC2 AMI data source
data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

# Security Groups
resource ""aws_security_group"" ""instance_sg"" {
  name_prefix = ""instance-sg""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port       = 80
    to_port         = 80
    protocol        = ""tcp""
    security_groups = [aws_security_group.alb_sg.id]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

resource ""aws_security_group"" ""alb_sg"" {
  name_prefix = ""alb-sg""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# EC2 Instances
resource ""aws_instance"" ""web_servers"" {
  count = 2

  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.private_subnets[count.index]

  vpc_security_group_ids = [aws_security_group.instance_sg.id]

  tags = {
    Name = ""web-server-${count.index + 1}""
  }
}

# Application Load Balancer
resource ""aws_lb"" ""alb"" {
  name               = ""streaming-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = module.vpc.public_subnets
}

# ALB Target Group
resource ""aws_lb_target_group"" ""web_tg"" {
  name     = ""web-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    path                = ""/""
    healthy_threshold   = 2
    unhealthy_threshold = 10
  }
}

# Target Group Attachment
resource ""aws_lb_target_group_attachment"" ""web_tg_attachment"" {
  count            = length(aws_instance.web_servers)
  target_group_arn = aws",,,,,,"Based on the examples provided and the current question, I'll help create a Terraform configuration that sets up a video streaming infrastructure with load balancing and DNS management. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# VPC Module
module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""streaming-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.available.names
  public_subnets  = [""10.0.1.0/24"", ""10.0.2.0/24""]
  private_subnets = [""10.0.3.0/24"", ""10.0.4.0/24""]

  enable_nat_gateway = true
  enable_vpn_gateway = false
}

# Get latest Ubuntu AMI
data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

# Security Groups
resource ""aws_security_group"" ""web_server"" {
  name        = ""web-server-sg""
  description = ""Security group for web servers""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port       = 80
    to_port         = 80
    protocol        = ""tcp""
    security_groups = [aws_security_group.alb.id]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

resource ""aws_security_group"" ""alb"" {
  name        = ""alb-sg""
  description = ""Security group for Application Load Balancer""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# EC2 Instances
resource ""aws_instance"" ""web_servers"" {
  count = 2

  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.private_subnets[count.index]

  vpc_security_group_ids = [aws_security_group.web_server.id]

  tags = {
    Name = ""web-server-${count.index + 1}""
  }
}

# Application Load Balancer
resource ""aws_lb"" ""web_alb"" {
  name               = ""streaming-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb.id]
  subnets            = module.vpc.public_subnets
}

# ALB Target Group
resource ""aws_lb_target_group"" ""web_tg"" {
  name     = ""streaming-tg""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    ",,,,,,"Based on the provided examples and the current question, I'll help create a Terraform configuration that sets up a video streaming infrastructure with load balancing, DNS management, and proper networking. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# VPC and Networking
module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""streaming-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.available.names
  public_subnets  = [""10.0.1.0/24"", ""10.0.2.0/24""]
  private_subnets = [""10.0.3.0/24"", ""10.0.4.0/24""]

  enable_nat_gateway = true
  single_nat_gateway = true
}

# EC2 AMI
data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

# Security Groups
resource ""aws_security_group"" ""instance_sg"" {
  name        = ""instance-sg""
  description = ""Security group for streaming instances""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port       = 80
    to_port         = 80
    protocol        = ""tcp""
    security_groups = [aws_security_group.alb_sg.id]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-sg""
  description = ""Security group for ALB""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# EC2 Instances
resource ""aws_instance"" ""streaming_servers"" {
  count = 2

  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.private_subnets[count.index % length(module.vpc.private_subnets)]

  vpc_security_group_ids = [aws_security_group.instance_sg.id]

  tags = {
    Name = ""streaming-server-${count.index + 1}""
  }
}

# Application Load Balancer
resource ""aws_lb"" ""streaming_alb"" {
  name               = ""streaming-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets           = module.vpc.public_subnets
}

# ALB Target Group
resource ""aws_lb_target_group"" ""streaming_tg"" {
  name     = ""streaming-tg""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    timeout             = 5
    ",,,,,
aws_ami,Create an AWS aws_ami resource for the latest Amazon Linux 2 AMI,"package terraform

default ami_latest_amazon_linux_2 = false

ami_latest_amazon_linux_2 {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_ami""
    resource.name == ""latest_amazon_linux_2""
    resource.expressions.filter[_].name.constant_value == ""name""
    resource.expressions.filter[_].values.constant_value[_] == ""amzn2-ami-hvm-*-x86_64-gp2""
    resource.expressions.most_recent.constant_value == true
    resource.expressions.owners.constant_value[_] == ""amazon""
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }
}",create aws_ami resource with name = any,"Based on the provided examples and your question, I'll help you create a data source to get the latest Amazon Linux 2 AMI. Here's the Terraform configuration:

```hcl
data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }

  filter {
    name   = ""virtualization-type""
    values = [""hvm""]
  }

  filter {
    name   = ""root-device-type""
    values = [""ebs""]
  }
}
```

This configuration:
1. Uses a data source (`data ""aws_ami""`) to query for AMIs
2. Sets `most_recent = true` to get the latest version
3. Specifies the owner as ""amazon"" to ensure we're getting official Amazon Linux 2 AMIs
4. Uses filters to:
   - Match the Amazon Linux 2 AMI name pattern
   - Ensure HVM virtualization
   - Ensure EBS-backed instances

You can reference this AMI in other resources using: `data.aws_ami.latest_amazon_linux_2.id`",,,,,,"Based on the examples provided and your request, I'll help you create a data source for the latest Amazon Linux 2 AMI. Note that we use a data source (data ""aws_ami"") rather than a resource because we're looking up an existing AMI rather than creating a new one.

Here's the Terraform configuration:

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }

  filter {
    name   = ""virtualization-type""
    values = [""hvm""]
  }

  filter {
    name   = ""root-device-type""
    values = [""ebs""]
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a data source to fetch the latest Amazon Linux 2 AMI
3. Uses filters to ensure we get:
   - The most recent version of the AMI
   - Only Amazon Linux 2 AMIs (through the name pattern)
   - HVM virtualization type
   - EBS-backed instances

You can reference this AMI in other resources using: `data.aws_ami.latest_amazon_linux_2.id`",,,,,,"Based on the provided examples and the current question, I'll help you create a Terraform configuration for getting the latest Amazon Linux 2 AMI. Note that this will be a data source rather than a resource since we're querying an existing AMI rather than creating one.

```hcl
data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }

  filter {
    name   = ""virtualization-type""
    values = [""hvm""]
  }

  filter {
    name   = ""root-device-type""
    values = [""ebs""]
  }
}
```

This configuration:
1. Uses a data source (`data ""aws_ami""`) to query for the AMI
2. Sets `most_recent = true` to get the latest version
3. Specifies the owner as ""amazon"" to ensure we're getting official Amazon AMIs
4. Uses filters to ensure we get:
   - Amazon Linux 2 AMI (via the name pattern)
   - HVM virtualization type
   - EBS-backed instance

You can reference this AMI in other resources using `data.aws_ami.amazon_linux_2.id`.",,,,,
aws_route53_zone,"Set up an aws_route_53 zone named ""example53.com""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false


# Validate aws_route53_zone resource
is_valid_r53_zone {
	some i
	resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
}
",1,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""","I'll help you create a Terraform configuration for setting up an AWS Route 53 zone named ""example53.com"". Based on the examples provided, here's the appropriate configuration:

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Route 53 hosted zone with the domain name ""example53.com""
3. Uses the `aws_route53_zone` resource type which is the correct resource for creating a DNS zone in Route 53

The configuration is minimal but complete for the specific requirement. You can add additional optional parameters like tags or comments if needed, but this will create a functional Route 53 zone.",,,,,,"I'll help you create a Terraform configuration for setting up an AWS Route 53 zone named ""example53.com"". Based on the examples provided, I'll create a simple and clean configuration.

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Route 53 hosted zone with the domain name ""example53.com""

The resource will generate a zone ID that can be referenced later if you need to add records to this zone. You can reference it using `aws_route53_zone.main.zone_id`.

This is a public hosted zone by default. If you need to make it a private hosted zone, you would need to add VPC configuration, but based on your current requirement, this configuration provides exactly what you've asked for.",,,,,,"I'll help you create a Terraform configuration for setting up an AWS Route 53 zone named ""example53.com"". Based on the examples provided, here's the appropriate configuration:

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}
```

This configuration:
1. Sets up the AWS provider (using us-east-1 as the default region)
2. Creates a Route 53 hosted zone with the domain name ""example53.com""

The resource will create a public hosted zone by default, which is suitable for domains that need to be accessible from the public internet. The zone will be assigned a unique zone ID that can be referenced when creating records in this zone.

If you need to reference this zone elsewhere in your Terraform configuration, you can use the reference `aws_route53_zone.example53.zone_id` to access the zone ID.",,,,,
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket where the bucket owner pays for fees.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""
        
        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""BucketOwner""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name
with payer = ""BucketOwner""","I'll help you create a Terraform configuration for an S3 bucket where the bucket owner pays for the fees. Based on Example 1 from the provided examples, I'll create a solution that includes both the S3 bucket and the request payment configuration.

```hcl
resource ""aws_s3_bucket"" ""my_bucket"" {
  bucket = ""my-owner-paid-bucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""my_bucket_payment"" {
  bucket = aws_s3_bucket.my_bucket.id
  payer  = ""BucketOwner""  # This ensures the bucket owner pays for fees
}
```

This configuration:
1. Creates an S3 bucket named ""my-owner-paid-bucket""
2. Sets up a request payment configuration that specifies the bucket owner as the payer for all request fees
3. Uses the bucket reference to ensure the payment configuration is associated with the correct bucket

The `payer = ""BucketOwner""` setting ensures that the bucket owner will pay for all fees associated with requests made to the bucket, rather than the requester.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket where the bucket owner pays for fees. Based on Example 1 from the reference documentation, I'll create a solution that includes both the S3 bucket and the request payment configuration.

```hcl
resource ""aws_s3_bucket"" ""my_bucket"" {
  bucket = ""my-bucket-name""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""my_bucket_payment"" {
  bucket = aws_s3_bucket.my_bucket.id
  payer  = ""BucketOwner""  # Specifies that the bucket owner pays for fees
}
```

This configuration:
1. Creates an S3 bucket with the name ""my-bucket-name""
2. Sets up a request payment configuration where the bucket owner pays for all request fees
3. Uses proper resource referencing with `aws_s3_bucket.my_bucket.id`

Note: Remember to replace ""my-bucket-name"" with your desired bucket name, keeping in mind that S3 bucket names must be globally unique across all AWS accounts.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket where the bucket owner pays for fees. Based on Example 1 from the provided examples, I'll create a configuration that includes both the S3 bucket and the request payment configuration.

```hcl
resource ""aws_s3_bucket"" ""my_bucket"" {
  bucket = ""my-secure-bucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""my_bucket_payment"" {
  bucket = aws_s3_bucket.my_bucket.id
  payer  = ""BucketOwner""
}
```

This configuration:
1. Creates an S3 bucket with the name ""my-secure-bucket""
2. Sets up a request payment configuration where the bucket owner pays for all request fees
3. Uses proper resource referencing with `aws_s3_bucket.my_bucket.id`

The `payer` is set to ""BucketOwner"", which means the owner of the bucket will pay for all requests, including those made by other AWS accounts.",,,,,
"aws_eks_cluster, aws_eks_fargate_profile, aws_iam_role, aws_iam_role, aws_iam_role_policy_attachment, aws_subnet, aws_subnet, aws_vpc, null_resource, aws_eks_cluster_auth","Create an IAM role for EKS Fargate profiles with the necessary policies for pod execution. Attach the AmazonEKSFargatePodExecutionRolePolicy to the IAM role. Define an AWS EKS Fargate profile for the 'kube-system' namespace, associating it with the EKS cluster and specifying the pod execution role ARN. Ensure that the specified subnets have the required resource tag for cluster association. Use a null_resource to patch the Kubernetes deployment 'coredns' in the 'kube-system' namespace, using the EKS cluster's endpoint, certificate authority, and authentication token for communication.","package terraform.validation

default has_aws_iam_role_fargate = false
default has_aws_iam_role_policy_attachment_fargate = false
default has_aws_eks_fargate_profile = false
default has_aws_eks_cluster_auth = false
default has_null_resource_k8s_patcher = false

has_aws_iam_role_fargate {
    role := input.planned_values.root_module.resources[_]
    role.type == ""aws_iam_role""
    role.name == ""eks-fargate-profile""
    role.values.name == ""eks-fargate-profile""
    role.values.assume_role_policy != null
}

has_aws_iam_role_policy_attachment_fargate {
    attachment := input.planned_values.root_module.resources[_]
    attachment.type == ""aws_iam_role_policy_attachment""
    attachment.name == ""eks-fargate-profile""
    attachment.values.policy_arn == ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
    attachment.values.role == input.planned_values.root_module.resources[_].values.name  # Ensure role is correctly referenced
}

has_aws_eks_fargate_profile {
    fargate_profile := input.configuration.root_module.resources[_]
    fargate_profile.type == ""aws_eks_fargate_profile""
    fargate_profile.name == ""kube-system""
    fargate_profile.expressions.cluster_name != null
    fargate_profile.expressions.fargate_profile_name.constant_value == ""kube-system""
    fargate_profile.expressions.pod_execution_role_arn != null
    count(fargate_profile.expressions.subnet_ids.references) == 4  # Ensure there are two subnet IDs
    fargate_profile.expressions.selector[_].namespace.constant_value == ""kube-system""
}

has_aws_eks_cluster_auth {
    cluster_auth := input.configuration.root_module.resources[_]
    cluster_auth.type == ""aws_eks_cluster_auth""
    cluster_auth.name != null  # Check for proper referencing
}

has_null_resource_k8s_patcher {
    k8s_patcher := input.configuration.root_module.resources[_]
    k8s_patcher.type == ""null_resource""
    k8s_patcher.name == ""k8s_patcher""
    k8s_patcher.depends_on != null
    count(k8s_patcher.expressions.triggers.references) == 8  # Check for three triggers
    k8s_patcher.provisioners[0].type == ""local-exec""
    k8s_patcher.provisioners[0].expressions.command != null
}

valid_configuration {
    has_aws_iam_role_fargate
    has_aws_iam_role_policy_attachment_fargate
    has_aws_eks_fargate_profile
    has_aws_eks_cluster_auth
    has_null_resource_k8s_patcher
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

resource ""aws_subnet"" ""test_eks_1"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  availability_zone = data.aws_availability_zones.available.names[0]

  tags = {
    ""kubernetes.io/cluster/test"" = ""shared""
  }
}

resource ""aws_subnet"" ""test_eks_2"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""

  availability_zone = data.aws_availability_zones.available.names[1]

  tags = {
    ""kubernetes.io/cluster/test"" = ""shared""
  }
}

resource ""aws_iam_role"" ""eks-cluster"" {
  name = ""eks-cluster-1""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""eks.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.eks-cluster.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.eks-cluster.name
}


resource ""aws_eks_cluster"" ""cluster"" {
  name     = ""test""
  role_arn = aws_iam_role.eks-cluster.arn

  vpc_config {
    subnet_ids = [
      aws_subnet.test_eks_1.id,
      aws_subnet.test_eks_2.id
    ]
  }

  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy,
    aws_iam_role_policy_attachment.eks_service_policy,
  ]
}

resource ""aws_iam_role"" ""eks-fargate-profile"" {
  name = ""eks-fargate-profile""

  assume_role_policy = jsonencode({
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""eks-fargate-pods.amazonaws.com""
      }
    }]
    Version = ""2012-10-17""
  })
}

resource ""aws_iam_role_policy_attachment"" ""eks-fargate-profile"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
  role       = aws_iam_role.eks-fargate-profile.name
}

resource ""aws_eks_fargate_profile"" ""kube-system"" {
  cluster_name           = aws_eks_cluster.cluster.name
  fargate_profile_name   = ""kube-system""
  pod_execution_role_arn = aws_iam_role.eks-fargate-profile.arn

  subnet_ids = [
    aws_subnet.test_eks_1.id,
    aws_subnet.test_eks_2.id
  ]

  selector {
    namespace = ""kube-system""
  }

  depends_on = [ aws_iam_role_policy_attachment.eks-fargate-profile ]
}

data ""aws_eks_cluster_auth"" ""eks"" {
  name = aws_eks_cluster.cluster.id
}

resource ""null_resource"" ""k8s_patcher"" {
  depends_on = [aws_eks_fargate_profile.kube-system]

  triggers = {
    endpoint = aws_eks_cluster.cluster.endpoint
    ca_crt   = base64decode(aws_eks_cluster.cluster.certificate_authority[0].data)
    token    = data.aws_eks_cluster_auth.eks.token
  }

  provisioner ""local-exec"" {
    command = <<EOH
      cat >/tmp/ca.crt <<EOF
      ${base64decode(aws_eks_cluster.cluster.certificate_authority[0].data)}
      EOF
      kubectl \
        --server=""${aws_eks_cluster.cluster.endpoint}"" \
        --certificate_authority=/tmp/ca.crt \
        --token=""${data.aws_eks_cluster_auth.eks.token}"" \
        patch deployment coredns \
        -n kube-system --type json \
        -p='[{""op"": ""remove"", ""path"": ""/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type""}]'
      EOH
  }

  lifecycle {
    ignore_changes = [triggers]
  }
}","Resource ""aws_iam_role"" ""eks-fargate-profile""
has one ""name""
has one ""assume_role_policy""
Resource ""aws_iam_role_policy_attachment"" ""eks-fargate-profile""
has one ""policy_arn""
has one ""role""
Resource ""aws_eks_fargate_profile"" ""kube-system""
has one ""cluster_name""
has one ""fargate_profile_name""
has one ""pod_execution_role_arn""
has one ""subnet_ids""
has one ""namespace"" in ""selector""
Data ""aws_eks_cluster_auth"" ""eks""
has one ""name""
Resource ""null_resource"" ""k8s_patcher""
has multiple ""triggers"" (endpoint, ca_crt, token)
uses a ""local-exec"" provisioner to execute a command that patches the Kubernetes deployment
has a ""lifecycle"" policy to ignore changes in ""triggers""","Based on the examples and requirements, I'll help create a Terraform configuration for an EKS Fargate profile with the necessary IAM roles and CoreDNS configuration. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
    null = {
      source  = ""hashicorp/null""
      version = ""~> 3.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM Role for Fargate Profile
resource ""aws_iam_role"" ""fargate_pod_execution_role"" {
  name = ""eks-fargate-pod-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""eks-fargate-pods.amazonaws.com""
      }
    }]
  })
}

# Attach the required policy for Fargate pod execution
resource ""aws_iam_role_policy_attachment"" ""fargate_pod_execution_role_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
  role       = aws_iam_role.fargate_pod_execution_role.name
}

# Subnets with required tags for EKS
resource ""aws_subnet"" ""fargate_subnet_1"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name                                = ""eks-fargate-subnet-1""
    ""kubernetes.io/cluster/eks-cluster"" = ""shared""
  }
}

resource ""aws_subnet"" ""fargate_subnet_2"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""

  tags = {
    Name                                = ""eks-fargate-subnet-2""
    ""kubernetes.io/cluster/eks-cluster"" = ""shared""
  }
}

# Fargate Profile for kube-system namespace
resource ""aws_eks_fargate_profile"" ""kube_system"" {
  cluster_name           = aws_eks_cluster.example.name
  fargate_profile_name   = ""kube-system""
  pod_execution_role_arn = aws_iam_role.fargate_pod_execution_role.arn
  subnet_ids             = [aws_subnet.fargate_subnet_1.id, aws_subnet.fargate_subnet_2.id]

  selector {
    namespace = ""kube-system""
  }

  depends_on = [aws_iam_role_policy_attachment.fargate_pod_execution_role_policy]
}

# Data source to get EKS cluster auth token
data ""aws_eks_cluster_auth"" ""cluster"" {
  name = aws_eks_cluster.example.name
}

# Null resource to patch CoreDNS
resource ""null_resource"" ""patch_coredns"" {
  depends_on = [aws_eks_fargate_profile.kube_system]

  provisioner ""local-exec"" {
    command = <<-EOT
      aws eks get-token --cluster-name ${aws_eks_cluster.example.name} | kubectl apply -f - &&
      kubectl patch deployment coredns \
        -n kube-system \
        --type json \
        -p='[{""op"": ""remove"", ""path"": ""/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type""},{""op"": ""replace"", ""path"": ""/spec/template/spec/containers/0/image"", ""value"": ""602401143452.dkr.ecr.${data.aws_region.current.name}.amazonaws.com/eks/coredns:v1.8.7-eksbuild.3""}]'
    EOT

    environment = {
      ",,,,,,"Based on the provided examples and the current question, I'll help create a Terraform configuration that sets up an EKS Fargate profile with the necessary IAM roles and CoreDNS patch. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
    null = {
      source  = ""hashicorp/null""
      version = ""~> 3.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM Role for Fargate Profile
resource ""aws_iam_role"" ""fargate_pod_execution_role"" {
  name = ""eks-fargate-pod-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""eks-fargate-pods.amazonaws.com""
      }
    }]
  })
}

# Attach the required policy for Fargate pod execution
resource ""aws_iam_role_policy_attachment"" ""fargate_pod_execution_role_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
  role       = aws_iam_role.fargate_pod_execution_role.name
}

# Subnet tags for Fargate
resource ""aws_ec2_tag"" ""subnet_tags"" {
  for_each    = toset([aws_subnet.example1.id, aws_subnet.example2.id])
  resource_id = each.value
  key         = ""kubernetes.io/cluster/${aws_eks_cluster.example.name}""
  value       = ""shared""
}

# Fargate Profile for kube-system
resource ""aws_eks_fargate_profile"" ""kube_system"" {
  cluster_name           = aws_eks_cluster.example.name
  fargate_profile_name   = ""kube-system""
  pod_execution_role_arn = aws_iam_role.fargate_pod_execution_role.arn
  subnet_ids             = [aws_subnet.example1.id, aws_subnet.example2.id]

  selector {
    namespace = ""kube-system""
  }

  depends_on = [
    aws_iam_role_policy_attachment.fargate_pod_execution_role_policy,
    aws_ec2_tag.subnet_tags
  ]
}

# Data source to get EKS cluster auth token
data ""aws_eks_cluster_auth"" ""cluster"" {
  name = aws_eks_cluster.example.name
}

# Null resource to patch CoreDNS
resource ""null_resource"" ""patch_coredns"" {
  depends_on = [aws_eks_fargate_profile.kube_system]

  provisioner ""local-exec"" {
    command = <<-EOT
      cat <<EOF | kubectl --server=""${aws_eks_cluster.example.endpoint}"" \
        --certificate-authority=<(echo ""${aws_eks_cluster.example.certificate_authority[0].data}"" | base64 --decode) \
        --token=""${data.aws_eks_cluster_auth.cluster.token}"" \
        apply -f -
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: coredns
        namespace: kube-system
      spec:
        template:
          metadata:
            annotations:
              eks.amazonaws.com/compute-type: fargate
    EOF
    EOT
  }

  triggers = {
    cluster_endpoint = aws_eks_cluster.example.endpoint
    profile_arn      = aws_eks_fargate_profile.kube_system.arn
  }
}
```

This configuration:

1. Creates an IAM role specifically for Fargate pod execution with the necessary assume role policy
2. Attaches the AmazonEKSFargatePodExecutionRolePolicy to the IAM role
3",,,,,,"I'll help you create a Terraform configuration for an EKS Fargate profile with the required IAM roles and CoreDNS patch. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
    null = {
      source  = ""hashicorp/null""
      version = ""~> 3.0""
    }
  }
}

# IAM Role for Fargate Profile
resource ""aws_iam_role"" ""fargate_pod_execution_role"" {
  name = ""eks-fargate-pod-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""eks-fargate-pods.amazonaws.com""
      }
    }]
  })
}

# Attach the required policy for Fargate pod execution
resource ""aws_iam_role_policy_attachment"" ""fargate_pod_execution_role_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
  role       = aws_iam_role.fargate_pod_execution_role.name
}

# EKS Fargate Profile for kube-system
resource ""aws_eks_fargate_profile"" ""kube_system"" {
  cluster_name           = aws_eks_cluster.example.name
  fargate_profile_name   = ""kube-system""
  pod_execution_role_arn = aws_iam_role.fargate_pod_execution_role.arn
  subnet_ids             = aws_subnet.private[*].id

  selector {
    namespace = ""kube-system""
  }

  depends_on = [aws_iam_role_policy_attachment.fargate_pod_execution_role_policy]
}

# Tag private subnets for EKS
resource ""aws_ec2_tag"" ""private_subnet_tags"" {
  count       = length(aws_subnet.private)
  resource_id = aws_subnet.private[count.index].id
  key         = ""kubernetes.io/cluster/${aws_eks_cluster.example.name}""
  value       = ""shared""
}

# Data source to get EKS cluster auth token
data ""aws_eks_cluster_auth"" ""cluster"" {
  name = aws_eks_cluster.example.name
}

# Null resource to patch CoreDNS
resource ""null_resource"" ""patch_coredns"" {
  depends_on = [aws_eks_fargate_profile.kube_system]

  provisioner ""local-exec"" {
    interpreter = [""/bin/bash"", ""-c""]
    command     = <<-EOT
      # Wait for CoreDNS pods to be created
      sleep 30
      
      # Update kubectl config
      aws eks update-kubeconfig --name ${aws_eks_cluster.example.name} --region ${data.aws_region.current.name}
      
      # Patch CoreDNS deployment
      kubectl patch deployment coredns \
        -n kube-system \
        --type json \
        -p='[{""op"": ""remove"", ""path"": ""/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type""},{""op"": ""replace"", ""path"": ""/spec/template/spec/affinity"", ""value"": {""nodeAffinity"": {""requiredDuringSchedulingIgnoredDuringExecution"": {""nodeSelectorTerms"": [{""matchExpressions"": [{""key"": ""eks.amazonaws.com/compute-type"", ""operator"": ""In"", ""values"": [""fargate""]}]}]}}}}]'
    EOT

    environment = {
      KUBECONFIG = ""~/.kube/config""
    }
  }
}

# Configure kubectl provider
provider ""kubernetes"" {
  host                   = aws_eks_cluster.example.endpoint
  cluster_ca_certificate = base64decode(aws_eks_cluster.example.certificate",,,,,
"aws_iam_role, aws_lambda_function, archive_file, aws_iam_policy_document","Create an AWS Lambda function. The function should be triggered by a nodejs18 runtime,use a ""lambda.js"" as the source code and a ""index.js"" as the handler. Please declare all necessary resources.","package main

import future.keywords.in

default allow = false

# Check for IAM Role for Lambda
aws_iam_role_for_lambda_valid(resources) {
    some resource in resources
    resource.type == ""aws_iam_role""
}

# Check for AWS Lambda Function with specific configurations
aws_lambda_function_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_function""
    resource.change.after.runtime == ""nodejs18.x""
    resource.change.after.handler == ""index.test""
    resource.change.after.filename == ""lambda_function_payload.zip""
}

# Check for Archive File for Lambda code
archive_file_for_lambda_valid(resources) {
    some resource in resources
    resource.type == ""archive_file""
    resource.values.source_file == ""lambda.js""
    resource.values.type == ""zip""
}

# Aggregate all checks
allow {
    aws_iam_role_for_lambda_valid(input.resource_changes)
    aws_lambda_function_valid(input.resource_changes)
    archive_file_for_lambda_valid(input.prior_state.values.root_module.resources)
}
",3,"data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""lambda.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""iam_for_lambda"" {
  name               = ""iam_for_lambda""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

data ""archive_file"" ""lambda"" {
  type        = ""zip""
  source_file = ""lambda.js""
  output_path = ""lambda_function_payload.zip""
}

resource ""aws_lambda_function"" ""test_lambda"" {
  filename      = ""lambda_function_payload.zip""
  function_name = ""lambda_function_name""
  role          = aws_iam_role.iam_for_lambda.arn
  handler       = ""index.test""

  runtime = ""nodejs18.x""
}","Needs to have a defined IAM role. Data for a .zip file, with source file = lambda.js, function name can be anything. For the aws_lambda_function use the defined zip, function name, set handler to anything, and runtime to ""nodejs.18x""","I'll help you create an AWS Lambda function with the specified requirements. Based on the examples provided, I'll include all necessary resources including the IAM role, policy document, archive file, and the Lambda function itself.

```hcl
# IAM policy document for Lambda role assumption
data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""lambda.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name               = ""lambda_execution_role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

# Basic Lambda execution policy attachment
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Archive file for Lambda function code
data ""archive_file"" ""lambda"" {
  type        = ""zip""
  source_file = ""lambda.js""
  output_path = ""lambda_function_payload.zip""
}

# Lambda function
resource ""aws_lambda_function"" ""lambda_function"" {
  filename      = ""lambda_function_payload.zip""
  function_name = ""my_lambda_function""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""

  source_code_hash = data.archive_file.lambda.output_base64sha256
  runtime          = ""nodejs18.x""

  environment {
    variables = {
      ENVIRONMENT = ""production""
    }
  }
}
```

This configuration includes:

1. An IAM policy document that allows Lambda to assume the role
2. An IAM role for the Lambda function
3. A basic execution policy attachment for Lambda
4. An archive file resource that zips the Lambda source code
5. The Lambda function itself with the specified:
   - nodejs18.x runtime
   - lambda.js as the source file (which gets zipped)
   - index.handler as the handler
   - Basic environment configuration

Make sure that your ""lambda.js"" file exists in the same directory as your Terraform configuration. The function will use the handler specified in ""index.handler"", which means it will look for a function named ""handler"" in ""index.js"".",,,,,,"I'll help you create an AWS Lambda function with the specified requirements. Based on the examples provided, I'll create a complete solution that includes the necessary IAM role, policy document, archive file, and Lambda function configuration.

```hcl
# IAM policy document for Lambda role assumption
data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""lambda.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name               = ""lambda_execution_role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

# Basic Lambda execution policy attachment
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Archive file for Lambda function code
data ""archive_file"" ""lambda"" {
  type        = ""zip""
  source_file = ""lambda.js""
  output_path = ""lambda_function_payload.zip""
}

# Lambda function
resource ""aws_lambda_function"" ""lambda_function"" {
  filename         = ""lambda_function_payload.zip""
  function_name    = ""my_lambda_function""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  source_code_hash = data.archive_file.lambda.output_base64sha256

  runtime = ""nodejs18.x""
}
```

This configuration:
1. Creates an IAM policy document that allows Lambda to assume the role
2. Creates an IAM role for the Lambda function
3. Attaches the basic Lambda execution policy to the role
4. Creates a ZIP archive from your `lambda.js` file
5. Creates the Lambda function with the specified runtime (nodejs18.x) and handler (index.handler)

Make sure you have the `lambda.js` file in your working directory before applying this configuration. The function will use `index.handler` as the entry point for execution.",,,,,,"I'll help you create an AWS Lambda function with the specified requirements. Based on the examples provided, I'll include all necessary resources including the IAM role, policy document, archive file, and the Lambda function itself.

```hcl
# IAM policy document for Lambda assume role
data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""lambda.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name               = ""lambda_execution_role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

# Basic Lambda execution policy attachment
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Archive file for Lambda source code
data ""archive_file"" ""lambda"" {
  type        = ""zip""
  source_file = ""lambda.js""
  output_path = ""lambda_function_payload.zip""
}

# Lambda function
resource ""aws_lambda_function"" ""lambda_function"" {
  filename      = ""lambda_function_payload.zip""
  function_name = ""my_lambda_function""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""

  source_code_hash = data.archive_file.lambda.output_base64sha256
  runtime          = ""nodejs18.x""

  environment {
    variables = {
      NODE_ENV = ""production""
    }
  }
}
```

This configuration:
1. Creates an IAM policy document that allows Lambda to assume the role
2. Creates an IAM role for the Lambda function
3. Attaches the basic Lambda execution policy to the role
4. Creates a zip archive from your lambda.js source file
5. Creates the Lambda function with:
   - nodejs18.x runtime
   - index.handler as the handler
   - Uses the lambda.js source code
   - Includes basic environment configuration

Make sure you have a `lambda.js` file in your working directory before applying this configuration.",,,,,
"aws_iam_role, aws_kinesisanalyticsv2_application, aws_iam_policy_document",Generate a basic Amazon Kinesis Analytics Application with a CloudWatch log stream,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kinesisanalytics.amazonaws.com"") 
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
}

has_valid_cloudwatch_log_group {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_cloudwatch_log_group""
    resource.values.name
}

has_valid_cloudwatch_log_stream {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_cloudwatch_log_stream""
    resource.values.name
    role := input.configuration.root_module.resources[i]
    role.expressions.log_group_name

}

has_valid_kinesis_analytics_application {
        some i
        resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_analytics_application""
    resource.values.name
    resource.values.cloudwatch_logging_options
    
}

has_valid_resources {
	has_valid_bucket
    has_valid_iam_role(input.resource_changes)
    has_valid_cloudwatch_log_group
    has_valid_cloudwatch_log_stream
    has_valid_kinesis_analytics_application
}",3,"provider ""aws"" {
  region = ""us-west-2""
}

data ""aws_iam_policy_document"" ""KinesisAssume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}


resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_role""
  assume_role_policy = data.aws_iam_policy_document.KinesisAssume.json
}

resource ""aws_kinesisanalyticsv2_application"" ""flink_app"" {
  name                 = ""example-application1""
  runtime_environment  = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_role.arn
}","Has one ""aws_cloudwatch_log_group"" resource, one ""aws_cloudwatch_log_stream"" resource, one ""aws_iam_role"" resource, one ""aws_s3_bucket"" resource, and one ""aws_kinesis_analytics_application"" resource with a ""cloudwatch_logging_options"" block
","I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with CloudWatch log stream integration. Based on the examples provided, I'll extend them to include CloudWatch logging capabilities.

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for Kinesis Analytics
data ""aws_iam_policy_document"" ""kinesis_assume_role"" {
  statement {
    effect = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}

resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_analytics_role""
  assume_role_policy = data.aws_iam_policy_document.kinesis_assume_role.json
}

# IAM Policy for CloudWatch Logs
resource ""aws_iam_role_policy"" ""cloudwatch_logs"" {
  name = ""kinesis_analytics_cloudwatch_logs""
  role = aws_iam_role.kinesis_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:DescribeLogStreams"",
          ""logs:PutLogEvents"",
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream""
        ]
        Resource = [""*""]
      }
    ]
  })
}

# CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""analytics_logs"" {
  name              = ""/aws/kinesis-analytics/analytics-app""
  retention_in_days = 14
}

# CloudWatch Log Stream
resource ""aws_cloudwatch_log_stream"" ""analytics_log_stream"" {
  name           = ""analytics-stream""
  log_group_name = aws_cloudwatch_log_group.analytics_logs.name
}

# Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""analytics_app"" {
  name = ""analytics-application""

  cloudwatch_logging_options {
    log_stream_arn = aws_cloudwatch_log_stream.analytics_log_stream.arn
    role_arn       = aws_iam_role.kinesis_role.arn
  }
}
```

This configuration creates:
1. An IAM role and policy document for Kinesis Analytics
2. A specific IAM policy for CloudWatch Logs access
3. A CloudWatch Log Group and Log Stream
4. A Kinesis Analytics Application with CloudWatch logging enabled

The application will be able to write logs to CloudWatch through the specified log stream. The logs are retained for 14 days by default, but you can adjust the retention_in_days value as needed.

Make sure to replace the region and any resource names according to your requirements. Also, consider adding any additional configurations specific to your use case, such as tags or additional application settings.",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with CloudWatch log stream integration. Based on the examples provided, I'll extend them to include CloudWatch logging.

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for Kinesis Analytics
data ""aws_iam_policy_document"" ""kinesis_assume_role"" {
  statement {
    effect = ""Allow""
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_analytics_role""
  assume_role_policy = data.aws_iam_policy_document.kinesis_assume_role.json
}

# IAM Policy for CloudWatch Logs
resource ""aws_iam_role_policy"" ""cloudwatch_logs"" {
  name = ""kinesis_cloudwatch_logs_policy""
  role = aws_iam_role.kinesis_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:PutLogEvents"",
          ""logs:CreateLogStream"",
          ""logs:CreateLogGroup"",
          ""logs:DescribeLogStreams"",
          ""logs:DescribeLogGroups""
        ]
        Resource = [""arn:aws:logs:*:*:*""]
      }
    ]
  })
}

# CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""kinesis_log_group"" {
  name              = ""/aws/kinesis-analytics/application""
  retention_in_days = 14
}

# CloudWatch Log Stream
resource ""aws_cloudwatch_log_stream"" ""kinesis_log_stream"" {
  name           = ""kinesis-analytics-log-stream""
  log_group_name = aws_cloudwatch_log_group.kinesis_log_group.name
}

# Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""kinesis-analytics-application""
  
  cloudwatch_logging_options {
    log_stream_arn = aws_cloudwatch_log_stream.kinesis_log_stream.arn
    role_arn       = aws_iam_role.kinesis_role.arn
  }
}
```

This configuration:
1. Creates an IAM role and policy for Kinesis Analytics to access CloudWatch Logs
2. Sets up a CloudWatch Log Group and Log Stream
3. Creates a Kinesis Analytics Application with CloudWatch logging enabled
4. Grants necessary permissions through IAM policies

The application will now log to CloudWatch using the specified log group and stream, with a 14-day retention period. You can adjust the retention period and names according to your needs.",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with CloudWatch log stream integration. Based on the examples provided, I'll extend them to include CloudWatch logging capabilities.

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

# IAM role for Kinesis Analytics
data ""aws_iam_policy_document"" ""kinesis_assume_role"" {
  statement {
    effect = ""Allow""
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_analytics_role""
  assume_role_policy = data.aws_iam_policy_document.kinesis_assume_role.json
}

# IAM policy for CloudWatch Logs
resource ""aws_iam_role_policy"" ""cloudwatch_logs"" {
  name = ""kinesis_cloudwatch_logs_policy""
  role = aws_iam_role.kinesis_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:PutLogEvents"",
          ""logs:CreateLogStream"",
          ""logs:CreateLogGroup"",
          ""logs:DescribeLogStreams"",
          ""logs:DescribeLogGroups""
        ]
        Resource = [""arn:aws:logs:*:*:*""]
      }
    ]
  })
}

# CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""kinesis_log_group"" {
  name              = ""/aws/kinesis-analytics/application-logs""
  retention_in_days = 14
}

# Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""example-kinesis-analytics-app""

  cloudwatch_logging_options {
    log_stream_arn = ""${aws_cloudwatch_log_group.kinesis_log_group.arn}:log-stream:kinesis-analytics-log-stream""
    role_arn       = aws_iam_role.kinesis_role.arn
  }
}

# CloudWatch Log Stream
resource ""aws_cloudwatch_log_stream"" ""kinesis_log_stream"" {
  name           = ""kinesis-analytics-log-stream""
  log_group_name = aws_cloudwatch_log_group.kinesis_log_group.name
}
```

This configuration creates:
1. An IAM role and policy for Kinesis Analytics to access CloudWatch Logs
2. A CloudWatch Log Group and Log Stream
3. A Kinesis Analytics Application with CloudWatch logging enabled

The application will be able to send logs to CloudWatch Logs, and the logs will be retained for 14 days. The IAM role has the necessary permissions to create and manage log streams and log events in CloudWatch Logs.

Make sure to adjust the region, names, and retention period according to your specific requirements.",,,,,
"aws_dynamodb_table, aws_iam_role, aws_lambda_event_source_mapping, aws_lambda_function, archive_file, aws_iam_policy_document",Create a Lambda event source mapping from a DynamoDB,"package main

import future.keywords.in

default allow = false

aws_dynamodb_table_valid(resources) {
    some resource in resources
    resource.type == ""aws_dynamodb_table""
}

aws_lambda_function_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_function""
}

# Check for Lambda event source mapping from DynamoDB
aws_lambda_event_source_mapping_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_event_source_mapping""
}

# Aggregate all checks
allow {
    aws_dynamodb_table_valid(input.resource_changes)
    aws_lambda_function_valid(input.resource_changes)
    aws_lambda_event_source_mapping_valid(input.resource_changes)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""example_table"" {
  name           = ""example_table""
  hash_key       = ""id""
  read_capacity  = 10
  write_capacity = 10

  attribute {
    name = ""id""
    type = ""S""
  }

  # Enable DynamoDB Streams
  stream_enabled   = true
  stream_view_type = ""NEW_AND_OLD_IMAGES"" # Choose as per your requirement

  # Other configurations for your table
}

data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""lambda.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""iam_for_lambda"" {
  name               = ""iam_for_lambda""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

resource ""aws_iam_role_policy"" ""dynamodb_lambda_policy"" {
  name   = ""lambda-dynamodb-policy""
  role   = aws_iam_role.iam_for_lambda.id
  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
        ""Sid"": ""AllowLambdaFunctionToCreateLogs"",
        ""Action"": [ 
            ""logs:*"" 
        ],
        ""Effect"": ""Allow"",
        ""Resource"": [ 
            ""arn:aws:logs:*:*:*"" 
        ]
    },
    {
        ""Sid"": ""AllowLambdaFunctionInvocation"",
        ""Effect"": ""Allow"",
        ""Action"": [
            ""lambda:InvokeFunction""
        ],
        ""Resource"": [
            ""${aws_dynamodb_table.example_table.arn}/stream/*""
        ]
    },
    {
        ""Sid"": ""APIAccessForDynamoDBStreams"",
        ""Effect"": ""Allow"",
        ""Action"": [
            ""dynamodb:GetRecords"",
            ""dynamodb:GetShardIterator"",
            ""dynamodb:DescribeStream"",
            ""dynamodb:ListStreams""
        ],
        ""Resource"": ""${aws_dynamodb_table.example_table.arn}/stream/*""
    }
  ]
}
EOF
}

data ""archive_file"" ""lambda_zip_file"" {
  type        = ""zip""
  source_file = ""./supplement/app.js""
  output_path = ""./supplement/app.zip""
}

resource ""aws_lambda_function"" ""example_lambda"" {
  filename      = data.archive_file.lambda_zip_file.output_path
  function_name = ""lambda_app_function""
  source_code_hash = data.archive_file.lambda_zip_file.output_base64sha256
  role          = aws_iam_role.iam_for_lambda.arn
  handler       = ""app.handler""
  runtime       = ""nodejs18.x""
}

resource ""aws_lambda_event_source_mapping"" ""dynamodb_lambda_mapping"" {
  event_source_arn  = aws_dynamodb_table.example_table.stream_arn
  function_name     = aws_lambda_function.example_lambda.arn
  starting_position = ""LATEST"" # or ""TRIM_HORIZON"" as per your use case

}

output ""dynamodb_arn"" {
  value = aws_dynamodb_table.example_table.arn
  description = ""The ARN of the DynamoDB table""
}

output ""lambda_processing_arn"" {
  value = aws_lambda_function.example_lambda.arn
  description = ""The ARN of the Lambda function processing the DynamoDB stream""
}","Needs to have an aws_lambda_function defined (IAM role, zip). Create a DynamoDB table. Create lambda source event from an event source from DynamoDB table and function name.","I'll help you create a Lambda event source mapping from a DynamoDB table using Terraform. This configuration will allow your Lambda function to be triggered by changes in your DynamoDB table.

```hcl
# IAM role for Lambda
data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""lambda.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""lambda_role"" {
  name               = ""lambda_dynamodb_role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

# IAM policy for the Lambda role to access DynamoDB
resource ""aws_iam_role_policy"" ""lambda_dynamodb_policy"" {
  name = ""lambda_dynamodb_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:DescribeStream"",
          ""dynamodb:GetRecords"",
          ""dynamodb:GetShardIterator"",
          ""dynamodb:ListStreams""
        ]
        Resource = ""${aws_dynamodb_table.example.stream_arn}""
      }
    ]
  })
}

# DynamoDB table with stream enabled
resource ""aws_dynamodb_table"" ""example"" {
  name           = ""example-table""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  stream_enabled = true
  stream_view_type = ""NEW_AND_OLD_IMAGES""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# Lambda function
resource ""aws_lambda_function"" ""example"" {
  filename      = ""lambda_function_payload.zip""
  function_name = ""process_dynamodb_records""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs18.x""
}

# Event source mapping
resource ""aws_lambda_event_source_mapping"" ""example"" {
  event_source_arn  = aws_dynamodb_table.example.stream_arn
  function_name     = aws_lambda_function.example.arn
  starting_position = ""LATEST""

  batch_size = 100
  enabled    = true

  depends_on = [
    aws_iam_role_policy.lambda_dynamodb_policy
  ]
}
```

This configuration:

1. Creates an IAM role and policy for the Lambda function to access DynamoDB streams
2. Creates a DynamoDB table with streaming enabled
3. Creates a Lambda function (assuming you have the function code in a zip file)
4. Sets up the event source mapping between the DynamoDB stream and the Lambda function

Key points:
- The DynamoDB table has `stream_enabled = true` and `stream_view_type = ""NEW_AND_OLD_IMAGES""` to capture both old and new versions of modified items
- The Lambda function has the necessary permissions to read from the DynamoDB stream
- The event source mapping is configured to start reading from the latest records (`starting_position = ""LATEST""`)
- The batch size is set to 100 records per Lambda invocation

Remember to:
1. Replace ""lambda_function_payload.zip"" with your actual Lambda function code package
2. Adjust the handler name to match your function's entry point
3. Modify the table name, function name, and other parameters as needed for your use case",,,,,,"I'll help you create a Lambda event source mapping from a DynamoDB table using Terraform. This configuration will allow your Lambda function to be triggered by changes in your DynamoDB table.

```hcl
# IAM role for Lambda
data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""lambda.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""lambda_role"" {
  name               = ""lambda_dynamodb_role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

# IAM policy for the Lambda role to access DynamoDB
resource ""aws_iam_role_policy"" ""lambda_dynamodb_policy"" {
  name = ""lambda_dynamodb_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:DescribeStream"",
          ""dynamodb:GetRecords"",
          ""dynamodb:GetShardIterator"",
          ""dynamodb:ListStreams""
        ]
        Resource = ""${aws_dynamodb_table.example.stream_arn}""
      }
    ]
  })
}

# DynamoDB table with stream enabled
resource ""aws_dynamodb_table"" ""example"" {
  name           = ""example-table""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  stream_enabled = true
  stream_view_type = ""NEW_AND_OLD_IMAGES""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# Lambda function
resource ""aws_lambda_function"" ""example"" {
  filename      = ""lambda_function_payload.zip""
  function_name = ""process_dynamodb_records""
  role         = aws_iam_role.lambda_role.arn
  handler      = ""index.handler""
  runtime      = ""nodejs18.x""
}

# Event source mapping
resource ""aws_lambda_event_source_mapping"" ""example"" {
  event_source_arn  = aws_dynamodb_table.example.stream_arn
  function_name     = aws_lambda_function.example.function_name
  starting_position = ""LATEST""

  batch_size = 100
  enabled    = true

  depends_on = [
    aws_iam_role_policy.lambda_dynamodb_policy
  ]
}
```

This configuration:

1. Creates an IAM role and policy for the Lambda function to access DynamoDB streams
2. Creates a DynamoDB table with streaming enabled
3. Creates a Lambda function (assumes you have a deployment package)
4. Sets up the event source mapping between the DynamoDB stream and the Lambda function

Key features:
- The DynamoDB table has streaming enabled with `stream_enabled = true`
- The stream view type is set to `NEW_AND_OLD_IMAGES` to capture both old and new versions of modified items
- The event source mapping is configured to start reading from the latest records (`starting_position = ""LATEST""`)
- Batch size is set to 100 records per Lambda invocation
- Appropriate IAM permissions are granted for the Lambda function to read from the DynamoDB stream

Remember to have your Lambda function code packaged in a zip file as specified in the `filename` attribute of the Lambda function resource.",,,,,,"I'll help you create a Lambda event source mapping from a DynamoDB table using Terraform. This configuration will allow the Lambda function to be triggered by changes in the DynamoDB table.

```hcl
# IAM role for Lambda
data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""lambda.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""lambda_role"" {
  name               = ""lambda_dynamodb_role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

# IAM policy for the Lambda role to access DynamoDB
resource ""aws_iam_role_policy"" ""lambda_dynamodb_policy"" {
  name = ""lambda_dynamodb_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:DescribeStream"",
          ""dynamodb:GetRecords"",
          ""dynamodb:GetShardIterator"",
          ""dynamodb:ListStreams""
        ]
        Resource = ""${aws_dynamodb_table.example.stream_arn}""
      }
    ]
  })
}

# DynamoDB table with stream enabled
resource ""aws_dynamodb_table"" ""example"" {
  name           = ""example-table""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  stream_enabled = true
  stream_view_type = ""NEW_AND_OLD_IMAGES""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# Lambda function
resource ""aws_lambda_function"" ""example"" {
  filename      = ""lambda_function_payload.zip""
  function_name = ""process_dynamodb_records""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs18.x""
}

# Event source mapping
resource ""aws_lambda_event_source_mapping"" ""example"" {
  event_source_arn  = aws_dynamodb_table.example.stream_arn
  function_name     = aws_lambda_function.example.arn
  starting_position = ""LATEST""

  batch_size = 100
  enabled    = true
}
```

This configuration:

1. Creates an IAM role and policy for the Lambda function to access DynamoDB streams
2. Creates a DynamoDB table with streaming enabled
3. Creates a Lambda function (assumes you have a deployment package)
4. Sets up the event source mapping between the DynamoDB stream and the Lambda function

The Lambda function will be triggered whenever records are modified in the DynamoDB table. The `starting_position` is set to ""LATEST"" which means it will only process new records after the mapping is created. You can change this to ""TRIM_HORIZON"" if you want to process all available records.

Remember to:
1. Provide the actual Lambda function code in a ZIP file
2. Adjust the handler name to match your function's entry point
3. Modify the table name, function name, and other parameters as needed for your use case
4. Add any additional IAM permissions that your Lambda function might need",,,,,
"aws_iam_role, aws_iam_role_policy, aws_kendra_data_source, aws_kendra_index",Generate a Kendra data source resource With URL Exclusion and Inclusion Patterns,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
    some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kendra.amazonaws.com"") 
}

has_valid_kendra_index {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_index""
    resource.values.name
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
}

has_valid_kendra_data_source{
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_data_source""
    resource.values.name
    resource.values.type
    resource.values.configuration[_].web_crawler_configuration[_].url_exclusion_patterns
    resource.values.configuration[_].web_crawler_configuration[_].url_inclusion_patterns
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
    role.expressions.index_id
}


has_valid_resources {
    has_valid_iam_role(input.resource_changes)
    has_valid_kendra_index
    has_valid_kendra_data_source
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""role"" {
  name = ""Kendra-Role""
  assume_role_policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": ""sts:AssumeRole"",
      ""Principal"": {
        ""Service"": ""kendra.amazonaws.com""
      },
      ""Effect"": ""Allow"",
      ""Sid"": """"
    }
  ]
}
EOF
}

resource ""aws_iam_role_policy"" ""policy"" {
  name = ""Kendra-Policy""
  role = aws_iam_role.role.id

  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:GetObject"",
        ""s3:ListBucket""
      ],
      ""Resource"": ""*""
    }
  ]
}
EOF
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example""
  description = ""example""
  edition     = ""DEVELOPER_EDITION""
  role_arn    = aws_iam_role.role.arn
}

resource ""aws_kendra_data_source"" ""example"" {
  index_id = aws_kendra_index.example.id
  name     = ""example""
  type     = ""WEBCRAWLER""
  role_arn = aws_iam_role.role.arn

  configuration {
    web_crawler_configuration {
      url_exclusion_patterns = [""example""]
      url_inclusion_patterns = [""hello""]

      urls {
        seed_url_configuration {
          seed_urls = [
            ""https://docs.aws.amazon.com/codedeploy/latest/userguide/getting-started-create-service-role.html""
          ]
        }
      }
    }
  }
}","Has one ""aws_iam_role"" resource, one ""aws_kendra_index"" resource, and one ""aws_kendra_data_source"" resource with ""url_exclusion_patterns"" and ""url_inclusion_patterns""","I'll help you generate a Terraform configuration for an AWS Kendra data source with URL exclusion and inclusion patterns. This will be based on the Kendra example provided but modified to include the specific URL pattern configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra-datasource-role""
  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra-datasource-policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:ListBucket""
        ]
        Resource = ""*""
      }
    ]
  })
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-index""
  description = ""Example Kendra Index""
  edition     = ""DEVELOPER_EDITION""
  role_arn    = aws_iam_role.kendra_role.arn
}

resource ""aws_kendra_data_source"" ""example"" {
  index_id = aws_kendra_index.example.id
  name     = ""example-datasource""
  type     = ""WEBCRAWLER""
  role_arn = aws_iam_role.kendra_role.arn

  configuration {
    web_crawler_configuration {
      urls {
        seed_url_configuration {
          seed_urls = [
            ""https://docs.aws.amazon.com/kendra/""
          ]
          
          url_inclusion_patterns = [
            ""https://docs.aws.amazon.com/kendra/*/developer-guide/*"",
            ""https://docs.aws.amazon.com/kendra/*/api-reference/*""
          ]

          url_exclusion_patterns = [
            ""https://docs.aws.amazon.com/kendra/*/troubleshooting/*"",
            ""https://docs.aws.amazon.com/kendra/*/release-notes/*""
          ]
        }
      }

      crawl_depth = 2
      max_links_per_page = 100
      
      url_inclusion_patterns = [
        ""https://docs.aws.amazon.com/kendra/*""
      ]
      
      url_exclusion_patterns = [
        ""https://docs.aws.amazon.com/kendra/latest/dg/archive*"",
        ""https://docs.aws.amazon.com/kendra/latest/dg/beta*""
      ]
    }
  }
}
```

This configuration creates:
1. A Kendra index
2. Required IAM role and policy
3. A Kendra data source with web crawler configuration that includes:
   - URL inclusion patterns at both the crawler and seed URL levels
   - URL exclusion patterns at both the crawler and seed URL levels
   - Basic crawler settings like crawl depth and max links per page

The inclusion and exclusion patterns are configured at two levels:
- At the seed URL level (within `seed_url_configuration`)
- At the general crawler level (directly under `web_crawler_configuration`)

This allows for fine-grained control over which URLs are crawled and indexed by Kendra. The patterns use wildcard matching to include or exclude specific URL patterns.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra data source with URL exclusion and inclusion patterns. This builds upon Example 1 but adds specific URL pattern configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra-crawler-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra-crawler-policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:ListBucket""
        ]
        Resource = ""*""
      }
    ]
  })
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-index""
  description = ""Example Kendra Index""
  role_arn    = aws_iam_role.kendra_role.arn
  edition     = ""DEVELOPER_EDITION""
}

resource ""aws_kendra_data_source"" ""example"" {
  index_id = aws_kendra_index.example.id
  name     = ""example-datasource""
  type     = ""WEBCRAWLER""
  role_arn = aws_iam_role.kendra_role.arn

  configuration {
    web_crawler_configuration {
      urls {
        seed_url_configuration {
          seed_urls = [
            ""https://docs.aws.amazon.com/kendra/""
          ]
          
          web_crawler_mode = ""HOST_ONLY""
        }

        url_inclusion_patterns = [
          ""https://docs.aws.amazon.com/kendra/*/developerguide/*"",
          ""https://docs.aws.amazon.com/kendra/*/apiref/*""
        ]

        url_exclusion_patterns = [
          ""https://docs.aws.amazon.com/kendra/*/release-notes/*"",
          ""https://docs.aws.amazon.com/kendra/*/sitemap.xml""
        ]
      }

      crawl_depth = 2
      max_links_per_page = 100
      
      url_inclusion_patterns = [
        ""*.aws.amazon.com/*""
      ]
      
      url_exclusion_patterns = [
        ""*/404/*"",
        ""*/search/*""
      ]
    }
  }
}
```

This configuration creates:
1. A Kendra index
2. Required IAM role and policy
3. A web crawler data source with:
   - URL inclusion patterns at both the crawler and seed URL levels
   - URL exclusion patterns at both the crawler and seed URL levels
   - Specific seed URL configuration with host-only crawling mode
   - Crawl depth and max links per page settings

The inclusion and exclusion patterns demonstrate both global patterns (at the web_crawler_configuration level) and specific patterns for the seed URLs. The patterns use wildcards (*) to match multiple URLs following similar patterns.

Key features:
- Includes documentation pages and API references
- Excludes release notes and sitemaps
- Global exclusions for 404 and search pages
- Global inclusion for AWS domain pages
- Crawl depth limited to 2 levels
- Maximum of 100 links per page will be processed

This configuration provides a good balance between specific content targeting and efficient crawling parameters.",,,,,,"I'll help you generate a Terraform configuration for an AWS Kendra data source with URL exclusion and inclusion patterns. This builds upon Example 1 but adds specific URL pattern configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra-crawler-role""
  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra-crawler-policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:ListBucket""
        ]
        Resource = ""*""
      }
    ]
  })
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-index""
  description = ""Example Kendra Index""
  edition     = ""DEVELOPER_EDITION""
  role_arn    = aws_iam_role.kendra_role.arn
}

resource ""aws_kendra_data_source"" ""example"" {
  index_id = aws_kendra_index.example.id
  name     = ""example-datasource""
  type     = ""WEBCRAWLER""
  role_arn = aws_iam_role.kendra_role.arn

  configuration {
    web_crawler_configuration {
      urls {
        seed_url_configuration {
          seed_urls = [
            ""https://docs.aws.amazon.com""
          ]
          
          url_inclusion_patterns = [
            ""https://docs.aws.amazon.com/*/latest/*"",
            ""https://docs.aws.amazon.com/*/guide/*""
          ]

          url_exclusion_patterns = [
            ""https://docs.aws.amazon.com/*/api/*"",
            ""https://docs.aws.amazon.com/*/release-notes/*"",
            ""*.pdf""
          ]
        }
      }

      crawl_depth = 2
      max_links_per_page = 100
      max_content_size_per_page_in_mega_bytes = 50
    }
  }

  language_code = ""en""
  
  schedule = ""cron(0 0 * * ? *)""

  description = ""Web crawler data source with URL inclusion and exclusion patterns""
}
```

This configuration creates:
1. A Kendra index
2. Required IAM role and policy
3. A web crawler data source with:
   - URL inclusion patterns to focus on documentation and user guides
   - URL exclusion patterns to skip API references, release notes, and PDF files
   - Basic crawler configurations like depth and page limits
   - A daily schedule for crawling

The inclusion patterns will ensure the crawler only processes URLs matching the specified patterns, while the exclusion patterns will prevent the crawler from processing URLs that match those patterns. This helps in targeting specific content while avoiding unnecessary crawling of unwanted pages.",,,,,
"aws_iam_role, aws_kinesis_firehose_delivery_stream, aws_s3_bucket, aws_iam_policy_document",Generate a Kinesis Firehose Delivery Stream with an extended S3 Destination,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""firehose.amazonaws.com"") 
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
}

has_valid_firehose_delivery_stream {
        some i
        resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_firehose_delivery_stream""
    resource.values.name
    resource.values.destination == ""extended_s3""
    resource.values.extended_s3_configuration[_]
        role := input.configuration.root_module.resources[i]
    role.expressions.extended_s3_configuration[_].role_arn
}

has_valid_resources {
        has_valid_iam_role(input.resource_changes)
        has_valid_bucket
    has_valid_firehose_delivery_stream
}",3,"provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""januarysixteenth"" {
  bucket = ""januarysixteenth""
}

data ""aws_iam_policy_document"" ""firehose_assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""firehose.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""firehose_role"" {
  name               = ""firehose_test_role""
  assume_role_policy = data.aws_iam_policy_document.firehose_assume_role.json
}

resource ""aws_kinesis_firehose_delivery_stream"" ""extended_s3_stream"" {
  name        = ""terraform-kinesis-firehose-extended-s3-test-stream""
  destination = ""extended_s3""

  extended_s3_configuration {
    role_arn   = aws_iam_role.firehose_role.arn
    bucket_arn = aws_s3_bucket.januarysixteenth.arn
  }
}","Has one ""aws_iam_role"" resource, one ""aws_s3_bucket"" resource, and one ""aws_kinesis_firehose_delivery_stream"" resource with an ""extended_s3_configuration"" block 
","I'll help you generate a Terraform configuration for a Kinesis Firehose Delivery Stream with an extended S3 destination, based on the provided examples. I'll create a simplified version that includes the essential components.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM Role for Firehose
data ""aws_iam_policy_document"" ""firehose_assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""firehose.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""firehose_role"" {
  name               = ""firehose_role""
  assume_role_policy = data.aws_iam_policy_document.firehose_assume_role.json
}

# S3 Bucket for Firehose destination
resource ""aws_s3_bucket"" ""firehose_bucket"" {
  bucket = ""my-firehose-destination-bucket""
}

# Kinesis Firehose Delivery Stream
resource ""aws_kinesis_firehose_delivery_stream"" ""extended_s3_stream"" {
  name        = ""terraform-kinesis-firehose-s3-stream""
  destination = ""extended_s3""

  extended_s3_configuration {
    role_arn        = aws_iam_role.firehose_role.arn
    bucket_arn      = aws_s3_bucket.firehose_bucket.arn
    buffering_size  = 64
    buffering_interval = 300
    
    prefix              = ""data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/""
    error_output_prefix = ""errors/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/!{firehose:error-output-type}/""

    processing_configuration {
      enabled = ""true""

      processors {
        type = ""AppendDelimiterToRecord""
      }
    }
  }
}
```

This configuration includes:

1. Required provider configuration for AWS
2. IAM role and policy document for Firehose to assume
3. S3 bucket as the destination
4. Kinesis Firehose Delivery Stream with extended S3 configuration including:
   - Basic buffering settings
   - Time-based prefix pattern for data organization
   - Error output prefix configuration
   - Simple processing configuration with newline delimiter

Note: You might want to add additional IAM policies to the role to allow Firehose to write to S3 and perform other necessary actions, depending on your specific requirements.

The stream will write data to S3 with a time-based prefix structure, and includes basic processing with a newline delimiter between records. The buffering is set to 64 MB or 300 seconds, whichever comes first.",,,,,,"I'll help you create a Kinesis Firehose Delivery Stream with an extended S3 destination based on the provided examples. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM role for Kinesis Firehose
data ""aws_iam_policy_document"" ""firehose_assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""firehose.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""firehose_role"" {
  name               = ""firehose_role""
  assume_role_policy = data.aws_iam_policy_document.firehose_assume_role.json
}

# S3 bucket for destination
resource ""aws_s3_bucket"" ""destination"" {
  bucket = ""firehose-destination-bucket""
}

# Kinesis Firehose Delivery Stream
resource ""aws_kinesis_firehose_delivery_stream"" ""extended_s3_stream"" {
  name        = ""terraform-kinesis-firehose-extended-s3-stream""
  destination = ""extended_s3""

  extended_s3_configuration {
    role_arn        = aws_iam_role.firehose_role.arn
    bucket_arn      = aws_s3_bucket.destination.arn
    buffering_size  = 64
    buffering_interval = 300

    prefix              = ""data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/""
    error_output_prefix = ""errors/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/!{firehose:error-output-type}/""

    processing_configuration {
      enabled = ""true""

      processors {
        type = ""AppendDelimiterToRecord""
      }
    }
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role for Firehose with the necessary assume role policy
3. Creates an S3 bucket as the destination
4. Creates a Kinesis Firehose Delivery Stream with:
   - Extended S3 as the destination
   - Basic buffering configuration
   - Time-based prefixes for data organization
   - Error output prefix configuration
   - A simple processing configuration that appends delimiters to records

The stream will buffer data until either 64 MB is accumulated or 300 seconds have passed, then deliver it to the specified S3 bucket with the defined prefix structure.

Note: You may want to add additional IAM policies to the Firehose role to grant permissions for S3 access and any other required services. Also, remember to adjust the bucket name to something unique as S3 bucket names must be globally unique.",,,,,,"I'll help you create a Kinesis Firehose Delivery Stream with an extended S3 destination using Terraform. This example will include the necessary resources and configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM role for Kinesis Firehose
data ""aws_iam_policy_document"" ""firehose_assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""firehose.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""firehose_role"" {
  name               = ""firehose_delivery_role""
  assume_role_policy = data.aws_iam_policy_document.firehose_assume_role.json
}

# S3 bucket for destination
resource ""aws_s3_bucket"" ""destination"" {
  bucket = ""my-firehose-destination-bucket""
}

# Kinesis Firehose Delivery Stream
resource ""aws_kinesis_firehose_delivery_stream"" ""extended_s3_stream"" {
  name        = ""terraform-kinesis-firehose-extended-s3-stream""
  destination = ""extended_s3""

  extended_s3_configuration {
    role_arn        = aws_iam_role.firehose_role.arn
    bucket_arn      = aws_s3_bucket.destination.arn
    buffering_size  = 64
    buffering_interval = 60
    
    prefix              = ""data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/""
    error_output_prefix = ""errors/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/!{firehose:error-output-type}/""
    
    compression_format = ""GZIP""

    processing_configuration {
      enabled = ""true""

      processors {
        type = ""AppendDelimiterToRecord""
        parameters {
          parameter_name  = ""Delimiter""
          parameter_value = ""\n""
        }
      }
    }
  }
}

# IAM policy for Firehose to access S3
resource ""aws_iam_role_policy"" ""firehose_s3_policy"" {
  name = ""firehose_s3_policy""
  role = aws_iam_role.firehose_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:AbortMultipartUpload"",
          ""s3:GetBucketLocation"",
          ""s3:GetObject"",
          ""s3:ListBucket"",
          ""s3:ListBucketMultipartUploads"",
          ""s3:PutObject""
        ]
        Resource = [
          aws_s3_bucket.destination.arn,
          ""${aws_s3_bucket.destination.arn}/*""
        ]
      }
    ]
  })
}
```

This configuration creates:
1. An IAM role and policy for Kinesis Firehose to assume
2. A destination S3 bucket
3. A Kinesis Firehose Delivery Stream with extended S3 configuration
4. The necessary IAM permissions for Firehose to write to S3

The Firehose stream includes:
- GZIP compression
- 64MB buffering size
- 60-second buffering interval
- A prefix pattern for organizing data by timestamp
- Error output handling
- A new line delimiter processor

Remember to:
- Adjust the bucket name to be globally unique
- Modify the region if needed
- Adjust the buffering settings based on your requirements
- Consider adding additional processors",,,,,
"aws_network_acl, aws_subnet, aws_vpc",Create an AWS VPC resource with a subnet and associated network ACLs to control both ingress and egress traffic,"package vpc_net_acl
import future.keywords.in

default valid := false

valid {
        some net_acl_resource in input.configuration.root_module.resources
        net_acl_resource.type == ""aws_network_acl""
        net_acl_resource.expressions.egress
        net_acl_resource.expressions.ingress
   
        some vpc_resource in input.configuration.root_module.resources
        vpc_resource.type == ""aws_vpc""
        vpc_resource.address in net_acl_resource.expressions.vpc_id.references

        some subnet in input.configuration.root_module.resources
        subnet.type == ""aws_subnet""
        vpc_resource.address in subnet.expressions.vpc_id.references
}",3,"resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""snet"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_network_acl"" ""ntwk_acl"" {
  vpc_id      = aws_vpc.main.id

  egress {
    protocol   = ""tcp""
    rule_no    = 200
    action     = ""allow""
    cidr_block = ""10.3.0.0/18""
    from_port  = 443
    to_port    = 443
   }

   ingress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""10.3.0.0/18""
    from_port  = 80
    to_port    = 80
  }
}","Has one resource ""aws_vpc""

Has one resource ""aws_subnet""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id""

Has one resource ""aws_network_acl""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id""
with one ""egress"" with correct parameters
with one ""ingress"" with correct parameters","I'll help you create a Terraform configuration for an AWS VPC with a subnet and network ACLs to control both ingress and egress traffic. Based on the examples provided and best practices, here's the solution:

```hcl
# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create Subnet
resource ""aws_subnet"" ""main"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""main-subnet""
  }
}

# Create Network ACL
resource ""aws_network_acl"" ""main"" {
  vpc_id     = aws_vpc.main.id
  subnet_ids = [aws_subnet.main.id]

  # Ingress rule - Allow HTTP
  ingress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  # Ingress rule - Allow HTTPS
  ingress {
    protocol   = ""tcp""
    rule_no    = 200
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  # Ingress rule - Allow SSH
  ingress {
    protocol   = ""tcp""
    rule_no    = 300
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 22
    to_port    = 22
  }

  # Egress rule - Allow all outbound traffic
  egress {
    protocol   = ""-1""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 0
    to_port    = 0
  }

  tags = {
    Name = ""main-nacl""
  }
}
```

This configuration:
1. Creates a VPC with DNS support enabled
2. Creates a subnet within the VPC
3. Creates a Network ACL with:
   - Ingress rules allowing HTTP (80), HTTPS (443), and SSH (22) traffic
   - Egress rule allowing all outbound traffic
4. Associates the Network ACL with the subnet

The Network ACL rules are evaluated in order based on the rule number, and the first match is applied. You can customize the rules by adding or modifying the ingress and egress blocks according to your security requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS VPC with a subnet and network ACLs to control both ingress and egress traffic. This will include the VPC, subnet, and NACL resources with specific rules.

```hcl
# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create Subnet
resource ""aws_subnet"" ""main"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""main-subnet""
  }
}

# Create Network ACL
resource ""aws_network_acl"" ""main"" {
  vpc_id     = aws_vpc.main.id
  subnet_ids = [aws_subnet.main.id]

  tags = {
    Name = ""main-nacl""
  }
}

# NACL Ingress Rules
resource ""aws_network_acl_rule"" ""allow_http_ingress"" {
  network_acl_id = aws_network_acl.main.id
  rule_number    = 100
  protocol       = ""tcp""
  rule_action    = ""allow""
  egress         = false
  cidr_block     = ""0.0.0.0/0""
  from_port      = 80
  to_port        = 80
}

resource ""aws_network_acl_rule"" ""allow_https_ingress"" {
  network_acl_id = aws_network_acl.main.id
  rule_number    = 110
  protocol       = ""tcp""
  rule_action    = ""allow""
  egress         = false
  cidr_block     = ""0.0.0.0/0""
  from_port      = 443
  to_port        = 443
}

resource ""aws_network_acl_rule"" ""allow_return_traffic_ingress"" {
  network_acl_id = aws_network_acl.main.id
  rule_number    = 120
  protocol       = ""tcp""
  rule_action    = ""allow""
  egress         = false
  cidr_block     = ""0.0.0.0/0""
  from_port      = 1024
  to_port        = 65535
}

# NACL Egress Rules
resource ""aws_network_acl_rule"" ""allow_http_egress"" {
  network_acl_id = aws_network_acl.main.id
  rule_number    = 100
  protocol       = ""tcp""
  rule_action    = ""allow""
  egress         = true
  cidr_block     = ""0.0.0.0/0""
  from_port      = 80
  to_port        = 80
}

resource ""aws_network_acl_rule"" ""allow_https_egress"" {
  network_acl_id = aws_network_acl.main.id
  rule_number    = 110
  protocol       = ""tcp""
  rule_action    = ""allow""
  egress         = true
  cidr_block     = ""0.0.0.0/0""
  from_port      = 443
  to_port        = 443
}

resource ""aws_network_acl_rule"" ""allow_return_traffic_egress"" {
  network_acl_id = aws_network_acl.main.id
  rule_number    = 120
  protocol       = ""tcp""
  rule_action    = ""allow""
  egress         = true
  cidr_block     = ""0.0.0.0/0""
  from_port      = 1024
  to_port        = 65535
}
```

This configuration creates:
1. A VPC with CIDR block 10.0.0.0/16
2. A subnet with CIDR block 10.0.1.0/24
3. A Network ACL",,,,,,"I'll help you create a Terraform configuration for an AWS VPC with a subnet and network ACLs to control both ingress and egress traffic. Here's the solution:

```hcl
# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create Subnet
resource ""aws_subnet"" ""main"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""main-subnet""
  }
}

# Create Network ACL
resource ""aws_network_acl"" ""main"" {
  vpc_id     = aws_vpc.main.id
  subnet_ids = [aws_subnet.main.id]

  tags = {
    Name = ""main-nacl""
  }
}

# Ingress rules
resource ""aws_network_acl_rule"" ""allow_ingress_http"" {
  network_acl_id = aws_network_acl.main.id
  rule_number    = 100
  protocol       = ""tcp""
  rule_action    = ""allow""
  egress         = false
  cidr_block     = ""0.0.0.0/0""
  from_port      = 80
  to_port        = 80
}

resource ""aws_network_acl_rule"" ""allow_ingress_https"" {
  network_acl_id = aws_network_acl.main.id
  rule_number    = 110
  protocol       = ""tcp""
  rule_action    = ""allow""
  egress         = false
  cidr_block     = ""0.0.0.0/0""
  from_port      = 443
  to_port        = 443
}

resource ""aws_network_acl_rule"" ""allow_ingress_return_traffic"" {
  network_acl_id = aws_network_acl.main.id
  rule_number    = 120
  protocol       = ""tcp""
  rule_action    = ""allow""
  egress         = false
  cidr_block     = ""0.0.0.0/0""
  from_port      = 1024
  to_port        = 65535
}

# Egress rules
resource ""aws_network_acl_rule"" ""allow_egress_http"" {
  network_acl_id = aws_network_acl.main.id
  rule_number    = 100
  protocol       = ""tcp""
  rule_action    = ""allow""
  egress         = true
  cidr_block     = ""0.0.0.0/0""
  from_port      = 80
  to_port        = 80
}

resource ""aws_network_acl_rule"" ""allow_egress_https"" {
  network_acl_id = aws_network_acl.main.id
  rule_number    = 110
  protocol       = ""tcp""
  rule_action    = ""allow""
  egress         = true
  cidr_block     = ""0.0.0.0/0""
  from_port      = 443
  to_port        = 443
}

resource ""aws_network_acl_rule"" ""allow_egress_ephemeral"" {
  network_acl_id = aws_network_acl.main.id
  rule_number    = 120
  protocol       = ""tcp""
  rule_action    = ""allow""
  egress         = true
  cidr_block     = ""0.0.0.0/0""
  from_port      = 1024
  to_port        = 65535
}
```

This configuration creates:
1. A VPC with CIDR block 10.0.0.0/16
2. A subnet with CIDR block 10.0.1.0/24
3. A Network ACL associated with the subnet
4. Ingress rules allowing:
   - HTTP (port",,,,,
aws_db_instance,"create a basic AWS RDS instance, with io1 storage type","package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.storage_type.constant_value == ""io1""
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage.constant_value >= 100
    expressions.allocated_storage.constant_value <= 65536
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 100
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  storage_type        = ""io1""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""","I'll help you create a Terraform configuration for an AWS RDS instance with io1 (Provisioned IOPS) storage type. Based on the examples provided and AWS best practices, here's the solution:

```hcl
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2""  # Set your desired AWS region
}

resource ""aws_db_instance"" ""default"" {
  allocated_storage     = 20
  storage_type         = ""io1""
  iops                 = 1000  # Required for io1 storage type (minimum 1000)
  db_name              = ""mydb""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
}
```

Key points about this configuration:
1. I've specified `storage_type = ""io1""` for Provisioned IOPS SSD
2. Added `iops = 1000` which is required for io1 storage type (minimum value is 1000)
3. Increased `allocated_storage` to 20 GB as io1 typically requires more storage
4. Included essential parameters like engine, username, password, and database name
5. Set `skip_final_snapshot = true` to allow for easier cleanup/deletion

This configuration will create an RDS instance with Provisioned IOPS (io1) storage, which is suitable for I/O-intensive database workloads requiring consistent performance.",,,,,,"I'll help you create a Terraform configuration for an AWS RDS instance with io1 (Provisioned IOPS) storage type. Based on the examples provided and AWS best practices, here's the solution:

```hcl
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2""  # Set your desired AWS region
}

resource ""aws_db_instance"" ""default"" {
  allocated_storage     = 20
  storage_type         = ""io1""
  iops                 = 1000  # Required for io1 storage type (min 1000)
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  db_name              = ""mydb""
  username             = ""foo""
  password             = ""foobarbaz""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
}
```

Key points about this configuration:
1. I've specified `storage_type = ""io1""` for Provisioned IOPS storage
2. Added `iops = 1000` which is required for io1 storage type (minimum value is 1000)
3. Included basic necessary parameters like allocated storage, engine, instance class, etc.
4. Added `skip_final_snapshot = true` to make cleanup easier
5. Included the provider block for AWS configuration

This configuration will create a basic RDS instance with io1 storage type and 1000 IOPS. You can adjust the IOPS value based on your performance requirements, but remember it must be between 1000 and 50000 for io1 storage type.",,,,,,"I'll help you create a Terraform configuration for an AWS RDS instance with io1 (Provisioned IOPS) storage type. Based on the examples provided and AWS best practices, here's the solution:

```hcl
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2""  # Set your desired AWS region
}

resource ""aws_db_instance"" ""default"" {
  allocated_storage     = 20
  storage_type         = ""io1""
  iops                 = 1000  # Required for io1 storage type
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  db_name              = ""mydb""
  username             = ""foo""
  password             = ""foobarbaz""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
}
```

Key points about this configuration:
1. I've specified `storage_type = ""io1""` for Provisioned IOPS SSD
2. When using io1 storage type, the `iops` parameter is required
3. The `iops` value must be between 1000-50000 for io1
4. I've included basic necessary parameters like allocated storage, engine details, and credentials
5. `skip_final_snapshot` is set to true to allow for easier cleanup during testing

This configuration will create a basic RDS instance with io1 storage type and 1000 IOPS.",,,,,
"aws_codebuild_project, aws_iam_policy, aws_iam_role, aws_iam_role_policy_attachment, aws_s3_bucket, aws_security_group, aws_subnet, aws_vpc, aws_iam_policy_document, aws_iam_policy_document",Infrastructure for an autograder for a CS class. The autograde needs to run students' code stored on GitHub in a specific environment and store the result. Make sure that the autograder prevents the students' code from reaching the internet. Accomplish this using AWS CodeBuild and AWS VPC.,"package autograder_high_jail

import rego.v1

codebuild_project_valid(codebuild_project, security_group, subnet, vpc) if {
	some artifact in codebuild_project.expressions.artifacts
	artifact.location
	artifact.type
	artifact.name

	some environment in codebuild_project.expressions.environment
	environment.compute_type
	environment.image
	environment.type

	some source in codebuild_project.expressions.source
	source.type.constant_value == ""GITHUB""
	source.location

	some vpc_config in codebuild_project.expressions.vpc_config
	security_group.address in vpc_config.security_group_ids.references
	subnet.address in vpc_config.subnets.references
	vpc.address in vpc_config.vpc_id.references
}

security_group_valid(security_group, vpc) if {
	vpc.address in security_group.expressions.vpc_id.references
}

subnet_valid(subnet, vpc) if {
	subnet.expressions.cidr_block
	vpc.address in subnet.expressions.vpc_id.references
}

vpc_valid(vpc) if {
	vpc.expressions.cidr_block
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources
	some codebuild_project in resources
	codebuild_project.type == ""aws_codebuild_project""
	some security_group in resources
	security_group.type == ""aws_security_group""
	some subnet in resources
	subnet.type == ""aws_subnet""
	some vpc in resources
	vpc.type == ""aws_vpc""
	codebuild_project_valid(codebuild_project, security_group, subnet, vpc)
	security_group_valid(security_group, vpc)
	subnet_valid(subnet, vpc)
	vpc_valid(vpc)
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""artifact-bucket-""
}

resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder_build""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip"" # include this
  }

  environment { # arguments required, exact value not specified
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1 # good to have, not required
    location        = ""github.com/source-location""
  }

  vpc_config {
    vpc_id             = aws_vpc.autograder_vpc.id
    subnets            = [aws_subnet.autograder_vpc_subnet.id]
    security_group_ids = [aws_security_group.autograder_vpc_securitygroup.id]
  }
}

resource ""aws_vpc"" ""autograder_vpc"" {
  cidr_block = ""10.0.0.0/16"" # extra value not specified
}

resource ""aws_subnet"" ""autograder_vpc_subnet"" {
  vpc_id     = aws_vpc.autograder_vpc.id
  cidr_block = ""10.0.0.0/24"" # include this
}

resource ""aws_security_group"" ""autograder_vpc_securitygroup"" {
  vpc_id = aws_vpc.autograder_vpc.id
}

resource ""aws_iam_role"" ""autograder_build_role"" {
  assume_role_policy = data.aws_iam_policy_document.autograder_build_policy_assume.json
}

data ""aws_iam_policy_document"" ""autograder_build_policy_assume"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }
  }
}

data ""aws_iam_policy_document"" ""autograder_build_policy"" {
  statement {
    effect = ""Allow""
    actions = [
      ""ec2:DescribeSubnets"",
      ""ec2:DescribeSecurityGroups"",
      ""ec2:DescribeVpcs""
    ]
    resources = [
      ""*""
    ]
  }

  statement {
    effect  = ""Allow""
    actions = [""s3:*""]
    resources = [
      aws_s3_bucket.artifact_bucket.arn,
      ""${aws_s3_bucket.artifact_bucket.arn}/*"",
    ]
  }
}

resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name        = ""lambda_policy""
  description = ""Grants permissions to Lambda to describe vpc, subnet, security group""

  policy = data.aws_iam_policy_document.autograder_build_policy.json
}

resource ""aws_iam_role_policy_attachment"" ""lambda_policy_attach"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}","have one ""aws_codebuild_project"" resource
  with ""artifacts"" argument block
    with ""type"" argument with value ""S3""
    with ""location"" argument
    with ""name"" argument
  with ""environment"" argument block
    with ""computer_type"" argument
    with ""image"" argument
    with ""type"" argument
  with ""source"" argument block
    with ""type"" argument with value ""GITHUB""
    with ""location"" argument
  with ""vpc_config"" argument block
    with ""vpc_id"" argument
    with ""subnets"" argument
    with ""security_group_ids"" argument
have one ""aws_vpc"" resource
  with ""cidr_block"" argument
have one ""aws_subnet"" argument
  with ""vpc_id"" argument
  with ""cidr_block"" argument
with one ""aws_security_group"" resource
  with ""vpc_id"" argument","Based on the examples provided and the current question, I'll create a Terraform configuration for an autograding system using AWS CodeBuild and VPC. I'll ensure the infrastructure prevents students' code from reaching the internet by properly configuring the VPC and security groups.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC Configuration
resource ""aws_vpc"" ""autograder_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""autograder-vpc""
  }
}

# Subnet Configuration
resource ""aws_subnet"" ""autograder_subnet"" {
  vpc_id     = aws_vpc.autograder_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""autograder-subnet""
  }
}

# Security Group
resource ""aws_security_group"" ""autograder_sg"" {
  name        = ""autograder-sg""
  description = ""Security group for autograder""
  vpc_id      = aws_vpc.autograder_vpc.id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""10.0.0.0/16""]  # Only allow internal VPC communication
  }
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class""
  service_role  = aws_iam_role.codebuild_role.arn
  build_timeout = ""30""

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-repo/autograder""
    git_clone_depth = 1
  }

  vpc_config {
    vpc_id             = aws_vpc.autograder_vpc.id
    subnets           = [aws_subnet.autograder_subnet.id]
    security_group_ids = [aws_security_group.autograder_sg.id]
  }
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""autograder-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""ec2:CreateNetworkInterface"",
          ""ec2:DescribeNetworkInterfaces"",
          ""ec2:DeleteNetworkInterface"",
          ""ec2:DescribeSubnets"",
          ""ec2:DescribeSecurityGroups"",
          ""ec2:DescribeVpcs""
        ]
        Resource = ""*""",,,,,,"Based on the current question and the provided examples, I'll create a Terraform configuration for an autograder using AWS CodeBuild and AWS VPC. This solution will include the necessary resources to run student code securely without internet access.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC Configuration
resource ""aws_vpc"" ""autograder_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""autograder-vpc""
  }
}

# Subnet Configuration
resource ""aws_subnet"" ""autograder_subnet"" {
  vpc_id     = aws_vpc.autograder_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""autograder-subnet""
  }
}

# Security Group
resource ""aws_security_group"" ""autograder_sg"" {
  name        = ""autograder-security-group""
  description = ""Security group for autograder CodeBuild""
  vpc_id      = aws_vpc.autograder_vpc.id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""10.0.0.0/16""]  # Only allow internal VPC communication
  }
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""ec2:CreateNetworkInterface"",
          ""ec2:DescribeNetworkInterfaces"",
          ""ec2:DeleteNetworkInterface"",
          ""ec2:DescribeVpcs"",
          ""ec2:DescribeSubnets"",
          ""ec2:DescribeSecurityGroups""
        ]
        Resource = [""*""]
      }
    ]
  })
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class""
  service_role  = aws_iam_role.codebuild_role.arn
  build_timeout = ""30""

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-repo/autograder""
    git_clone_depth = 1
  }

  vpc_config {
    vpc_id             = aws_vpc.autograder_vpc.id
    subnets            = [aws_subnet.autograder_subnet.id]
    security_group",,,,,,"Based on the provided examples and the current question, I'll create a Terraform configuration for an autograder using AWS CodeBuild and AWS VPC, ensuring the student code cannot reach the internet. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC Configuration
resource ""aws_vpc"" ""autograder_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""autograder-vpc""
  }
}

# Private Subnet
resource ""aws_subnet"" ""private_subnet"" {
  vpc_id            = aws_vpc.autograder_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-east-1a""

  tags = {
    Name = ""autograder-private-subnet""
  }
}

# Security Group
resource ""aws_security_group"" ""codebuild_sg"" {
  name        = ""codebuild-sg""
  description = ""Security group for CodeBuild""
  vpc_id      = aws_vpc.autograder_vpc.id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = []  # Empty list prevents all outbound internet access
  }
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""ec2:CreateNetworkInterface"",
          ""ec2:DescribeNetworkInterfaces"",
          ""ec2:DeleteNetworkInterface"",
          ""ec2:DescribeVpcs"",
          ""ec2:DescribeSubnets"",
          ""ec2:DescribeSecurityGroups""
        ]
        Resource = ""*""
      }
    ]
  })
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class""
  service_role  = aws_iam_role.codebuild_role.arn
  build_timeout = ""30""

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-repo/autograder""
    git_clone_depth = 1
  }

  vpc_config {
    vpc_id             = aws_vpc.autograder_vpc.id
    subnets            = [aws_subnet.private_subnet.id]
    security_group_ids = [",,,,,
"aws_iam_group, aws_iam_group_membership, aws_iam_user, aws_iam_user",Create two basic AWS IAM users that are a part of a basic IAM group,"package iam_group_two_users
import future.keywords.in

default valid := false

valid {
    some user1 in input.configuration.root_module.resources
    user1.type == ""aws_iam_user""

    some user2 in input.configuration.root_module.resources
    user2.type == ""aws_iam_user""

    not user1 == user2

    some group in input.configuration.root_module.resources
    group.type == ""aws_iam_group""

    some group_membership in input.configuration.root_module.resources
    group_membership.type == ""aws_iam_group_membership""
    user1.address in group_membership.expressions.users.references
    user2.address in group_membership.expressions.users.references
    group.address in group_membership.expressions.group.references
}",3,"resource ""aws_iam_group_membership"" ""team"" {
  name = ""tf-testing-group-membership""

  users = [
    aws_iam_user.user_one.name,
    aws_iam_user.user_two.name,
  ]

  group = aws_iam_group.group.name
}

resource ""aws_iam_group"" ""group"" {
  name = ""test-group""
}

resource ""aws_iam_user"" ""user_one"" {
  name = ""test-user""
}

resource ""aws_iam_user"" ""user_two"" {
  name = ""test-user-two""
}","Has one ""aws_iam_group"" resource
with one ""name"" with any value

Has one ""aws_iam_user"" resource
with one ""name"" with any value

Has another ""aws_iam_user"" resource
with one ""name"" with any value

Has one ""aws_iam_group_membership"" resource
with one ""name"" with any value
with one ""users"" with the two user resource names
with one ""group"" with the group resource name","I'll help you create a Terraform configuration that creates two IAM users and adds them to a single IAM group. Here's the solution:

```hcl
# Create the IAM group
resource ""aws_iam_group"" ""developers"" {
  name = ""developers""
  path = ""/users/""
}

# Create first IAM user
resource ""aws_iam_user"" ""user1"" {
  name = ""developer1""
  path = ""/users/""

  tags = {
    Role = ""Developer""
    Team = ""Engineering""
  }
}

# Create second IAM user
resource ""aws_iam_user"" ""user2"" {
  name = ""developer2""
  path = ""/users/""

  tags = {
    Role = ""Developer""
    Team = ""Engineering""
  }
}

# Add first user to the group
resource ""aws_iam_group_membership"" ""team"" {
  name = ""developers-group-membership""

  users = [
    aws_iam_user.user1.name,
    aws_iam_user.user2.name
  ]

  group = aws_iam_group.developers.name
}
```

This configuration:
1. Creates an IAM group named ""developers""
2. Creates two IAM users named ""developer1"" and ""developer2""
3. Adds both users to the developers group using the aws_iam_group_membership resource
4. Includes appropriate tags for better resource management
5. Uses the /users/ path for both the group and users for consistent organization

The configuration follows AWS best practices and provides a clean way to manage IAM users and groups through Terraform.",,,,,,"I'll help you create a Terraform configuration that creates two IAM users and adds them to a single IAM group. Here's the solution:

```hcl
# Create the IAM group
resource ""aws_iam_group"" ""developers"" {
  name = ""developers""
  path = ""/users/""
}

# Create first IAM user
resource ""aws_iam_user"" ""user1"" {
  name = ""developer1""
  path = ""/users/""

  tags = {
    Role = ""Developer""
    Team = ""Engineering""
  }
}

# Create second IAM user
resource ""aws_iam_user"" ""user2"" {
  name = ""developer2""
  path = ""/users/""

  tags = {
    Role = ""Developer""
    Team = ""Engineering""
  }
}

# Add first user to the group
resource ""aws_iam_group_membership"" ""team"" {
  name = ""developers-group-membership""

  users = [
    aws_iam_user.user1.name,
    aws_iam_user.user2.name
  ]

  group = aws_iam_group.developers.name
}
```

This configuration:
1. Creates an IAM group named ""developers""
2. Creates two IAM users: ""developer1"" and ""developer2""
3. Adds both users to the ""developers"" group using the aws_iam_group_membership resource
4. Includes appropriate tags for the users
5. Uses proper path structure
6. Maintains proper references between resources

All resources are properly defined with correct syntax and structure, following AWS IAM best practices.",,,,,,"I'll help you create a Terraform configuration that creates two IAM users and adds them to a single IAM group. I'll combine the concepts from both example references and add the necessary group membership configuration.

```hcl
# Create the IAM group
resource ""aws_iam_group"" ""developers"" {
  name = ""developers""
  path = ""/users/""
}

# Create first IAM user
resource ""aws_iam_user"" ""user1"" {
  name = ""developer1""
  path = ""/users/""

  tags = {
    Role = ""Developer""
    Team = ""Engineering""
  }
}

# Create second IAM user
resource ""aws_iam_user"" ""user2"" {
  name = ""developer2""
  path = ""/users/""

  tags = {
    Role = ""Developer""
    Team = ""Engineering""
  }
}

# Add users to the group
resource ""aws_iam_group_membership"" ""developer_team"" {
  name = ""developer-group-membership""

  users = [
    aws_iam_user.user1.name,
    aws_iam_user.user2.name
  ]

  group = aws_iam_group.developers.name
}
```

This configuration:
1. Creates an IAM group named ""developers""
2. Creates two IAM users: ""developer1"" and ""developer2""
3. Adds both users to the developers group using the aws_iam_group_membership resource
4. Includes appropriate tags for better resource management
5. Uses proper path structure for both users and group
6. Maintains proper references between resources using the resource attributes

All resources are properly nested and formatted according to HCL syntax requirements.",,,,,
"aws_codebuild_project, aws_iam_role",create a Basic AWS codebuild project resource with example iam role and an example GITHUB source,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for aws_iam_role resource
has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""codebuild.amazonaws.com"") 
} 

has_valid_codebuild {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_codebuild_project""
        resource.values.artifacts
    resource.values.environment
    resource.values.name
    resource.values.source[_].type == ""GITHUB""
    role := input.configuration.root_module.resources[i]
    role.expressions.service_role
}

has_valid_resources {
        has_valid_iam_role(input.resource_changes)
    has_valid_codebuild
}
",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""test_role"" {
  name               = ""test_role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}


resource ""aws_codebuild_project"" ""example"" {
  name          = ""test-project""
  service_role  = aws_iam_role.test_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    location        = ""github.com/source-location""
    git_clone_depth = 1
  }
}","Has one ""aws_iam_role"" resource and one ""aws_codebuild_project"" resource with the ""type"" argument within the ""source"" block equal to ""GITHUB""","I'll help you create a basic AWS CodeBuild project with an IAM role and GitHub source. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-basic-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# Basic CodeBuild project
resource ""aws_codebuild_project"" ""basic_project"" {
  name          = ""basic-build-project""
  description   = ""Basic CodeBuild project with GitHub source""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:5.0""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
  }

  source_version = ""main""
}
```

This configuration creates:
1. A basic IAM role that CodeBuild can assume
2. A CodeBuild project that:
   - Uses GitHub as the source
   - Has no artifacts (but you can add them if needed)
   - Uses the standard Linux container environment
   - Has a 60-minute build timeout

Remember to:
1. Replace the GitHub repository URL (`location`) with your actual repository URL
2. Adjust the region if needed
3. Add any additional IAM permissions to the role based on your specific needs
4. Modify the compute type, image, or other parameters according to your requirements",,,,,,"I'll help you create a basic AWS CodeBuild project with an IAM role and GitHub source. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-basic-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# Basic CodeBuild project
resource ""aws_codebuild_project"" ""basic_project"" {
  name          = ""basic-project""
  description   = ""Basic CodeBuild project with GitHub source""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:7.0""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
  }

  source_version = ""main""
}
```

This configuration creates:
1. A basic IAM role that CodeBuild can assume
2. A CodeBuild project that:
   - Uses GitHub as the source
   - Has no artifacts (but you can add them if needed)
   - Uses the standard Linux container environment
   - Has a 60-minute build timeout

Remember to:
1. Replace the GitHub repository URL (`location`) with your actual repository URL
2. Adjust the region if needed
3. Add any additional IAM policies to the role depending on what your build needs to access

This is a minimal configuration that you can build upon based on your specific requirements.",,,,,,"I'll help you create a basic AWS CodeBuild project with an IAM role and GitHub source. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-basic-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# Basic CodeBuild Project
resource ""aws_codebuild_project"" ""basic_project"" {
  name          = ""basic-project""
  description   = ""Basic CodeBuild project with GitHub source""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:7.0""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
  }

  source_version = ""main""
}
```

This configuration creates:
1. A basic IAM role that CodeBuild can assume
2. A CodeBuild project that:
   - Uses GitHub as the source
   - Has no artifacts (but you can add them if needed)
   - Uses the standard AWS CodeBuild environment
   - Has a 60-minute timeout
   - Uses the smallest compute type for cost efficiency

Remember to:
- Replace the GitHub repository URL with your actual repository
- Add any necessary IAM role policies depending on what your build needs to access
- Adjust the environment settings based on your specific build requirements",,,,,
"aws_chime_voice_connector, aws_chime_voice_connector, aws_chime_voice_connector_group","Create two AWS Chime Voice Connectors, both with encryption. Then create an aws_chime_voice_connector_group resource with each connector as a connector argument","package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc1 in input.configuration.root_module.resources
    vc1.type == ""aws_chime_voice_connector""
    vc1.expressions.require_encryption.constant_value == true

    some vc2 in input.configuration.root_module.resources
    vc2.type == ""aws_chime_voice_connector""
    vc2.expressions.require_encryption.constant_value == true

    not vc1 == vc2
    
    some vcgroup in input.configuration.root_module.resources
    vcgroup.type == ""aws_chime_voice_connector_group""

    some connector1 in vcgroup.expressions.connector
    vc1.address in connector1.voice_connector_id.references
    some connector2 in vcgroup.expressions.connector
    vc2.address in connector2.voice_connector_id.references
    
    not connector1 == connector2
}
",3,"resource ""aws_chime_voice_connector"" ""vc1"" {
  name               = ""connector-test-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""vc2"" {
  name               = ""connector-test-2""
  require_encryption = true
  aws_region         = ""us-west-2""
}

resource ""aws_chime_voice_connector_group"" ""group"" {
  name = ""test-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.vc1.id
    priority           = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.vc2.id
    priority           = 3
  }
}","Has one resource ""aws_chime_voice_connector""
with require_encryption=""true""

Has another resource ""aws_chime_voice_connector""
with require_encryption=""true""

Has one resource ""aws_chime_voice_connector_group""
with a connector for each voice connector with the correct id","Based on the examples provided, I'll help you create two AWS Chime Voice Connectors with encryption and group them together using an aws_chime_voice_connector_group resource. Here's the Terraform configuration:

```hcl
resource ""aws_chime_voice_connector"" ""vc1"" {
  name               = ""connector-test-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""vc2"" {
  name               = ""connector-test-2""
  require_encryption = true
  aws_region         = ""us-west-2""
}

resource ""aws_chime_voice_connector_group"" ""group"" {
  name = ""test-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.vc1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.vc2.id
    priority          = 2
  }
}
```

This configuration:
1. Creates two voice connectors (vc1 and vc2) with encryption enabled
2. Places them in different AWS regions for redundancy
3. Creates a voice connector group that includes both connectors
4. Assigns priorities to each connector (1 and 2, where lower number means higher priority)

The voice connectors will be referenced in the group using their IDs, and the group will manage both connectors as specified in the connector arguments.",,,,,,"I'll help you create two AWS Chime Voice Connectors with encryption and group them together using an aws_chime_voice_connector_group resource. Based on the examples provided, here's the solution:

```hcl
resource ""aws_chime_voice_connector"" ""vc1"" {
  name               = ""connector-test-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""vc2"" {
  name               = ""connector-test-2""
  require_encryption = true
  aws_region         = ""us-west-2""
}

resource ""aws_chime_voice_connector_group"" ""group"" {
  name = ""test-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.vc1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.vc2.id
    priority          = 2
  }
}
```

This configuration:
1. Creates two voice connectors (vc1 and vc2) with encryption enabled
2. Places them in different AWS regions for redundancy
3. Creates a voice connector group that includes both connectors
4. Assigns priorities to each connector (1 and 2, where lower number means higher priority)

The connectors will be referenced in the group using their IDs, and the priority values determine their order of usage in the group.",,,,,,"Based on your request, I'll help you create two AWS Chime Voice Connectors with encryption enabled and group them together using an aws_chime_voice_connector_group resource. Here's the Terraform configuration:

```hcl
resource ""aws_chime_voice_connector"" ""vc1"" {
  name               = ""connector-test-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""vc2"" {
  name               = ""connector-test-2""
  require_encryption = true
  aws_region         = ""us-west-2""
}

resource ""aws_chime_voice_connector_group"" ""group"" {
  name = ""test-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.vc1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.vc2.id
    priority          = 2
  }
}
```

This configuration:
1. Creates two voice connectors (vc1 and vc2) with encryption required
2. Creates a voice connector group that includes both connectors
3. Sets priorities for the connectors (1 and 2, where lower number means higher priority)
4. Places the connectors in different AWS regions for redundancy

The voice connectors will be created first, and then the group will be created with references to both connectors.",,,,,
"aws_cloudfront_distribution, aws_cloudfront_origin_access_control, aws_route53_record, aws_route53_zone, aws_s3_bucket","The infrastructure for a video streaming site that distribute video content globally. Accomplish this using the following services: AWS CloudFront, AWS Route53, and AWS S3. Add any necessary services.","package netflix_middle

import data.set
import rego.v1

bucket_valid(bucket) := true

access_control_valid(access_control) if {
	access_control.expressions.name
	access_control.expressions.origin_access_control_origin_type.constant_value == ""s3""
	access_control.expressions.signing_behavior
	access_control.expressions.signing_protocol
}

distribution_valid(distribution, access_control, bucket) if {
	some origin in distribution.expressions.origin
	bucket.address in origin.domain_name.references
	access_control.address in origin.origin_access_control_id.references

	some cache_behavior in distribution.expressions.default_cache_behavior
	{method | method := cache_behavior.allowed_methods.constant_value[_]} == {""GET"", ""HEAD""}
	{method | method := cache_behavior.cached_methods.constant_value[_]} == {""GET"", ""HEAD""}
	cache_behavior.viewer_protocol_policy.constant_value == ""allow-all""

	origin.origin_id == cache_behavior.target_origin_id

	some restrictions in distribution.expressions.restrictions
	some restriction in restrictions.geo_restriction
	restriction.restriction_type
	restriction.locations
}

zone_valid(zone) if {
	zone.expressions.name
}

record_valid(record, zone, distribution) if {
	record.expressions.type
	record.expressions.name
	zone.address in record.expressions.zone_id.references

	some alias in record.expressions.alias
	distribution.address in alias.name.references
	distribution.address in alias.zone_id.references
	alias.evaluate_target_health
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	some access_control in resources
	access_control.type == ""aws_cloudfront_origin_access_control""

	some distribution in resources
	distribution.type == ""aws_cloudfront_distribution""

	some zone in resources
	zone.type == ""aws_route53_zone""

	some record in resources
	record.type == ""aws_route53_record""

	bucket_valid(bucket)
	access_control_valid(access_control)
	distribution_valid(distribution, access_control, bucket)
	zone_valid(zone)
	record_valid(record, zone, distribution)
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_s3_bucket"" ""video_content"" {
  bucket_prefix = ""video-content-""
}

resource ""aws_s3_object"" ""put_website"" {
  bucket = aws_s3_bucket.video_content.bucket
  key = ""index.html""
  source = ""./supplement/index.html""
  content_type = ""text/html""
}

resource ""aws_s3_object"" ""put_website_css"" {
  bucket = aws_s3_bucket.video_content.bucket
  key = ""css/style.css""
  source = ""./supplement/css/style.css""
  content_type = ""text/css""
}

resource ""aws_s3_bucket_policy"" ""bucket_policy"" {
  bucket = aws_s3_bucket.video_content.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""cloudfront.amazonaws.com""
        }
        Action = ""s3:GetObject""
        Resource = ""${aws_s3_bucket.video_content.arn}/*""
        Condition = {
          StringEquals = {
            ""AWS:SourceArn"" = aws_cloudfront_distribution.my_distribution.arn
          }
        }
      }
    ]
  })
}


locals {
  s3_origin_id = ""s3_video_content_origin""
}

resource ""aws_cloudfront_origin_access_control"" ""s3_oac"" {
  name                              = ""s3-oac""
  description                       = ""aws access control policy""
  origin_access_control_origin_type = ""s3""
  signing_behavior                  = ""always""
  signing_protocol                  = ""sigv4""
}

resource ""aws_cloudfront_distribution"" ""my_distribution"" {
  origin {
    domain_name              = aws_s3_bucket.video_content.bucket_regional_domain_name
    origin_access_control_id = aws_cloudfront_origin_access_control.s3_oac.id
    origin_id                = local.s3_origin_id
  }

  enabled             = true
  default_root_object = ""index.html""

  default_cache_behavior {
    allowed_methods  = [""GET"", ""HEAD""]
    cached_methods   = [""GET"", ""HEAD""]
    target_origin_id = local.s3_origin_id

    forwarded_values {
      query_string = false

      cookies {
        forward = ""none""
      }
    }

    viewer_protocol_policy = ""allow-all""
  }

  price_class = ""PriceClass_200""

  restrictions {
    geo_restriction {
      restriction_type = ""whitelist""
      locations        = [""US"", ""CA"", ""GB"", ""DE""]
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }
}

# Route53
resource ""aws_route53_zone"" ""my_zone"" {
  name = ""my-test-cloudfront.com""
}

resource ""aws_route53_record"" ""cdn_ipv4"" {
  type    = ""A""
  name    = ""cdn""
  zone_id = aws_route53_zone.my_zone.zone_id

  alias {
    name                   = aws_cloudfront_distribution.my_distribution.domain_name
    zone_id                = aws_cloudfront_distribution.my_distribution.hosted_zone_id
    evaluate_target_health = true
  }
}","have one ""aws_cloudfront_origin_access_control"" resource
    with ""name"" argument
    with ""origin_access_control_origin_type"" with value ""s3""
    with ""signing_behavior"" argument
    with ""signing_protocol"" argument
have one ""aws_cloudfront_distribution"" resource
    with ""origin.domain_name"" argument linking to ""aws_s3_bucket"" resource
    with ""origin.origin_access_control_id"" argument linking to ""aws_cloudfront_origin_access_control"" resource
    with ""origin.origin_id"" argument
    with ""default_cache_behavior.allowed_methods"" argument with value [""GET"", ""HEAD""]
    with ""default_cache_behavior.cached_methods"" argument with value [""GET"", ""HEAD""]
    with ""default_cache_behavior.target_origin_id"" argument having the same value as ""origin.origin_id"" argument
    with ""default_cache_behavior.viewer_protocol_policy"" argument with value ""allow-all""
    with ""restrictions.geo_restriction.restriction_type"" argument
    with ""restrictions.geo_restriction.locations"" argument
    with ""viewer_certificate.cloudfront_default_certificate"" argument
have one ""aws_s3_bucket"" resource
have one ""aws_route53_zone"" resource
    with ""name"" argument
have one ""aws_route53_record"" resource
    with ""type"" argument
    with ""name"" argument
    with ""zone_id"" argument linking to ""aws_route53_zone"" resource
    with ""alias.name"" argument linking to ""aws_cloudfront_distribution"" resource
    with ""alias.zone_id"" argument linking to ""aws_cloudfront_distribution"" resource
    with ""alias.evaluate_target_health"" argument","I'll help you create a Terraform configuration for a global video streaming infrastructure using AWS CloudFront, Route53, and S3. This solution will be based on the provided examples while incorporating best practices.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket for video content
resource ""aws_s3_bucket"" ""video_content"" {
  bucket_prefix = ""video-content-""
}

# S3 bucket website configuration
resource ""aws_s3_bucket_website_configuration"" ""video_site"" {
  bucket = aws_s3_bucket.video_content.id

  index_document {
    suffix = ""index.html""
  }
}

# Sample website content
resource ""aws_s3_object"" ""index_html"" {
  bucket       = aws_s3_bucket.video_content.bucket
  key          = ""index.html""
  source       = ""./supplement/index.html""
  content_type = ""text/html""
}

# CloudFront Origin Access Control
resource ""aws_cloudfront_origin_access_control"" ""s3_oac"" {
  name                              = ""video-streaming-oac""
  description                       = ""Video streaming S3 bucket access control""
  origin_access_control_origin_type = ""s3""
  signing_behavior                  = ""always""
  signing_protocol                  = ""sigv4""
}

# S3 bucket policy for CloudFront access
resource ""aws_s3_bucket_policy"" ""allow_cloudfront_access"" {
  bucket = aws_s3_bucket.video_content.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowCloudFrontServicePrincipal""
        Effect    = ""Allow""
        Principal = {
          Service = ""cloudfront.amazonaws.com""
        }
        Action   = ""s3:GetObject""
        Resource = ""${aws_s3_bucket.video_content.arn}/*""
        Condition = {
          StringEquals = {
            ""AWS:SourceArn"" = aws_cloudfront_distribution.video_distribution.arn
          }
        }
      }
    ]
  })
}

locals {
  s3_origin_id = ""video_content_origin""
}

# CloudFront distribution
resource ""aws_cloudfront_distribution"" ""video_distribution"" {
  enabled             = true
  is_ipv6_enabled    = true
  default_root_object = ""index.html""
  price_class         = ""PriceClass_200""

  origin {
    domain_name              = aws_s3_bucket.video_content.bucket_regional_domain_name
    origin_access_control_id = aws_cloudfront_origin_access_control.s3_oac.id
    origin_id                = local.s3_origin_id
  }

  default_cache_behavior {
    allowed_methods  = [""GET"", ""HEAD"", ""OPTIONS""]
    cached_methods   = [""GET"", ""HEAD""]
    target_origin_id = local.s3_origin_id

    forwarded_values {
      query_string = false
      cookies {
        forward = ""none""
      }
    }

    viewer_protocol_policy = ""redirect-to-https""
    min_ttl                = 0
    default_ttl            = 3600
    max_ttl                = 86400
  }

  restrictions {
    geo_restriction {
      restriction_type = ""whitelist""
      locations        = [""US"", ""CA"", ""GB"", ""DE""]
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }

  tags = {
    Environment = ""production""
    Service     = ""video-streaming""
  }
}

# Route53 configuration
resource ""aws_route53_zone"" ""main"" {",,,,,,"I'll help you create a Terraform configuration for a global video streaming infrastructure using AWS CloudFront, Route53, and S3. Based on the provided examples, I'll create a solution that includes all necessary components and follows best practices.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket for video content
resource ""aws_s3_bucket"" ""video_content"" {
  bucket_prefix = ""video-content-""
}

# S3 bucket policy to allow CloudFront access
resource ""aws_s3_bucket_policy"" ""bucket_policy"" {
  bucket = aws_s3_bucket.video_content.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""cloudfront.amazonaws.com""
        }
        Action = ""s3:GetObject""
        Resource = ""${aws_s3_bucket.video_content.arn}/*""
        Condition = {
          StringEquals = {
            ""AWS:SourceArn"" = aws_cloudfront_distribution.video_distribution.arn
          }
        }
      }
    ]
  })
}

locals {
  s3_origin_id = ""video_content_origin""
}

# CloudFront Origin Access Control
resource ""aws_cloudfront_origin_access_control"" ""video_oac"" {
  name                              = ""video-streaming-oac""
  description                       = ""Origin Access Control for video streaming""
  origin_access_control_origin_type = ""s3""
  signing_behavior                  = ""always""
  signing_protocol                  = ""sigv4""
}

# CloudFront Distribution
resource ""aws_cloudfront_distribution"" ""video_distribution"" {
  origin {
    domain_name              = aws_s3_bucket.video_content.bucket_regional_domain_name
    origin_access_control_id = aws_cloudfront_origin_access_control.video_oac.id
    origin_id                = local.s3_origin_id
  }

  enabled             = true
  default_root_object = ""index.html""
  price_class         = ""PriceClass_200""

  default_cache_behavior {
    allowed_methods  = [""GET"", ""HEAD""]
    cached_methods   = [""GET"", ""HEAD""]
    target_origin_id = local.s3_origin_id

    forwarded_values {
      query_string = false
      cookies {
        forward = ""none""
      }
    }

    viewer_protocol_policy = ""redirect-to-https""
    min_ttl                = 0
    default_ttl            = 3600
    max_ttl                = 86400
  }

  restrictions {
    geo_restriction {
      restriction_type = ""whitelist""
      locations        = [""US"", ""CA"", ""GB"", ""DE""]
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }

  tags = {
    Environment = ""production""
    Service     = ""video-streaming""
  }
}

# Route53 Zone
resource ""aws_route53_zone"" ""video_zone"" {
  name = ""video-streaming.example.com""
}

# Route53 Record for CloudFront Distribution
resource ""aws_route53_record"" ""video_distribution"" {
  zone_id = aws_route53_zone.video_zone.zone_id
  name    = ""stream""
  type    = ""A""

  alias {
    name                   = aws_cloudfront_distribution.video_distribution.domain_name
    zone_id                = aws_cloudfront_distribution.video_distribution.hosted_zone_id
    evaluate_target_health = false
  }
}

# Output the CloudFront distribution domain name
output ""cloudfront_domain_name"" {
  value = aws_cloudfront_distribution.",,,,,,"I'll help you create a Terraform configuration for a global video streaming infrastructure using AWS CloudFront, Route53, and S3. This solution will be based on the provided examples while incorporating best practices.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 Bucket for video content
resource ""aws_s3_bucket"" ""video_content"" {
  bucket_prefix = ""video-content-""
}

# S3 bucket policy to allow CloudFront access
resource ""aws_s3_bucket_policy"" ""bucket_policy"" {
  bucket = aws_s3_bucket.video_content.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""cloudfront.amazonaws.com""
        }
        Action = ""s3:GetObject""
        Resource = ""${aws_s3_bucket.video_content.arn}/*""
        Condition = {
          StringEquals = {
            ""AWS:SourceArn"" = aws_cloudfront_distribution.video_distribution.arn
          }
        }
      }
    ]
  })
}

# CloudFront Origin Access Control
resource ""aws_cloudfront_origin_access_control"" ""video_oac"" {
  name                              = ""video-streaming-oac""
  description                       = ""Origin Access Control for video streaming""
  origin_access_control_origin_type = ""s3""
  signing_behavior                  = ""always""
  signing_protocol                  = ""sigv4""
}

locals {
  s3_origin_id = ""video_content_origin""
}

# CloudFront Distribution
resource ""aws_cloudfront_distribution"" ""video_distribution"" {
  origin {
    domain_name              = aws_s3_bucket.video_content.bucket_regional_domain_name
    origin_access_control_id = aws_cloudfront_origin_access_control.video_oac.id
    origin_id                = local.s3_origin_id
  }

  enabled             = true
  default_root_object = ""index.html""
  price_class         = ""PriceClass_200""

  default_cache_behavior {
    allowed_methods  = [""GET"", ""HEAD""]
    cached_methods   = [""GET"", ""HEAD""]
    target_origin_id = local.s3_origin_id

    forwarded_values {
      query_string = false
      cookies {
        forward = ""none""
      }
    }

    viewer_protocol_policy = ""redirect-to-https""
    min_ttl                = 0
    default_ttl            = 3600
    max_ttl                = 86400
  }

  restrictions {
    geo_restriction {
      restriction_type = ""whitelist""
      locations        = [""US"", ""CA"", ""GB"", ""DE""]
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }

  tags = {
    Environment = ""production""
    Service     = ""video-streaming""
  }
}

# Route53 Zone
resource ""aws_route53_zone"" ""streaming_zone"" {
  name = ""streaming.example.com""
}

# Route53 A Record for CloudFront
resource ""aws_route53_record"" ""cdn_alias"" {
  zone_id = aws_route53_zone.streaming_zone.zone_id
  name    = ""cdn.streaming.example.com""
  type    = ""A""

  alias {
    name                   = aws_cloudfront_distribution.video_distribution.domain_name
    zone_id                = aws_cloudfront_distribution.video_distribution.hosted_zone_id
    evaluate_target_health = true
  }
}

# Output values
output ""cloudfront_domain_name"" {
  value = aws_cloudfront_distribution.video_distribution.domain_name",,,,,
aws_db_instance,create a basic AWS SQL database,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        requirement1(resource.expressions)
        requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
        expressions.allocated_storage
        expressions.engine.constant_value == ""mysql""
    expressions.username
        is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
        expressions.instance_class
        is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
        expressions.password
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

requirement3(expressions) {
        expressions.snapshot_identifier
}

requirement3(expressions) {
        expressions.replicate_source_db
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 10
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""","I'll help you create a basic AWS SQL (RDS) database using Terraform. Based on the examples provided, I'll create a secure and standard configuration.

```hcl
# Define the AWS provider
provider ""aws"" {
  region = ""us-west-2""  # Specify your desired AWS region
}

# Create the RDS instance
resource ""aws_db_instance"" ""main"" {
  identifier           = ""my-sql-database""
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  db_name             = ""mydb""
  username            = ""admin""
  password            = ""your-secure-password""  # Replace with secure password

  # Additional security and maintenance settings
  skip_final_snapshot = true
  publicly_accessible = false
  multi_az           = false

  # Backup configuration
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  # Performance and security settings
  parameter_group_name = ""default.mysql8.0""
  port                = 3306

  # Tags
  tags = {
    Name        = ""my-sql-database""
    Environment = ""production""
  }
}
```

This configuration creates a basic MySQL database in AWS RDS with the following features:

1. Uses MySQL 8.0 as the database engine
2. Allocates 20GB of GP2 storage
3. Uses db.t3.micro instance type (suitable for development/testing)
4. Includes basic backup configuration
5. Sets up maintenance windows
6. Includes necessary security settings
7. Adds basic tagging

Remember to:
1. Replace ""your-secure-password"" with a strong password
2. Adjust the region as needed
3. Modify the allocated storage, instance class, and other parameters based on your specific requirements
4. Consider enabling encryption at rest if handling sensitive data
5. Review and adjust the backup and maintenance windows according to your needs

Also note that this is a basic configuration. For production environments, you might want to add additional security measures like:
- VPC configuration
- Security groups
- Enhanced monitoring
- Performance insights
- Multi-AZ deployment",,,,,,"Based on the examples provided and the request to create a basic AWS SQL database, I'll create a Terraform configuration that sets up a basic AWS RDS instance with MySQL. Here's a secure and standardized configuration:

```hcl
# Define the AWS provider
provider ""aws"" {
  region = ""us-east-1""  # Specify your desired AWS region
}

# Create the RDS instance
resource ""aws_db_instance"" ""main"" {
  identifier           = ""my-sql-database""
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  db_name             = ""mydb""
  username            = ""admin""
  password            = ""your-secure-password""
  
  # Security settings
  multi_az            = false
  publicly_accessible = false
  
  # Backup and maintenance
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""
  
  # Additional settings
  parameter_group_name = ""default.mysql8.0""
  skip_final_snapshot  = true
  
  # Tags
  tags = {
    Name        = ""my-sql-database""
    Environment = ""production""
  }
}
```

This configuration includes:

1. An AWS provider block specifying the region
2. An RDS instance resource with:
   - Basic configuration (storage, engine, instance class)
   - Security settings
   - Backup and maintenance windows
   - Parameter group specification
   - Appropriate tags for resource management

The configuration uses MySQL 8.0 with general-purpose SSD storage and includes best practices for backup and maintenance windows. Remember to replace ""your-secure-password"" with a strong password before deploying.",,,,,,"Based on the examples provided and the current question to create a basic AWS SQL database, I'll create a Terraform configuration that sets up a basic AWS RDS instance with MySQL. Here's a secure and straightforward implementation:

```hcl
# Configure the AWS Provider
provider ""aws"" {
  region = ""us-west-2""  # Specify your desired AWS region
}

# Create the RDS instance
resource ""aws_db_instance"" ""main"" {
  identifier           = ""my-sql-database""
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  db_name             = ""mydb""
  username            = ""admin""
  password            = ""your-secure-password""
  
  # Additional security settings
  skip_final_snapshot = true
  publicly_accessible = false
  
  # Backup and maintenance settings
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""
  
  # Performance and monitoring
  monitoring_interval    = 0
  parameter_group_name   = ""default.mysql8.0""
  
  tags = {
    Name        = ""my-sql-database""
    Environment = ""production""
  }
}
```

This configuration:
1. Sets up an AWS provider
2. Creates an RDS instance with MySQL 8.0
3. Includes basic security, backup, and maintenance settings
4. Uses GP2 storage type with 20GB allocation
5. Uses the cost-effective db.t3.micro instance class
6. Includes proper tagging for resource management

Remember to replace ""your-secure-password"" with a strong password that meets AWS RDS requirements. Also, adjust the region, storage size, and instance class according to your specific needs.",,,,,
"aws_internet_gateway, aws_route_table, aws_vpc","sets up a dedicated-tenancy AWS VPC with DNS support and hostnames enabled, an internet gateway, and a route table for outbound internet access, all tagged with a variable name.","package terraform.validation

default is_valid_vpc = false
default is_valid_internet_gateway = false
default is_valid_route_table = false

# Validate aws_vpc resource
is_valid_vpc {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_vpc""
    resource.expressions.cidr_block != null
    resource.expressions.enable_dns_support.constant_value == true
    resource.expressions.enable_dns_hostnames.constant_value == true
}

# Validate aws_internet_gateway resource
is_valid_internet_gateway {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_internet_gateway""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.dgraph.id""
    resource.expressions.tags.constant_value[""Name""] != null
}

# Validate aws_route_table resource
is_valid_route_table {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route_table""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.dgraph.id""
    resource.expressions.route.references[0] == ""aws_internet_gateway.dgraph_gw.id""
    resource.expressions.tags.constant_value[""Name""] != null
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""dgraph"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_support   = true
  instance_tenancy     = ""dedicated""

  # For enabling assignment of private dns addresses within AWS.
  enable_dns_hostnames = true

  tags = {
    Name = ""var.name""
  }
}

resource ""aws_internet_gateway"" ""dgraph_gw"" {
  vpc_id = aws_vpc.dgraph.id

  tags = {
    Name = ""var.name""
  }
}

resource ""aws_route_table"" ""dgraph_igw"" {
  vpc_id = aws_vpc.dgraph.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.dgraph_gw.id
  }

  tags = {
    Name = ""var.name""
  }
}","Has one ""aws_vpc"" resource
with a specified ""cidr_block""
with ""enable_dns_support"" and ""enable_dns_hostnames"" set to true

Has one ""aws_internet_gateway"" resource
with ""vpc_id"" referencing the ""aws_vpc"" resource
with a ""tags"" map including a ""Name"" key referencing a variable

Has one ""aws_route_table"" resource
with ""vpc_id"" referencing the ""aws_vpc"" resource
containing a default route for internet access through the ""aws_internet_gateway""
with a ""tags"" map including a ""Name"" key referencing a variable","Based on the provided examples and the current question, I'll help you create a Terraform configuration for setting up a dedicated-tenancy AWS VPC with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

variable ""name"" {
  description = ""Name tag for the resources""
  type        = string
}

resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_support   = true
  enable_dns_hostnames = true
  instance_tenancy     = ""dedicated""

  tags = {
    Name = var.name
  }
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = var.name
  }
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = var.name
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC with dedicated tenancy, DNS support, and DNS hostnames enabled
3. Attaches an Internet Gateway to the VPC
4. Creates a route table with a route to the Internet Gateway
5. All resources are tagged with the variable ""name""
6. Uses proper reference syntax for resource dependencies
7. Follows HCL formatting standards

You can apply this configuration by setting the `name` variable value either through a tfvars file or command line argument.",,,,,,"Based on the examples provided and the current question, I'll help you create a Terraform configuration for setting up a dedicated-tenancy AWS VPC with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

variable ""name"" {
  description = ""Name tag for the resources""
  type        = string
}

resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_support   = true
  enable_dns_hostnames = true
  instance_tenancy     = ""dedicated""

  tags = {
    Name = var.name
  }
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = var.name
  }
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = var.name
  }
}
```

This configuration:
1. Sets up the required AWS provider and terraform version
2. Defines a variable for the name tag
3. Creates a VPC with dedicated tenancy, DNS support, and DNS hostnames enabled
4. Creates an Internet Gateway attached to the VPC
5. Creates a route table with a route to the internet via the Internet Gateway
6. Tags all resources with the provided name variable

To use this configuration, you would need to provide a value for the `name` variable either through a tfvars file or when running terraform apply.",,,,,,"Based on the provided examples and the current question, I'll help you create a Terraform configuration for setting up a dedicated-tenancy AWS VPC with the required components. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

variable ""name"" {
  description = ""Name tag for the resources""
  type        = string
}

resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_support   = true
  enable_dns_hostnames = true
  instance_tenancy     = ""dedicated""

  tags = {
    Name = var.name
  }
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = var.name
  }
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = var.name
  }
}
```

This configuration:
1. Sets up the required AWS provider and version constraints
2. Creates a VPC with dedicated tenancy, DNS support, and DNS hostnames enabled
3. Attaches an Internet Gateway to the VPC
4. Creates a route table with a route to the Internet Gateway
5. Tags all resources with the provided name variable
6. Uses proper resource dependencies through reference expressions

You can use this configuration by setting the `name` variable when applying the configuration:
```
terraform apply -var=""name=my-vpc""
```",,,,,
"aws_elastic_beanstalk_application, aws_elastic_beanstalk_application_version, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment, aws_route53_record, aws_route53_zone, aws_s3_bucket, aws_s3_object","Configure Route 53 resources to manage a domain and directs traffic to an Elastic Beanstalk environment. 
Name the zone ""main"", the environment ""eb-env"", the iam role ""eb_ec2_role"", the instance profile ""eb_ec2_profile"", the bucket (with unique name), the object ""examplebucket_object"", the application ""myapp"" and the application version ""version""","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_s3_bucket = false

default is_valid_s3_object = false

default is_valid_eb_app = false

default is_valid_eb_env = false

default is_valid_r53_zone = false

default is_valid_r53_record = false



# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.name
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records.references[0] == ""aws_elastic_beanstalk_environment.eb-env.cname""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.main.zone_id""

}

is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0] == ""aws_iam_role.eb_ec2_role.name""
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0] == ""aws_iam_role.eb_ec2_role.name""
}

is_valid_s3_bucket {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    resource.name
    # resource.expressions.bucket.constant_value == ""sampleapril26426""
}

is_valid_s3_object {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_s3_object""
    # resource.expressions.bucket.references[0] == ""aws_s3_bucket.sampleapril26426.id""
    resource.expressions.key
    resource.expressions.source
    
}

# Validate aws_eb_app
is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}

is_valid_eb_app_version {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application_version""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.myapp.name""
    resource.expressions.bucket.references[0] == ""aws_s3_object.examplebucket_object.bucket""
    resource.expressions.key.references[0] == ""aws_s3_object.examplebucket_object.key""
}
# Validate aws_eb_env
is_valid_eb_env {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.myapp.name""
    resource.expressions.solution_stack_name
    resource.expressions.version_label.references[0] == ""aws_elastic_beanstalk_application_version.version.name""
        some a
    resource.expressions.setting[a].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
}


# Combine all checks into a final rule
is_configuration_valid {
        is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
        is_valid_r53_zone
        is_valid_r53_record
    is_valid_s3_bucket
    is_valid_s3_object 
    is_valid_eb_app_version
    is_valid_eb_app
    is_valid_eb_env
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}


# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Assuming you have a hosted zone for your domain in Route 53.
# Route 53 Hosted Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example56.com""
}

# DNS Record to point to the Elastic Beanstalk environment
resource ""aws_route53_record"" ""www"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""app.example56.com""
  type    = ""CNAME""
  ttl     = 60
  records = [aws_elastic_beanstalk_environment.eb-env.cname]
}

resource ""aws_s3_bucket"" ""sample_bucket"" {
  bucket_prefix = ""sample-""
}

resource ""aws_s3_object"" ""examplebucket_object"" {
  key    = ""randofile""
  bucket = aws_s3_bucket.sample_bucket.id
  source = ""./supplement/app.zip""
}

# Define the Elastic Beanstalk application
resource ""aws_elastic_beanstalk_application"" ""myapp"" {
  name        = ""something-application""
}

# Define the Elastic Beanstalk application version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""app-version""
  application = aws_elastic_beanstalk_application.myapp.name
  bucket = aws_s3_object.examplebucket_object.bucket
  key    = aws_s3_object.examplebucket_object.key
}


# Elastic Beanstalk environment
resource ""aws_elastic_beanstalk_environment"" ""eb-env"" {
  name                = ""awesomeenv""
  application         = aws_elastic_beanstalk_application.myapp.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  version_label = aws_elastic_beanstalk_application_version.version.name

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""

Has one ""aws_s3_bucket"" resource
    with ""bucket""

Has one ""aws_s3_object"" resource
    with ""key""
    with ""source""
    with ""bucket"" referencing the ""aws_s3_bucket"" resource

Has one ""aws_elastic_beanstalk_application_version"" resource
    with ""name
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""bucket"" referencing the ""bucket"" from the ""aws_s3_object"" resource
    with ""key"" referencing the ""key"" from the ""aws_s3_object"" resource

Has one ""aws_elastic_beanstalk_environment"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with ""version_label"" referencing the ""aws_elastic_beanstalk_application_version"" resource
    with one ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource

Has one ""aws_route53_zone"" resource
    with ""name""

Has one ""aws_route53_record"" resources
    with ""name""
    with ""type""
    with ""ttl""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
    with ""records"" referencing one of the ""aws_elastic_beanstalk_environment"" resources
","Based on the examples provided, I'll help you create a Terraform configuration that sets up Route 53 resources and an Elastic Beanstalk environment with all the specified names. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

resource ""aws_s3_bucket"" ""app_bucket"" {
  bucket = ""my-unique-bucket-name-${random_string.bucket_suffix.result}""
}

resource ""random_string"" ""bucket_suffix"" {
  length  = 8
  special = false
  upper   = false
}

resource ""aws_s3_object"" ""examplebucket_object"" {
  key    = ""application-version.zip""
  bucket = aws_s3_bucket.app_bucket.id
  source = ""./supplement/app.zip""
}

resource ""aws_elastic_beanstalk_application"" ""myapp"" {
  name = ""my-application""
}

resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""app-version""
  application = aws_elastic_beanstalk_application.myapp.name
  bucket      = aws_s3_bucket.app_bucket.id
  key         = aws_s3_object.examplebucket_object.key
}

resource ""aws_elastic_beanstalk_environment"" ""eb-env"" {
  name                = ""my-environment""
  application         = aws_elastic_beanstalk_application.myapp.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  version_label       = aws_elastic_beanstalk_application_version.version.name

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

resource ""aws_route53_record"" ""www"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""
  type    = ""CNAME""
  ttl     = 300
  records = [aws_elastic_beanstalk_environment.eb-env.cname]
}
```

This configuration:
1. Creates an IAM role and instance profile for Elastic Beanstalk
2. Creates a unique S3 bucket using a random suffix
3. Creates an S3 object for the application code
4. Sets up an Elastic Beanstalk application and version
5. Creates an Elastic Beanstalk environment
6. Sets up Route 53 zone and record to direct traffic to the Elastic Beanstalk environment

All resources are name",,,,,,"I'll help you create a Terraform configuration that sets up Route 53 resources with Elastic Beanstalk environment based on your requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

resource ""aws_s3_bucket"" ""app_bucket"" {
  bucket = ""my-unique-eb-app-bucket-name-2024""
}

resource ""aws_s3_object"" ""examplebucket_object"" {
  key    = ""application-package""
  bucket = aws_s3_bucket.app_bucket.id
  source = ""./app.zip""  # Make sure this file exists in your local directory
}

resource ""aws_elastic_beanstalk_application"" ""myapp"" {
  name        = ""my-application""
  description = ""My Application""
}

resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""app-version-1.0""
  application = aws_elastic_beanstalk_application.myapp.name
  bucket      = aws_s3_bucket.app_bucket.id
  key         = aws_s3_object.examplebucket_object.key
}

resource ""aws_elastic_beanstalk_environment"" ""eb-env"" {
  name                = ""my-environment""
  application         = aws_elastic_beanstalk_application.myapp.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  version_label      = aws_elastic_beanstalk_application_version.version.name

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

resource ""aws_route53_record"" ""www"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""
  type    = ""CNAME""
  ttl     = 300
  records = [aws_elastic_beanstalk_environment.eb-env.cname]
}
```

This configuration:
1. Creates an IAM role and instance profile for Elastic Beanstalk
2. Sets up an S3 bucket and object for the application code
3. Creates an Elastic Beanstalk application and version
4. Configures an Elastic Beanstalk environment
5. Sets up Route 53 zone and record to direct traffic to the Elastic Beanstalk environment

Note: Make sure to:
- Replace the S3 bucket name with a globally unique name
- Have an application package (app.zip) in your local directory
- Update the",,,,,,"I'll help you create a Terraform configuration that sets up Route 53 resources and an Elastic Beanstalk environment with the specified names. Based on Example 1 and Example 2, I'll combine the relevant components and adjust them according to your requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

resource ""aws_s3_bucket"" ""app_bucket"" {
  bucket = ""my-unique-eb-app-bucket-${random_string.bucket_suffix.result}""
}

resource ""random_string"" ""bucket_suffix"" {
  length  = 8
  special = false
  upper   = false
}

resource ""aws_s3_object"" ""examplebucket_object"" {
  key    = ""application-package.zip""
  bucket = aws_s3_bucket.app_bucket.id
  source = ""./supplement/app.zip""
}

resource ""aws_elastic_beanstalk_application"" ""myapp"" {
  name = ""my-application""
}

resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""app-version""
  application = aws_elastic_beanstalk_application.myapp.name
  bucket      = aws_s3_bucket.app_bucket.id
  key         = aws_s3_object.examplebucket_object.key
}

resource ""aws_elastic_beanstalk_environment"" ""eb-env"" {
  name                = ""my-environment""
  application         = aws_elastic_beanstalk_application.myapp.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  version_label       = aws_elastic_beanstalk_application_version.version.name

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

resource ""aws_route53_record"" ""www"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""
  type    = ""CNAME""
  ttl     = 300
  records = [aws_elastic_beanstalk_environment.eb-env.cname]
}
```

This configuration:
1. Creates an IAM role and instance profile for Elastic Beanstalk
2. Creates a unique S3 bucket using a random suffix
3. Sets up the Elastic Beanstalk application and version
4. Creates the Elastic Beanstalk environment
5. Sets up Route 53 zone and record to direct traffic to the Elastic Beanstalk environment

The names",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer with IP Target Group,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.values.subnet_mapping != null
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.ipv4_ipam_pool_id.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.ipv6_cider_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value != ""forward""
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    resources.values.target_type == ""ip""
    
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group""
        resource.expressions.port != null
        resource.expressions.protocol != null
        resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
        is_valid_lb
        is_valid_lb_listener
        is_valid_lb_target_group_attachment
        is_valid_lb_target_group
        is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.public_subnets[0]
  security_groups = [ aws_security_group.instance-sg.id ]
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
  target_type = ""ip""
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.private_ip
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id
    with target_type = ""ip""

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you create a Terraform configuration for an AWS Load Balancer with an IP Target Group. This is similar to the previous examples but modified to use IP targets instead of instance targets.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs            = data.aws_availability_zones.available.names
  public_subnets = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

resource ""aws_security_group"" ""lb-sg"" {
  name   = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port         = 80
  to_port          = 80
  ip_protocol      = ""tcp""
  cidr_ipv4        = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port         = 0
  to_port          = 0
  ip_protocol      = ""-1""
  cidr_ipv4        = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_lb"" ""ip_target_lb"" {
  name               = ""ip-target-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
  subnets           = module.vpc.public_subnets
}

resource ""aws_lb_target_group"" ""ip_target_group"" {
  name        = ""ip-target-group""
  port        = 80
  protocol    = ""HTTP""
  target_type = ""ip""
  vpc_id      = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 3
  }
}

resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.ip_target_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.ip_target_group.arn
  }
}

# Example of adding an IP target to the target group
resource ""aws_lb_target_group_attachment"" ""ip_target"" {
  target_group_arn = aws_lb_target_group.ip_target_group.arn
  target_id        = ""10.0.4.10""  # Replace with your desired IP address
  port             = 80
}
```

This configuration creates:
1. A VPC with public subnets
2. An Application Load Balancer (ALB)
3. A security group for the ALB
4. An IP-based target group (specified by `target_type = ""ip""`)
5. A listener that forwards traffic to the target group
6. An example target group attachment for an IP address

Key differences from the instance-based target group:
- The target group's `target_type` is set to ""ip""",,,,,,"I'll help you create a Terraform configuration for an AWS Load Balancer with an IP Target Group. This configuration will be based on the examples provided but modified to use IP targets instead of instance targets.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs            = data.aws_availability_zones.available.names
  public_subnets = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

resource ""aws_security_group"" ""lb-sg"" {
  name   = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port         = 80
  to_port          = 80
  ip_protocol      = ""tcp""
  cidr_ipv4        = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port         = 0
  to_port          = 0
  ip_protocol      = ""-1""
  cidr_ipv4        = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_lb"" ""ip_target_lb"" {
  name               = ""ip-target-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
  subnets           = module.vpc.public_subnets
}

resource ""aws_lb_target_group"" ""ip_target_group"" {
  name        = ""ip-target-group""
  port        = 80
  protocol    = ""HTTP""
  target_type = ""ip""
  vpc_id      = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 3
  }
}

resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.ip_target_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.ip_target_group.arn
  }
}

# Example of adding an IP target to the target group
resource ""aws_lb_target_group_attachment"" ""ip_target"" {
  target_group_arn = aws_lb_target_group.ip_target_group.arn
  target_id        = ""10.0.4.10""  # Replace with your desired IP address
  port             = 80
}
```

Key differences from the instance-based target group example:

1. The target group is configured with `target_type = ""ip""` instead of the default instance type.
2. The target group attachment uses an IP address as the `target_id` instead of an instance ID.
3. Added health check configuration in the target group for better control over target health monitoring.
4. Simplified the configuration by removing instance-related resources since we're using IP targets.

This configuration creates:
- A VPC with",,,,,,"I'll help you create a Terraform configuration for an AWS Load Balancer with an IP Target Group. This is similar to the previous examples but modified to use IP targets instead of instance targets.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs            = data.aws_availability_zones.available.names
  public_subnets = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

# Security Group for the Load Balancer
resource ""aws_security_group"" ""lb-sg"" {
  name   = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port         = 80
  to_port          = 80
  ip_protocol      = ""tcp""
  cidr_ipv4        = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port         = 0
  to_port          = 0
  ip_protocol      = ""-1""
  cidr_ipv4        = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

# Application Load Balancer
resource ""aws_lb"" ""application_lb"" {
  name               = ""ip-target-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
  subnets           = module.vpc.public_subnets
}

# Target Group for IP targets
resource ""aws_lb_target_group"" ""ip_target_group"" {
  name        = ""ip-target-group""
  port        = 80
  protocol    = ""HTTP""
  target_type = ""ip""
  vpc_id      = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval           = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 3
  }
}

# Listener for the Load Balancer
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.application_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.ip_target_group.arn
  }
}

# Example of attaching an IP target to the target group
resource ""aws_lb_target_group_attachment"" ""ip_target"" {
  target_group_arn = aws_lb_target_group.ip_target_group.arn
  target_id        = ""10.0.4.10""  # Replace with your desired IP address
  port             = 80
}
```

This configuration creates:
1. A VPC with public subnets
2. A security group for the load balancer
3. An Application Load Balancer
4. An IP-based target group
5. A listener that forwards traffic to the target group
6. An example target group attachment for an IP address

Key differences from the instance-based target group:
-",,,,,
"aws_db_instance, aws_db_instance, aws_elastic_beanstalk_application, aws_elastic_beanstalk_environment, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment","Configure production and staging Elastic Beanstalk environments with a respective db_instance from RDS. Name the instance profile ""eb_ec2_profile"", the production database ""prod_db"", and the staging database ""staging_db""
","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_eb_app = false

default is_valid_eb_env = false

default is_valid_db_instance = false


is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0]
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0]
}

# Validate aws_eb_app
is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}

# Validate aws_eb_env
is_valid_eb_env {
        some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.expressions.name
    resource.expressions.application.references[0]
    resource.expressions.solution_stack_name
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_elastic_beanstalk_environment""
    resource2.expressions.name
    resource2.expressions.application.references[0]
    resource2.expressions.solution_stack_name
    some a, b, c, d, e, f, g, h
    resource.expressions.setting[a].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource.expressions.setting[b].value.references[0] == ""aws_db_instance.prod_db.username""
    resource.expressions.setting[c].value.references[0] == ""aws_db_instance.prod_db.password""
    resource.expressions.setting[d].value.references[0] == ""aws_db_instance.prod_db.address""
    resource2.expressions.setting[e].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource2.expressions.setting[f].value.references[0] == ""aws_db_instance.staging_db.username""
    resource2.expressions.setting[g].value.references[0] == ""aws_db_instance.staging_db.password""
    resource2.expressions.setting[h].value.references[0] == ""aws_db_instance.staging_db.address""

}

is_valid_db_instance {
                some i, j
        resource := input.configuration.root_module.resources[i]
    resource2 := input.configuration.root_module.resources[j]
    resource.type == ""aws_db_instance""
    resource2.type == ""aws_db_instance""
    has_required_main_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_main_db_arguments {
                some i, j
    resource := input.configuration.root_module.resources[i]
    resource2 := input.configuration.root_module.resources[j]
    resource.type == ""aws_db_instance""
    resource2.type == ""aws_db_instance""
    # Check for allocated_storage, engine, instance_class, username, password, and skip_final_snapshot
    requirement1(resource.expressions)
    requirement1(resource2.expressions)
    # Check for instance_class validity
    requirement2(resource.expressions)
    requirement2(resource2.expressions)
}



# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
    expressions.allocated_storage
    expressions.engine
    expressions.username
    expressions.password
    is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
    expressions.snapshot_identifier
}

# Check for instance_class validity
requirement2(expressions) {
    expressions.instance_class
    is_valid_instance_class(expressions.instance_class.constant_value)
}


# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}


# Combine all checks into a final rule
is_configuration_valid {
        is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
    is_valid_eb_app
    is_valid_eb_env
    is_valid_db_instance
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}


# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}


# RDS Database for Production
resource ""aws_db_instance"" ""prod_db"" {
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  username             = ""dbuser""
  password             = ""prodsecret""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
  identifier = ""production""
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""my_app"" {
  name        = ""my-app""
  description = ""My awesome application""
}

# Elastic Beanstalk Environment for Production
resource ""aws_elastic_beanstalk_environment"" ""prod_env"" {
  name                = ""my-app-prod""
  application         = aws_elastic_beanstalk_application.my_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.prod_db.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.prod_db.username
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.prod_db.password
  }

  setting {
      namespace = ""aws:autoscaling:launchconfiguration""
      name      = ""IamInstanceProfile""
      value     = aws_iam_instance_profile.eb_ec2_profile.name
    }
}

# RDS Database for Staging
resource ""aws_db_instance"" ""staging_db"" {
  allocated_storage    = 10
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  username             = ""dbuser""
  password             = ""stagingsecret""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
  identifier = ""staging""
}

# Elastic Beanstalk Environment for Staging
resource ""aws_elastic_beanstalk_environment"" ""staging_env"" {
  name                = ""my-app-staging""
  application         = aws_elastic_beanstalk_application.my_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.staging_db.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.staging_db.username
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.staging_db.password
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""

Has a ""aws_elastic_beanstalk_environment"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with one ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource
    with a ""setting"" block that has a value that references the ""address"" from the production ""db_instance"" resource
    with a ""setting"" block that has a value that references the ""username"" from the production ""db_instance"" resource 
    with a ""setting"" block that has a value that references the ""password"" from the production ""db_instance"" resource

Has a ""aws_elastic_beanstalk_environment"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with one ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource
    with a ""setting"" block that has a ""value""  that references the ""address"" from the staging ""db_instance"" resource
    with a ""setting"" block that has a ""value""  that references the ""username"" from the staging ""db_instance"" resource 
    with a ""setting"" block that has a ""value""  that references the ""password"" from the staging ""db_instance"" resource  

Has two ""aws_db_instance"" resources
    with the ""identifier"" for one set to ""production"" and ""identifier"" for the other set to ""staging""
    with ""instance_class""
    with ""allocated_storage""
    with ""engine""
    with ""username""
    with ""password""
    with ""backup_retention_period"" set to a non-zero number
    with ""skip_final_snapshot""

","I'll help you create a Terraform configuration for production and staging Elastic Beanstalk environments with their respective RDS instances. This solution will be based on the examples provided while incorporating your specific naming requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM Role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Production RDS Instance
resource ""aws_db_instance"" ""prod_db"" {
  identifier           = ""prod-db""
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class      = ""db.t3.medium""
  username            = ""produser""
  password            = ""prodpassword123""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot = true
  multi_az            = true
  backup_retention_period = 7
}

# Staging RDS Instance
resource ""aws_db_instance"" ""staging_db"" {
  identifier           = ""staging-db""
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class      = ""db.t3.micro""
  username            = ""staginguser""
  password            = ""stagingpassword123""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot = true
  multi_az            = false
  backup_retention_period = 3
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""app"" {
  name        = ""my-application""
  description = ""Production and Staging Application""
}

# Production Environment
resource ""aws_elastic_beanstalk_environment"" ""production"" {
  name                = ""production-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  tier                = ""WebServer""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.prod_db.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USER""
    value     = aws_db_",,,,,,"I'll help you create a Terraform configuration for production and staging Elastic Beanstalk environments with their respective RDS instances. Based on the examples provided, I'll create a configuration that follows similar patterns but adapts to your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM Role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Production RDS Instance
resource ""aws_db_instance"" ""prod_db"" {
  identifier           = ""prod-db""
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine              = ""mysql""
  engine_version      = ""5.7""
  instance_class      = ""db.t3.medium""
  username            = ""produser""
  password            = ""prodpassword123""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot = true
  multi_az            = true
  backup_retention_period = 7

  tags = {
    Environment = ""Production""
  }
}

# Staging RDS Instance
resource ""aws_db_instance"" ""staging_db"" {
  identifier           = ""staging-db""
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine              = ""mysql""
  engine_version      = ""5.7""
  instance_class      = ""db.t3.micro""
  username            = ""staginguser""
  password            = ""stagingpassword123""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot = true
  multi_az            = false

  tags = {
    Environment = ""Staging""
  }
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""app"" {
  name        = ""my-application""
  description = ""My Application with Production and Staging environments""
}

# Production Environment
resource ""aws_elastic_beanstalk_environment"" ""production"" {
  name                = ""production-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  tier                = ""WebServer""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.prod_db.address
  }

  setting {
    namespace = ""aws",,,,,,"I'll help you create a Terraform configuration for production and staging Elastic Beanstalk environments with their respective RDS instances. This solution will be based on the provided examples while adapting them for your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM Role and Instance Profile
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Production RDS Instance
resource ""aws_db_instance"" ""prod_db"" {
  identifier           = ""prod-db""
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class      = ""db.t3.medium""
  username            = ""produser""
  password            = ""prodpassword123""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot = true
  multi_az            = true
  backup_retention_period = 7
}

# Staging RDS Instance
resource ""aws_db_instance"" ""staging_db"" {
  identifier           = ""staging-db""
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class      = ""db.t3.micro""
  username            = ""staginguser""
  password            = ""stagingpassword123""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot = true
  multi_az            = false
  backup_retention_period = 3
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""app"" {
  name        = ""my-application""
  description = ""Production and Staging Application""
}

# Production Environment
resource ""aws_elastic_beanstalk_environment"" ""production"" {
  name                = ""production-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  tier                = ""WebServer""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.prod_db.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USER""
    value     = aws_db_instance.prod_db.username
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize",,,,,
"aws_vpc, aws_vpc_dhcp_options, aws_vpc_dhcp_options_association","sets up a basic AWS network infrastructure consisting of a Virtual Private Cloud (VPC) with a custom DHCP options set. The VPC is configured with a CIDR block of 192.168.0.0/16. The DHCP options include a domain name (windomain.local), a combination of custom and Google's DNS servers (192.168.56.102 and 8.8.8.8), and a NetBIOS name server (192.168.56.102). These DHCP options are then associated with the created VPC, customizing its DHCP settings.






","package terraform.validation

# Set default validation states
default is_valid_vpc = false

default is_valid_dhcp_options = false

default is_valid_dhcp_options_association = false

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value == ""192.168.0.0/16""
}

# Validate aws_vpc_dhcp_options resource
is_valid_dhcp_options {
        some i
        resource := input.planned_values.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options""
        resource.values.domain_name == ""windomain.local""
    resource.values.domain_name_servers == [""192.168.56.102"", ""8.8.8.8""]
        resource.values.netbios_name_servers == [""192.168.56.102""]
}

# Validate aws_vpc_dhcp_options_association resource
is_valid_dhcp_options_association {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options_association""
        resource.expressions.dhcp_options_id.references[0] == ""aws_vpc_dhcp_options.default.id""
        resource.expressions.vpc_id.references[0] == ""aws_vpc.default.id""
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""default"" {
  cidr_block = ""192.168.0.0/16""
}

resource ""aws_vpc_dhcp_options"" ""default"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = concat([""192.168.56.102""], [""8.8.8.8""])
  netbios_name_servers = [""192.168.56.102""]
}

resource ""aws_vpc_dhcp_options_association"" ""default"" {
  vpc_id          = aws_vpc.default.id
  dhcp_options_id = aws_vpc_dhcp_options.default.id
}","Has one ""aws_vpc"" resource:
Configured with a ""cidr_block"" of ""192.168.0.0/16""
Includes associated custom DHCP options

Has one ""aws_vpc_dhcp_options"" resource:
Specifies a ""domain_name"" of ""windomain.local""
Includes DNS servers with a combination of a custom DNS server (""192.168.56.102"") and Google's DNS server (""8.8.8.8"")
Configures a ""NetBIOS"" name server at ""192.168.56.102""

Has one ""aws_vpc_dhcp_options_association"" resource:
Associates the ""aws_vpc_dhcp_options"" with the created ""aws_vpc""
Customizes DHCP settings specifically for the created VPC environment","Based on your requirements, I'll help you create a Terraform configuration for setting up a VPC with custom DHCP options. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create VPC
resource ""aws_vpc"" ""default"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""default"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""default"" {
  vpc_id          = aws_vpc.default.id
  dhcp_options_id = aws_vpc_dhcp_options.default.id
}
```

This configuration:
1. Sets up the required AWS provider configuration
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

I've also added some best practices:
- Enabled DNS hostnames and DNS support in the VPC
- Added tags for better resource identification
- Used proper resource naming conventions
- Included provider version constraints

The configuration will create all necessary resources and link them together automatically using Terraform's resource references.",,,,,,"Based on your requirements, I'll help you create a Terraform configuration for setting up a VPC with custom DHCP options. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

resource ""aws_vpc"" ""default"" {
  cidr_block = ""192.168.0.0/16""
  
  tags = {
    Name = ""main-vpc""
  }
}

resource ""aws_vpc_dhcp_options"" ""default"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

resource ""aws_vpc_dhcp_options_association"" ""default"" {
  vpc_id          = aws_vpc.default.id
  dhcp_options_id = aws_vpc_dhcp_options.default.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates DHCP options with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options with the VPC

I've also added tags to make the resources more identifiable in the AWS console. The configuration follows AWS best practices and includes proper resource naming and organization.",,,,,,"Based on your requirements, I'll help you create a Terraform configuration for setting up a VPC with custom DHCP options. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

resource ""aws_vpc"" ""default"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

resource ""aws_vpc_dhcp_options"" ""default"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

resource ""aws_vpc_dhcp_options_association"" ""default"" {
  vpc_id          = aws_vpc.default.id
  dhcp_options_id = aws_vpc_dhcp_options.default.id
}
```

This configuration:

1. Sets up the required AWS provider and version constraints
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Configures DHCP options with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options with the VPC
5. Includes appropriate tags for better resource management

The configuration enables DNS support and DNS hostnames in the VPC for proper name resolution. All resources are properly linked using Terraform references.",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer with Lambda Target Group,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.values.subnet_mapping != null
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.ipv4_ipam_pool_id.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.ipv6_cider_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value != ""forward""
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    resources.values.target_type == ""lambda""
    
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group""
        resource.expressions.port != null
        resource.expressions.protocol != null
        resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
        is_valid_lb
        is_valid_lb_listener
        is_valid_lb_target_group_attachment
        is_valid_lb_target_group
        is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_iam_role"" ""lambda_role"" {
  name = ""example-lambda-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy_attachment"" ""lambda_policy_attachment"" {
  name       = ""example-lambda-policy-attachment""
  roles      = [aws_iam_role.lambda_role.name]
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}

data ""archive_file"" ""lambda-func"" {
  type        = ""zip""
  source_file = ""./supplement/index.js""
  output_path = ""./supplement/lambda_func.zip""
}

resource ""aws_lambda_function"" ""example"" {
  function_name = ""example-lambda""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs18.x""

  filename         = data.archive_file.lambda-func.output_path
  source_code_hash = data.archive_file.lambda-func.output_base64sha256
}

resource ""aws_lambda_permission"" ""elb_invocation"" {
  statement_id  = ""AllowExecutionFromALB""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.example.function_name
  principal     = ""elasticloadbalancing.amazonaws.com""
  source_arn    = aws_lb_target_group.target_group.arn
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  target_type = ""lambda""
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_lambda_function.example.arn
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id
    with target_type = ""lambda""

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you create a Terraform configuration for an Application Load Balancer with a Lambda target group. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""lambda-alb-vpc""
  cidr = ""10.0.0.0/16""

  azs            = data.aws_availability_zones.available.names
  public_subnets = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name   = ""alb-sg""
  vpc_id = module.vpc.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Lambda Function
resource ""aws_lambda_function"" ""example"" {
  filename      = ""lambda_function.zip""  # You need to provide the Lambda function code
  function_name = ""alb-lambda-target""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs18.x""
}

# IAM Role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda-alb-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Lambda permission to allow ALB invocation
resource ""aws_lambda_permission"" ""with_lb"" {
  statement_id  = ""AllowExecutionFromlb""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.example.function_name
  principal     = ""elasticloadbalancing.amazonaws.com""
}

# Application Load Balancer
resource ""aws_lb"" ""example"" {
  name               = ""lambda-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = module.vpc.public_subnets
}

# Target Group for Lambda
resource ""aws_lb_target_group"" ""lambda"" {
  name        = ""lambda-target-group""
  target_type = ""lambda""
}

# Target Group Attachment
resource ""aws_lb_target_group_attachment"" ""lambda"" {
  target_group_arn = aws_lb_target_group.lambda.arn
  target_id        = aws_lambda_function.example.arn
  depends_on       = [aws_lambda_permission.with_lb]
}

# Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.lambda.arn
  }
}
```

This configuration creates:
1. A VPC with public subnets
2. An Application",,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Lambda target group. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create VPC and networking components
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""lambda-alb-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.available.names
  public_subnets  = [""10.0.1.0/24"", ""10.0.2.0/24""]
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Create Lambda Function
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda-alb-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_lambda_function"" ""test_lambda"" {
  filename      = ""lambda_function_payload.zip""  # Make sure to provide your Lambda function code
  function_name = ""alb-lambda-function""
  role         = aws_iam_role.lambda_role.arn
  handler      = ""index.handler""
  runtime      = ""nodejs18.x""

  vpc_config {
    subnet_ids         = module.vpc.public_subnets
    security_group_ids = [aws_security_group.alb_sg.id]
  }
}

# Create ALB
resource ""aws_lb"" ""lambda_alb"" {
  name               = ""lambda-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = module.vpc.public_subnets
}

# Create Target Group
resource ""aws_lb_target_group"" ""lambda_tg"" {
  name        = ""lambda-target-group""
  target_type = ""lambda""
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.lambda_alb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.lambda_tg.arn
  }
}

# Attach Lambda to Target Group
resource ""aws_lb_target_group_attachment"" ""lambda_attachment"" {
  target_group_arn = aws_lb_target_group.lambda_tg.arn
  target_id        = aws_lambda_function.test_lambda.arn
  depends_on       = [aws_lambda_permission.with_lb]
}

# Add Lambda permission for ALB
resource ""aws_lambda_permission"" ""with_lb"" {
  statement_id  = ""AllowExecutionFromALB""
  action        = """,,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Lambda target group. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC and networking components
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""lambda-alb-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.available.names
  public_subnets  = [""10.0.1.0/24"", ""10.0.2.0/24""]
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-lambda-sg""
  description = ""Security group for ALB with Lambda target""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Lambda Function
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda-alb-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_lambda_function"" ""test_lambda"" {
  filename      = ""lambda_function_payload.zip""  # Make sure to provide your Lambda function code
  function_name = ""alb-lambda-function""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs18.x""

  environment {
    variables = {
      foo = ""bar""
    }
  }
}

# Lambda permission for ALB
resource ""aws_lambda_permission"" ""with_lb"" {
  statement_id  = ""AllowExecutionFromlb""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.test_lambda.function_name
  principal     = ""elasticloadbalancing.amazonaws.com""
}

# Application Load Balancer
resource ""aws_lb"" ""lambda_alb"" {
  name               = ""lambda-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = module.vpc.public_subnets
}

# Target Group for Lambda
resource ""aws_lb_target_group"" ""lambda_tg"" {
  name        = ""lambda-target-group""
  target_type = ""lambda""
}

# Target Group Attachment
resource ""aws_lb_target_group_attachment"" ""lambda_tg_attachment"" {
  target_group_arn = aws_lb_target_group.lambda_tg.arn
  target_id        = aws_lambda_function.test_lambda.arn
  depends_on       = [aws_lambda_permission.with_lb]
}

# Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.lambda_alb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws",,,,,
"aws_cloudwatch_log_group, aws_cloudwatch_log_resource_policy, aws_route53_query_log, aws_route53_zone, aws_iam_policy_document","Configure a query log that can create a log stream and put log events using Route 53 resources. Name the zone ""primary"", the cloudwatch log group ""aws_route53_example_com"", and the cloudwatch log resource policy ""route53-query-logging-policy""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_cloudwatch_log_group = false

default is_valid_cloudwatch_log_resource_policy = false

default is_valid_route53_query_log = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name
}

is_valid_cloudwatch_log_group {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_cloudwatch_log_group""
}

is_valid_cloudwatch_log_resource_policy {
    some i
    resource := input.resource_changes[i]
    resource.type == ""aws_cloudwatch_log_resource_policy""
    contains(resource.change.after.policy_document, ""logs:PutLogEvents"")
    contains(resource.change.after.policy_document, ""logs:CreateLogStream"")
    resource.change.after.policy_name
}

is_valid_route53_query_log {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_query_log""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.primary.zone_id""
    resource.expressions.cloudwatch_log_group_arn.references[0] == ""aws_cloudwatch_log_group.aws_route53_example_com.arn""
    resource.depends_on[0] == ""aws_cloudwatch_log_resource_policy.route53-query-logging-policy""
}

# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_cloudwatch_log_group
    is_valid_cloudwatch_log_resource_policy
    is_valid_route53_query_log
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}


resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""
}

resource ""aws_cloudwatch_log_group"" ""aws_route53_example_com"" {
  name              = ""/aws/route53/${aws_route53_zone.primary.name}""
  retention_in_days = 30
}

# Example CloudWatch log resource policy to allow Route53 to write logs
# to any log group under /aws/route53/*

data ""aws_iam_policy_document"" ""route53-query-logging-policy"" {
  statement {
    actions = [
      ""logs:CreateLogStream"",
      ""logs:PutLogEvents"",
    ]

    resources = [""arn:aws:logs:*:*:log-group:/aws/route53/*""]

    principals {
      identifiers = [""route53.amazonaws.com""]
      type        = ""Service""
    }
  }
}

resource ""aws_cloudwatch_log_resource_policy"" ""route53-query-logging-policy"" {
  policy_document = data.aws_iam_policy_document.route53-query-logging-policy.json
  policy_name     = ""route53-query-logging-policy""
}

resource ""aws_route53_query_log"" ""example_com"" {
  depends_on = [aws_cloudwatch_log_resource_policy.route53-query-logging-policy]

  cloudwatch_log_group_arn = aws_cloudwatch_log_group.aws_route53_example_com.arn
  zone_id                  = aws_route53_zone.primary.zone_id
}","Has one ""aws_route53_zone"" resource
    with ""name""

Has one ""aws_cloudwatch_log_group"" resource

Has one ""aws_cloudwatch_log_resource_policy"" resource
    with ""policy_document"" enabling ""logs:CreateLogStream"" and ""logs:PutLogEvents""
    with ""policy_name""

Has one ""aws_route53_query_log"" resource
    with ""cloudwatch_log_group_arn"" referencing the ""aws_cloudwatch_log_group"" resource
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
    and depends on the ""aws_cloudwatch_log_resource_policy"" resource","I'll help you create a Terraform configuration for Route 53 query logging with CloudWatch Log Group integration. Based on the requirements, here's the solution:

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

# Create the CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""route53_log_group"" {
  name              = ""aws_route53_example_com""
  retention_in_days = 30
}

# Create the CloudWatch Log Resource Policy
resource ""aws_cloudwatch_log_resource_policy"" ""route53_log_policy"" {
  policy_name = ""route53-query-logging-policy""

  policy_document = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""route53.amazonaws.com""
        }
        Action = [
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = ""${aws_cloudwatch_log_group.route53_log_group.arn}:*""
      }
    ]
  })
}

# Create the Route 53 Zone
resource ""aws_route53_zone"" ""primary"" {
  name = ""example.com""
}

# Create the Route 53 Query Log
resource ""aws_route53_query_log"" ""query_log"" {
  depends_on = [aws_cloudwatch_log_resource_policy.route53_log_policy]

  cloudwatch_log_group_arn = aws_cloudwatch_log_group.route53_log_group.arn
  zone_id                  = aws_route53_zone.primary.zone_id
}
```

This configuration:

1. Creates a CloudWatch Log Group named ""aws_route53_example_com""
2. Establishes a CloudWatch Log Resource Policy named ""route53-query-logging-policy"" that allows Route 53 to create log streams and put log events
3. Sets up a Route 53 hosted zone named ""primary""
4. Configures query logging for the Route 53 zone to send logs to the CloudWatch Log Group

The policy document grants the necessary permissions for Route 53 to write logs to CloudWatch Logs. The `depends_on` block ensures that the log resource policy is created before attempting to set up the query log.",,,,,,"I'll help you create a Terraform configuration for Route 53 query logging with CloudWatch Log Group integration. Based on the requirements, here's the solution:

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

# Create the Route 53 zone
resource ""aws_route53_zone"" ""primary"" {
  name = ""example.com""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""aws_route53_example_com"" {
  name              = ""aws_route53_example_com""
  retention_in_days = 30
}

# Create CloudWatch Log Resource Policy
data ""aws_iam_policy_document"" ""route53_query_logging_policy"" {
  statement {
    actions = [
      ""logs:CreateLogStream"",
      ""logs:PutLogEvents""
    ]

    resources = [""${aws_cloudwatch_log_group.aws_route53_example_com.arn}:*""]

    principals {
      identifiers = [""route53.amazonaws.com""]
      type        = ""Service""
    }
  }
}

resource ""aws_cloudwatch_log_resource_policy"" ""route53_query_logging_policy"" {
  policy_name     = ""route53-query-logging-policy""
  policy_document = data.aws_iam_policy_document.route53_query_logging_policy.json
}

# Create Route 53 Query Log
resource ""aws_route53_query_log"" ""query_log"" {
  depends_on = [aws_cloudwatch_log_resource_policy.route53_query_logging_policy]

  cloudwatch_log_group_arn = aws_cloudwatch_log_group.aws_route53_example_com.arn
  zone_id                  = aws_route53_zone.primary.zone_id
}
```

This configuration:
1. Creates a Route 53 hosted zone named ""primary""
2. Creates a CloudWatch Log Group named ""aws_route53_example_com""
3. Creates a CloudWatch Log Resource Policy named ""route53-query-logging-policy"" that allows Route 53 to create log streams and put log events
4. Sets up Route 53 query logging to send logs to the specified CloudWatch Log Group

The configuration includes necessary permissions and dependencies to ensure proper setup of the query logging functionality.",,,,,,"I'll help you create a Route 53 query logging configuration with CloudWatch Log Group and the necessary IAM policies. Here's the Terraform configuration:

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

# Create the CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""route53_logs"" {
  name              = ""aws_route53_example_com""
  retention_in_days = 30
}

# Create the IAM policy for Route 53 query logging
data ""aws_iam_policy_document"" ""route53_query_logging_policy"" {
  statement {
    actions = [
      ""logs:CreateLogStream"",
      ""logs:PutLogEvents""
    ]

    resources = [""${aws_cloudwatch_log_group.route53_logs.arn}:*""]

    principals {
      identifiers = [""route53.amazonaws.com""]
      type        = ""Service""
    }
  }
}

# Attach the policy to the CloudWatch Log Group
resource ""aws_cloudwatch_log_resource_policy"" ""route53_query_logging_policy"" {
  policy_name     = ""route53-query-logging-policy""
  policy_document = data.aws_iam_policy_document.route53_query_logging_policy.json
}

# Create the Route 53 zone
resource ""aws_route53_zone"" ""primary"" {
  name = ""example.com""
}

# Enable query logging for the zone
resource ""aws_route53_query_log"" ""query_log"" {
  depends_on = [aws_cloudwatch_log_resource_policy.route53_query_logging_policy]

  cloudwatch_log_group_arn = aws_cloudwatch_log_group.route53_logs.arn
  zone_id                  = aws_route53_zone.primary.zone_id
}
```

This configuration:
1. Creates a CloudWatch Log Group named ""aws_route53_example_com""
2. Creates an IAM policy document that allows Route 53 to create log streams and put log events
3. Attaches the policy to the CloudWatch Log Group with the specified policy name
4. Creates a Route 53 hosted zone named ""primary""
5. Enables query logging for the zone, directing logs to the CloudWatch Log Group

The configuration includes all necessary permissions and dependencies to ensure proper functionality of Route 53 query logging.",,,,,
"aws_db_instance, aws_db_subnet_group, aws_elastic_beanstalk_application, aws_elastic_beanstalk_environment, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment, aws_internet_gateway, aws_route_table, aws_route_table_association, aws_route_table_association, aws_security_group, aws_subnet, aws_subnet, aws_vpc","Configure an AWS VPC with internet gateway, two public subnets, a security group, and a route table associated with the public subnets. Then connect these resources to two Elastic Beanstalk environments. These environments are also connected to a shared RDS database that is linked to the subnets. Name the database ""shared_rds"", the security group ""eb_env_sg"", the vpc ""eb_vpc"", and the subnets ""eb_subnet_public_1"" and ""eb_subnet_public_2""","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_eb_app = false

default is_valid_eb_env = false

default is_valid_db_instance = false

default is_valid_vpc = false

default is_valid_internet_gateway = false

default is_valid_subnet = false

default is_valid_subnet_group = false

default is_valid_security_group = false

default is_valid_route_table = false

default is_valid_route_table_association = false


is_valid_vpc {
                some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_vpc""
    resource.expressions.cidr_block
}

is_valid_internet_gateway {
                        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_internet_gateway""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.eb_vpc.id""
}

is_valid_subnet {
                some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_subnet""
    resource.expressions.cidr_block
    resource.expressions.availability_zone
    
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_subnet""
    resource2.expressions.cidr_block
    resource2.expressions.availability_zone
    resource2.expressions.vpc_id.references[0] == resource.expressions.vpc_id.references[0]

}

is_valid_subnet_group {
                some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_db_subnet_group""
    resource.expressions.subnet_ids.references[0]

}

is_valid_security_group {
                some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_security_group""
    resource.expressions.vpc_id.references[0]
    resource.expressions.egress
    resource.expressions.ingress
}

is_valid_route_table {
                some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route_table""
    resource.expressions.route.references[0]
    resource.expressions.vpc_id.references
}

is_valid_route_table_association {
                some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route_table_association""
    resource.expressions.subnet_id.references[0]
    resource.expressions.route_table_id.references[0]
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_route_table_association""
    resource2.expressions.subnet_id.references[0]
    resource2.expressions.route_table_id.references[0]
    resource2.expressions.route_table_id.references[0] == resource.expressions.route_table_id.references[0]
    resource2.expressions.subnet_id.references[0] != resource.expressions.subnet_id.references[0]

}

is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0]
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0]
}

# Validate aws_eb_app
is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}

# Validate aws_eb_env
is_valid_eb_env {
        some a, b
    resource := input.configuration.root_module.resources[a]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.expressions.name
    resource.expressions.application.references[0]
    resource.expressions.solution_stack_name
    
    resource2 := input.configuration.root_module.resources[b]
    resource2.type == ""aws_elastic_beanstalk_environment""
    resource2.expressions.name
    resource2.expressions.application.references[0]
    resource2.expressions.solution_stack_name
    
    some c, d, e, f, g, h, i, j, k, l, m, n, o, p
    resource.expressions.setting[c].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource.expressions.setting[d].value.references[0] == ""aws_db_instance.shared_rds.username""
    resource.expressions.setting[e].value.references[0] == ""aws_db_instance.shared_rds.password""
    resource.expressions.setting[f].value.references[0] == ""aws_db_instance.shared_rds.address""
    resource.expressions.setting[g].value.references[0] == ""aws_security_group.eb_env_sg.id""
    resource.expressions.setting[h].value.references[0] == ""aws_vpc.eb_vpc.id""
    resource.expressions.setting[i].value.references[0] == ""aws_subnet.eb_subnet_public_1.id""
    resource.expressions.setting[i].value.references[2] == ""aws_subnet.eb_subnet_public_2.id""
    
    resource2.expressions.setting[j].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource2.expressions.setting[k].value.references[0] == ""aws_db_instance.shared_rds.username""
    resource2.expressions.setting[l].value.references[0] == ""aws_db_instance.shared_rds.password""
    resource2.expressions.setting[m].value.references[0] == ""aws_db_instance.shared_rds.address""
    resource2.expressions.setting[n].value.references[0] == ""aws_security_group.eb_env_sg.id""
    resource2.expressions.setting[o].value.references[0] == ""aws_vpc.eb_vpc.id""
    resource2.expressions.setting[p].value.references[0] == ""aws_subnet.eb_subnet_public_1.id""
    resource2.expressions.setting[p].value.references[2] == ""aws_subnet.eb_subnet_public_2.id""
}

is_valid_db_instance {
                some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_db_instance""
    has_required_main_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_main_db_arguments {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_db_instance""
    resource.expressions.db_subnet_group_name.references[0]
    resource.expressions.vpc_security_group_ids.references[0]
    # Check for allocated_storage, engine, instance_class, username, password, and skip_final_snapshot
    requirement1(resource.expressions)
    # Check for instance_class validity
    requirement2(resource.expressions)
}



# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
    expressions.allocated_storage
    expressions.engine
    expressions.username
    expressions.password
    is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
    expressions.snapshot_identifier
}

# Check for instance_class validity
requirement2(expressions) {
    expressions.instance_class
    is_valid_instance_class(expressions.instance_class.constant_value)
}


# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}


# Combine all checks into a final rule
is_configuration_valid {
        is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
    is_valid_eb_app
    is_valid_eb_env
    is_valid_db_instance
    is_valid_vpc
    is_valid_internet_gateway
    is_valid_subnet
    is_valid_subnet_group
    is_valid_security_group
    is_valid_route_table
    is_valid_route_table_association
    
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# VPC
resource ""aws_vpc"" ""eb_vpc"" {
  cidr_block = ""10.0.0.0/16""
  enable_dns_support   = true
  enable_dns_hostnames = true
}

# Internet Gateway
resource ""aws_internet_gateway"" ""eb_igw"" {
  vpc_id = aws_vpc.eb_vpc.id
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Subnets
resource ""aws_subnet"" ""eb_subnet_public_1"" {
  vpc_id     = aws_vpc.eb_vpc.id
  cidr_block = ""10.0.1.0/24""

  map_public_ip_on_launch = true
  availability_zone       = data.aws_availability_zones.available.names[0]
}

resource ""aws_subnet"" ""eb_subnet_public_2"" {
  vpc_id     = aws_vpc.eb_vpc.id
  cidr_block = ""10.0.2.0/24""

  map_public_ip_on_launch = true
  availability_zone       = data.aws_availability_zones.available.names[1]
}

# Security groups for Elastic Beanstalk environments
resource ""aws_security_group"" ""eb_env_sg"" {
  name        = ""eb-env-sg""
  description = ""Security group for Elastic Beanstalk environments""
  vpc_id      = aws_vpc.eb_vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.eb_env_sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  from_port         = 80
  ip_protocol       = ""tcp""
  to_port           = 80
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress2"" {
  security_group_id = aws_security_group.eb_env_sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  from_port         = 443
  ip_protocol       = ""tcp""
  to_port           = 443
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.eb_env_sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  ip_protocol       = ""-1""
}

# DB subnet group for RDS instance
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name       = ""rds-subnet-group""
  subnet_ids = [aws_subnet.eb_subnet_public_1.id, aws_subnet.eb_subnet_public_2.id]
}

resource ""aws_route_table"" ""eb_route_table"" {
  vpc_id = aws_vpc.eb_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.eb_igw.id
  }
}

resource ""aws_route_table_association"" ""eb_route_table_association_1"" {
  subnet_id      = aws_subnet.eb_subnet_public_1.id
  route_table_id = aws_route_table.eb_route_table.id
}

resource ""aws_route_table_association"" ""eb_route_table_association_2"" {
  subnet_id      = aws_subnet.eb_subnet_public_2.id
  route_table_id = aws_route_table.eb_route_table.id
}

# RDS instance
resource ""aws_db_instance"" ""shared_rds"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""dbadmin""
  password             = ""securepassword"" 
  backup_retention_period = 0
  skip_final_snapshot  = true
  db_subnet_group_name  = aws_db_subnet_group.rds_subnet_group.name

  vpc_security_group_ids = [aws_security_group.eb_env_sg.id]
}

# Elastic Beanstalk application
resource ""aws_elastic_beanstalk_application"" ""microservice_app"" {
  name        = ""MicroserviceApplication""
  description = ""An application for microservices""
}

resource ""aws_elastic_beanstalk_environment"" ""microservice_env1"" {
  name                = ""microservice-env1""
  application         = aws_elastic_beanstalk_application.microservice_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  # Elastic Beanstalk environment variables for RDS connection
  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOSTNAME""
    value     = aws_db_instance.shared_rds.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.shared_rds.username
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.shared_rds.password
  }

  setting {
    namespace = ""aws:ec2:vpc""
    name      = ""VPCId""
    value     = aws_vpc.eb_vpc.id
  }

  setting {
    namespace = ""aws:ec2:vpc""
    name      = ""Subnets""
    value     = ""${aws_subnet.eb_subnet_public_1.id},${aws_subnet.eb_subnet_public_2.id}""
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""SecurityGroups""
    value     = aws_security_group.eb_env_sg.id
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

resource ""aws_elastic_beanstalk_environment"" ""microservice_env2"" {
  name                = ""microservice-env2""
  application         = aws_elastic_beanstalk_application.microservice_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  # Elastic Beanstalk environment variables for RDS connection
  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOSTNAME""
    value     = aws_db_instance.shared_rds.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.shared_rds.username
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.shared_rds.password
  }

  setting {
    namespace = ""aws:ec2:vpc""
    name      = ""VPCId""
    value     = aws_vpc.eb_vpc.id
  }

  setting {
    namespace = ""aws:ec2:vpc""
    name      = ""Subnets""
    value     = ""${aws_subnet.eb_subnet_public_1.id},${aws_subnet.eb_subnet_public_2.id}""
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""SecurityGroups""
    value     = aws_security_group.eb_env_sg.id
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has one ""aws_vpc"" resource
    with ""cidr_block"" set to a valid value

Has one ""aws_internet_gateway"" resource
    with ""vpc_id"" referencing the ""aws_vpc"" resource

Has two ""aws_subnet"" resources
    with ""vpc_id"" referencing the ""aws_vpc"" resource
    with ""cidr_block"" set to a valid value
    with ""availability_zone"" set to a zone within the region

Has one ""aws_security_group"" resource
     with ""vpc_id"" referencing the ""aws_vpc"" resource
     with ""ingress"" blocks
     with ""egress"" block

Has one ""aws_db_subnet_group"" resource
     with ""subnet_ids"" referencing the ""aws_subnet"" resources

Has one ""aws_route_table"" resources
     with ""vpc_id"" referencing the ""aws_vpc"" resource
     with the ""route"" block containing ""gateway_id"" referencing the ""aws_internet_gateway"" resource

Has two ""aws_route_table_association"" resources
     with ""subnet_id"" referencing one of the ""aws_subnet"" resources
     with ""route_table_id"" referencing the ""aws_route_table"" resource

Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""


Has two ""aws_elastic_beanstalk_environment"" resources
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with a ""setting"" block that has a ""value"" that references the ""aws_iam_instance_profile"" resource
    with a ""setting"" block that has a ""value"" that references the ""id"" from all the ""aws_subnet"" resources
    with a ""setting"" block that has a ""value"" that references the ""id"" from the ""aws_vpc"" resource
    with a ""setting"" block that has a ""value"" that references the ""id"" from the ""aws_security_group"" resource
    with a ""setting"" block that has a ""value"" that references the ""address"" from the ""db_instance"" resource
    with a ""setting"" block that has a ""value"" that references the ""username"" from the ""db_instance"" resource 
    with a ""setting"" block that has a ""value"" that references the ""password"" from the ""db_instance"" resource 

Has a ""aws_db_instance"" resource
    with ""instance_class""
    with ""allocated_storage""
    with ""engine""
    with ""username""
    with ""password""
    with ""backup_retention_period"" set to a non-zero number
    with ""skip_final_snapshot""
    with ""db_subnet_group_name"" referencing the ""aws_db_subnet_group"" resource
    with ""vpc_security_group_ids"" referencing the ""aws_security_group"" resource","I'll help you create a Terraform configuration for your requirements. This solution combines aspects from the provided examples while incorporating the specific naming and architecture you've requested.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC
resource ""aws_vpc"" ""eb_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""eb_vpc""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""eb_igw"" {
  vpc_id = aws_vpc.eb_vpc.id

  tags = {
    Name = ""eb_igw""
  }
}

# Fetch Available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Public Subnets
resource ""aws_subnet"" ""eb_subnet_public_1"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.1.0/24""
  availability_zone       = data.aws_availability_zones.available.names[0]
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_1""
  }
}

resource ""aws_subnet"" ""eb_subnet_public_2"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.2.0/24""
  availability_zone       = data.aws_availability_zones.available.names[1]
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_2""
  }
}

# Route Table
resource ""aws_route_table"" ""eb_public_rt"" {
  vpc_id = aws_vpc.eb_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.eb_igw.id
  }

  tags = {
    Name = ""eb_public_rt""
  }
}

# Route Table Associations
resource ""aws_route_table_association"" ""public_1"" {
  subnet_id      = aws_subnet.eb_subnet_public_1.id
  route_table_id = aws_route_table.eb_public_rt.id
}

resource ""aws_route_table_association"" ""public_2"" {
  subnet_id      = aws_subnet.eb_subnet_public_2.id
  route_table_id = aws_route_table.eb_public_rt.id
}

# Security Group
resource ""aws_security_group"" ""eb_env_sg"" {
  name        = ""eb_env_sg""
  description = ""Security group for Elastic Beanstalk environments""
  vpc_id      = aws_vpc.eb_vpc.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""eb_db_subnet_group"" {
  name       = ""eb-db-subnet-group""
  subnet_ids = [aws_subnet.eb_subnet_public_1.id, aws_subnet.eb_subnet",,,,,,"I'll help you create a Terraform configuration that sets up the requested AWS infrastructure. Based on the provided examples and your specific requirements, here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC
resource ""aws_vpc"" ""eb_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""eb_vpc""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""eb_igw"" {
  vpc_id = aws_vpc.eb_vpc.id

  tags = {
    Name = ""eb_igw""
  }
}

# Public Subnets
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

resource ""aws_subnet"" ""eb_subnet_public_1"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.1.0/24""
  availability_zone       = data.aws_availability_zones.available.names[0]
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_1""
  }
}

resource ""aws_subnet"" ""eb_subnet_public_2"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.2.0/24""
  availability_zone       = data.aws_availability_zones.available.names[1]
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_2""
  }
}

# Route Table
resource ""aws_route_table"" ""eb_public_rt"" {
  vpc_id = aws_vpc.eb_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.eb_igw.id
  }

  tags = {
    Name = ""eb_public_rt""
  }
}

# Route Table Associations
resource ""aws_route_table_association"" ""public_1"" {
  subnet_id      = aws_subnet.eb_subnet_public_1.id
  route_table_id = aws_route_table.eb_public_rt.id
}

resource ""aws_route_table_association"" ""public_2"" {
  subnet_id      = aws_subnet.eb_subnet_public_2.id
  route_table_id = aws_route_table.eb_public_rt.id
}

# Security Group
resource ""aws_security_group"" ""eb_env_sg"" {
  name        = ""eb_env_sg""
  description = ""Security group for Elastic Beanstalk environments""
  vpc_id      = aws_vpc.eb_vpc.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""eb_db_subnet_group"" {
  name       = ""eb-db-subnet-group""
  subnet_ids = [aws_subnet.eb_subnet_public_1.id, aws_subnet.eb_subnet_public_2.id]",,,,,,"I'll help you create a Terraform configuration that sets up the requested AWS infrastructure. Here's a solution based on the provided examples and requirements:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC Configuration
resource ""aws_vpc"" ""eb_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""eb_vpc""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""eb_igw"" {
  vpc_id = aws_vpc.eb_vpc.id

  tags = {
    Name = ""eb_igw""
  }
}

# Fetch Available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Public Subnets
resource ""aws_subnet"" ""eb_subnet_public_1"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.1.0/24""
  availability_zone       = data.aws_availability_zones.available.names[0]
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_1""
  }
}

resource ""aws_subnet"" ""eb_subnet_public_2"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.2.0/24""
  availability_zone       = data.aws_availability_zones.available.names[1]
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_2""
  }
}

# Route Table
resource ""aws_route_table"" ""eb_public_rt"" {
  vpc_id = aws_vpc.eb_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.eb_igw.id
  }

  tags = {
    Name = ""eb_public_rt""
  }
}

# Route Table Associations
resource ""aws_route_table_association"" ""public1"" {
  subnet_id      = aws_subnet.eb_subnet_public_1.id
  route_table_id = aws_route_table.eb_public_rt.id
}

resource ""aws_route_table_association"" ""public2"" {
  subnet_id      = aws_subnet.eb_subnet_public_2.id
  route_table_id = aws_route_table.eb_public_rt.id
}

# Security Group
resource ""aws_security_group"" ""eb_env_sg"" {
  name        = ""eb_env_sg""
  description = ""Security group for Elastic Beanstalk environments""
  vpc_id      = aws_vpc.eb_vpc.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""eb_db_subnet_group"" {
  name       = ""eb-db-subnet-group""
  subnet_ids = [aws_subnet.eb_subnet_public_1.id, aws_subnet.eb_subnet_public_2",,,,,
"aws_codebuild_project, aws_iam_policy, aws_iam_role, aws_iam_role_policy_attachment, aws_s3_bucket, aws_iam_policy_document, aws_iam_policy_document",Infrastructure for an autograder for a CS class. The autograder needs to run students' code stored on GitHub in a specific environment and store the result. Accomplish this using AWS CodeBuild.,"package autograder_high

import rego.v1

codebuild_project_valid(codebuild_project) if {
        some artifact in codebuild_project.expressions.artifacts
        artifact.location
        artifact.name
        artifact.type
        some environment in codebuild_project.expressions.environment
        environment.compute_type
        environment.image
        environment.type

        some source in codebuild_project.expressions.source
        source.type.constant_value == ""GITHUB""
        source.location
}

default valid := false

valid if {
        resources := input.configuration.root_module.resources
        some codebuild_project in resources
        codebuild_project.type == ""aws_codebuild_project""
        some security_group in resources
        codebuild_project_valid(codebuild_project)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""artifact-bucket-""
}

resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder_build""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip"" # include this
  }

  environment { # arguments required, exact value not specified
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1 # good to have, not required
    location        = ""github.com/source-location""
  }
}

resource ""aws_iam_role"" ""autograder_build_role"" {
  assume_role_policy = data.aws_iam_policy_document.autograder_build_policy_assume.json
}

data ""aws_iam_policy_document"" ""autograder_build_policy_assume"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }
  }

}

data ""aws_iam_policy_document"" ""autograder_build_policy"" {
  statement {
    effect = ""Allow""
    actions = [
      ""ec2:DescribeSubnets"",
      ""ec2:DescribeSecurityGroups"",
      ""ec2:DescribeVpcs""
    ]
    resources = [
      ""*""
    ]
  }
}

resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name        = ""lambda_policy""
  description = ""Grants permissions to Lambda to describe vpc, subnet, security group""

  policy = data.aws_iam_policy_document.autograder_build_policy.json
}

resource ""aws_iam_role_policy_attachment"" ""lambda_policy_attach"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}","have one ""aws_codebuild_project"" resource
  with ""artifacts"" argument block
    with ""type"" argument
    with ""location"" argument
    with ""name"" argument
  with ""environment"" argument block
    with ""computer_type"" argument
    with ""image"" argument
    with ""type"" argument
  with ""source"" argument block
    with ""type"" argument with value ""GITHUB""
    with ""location"" argument","I'll help you create a Terraform configuration for an autograder using AWS CodeBuild. Based on the examples provided, I'll create a solution that includes AWS CodeBuild and the necessary IAM roles and policies.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket to store build artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""autograder-artifacts-""
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder-project""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""grading-results.zip""
  }

  environment {
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1
    location        = ""github.com/your-repository-url""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""autograder_build_role"" {
  name = ""autograder-build-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild
resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name = ""autograder-build-policy""
  
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = ""*""
      },
      {
        Effect = ""Allow""
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
        Resource = [
          aws_s3_bucket.artifact_bucket.arn,
          ""${aws_s3_bucket.artifact_bucket.arn}/*""
        ]
      }
    ]
  })
}

# Attach the policy to the role
resource ""aws_iam_role_policy_attachment"" ""autograder_policy_attachment"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}
```

This configuration creates:

1. An S3 bucket to store the grading results
2. A CodeBuild project that:
   - Uses a standard Linux environment
   - Sources code from GitHub
   - Stores results in the S3 bucket
3. Required IAM roles and policies for CodeBuild to:
   - Access the source code
   - Store results in S3
   - Create CloudWatch logs

To use this configuration:

1. Replace `""github.com/your-repository-url""` with your actual GitHub repository URL
2. Make sure you have configured GitHub authentication for CodeBuild
3. Adjust the compute environment specifications if needed
4. Customize the artifact naming and storage structure as required

The configuration provides the basic infrastructure needed to run an autograder. You'll need to provide the actual",,,,,,"I'll help you create a Terraform configuration for an autograder using AWS CodeBuild. Based on the example and requirements, here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket to store build artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""autograder-artifacts-""
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder-project""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip""
  }

  environment {
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1
    location        = ""github.com/source-location""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""autograder_build_role"" {
  name = ""autograder_build_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild
resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name = ""autograder_build_policy""
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.artifact_bucket.arn,
          ""${aws_s3_bucket.artifact_bucket.arn}/*""
        ]
      },
      {
        Effect = ""Allow""
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = [""*""]
      }
    ]
  })
}

# Attach the policy to the role
resource ""aws_iam_role_policy_attachment"" ""autograder_policy_attachment"" {
  policy_arn = aws_iam_policy.autograder_build_policy.arn
  role       = aws_iam_role.autograder_build_role.name
}
```

This configuration creates:

1. An S3 bucket to store the build artifacts and results
2. A CodeBuild project that:
   - Uses GitHub as the source
   - Runs in a Linux container environment
   - Stores artifacts in the S3 bucket
3. Necessary IAM roles and policies to allow CodeBuild to:
   - Access the GitHub repository
   - Store artifacts in S3
   - Create CloudWatch logs

The CodeBuild project will pull code from the specified GitHub repository, run it in the defined environment, and store the results in the S3 bucket. You'll need to:

1. Replace ""github.com/source-location"" with your actual GitHub repository URL
2. Configure GitHub authentication for CodeBuild
3. Customize the build environment and specifications according to your specific requirements through a buildspec.yml file in your repository

This configuration provides the basic infrastructure needed for an autograder system while maintaining security through proper IAM roles and policies.",,,,,,"I'll help you create a Terraform configuration for an autograder using AWS CodeBuild. Based on the examples provided, I'll create a solution that includes AWS CodeBuild project with necessary IAM roles and an S3 bucket for storing results.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket for storing build artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""autograder-artifacts-""
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder-project""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""autograder-results.zip""
  }

  environment {
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1
    location        = ""github.com/source-location""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""autograder_build_role"" {
  name = ""autograder-build-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild role
resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name = ""autograder-build-policy""
  description = ""Policy for autograder CodeBuild project""

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = ""*""
      },
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:GetObjectVersion""
        ]
        Resource = [
          aws_s3_bucket.artifact_bucket.arn,
          ""${aws_s3_bucket.artifact_bucket.arn}/*""
        ]
      }
    ]
  })
}

# Attach the policy to the role
resource ""aws_iam_role_policy_attachment"" ""autograder_policy_attachment"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}
```

This configuration:
1. Sets up an S3 bucket to store the build artifacts
2. Creates a CodeBuild project that:
   - Uses a standard Linux environment
   - Pulls code from GitHub
   - Stores results in the S3 bucket
3. Creates necessary IAM roles and policies for CodeBuild to:
   - Assume the role
   - Write logs to CloudWatch
   - Access the S3 bucket for storing results

You'll need to:
1. Replace ""github.com/source-location"" with your actual GitHub repository URL
2. Configure GitHub authentication for CodeBuild
3. Customize the compute environment and buildspec as needed for your specific autograding requirements

This provides a basic setup that can be extended based on specific needs.",,,,,
"aws_lightsail_disk, aws_lightsail_disk, aws_lightsail_disk_attachment, aws_lightsail_disk_attachment, aws_lightsail_instance, aws_availability_zones",Create a Lightsail instance with multiple attached disks,"package terraform.validation

default has_valid_resources = false

# Rule for multiple aws_lightsail_disk resources
has_valid_lightsail_disks {
    count([disk | disk := input.planned_values.root_module.resources[_]; disk.type == ""aws_lightsail_disk""; disk.values.name; disk.values.size_in_gb; disk.values.availability_zone]) > 0
}

# Rule for multiple aws_lightsail_disk_attachment resources
has_valid_lightsail_disk_attachments {
    count([attachment | attachment := input.planned_values.root_module.resources[_]; attachment.type == ""aws_lightsail_disk_attachment""; attachment.values.disk_name; attachment.values.instance_name; attachment.values.disk_path]) > 0
}

# Rule for aws_lightsail_instance resource with specific arguments
has_valid_lightsail_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id
    resource.values.bundle_id
}

# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lightsail_disks
    has_valid_lightsail_disk_attachments
    has_valid_lightsail_instance
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""

  filter {
    name   = ""opt-in-status""
    values = [""opt-in-not-required""]
  }
}

resource ""aws_lightsail_disk"" ""test1"" {
  name              = ""test-disk-1""
  size_in_gb        = 8
  availability_zone = data.aws_availability_zones.available.names[0]
}

resource ""aws_lightsail_disk"" ""test2"" {
  name              = ""test-disk-2""
  size_in_gb        = 8
  availability_zone = data.aws_availability_zones.available.names[0]
}

resource ""aws_lightsail_instance"" ""test"" {
  name              = ""test-instance""
  availability_zone = data.aws_availability_zones.available.names[0]
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_1_0""
}

resource ""aws_lightsail_disk_attachment"" ""test1"" {
  disk_name     = aws_lightsail_disk.test1.name
  instance_name = aws_lightsail_instance.test.name
  disk_path     = ""/dev/xvdf""
}

resource ""aws_lightsail_disk_attachment"" ""test2"" {
  disk_name     = aws_lightsail_disk.test2.name
  instance_name = aws_lightsail_instance.test.name
  disk_path     = ""/dev/xvdg""
}","have multiple aws_lightsail_disk resource
    with name
    with size_in_gb
    with availability_zone

have multiple aws_lightsail_disk_attachment
    with disk_name
    with instance_name
    with disk_path

have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument","I'll help you create a Terraform configuration for a Lightsail instance with multiple attached disks. This solution will use the AWS provider and create a Lightsail instance with two attached disks.

```hcl
# Configure AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""

  filter {
    name   = ""opt-in-status""
    values = [""opt-in-not-required""]
  }
}

# Create Lightsail Instance
resource ""aws_lightsail_instance"" ""main"" {
  name              = ""multi-disk-instance""
  availability_zone = data.aws_availability_zones.available.names[0]
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_1_0""
}

# Create First Disk
resource ""aws_lightsail_disk"" ""disk1"" {
  name              = ""disk-1""
  size_in_gb        = 8
  availability_zone = data.aws_availability_zones.available.names[0]
}

# Create Second Disk
resource ""aws_lightsail_disk"" ""disk2"" {
  name              = ""disk-2""
  size_in_gb        = 16
  availability_zone = data.aws_availability_zones.available.names[0]
}

# Attach First Disk
resource ""aws_lightsail_disk_attachment"" ""disk1_attachment"" {
  disk_name     = aws_lightsail_disk.disk1.name
  instance_name = aws_lightsail_instance.main.name
  disk_path     = ""/dev/xvdf""
}

# Attach Second Disk
resource ""aws_lightsail_disk_attachment"" ""disk2_attachment"" {
  disk_name     = aws_lightsail_disk.disk2.name
  instance_name = aws_lightsail_instance.main.name
  disk_path     = ""/dev/xvdg""
}
```

This configuration:
1. Sets up the AWS provider
2. Gets available Availability Zones
3. Creates a Lightsail instance
4. Creates two different sized disks (8GB and 16GB)
5. Attaches both disks to the instance with different disk paths

The disks will be mounted at `/dev/xvdf` and `/dev/xvdg` respectively. You can modify the disk sizes, names, and paths according to your needs. Remember that the instance and disks must be in the same availability zone.",,,,,,"I'll help you create a Terraform configuration for a Lightsail instance with multiple attached disks. Based on the examples provided, I'll create a solution that combines multiple disk attachments to a single instance.

```hcl
# Configure AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""

  filter {
    name   = ""opt-in-status""
    values = [""opt-in-not-required""]
  }
}

# Create Lightsail Instance
resource ""aws_lightsail_instance"" ""main"" {
  name              = ""multi-disk-instance""
  availability_zone = data.aws_availability_zones.available.names[0]
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_1_0""
}

# Create first disk
resource ""aws_lightsail_disk"" ""disk1"" {
  name              = ""disk-1""
  size_in_gb        = 8
  availability_zone = data.aws_availability_zones.available.names[0]
}

# Create second disk
resource ""aws_lightsail_disk"" ""disk2"" {
  name              = ""disk-2""
  size_in_gb        = 16
  availability_zone = data.aws_availability_zones.available.names[0]
}

# Attach first disk
resource ""aws_lightsail_disk_attachment"" ""disk1_attachment"" {
  disk_name     = aws_lightsail_disk.disk1.name
  instance_name = aws_lightsail_instance.main.name
  disk_path     = ""/dev/xvdf""
}

# Attach second disk
resource ""aws_lightsail_disk_attachment"" ""disk2_attachment"" {
  disk_name     = aws_lightsail_disk.disk2.name
  instance_name = aws_lightsail_instance.main.name
  disk_path     = ""/dev/xvdg""
}
```

This configuration:
1. Sets up the AWS provider
2. Gets available Availability Zones
3. Creates a Lightsail instance
4. Creates two different disks with different sizes (8GB and 16GB)
5. Attaches both disks to the instance using different disk paths (/dev/xvdf and /dev/xvdg)

The disks will be attached to the instance and can be formatted and mounted within the operating system. Make sure to use different disk paths for each attachment to avoid conflicts.",,,,,,"I'll help you create a Terraform configuration for a Lightsail instance with multiple attached disks. Based on the examples provided, I'll create a solution that combines multiple disk attachments.

```hcl
# Configure AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""

  filter {
    name   = ""opt-in-status""
    values = [""opt-in-not-required""]
  }
}

# Create Lightsail Instance
resource ""aws_lightsail_instance"" ""main"" {
  name              = ""multi-disk-instance""
  availability_zone = data.aws_availability_zones.available.names[0]
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_1_0""
}

# Create first disk
resource ""aws_lightsail_disk"" ""disk1"" {
  name              = ""disk-1""
  size_in_gb        = 8
  availability_zone = data.aws_availability_zones.available.names[0]
}

# Create second disk
resource ""aws_lightsail_disk"" ""disk2"" {
  name              = ""disk-2""
  size_in_gb        = 16
  availability_zone = data.aws_availability_zones.available.names[0]
}

# Attach first disk
resource ""aws_lightsail_disk_attachment"" ""disk1_attachment"" {
  disk_name     = aws_lightsail_disk.disk1.name
  instance_name = aws_lightsail_instance.main.name
  disk_path     = ""/dev/xvdf""
}

# Attach second disk
resource ""aws_lightsail_disk_attachment"" ""disk2_attachment"" {
  disk_name     = aws_lightsail_disk.disk2.name
  instance_name = aws_lightsail_instance.main.name
  disk_path     = ""/dev/xvdg""
}
```

This configuration:
1. Sets up the AWS provider
2. Gets available Availability Zones
3. Creates a Lightsail instance
4. Creates two separate disks of different sizes (8GB and 16GB)
5. Attaches both disks to the instance with different disk paths

The disks will be mounted at `/dev/xvdf` and `/dev/xvdg` respectively. Make sure to initialize these disks and create file systems on them after they're attached to the instance.",,,,,
aws_kinesis_stream,Generate a basic Amazon Kinesis stream,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_kinesis_stream {
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_stream""
    resource.values.name
}

has_valid_resources {
	has_valid_kinesis_stream
}",2,"provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_stream"" ""test_stream"" {
  name             = ""drow1""
  shard_count      = 1
  retention_period = 48

  shard_level_metrics = [
    ""IncomingBytes"",
    ""OutgoingBytes"",
  ]

  stream_mode_details {
    stream_mode = ""PROVISIONED""
  }

  tags = {
    Environment = ""test""
  }
}","Has one ""aws_kinesis_stream"" resource","I'll help you generate a Terraform configuration for a basic Amazon Kinesis stream. Based on the provided examples and best practices, here's a solution:

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_stream"" ""main"" {
  name             = ""my-kinesis-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""production""
    Name        = ""my-kinesis-stream""
  }
}
```

This configuration:
1. Sets up the AWS provider for the us-west-2 region
2. Creates a Kinesis stream with:
   - A name ""my-kinesis-stream""
   - 1 shard (minimum required)
   - Default retention period of 24 hours
   - Basic tags for resource identification

You can customize the configuration by adjusting:
- The `shard_count` based on your throughput needs
- The `retention_period` (in hours) based on your data retention requirements
- The tags according to your organization's tagging strategy",,,,,,"I'll help you generate a Terraform configuration for a basic Amazon Kinesis stream. Based on the provided examples and best practices, here's a solution:

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_stream"" ""basic_stream"" {
  name             = ""basic-kinesis-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""production""
    Name        = ""basic-kinesis-stream""
  }
}
```

This configuration:
1. Sets up the AWS provider for the us-west-2 region
2. Creates a Kinesis stream with:
   - A descriptive name ""basic-kinesis-stream""
   - 1 shard (minimum requirement)
   - Default retention period of 24 hours
   - Basic tags for resource management

You can modify the `shard_count` and `retention_period` based on your specific needs. The retention period can be between 24 to 168 hours.",,,,,,"I'll help you generate a Terraform configuration for a basic Amazon Kinesis stream. Based on the examples provided and best practices, here's a configuration that creates a basic Kinesis stream:

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_stream"" ""main"" {
  name             = ""my-kinesis-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""production""
    Name        = ""my-kinesis-stream""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Kinesis stream with:
   - A descriptive name
   - 1 shard (minimum configuration)
   - Default retention period of 24 hours
   - Basic tags for resource management

You can adjust the `shard_count` and `retention_period` based on your specific needs. The `shard_count` determines the stream's capacity, and the `retention_period` (in hours) determines how long data records are accessible after being added to the stream.",,,,,
"aws_instance, aws_placement_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc",Create 3 EC2 instances within an EC2 placement group usign clustering.,"package main

import future.keywords.in

default allow = false

# Check for VPC with public and private subnets
vpc_exists {
    some i
    input.resource_changes[i].type == ""aws_vpc""
}

public_subnet_exists(resources) {
    some resource in resources
    resource.type == ""aws_subnet""
    resource.change.after.map_public_ip_on_launch == true
}

private_subnet_exists(resources) {
    some resource in resources
    resource.type == ""aws_subnet""
    resource.change.after.map_public_ip_on_launch == false
}

aws_instances[resource] {
    resource := input.resource_changes[_]
    resource.type == ""aws_instance""
}

# Aggregate all checks
allow {
    vpc_exists
    public_subnet_exists(input.resource_changes)
    private_subnet_exists(input.resource_changes)
    count(aws_instances) == 3
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

# Create a Virtual Private Cloud (VPC)
resource ""aws_vpc"" ""my_vpc"" {
  cidr_block = ""10.0.0.0/16""
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

# Create Public Subnet
resource ""aws_subnet"" ""public_subnet"" {
  vpc_id                  = aws_vpc.my_vpc.id
  cidr_block              = ""10.0.1.0/24""
  availability_zone       = data.aws_availability_zones.azs.names[0]
  map_public_ip_on_launch = true
}

# Create Private Subnet
resource ""aws_subnet"" ""private_subnet"" {
  vpc_id            = aws_vpc.my_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = data.aws_availability_zones.azs.names[1]
}

# Create EC2 Placement Group
resource ""aws_placement_group"" ""my_placement_group"" {
  name     = ""my-placement-group""
  strategy = ""cluster""
}

# Create Security Group for EC2 instances
resource ""aws_security_group"" ""ec2_sg"" {
  vpc_id = aws_vpc.my_vpc.id
  name   = ""ec2_sg""
}

data ""aws_ami"" ""amzn2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

# Create EC2 instances in the Placement Group in the Private Subnet
resource ""aws_instance"" ""ec2_instance"" {
  count           = 3
  ami             = data.aws_ami.amzn2.id
  instance_type   = ""m5.large""
  subnet_id       = aws_subnet.private_subnet.id
  security_groups = [aws_security_group.ec2_sg.id]
  placement_group = aws_placement_group.my_placement_group.name
}",,"I'll help you create a Terraform configuration for 3 EC2 instances within a placement group using clustering strategy. Based on the provided examples, I'll create a solution that follows AWS best practices.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create EC2 Placement Group
resource ""aws_placement_group"" ""cluster_placement_group"" {
  name     = ""cluster-placement-group""
  strategy = ""cluster""
}

# Get latest Amazon Linux 2 AMI
data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

# Create Security Group for EC2 instances
resource ""aws_security_group"" ""instance_sg"" {
  name_prefix = ""instance_sg""
  
  # You can add specific security group rules here if needed
}

# Create EC2 instances in the Placement Group
resource ""aws_instance"" ""cluster_instances"" {
  count = 3

  ami           = data.aws_ami.amazon_linux_2.id
  instance_type = ""m5.large""  # Using m5.large as it supports placement groups

  placement_group = aws_placement_group.cluster_placement_group.id
  vpc_security_group_ids = [aws_security_group.instance_sg.id]

  tags = {
    Name = ""cluster-instance-${count.index + 1}""
  }
}

# Output the instance IDs
output ""instance_ids"" {
  value = aws_instance.cluster_instances[*].id
}
```

This configuration:
1. Creates a placement group with clustering strategy
2. Uses the latest Ubuntu 24.04 AMI
3. Creates a security group for the instances
4. Launches 3 EC2 instances of type m5.large within the placement group
5. Tags each instance with a unique name
6. Outputs the instance IDs

Key points:
- The placement group uses the ""cluster"" strategy, which packs instances close together for high-performance computing
- M5.large instance type is used as it supports placement groups
- The instances are created in the same placement group for low-latency network performance
- A count parameter is used to create multiple instances
- Each instance gets a unique name via the tags

Note: You might want to add specific security group rules and VPC configurations depending on your network requirements.",,,,,,"I'll help you create a Terraform configuration for 3 EC2 instances within a placement group using clustering strategy. Based on the provided examples, I'll create a solution that follows AWS best practices.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create EC2 Placement Group
resource ""aws_placement_group"" ""cluster_placement_group"" {
  name     = ""cluster-placement-group""
  strategy = ""cluster""
}

# Get latest Amazon Linux 2 AMI
data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

# Create Security Group for EC2 instances
resource ""aws_security_group"" ""instance_sg"" {
  name_prefix = ""instance_sg""
  
  # You can add specific security group rules here if needed
}

# Create EC2 instances in the Placement Group
resource ""aws_instance"" ""cluster_instances"" {
  count = 3

  ami           = data.aws_ami.amazon_linux_2.id
  instance_type = ""m5.large""  # Using m5.large as it supports placement groups

  placement_group = aws_placement_group.cluster_placement_group.id
  vpc_security_group_ids = [aws_security_group.instance_sg.id]

  tags = {
    Name = ""cluster-instance-${count.index + 1}""
  }
}

# Output the instance IDs
output ""instance_ids"" {
  value = aws_instance.cluster_instances[*].id
}
```

This configuration will:

1. Create a placement group with clustering strategy
2. Fetch the latest Amazon Linux 2 AMI
3. Create a security group for the instances
4. Launch 3 EC2 instances within the placement group
5. Output the instance IDs

Key points about this configuration:
- Uses clustering strategy for the placement group, which is ideal for applications that need low latency and high network throughput
- Uses m5.large instance type which supports placement groups
- Instances are automatically spread across the placement group
- Each instance gets a unique name tag with an incrementing number
- Basic security group is included (you can add specific rules as needed)

Note: Remember that placement groups with clustering strategy have specific requirements:
- Instances must be launched in a single Availability Zone
- Not all instance types support placement groups
- It's recommended to use similar instance types within a cluster placement group",,,,,,"I'll help you create a Terraform configuration for 3 EC2 instances within a placement group using clustering strategy. Based on the provided examples, I'll create a solution that follows AWS best practices.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create EC2 Placement Group
resource ""aws_placement_group"" ""cluster_placement"" {
  name     = ""cluster-placement-group""
  strategy = ""cluster""
}

# Get latest Amazon Linux 2 AMI
data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

# Create Security Group for EC2 instances
resource ""aws_security_group"" ""instance_sg"" {
  name_prefix = ""instance_sg""
  description = ""Security group for EC2 instances in placement group""

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Create EC2 instances in the Placement Group
resource ""aws_instance"" ""cluster_instances"" {
  count = 3

  ami           = data.aws_ami.amazon_linux_2.id
  instance_type = ""m5.large""  # Using m5.large as it supports placement groups

  placement_group = aws_placement_group.cluster_placement.id
  vpc_security_group_ids = [aws_security_group.instance_sg.id]

  tags = {
    Name = ""cluster-instance-${count.index + 1}""
  }
}

# Output the instance IDs
output ""instance_ids"" {
  value = aws_instance.cluster_instances[*].id
}
```

This configuration will:
1. Create a placement group with clustering strategy
2. Use the latest Ubuntu 24.04 AMI
3. Create a security group with basic SSH access
4. Launch 3 EC2 instances (m5.large) within the placement group
5. Tag each instance with a unique name
6. Output the instance IDs

The clustering strategy places instances close together in the same Availability Zone, which provides low-latency network performance necessary for high-performance computing applications.

Note: When using cluster placement groups, it's recommended to:
- Use supported instance types (like m5.large)
- Launch all instances at the same time
- Use the same instance type for all instances in the group
- Launch instances in a VPC for optimal performance",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer that Specifying private IP addresses for an internal-facing load balancer,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource

is_valid_lb {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.expressions.load_balancer_type.constant_value == ""network""
    resource.expressions.subnet_mapping[0].private_ipv4_address != null
    resource.expressions.subnet_mapping[0].subnet_id != null
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    valid_type := {""instance"", ""ip"", ""alb""}
    type := resources.values.target_type
    valid_type[type]

    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_target_group""
    resource.expressions.port != null
    resource.expressions.protocol != null
    resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
    is_valid_lb
    is_valid_lb_listener
    is_valid_lb_target_group_attachment
    is_valid_lb_target_group
    is_valid_instance
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.azs.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""     # Replace with your desired instance type
  subnet_id     = module.vpc.public_subnets[0]
}

resource ""aws_lb"" ""test"" {
  load_balancer_type = ""network""
  
  subnet_mapping {
    subnet_id            = module.vpc.public_subnet_arns[0]
    private_ipv4_address = ""10.0.4.30""
  }

  subnet_mapping {
    subnet_id            = module.vpc.public_subnet_arns[1]
    private_ipv4_address = ""10.0.5.15""
  }
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.id
}","has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument

has one ""aws_lb"" resource
    with ""subnet_mapping""
        with private_ipv4_address
    with load_balancer_type = ""network""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument
    with one protocol argument
        with one of following values: TCP, UDP, TLS, TCP_UDP

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument

has at least one ""aws_eip""","I'll help you create a Terraform configuration for an internal-facing Application Load Balancer with specified private IP addresses. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.available.names
  private_subnets = [""10.0.1.0/24"", ""10.0.2.0/24"", ""10.0.3.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.private_subnets[0]
}

resource ""aws_lb"" ""internal"" {
  name               = ""internal-lb""
  internal           = true
  load_balancer_type = ""application""

  subnet_mapping {
    subnet_id            = module.vpc.private_subnets[0]
    private_ipv4_address = ""10.0.1.15""
  }

  subnet_mapping {
    subnet_id            = module.vpc.private_subnets[1]
    private_ipv4_address = ""10.0.2.15""
  }
}

resource ""aws_lb_listener"" ""internal_listener"" {
  load_balancer_arn = aws_lb.internal.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  name     = ""internal-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id        = aws_instance.my_instance.id
  port             = 80
}
```

This configuration:
1. Creates an internal Application Load Balancer using private subnets
2. Specifies private IP addresses (10.0.1.15 and 10.0.2.15) for the load balancer in each subnet
3. Sets up a listener on port 80
4. Creates a target group and attaches an EC2 instance to it
5. Configures health checks for the target group

The load balancer is marked as internal (internal = true) and will only be accessible from within the VPC or connected networks. The private IP addresses are specified in the subnet_mapping blocks, allowing you to control exactly where in your subnet the load balancer endpoints will be placed.",,,,,,"I'll help you create a Terraform configuration for an internal-facing Application Load Balancer (ALB) with specified private IP addresses. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""azs"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.azs.names
  private_subnets = [""10.0.1.0/24"", ""10.0.2.0/24"", ""10.0.3.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.private_subnets[0]
}

resource ""aws_lb"" ""internal"" {
  name               = ""internal-lb""
  internal           = true
  load_balancer_type = ""application""

  subnet_mapping {
    subnet_id            = module.vpc.private_subnets[0]
    private_ipv4_address = ""10.0.1.15""
  }

  subnet_mapping {
    subnet_id            = module.vpc.private_subnets[1]
    private_ipv4_address = ""10.0.2.15""
  }
}

resource ""aws_lb_listener"" ""internal_listener"" {
  load_balancer_arn = aws_lb.internal.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  name     = ""internal-tg""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    timeout             = 5
    path                = ""/""
    port                = ""traffic-port""
    protocol            = ""HTTP""
    unhealthy_threshold = 2
  }
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id        = aws_instance.my_instance.id
}
```

This configuration:

1. Creates an internal Application Load Balancer using private subnets
2. Specifies private IP addresses for each subnet mapping (10.0.1.15 and 10.0.2.15)
3. Sets up a listener on port 80
4. Creates a target group and attaches an EC2 instance to it
5. Uses the VPC module to create the necessary networking infrastructure
6. The load balancer is marked as internal with `internal = true`

Key features:
- The load balancer is internal-facing (not accessible from the internet)
- Uses specific private IP addresses in the subnet mappings
- Deployed across multiple availability zones for high availability
- Includes health checks for the target group
- EC2 instance is placed in a private subnet

Make sure the private IP addresses specified in the subnet mappings fall within the CIDR ranges of your respective private subnets.",,,,,,"I'll help you create a Terraform configuration for an internal Application Load Balancer with specified private IP addresses. This configuration will be based on the examples provided while adapting it for internal use with private IP addresses.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""
}

data ""aws_availability_zones"" ""azs"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.azs.names
  private_subnets = [""10.0.1.0/24"", ""10.0.2.0/24"", ""10.0.3.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.private_subnets[0]
}

resource ""aws_lb"" ""internal"" {
  name               = ""internal-lb""
  internal           = true
  load_balancer_type = ""application""

  subnet_mapping {
    subnet_id            = module.vpc.private_subnets[0]
    private_ipv4_address = ""10.0.1.15""
  }

  subnet_mapping {
    subnet_id            = module.vpc.private_subnets[1]
    private_ipv4_address = ""10.0.2.15""
  }

  enable_deletion_protection = false
}

resource ""aws_lb_listener"" ""internal_listener"" {
  load_balancer_arn = aws_lb.internal.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  name     = ""internal-tg""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval           = 30
    timeout            = 5
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    unhealthy_threshold = 2
  }
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id        = aws_instance.my_instance.id
  port             = 80
}
```

This configuration creates:

1. An internal Application Load Balancer (ALB) with specific private IP addresses in each subnet
2. The ALB is marked as internal using `internal = true`
3. Subnet mappings with specific private IPv4 addresses for each subnet
4. A target group and listener for HTTP traffic on port 80
5. An EC2 instance in the private subnet as the target
6. All necessary VPC infrastructure using the AWS VPC module

Key features:
- The load balancer is internal-facing (`internal = true`)
- Private IP addresses are specified for each subnet using `private_ipv4_address`
- The load balancer is placed in private subnets
- The configuration includes proper health checks and target group settings

Make sure the private IP addresses you specify (10.0.1.15 and 10.0.2",,,,,
"aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_s3_bucket",An AWS service that holds a web server which allows you to upload cat pictures and provides random cat pictures on demand. Accomplishes this with API Gateway and S3.,"package caas_high

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
	api.address in cat.expressions.parent_id.references
	api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
	method.expressions.http_method.constant_value == method_string
	resource.address in method.expressions.resource_id.references
	api.address in method.expressions.rest_api_id.references
	method.expressions.authorization
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some api in resources
	api.type == ""aws_api_gateway_rest_api""

	some cat in resources
	cat.type == ""aws_api_gateway_resource""

	some method_get in resources
	method_get.type == ""aws_api_gateway_method""

	some method_put in resources
	method_put.type == ""aws_api_gateway_method""

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	api_valid(api)
	cat_valid(cat, api)
	method_valid(method_get, ""GET"", cat, api)
	method_valid(method_put, ""PUT"", cat, api)
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource
    with ""http_method"" argument with values ""GET"" and ""PUT""
    with ""authorization"" argument
have one ""aws_s3_bucket"" resource","I'll help you create a Terraform configuration for a service that allows uploading and retrieving cat pictures using AWS API Gateway and S3. Based on Example 1 from the provided context, I'll adapt it to meet your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket_prefix = ""cat-pictures-""
}

# IAM role for API Gateway to access S3
resource ""aws_iam_role"" ""api_gateway_role"" {
  name = ""api_gateway_s3_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""apigateway.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for S3 access
resource ""aws_iam_role_policy"" ""api_gateway_s3_policy"" {
  name = ""api_gateway_s3_policy""
  role = aws_iam_role.api_gateway_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*""
        ]
      }
    ]
  })
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat-pictures-api""
}

# API Gateway resource for /cats endpoint
resource ""aws_api_gateway_resource"" ""cats"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# GET method for retrieving random cat picture
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cats.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# POST method for uploading cat pictures
resource ""aws_api_gateway_method"" ""upload_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cats.id
  http_method   = ""POST""
  authorization = ""NONE""
}

# Integration for GET method
resource ""aws_api_gateway_integration"" ""get_cat_integration"" {
  rest_api_id             = aws_api_gateway_rest_api.cat_api.id
  resource_id             = aws_api_gateway_resource.cats.id
  http_method             = aws_api_gateway_method.get_cat.http_method
  integration_http_method = ""GET""
  type                    = ""AWS""
  credentials             = aws_iam_role.api_gateway_role.arn
  uri                     = ""arn:aws:apigateway:${data.aws_region.current.name}:s3:path/${aws_s3_bucket.cat_pictures.id}/{key}""

  request_parameters = {
    ""integration.request.path.key"" = ""method.request.path.key""
  }
}

# Integration for POST method
resource ""aws_api_gateway_integration"" ""uploa",,,,,,"I'll help create a Terraform configuration for a service that allows uploading and retrieving cat pictures using AWS API Gateway and S3. Based on Example 1 provided, I'll adapt it to meet your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket_prefix = ""cat-pictures-""
}

# IAM role for API Gateway to access S3
resource ""aws_iam_role"" ""api_gateway_role"" {
  name = ""api_gateway_s3_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""apigateway.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for S3 access
resource ""aws_iam_role_policy"" ""api_gateway_s3_policy"" {
  name = ""api_gateway_s3_policy""
  role = aws_iam_role.api_gateway_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*""
        ]
      }
    ]
  })
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat-pictures-api""
}

# API Gateway resource for cats
resource ""aws_api_gateway_resource"" ""cats"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# GET method for random cat picture
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cats.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# PUT method for uploading cat picture
resource ""aws_api_gateway_method"" ""put_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cats.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

# Integration for GET method
resource ""aws_api_gateway_integration"" ""get_cat"" {
  rest_api_id             = aws_api_gateway_rest_api.cat_api.id
  resource_id             = aws_api_gateway_resource.cats.id
  http_method             = aws_api_gateway_method.get_cat.http_method
  type                    = ""AWS""
  integration_http_method = ""GET""
  credentials             = aws_iam_role.api_gateway_role.arn
  uri                     = ""arn:aws:apigateway:${var.region}:s3:path/${aws_s3_bucket.cat_pictures.id}/random""
}

# Integration for PUT method
resource ""aws_api_gateway_integration"" ""put_cat"" {
  rest_api_id             = aws_api_gateway_rest_api.cat_api.id
  resource_id             = aws_api_gateway_resource.cats.id
  http",,,,,,"I'll help you create a Terraform configuration for a web service that handles cat picture uploads and retrieval using AWS API Gateway and S3. Based on Example 1 from the reference, I'll adapt it to meet your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket to store cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket_prefix = ""cat-pictures-""
}

# IAM role for API Gateway to access S3
resource ""aws_iam_role"" ""api_gateway_role"" {
  name = ""api_gateway_s3_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""apigateway.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for S3 access
resource ""aws_iam_role_policy"" ""api_gateway_s3_policy"" {
  name = ""api_gateway_s3_policy""
  role = aws_iam_role.api_gateway_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*""
        ]
      }
    ]
  })
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat-pictures-api""
}

# API Gateway resource for /cats endpoint
resource ""aws_api_gateway_resource"" ""cats"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# GET method for retrieving random cat picture
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cats.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# PUT method for uploading cat pictures
resource ""aws_api_gateway_method"" ""put_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cats.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

# Integration for GET method
resource ""aws_api_gateway_integration"" ""get_cat"" {
  rest_api_id             = aws_api_gateway_rest_api.cat_api.id
  resource_id             = aws_api_gateway_resource.cats.id
  http_method             = aws_api_gateway_method.get_cat.http_method
  type                    = ""AWS""
  integration_http_method = ""GET""
  credentials             = aws_iam_role.api_gateway_role.arn
  uri                     = ""arn:aws:apigateway:${data.aws_region.current.name}:s3:path/${aws_s3_bucket.cat_pictures.id}/{key}""

  request_parameters = {
    ""integration.request.path.key"" = ""method.request.path.key""
  }
}

# Integration for PUT method
resource ""aws_api_gateway_integration"" ""put_cat""",,,,,
"aws_msk_serverless_cluster, aws_security_group, aws_subnet, aws_subnet, aws_subnet, aws_vpc, aws_availability_zones",Create a serverless MSK with 3 broker nodes.,"package main

import future.keywords.in

default allow = false

# Check if MSK cluster is serverless
is_serverless(resource) {
resource.type == ""aws_msk_serverless_cluster""
}


# Check if cluster spans three AZs
has_three_azs(resource) {
resource.type == ""aws_msk_serverless_cluster""
count(resource.expressions.vpc_config[_].subnet_ids.references) == 6
}

# Main rule combining all checks
allow {
some resource in input.configuration.root_module.resources
is_serverless(resource)
has_three_azs(resource)
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""vpc"" {
    cidr_block = ""192.168.0.0/22""
    enable_dns_support = true
    enable_dns_hostnames = true
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

resource ""aws_subnet"" ""subnet_az1"" {
    availability_zone = data.aws_availability_zones.azs.names[0]
    cidr_block = ""192.168.0.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az2"" {
    availability_zone = data.aws_availability_zones.azs.names[1]
    cidr_block = ""192.168.1.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az3"" {
    availability_zone = data.aws_availability_zones.azs.names[2]
    cidr_block = ""192.168.2.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.vpc.id
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }
}

resource ""aws_route_table_association"" ""first"" {
  subnet_id      = aws_subnet.subnet_az1.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""second"" {
  subnet_id      = aws_subnet.subnet_az2.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""third"" {
  subnet_id      = aws_subnet.subnet_az3.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_security_group"" ""sg"" {
    vpc_id = aws_vpc.vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4 = ""0.0.0.0/0""
  ip_protocol = ""-1""
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  ip_protocol       = ""-1""
}

resource ""aws_msk_serverless_cluster"" ""example"" {
    cluster_name = ""Example""

    vpc_config {
        subnet_ids = [aws_subnet.subnet_az1.id, aws_subnet.subnet_az2.id, aws_subnet.subnet_az3.id]
        security_group_ids = [aws_security_group.sg.id]
    }

    client_authentication {
        sasl {
            iam {
                enabled = true
            }
        }
    }
}","Has one ""aws_msk_serverless_cluster"" resource
with one ""vpc_config""
with one ""subnet_ids"" with 6 references (3 subnets)","I'll help you create a Terraform configuration for a serverless MSK cluster with 3 broker nodes. This will be based on Example 2 from the reference material, as it specifically deals with serverless MSK configuration.

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create VPC
resource ""aws_vpc"" ""vpc"" {
  cidr_block           = ""192.168.0.0/22""
  enable_dns_support   = true
  enable_dns_hostnames = true
}

# Get available AZs
data ""aws_availability_zones"" ""azs"" {
  state = ""available""
}

# Create subnets in 3 AZs
resource ""aws_subnet"" ""subnet_az1"" {
  availability_zone       = data.aws_availability_zones.azs.names[0]
  cidr_block             = ""192.168.0.0/24""
  vpc_id                 = aws_vpc.vpc.id
  map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az2"" {
  availability_zone       = data.aws_availability_zones.azs.names[1]
  cidr_block             = ""192.168.1.0/24""
  vpc_id                 = aws_vpc.vpc.id
  map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az3"" {
  availability_zone       = data.aws_availability_zones.azs.names[2]
  cidr_block             = ""192.168.2.0/24""
  vpc_id                 = aws_vpc.vpc.id
  map_public_ip_on_launch = true
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.vpc.id
}

# Create Route Table
resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }
}

# Associate Route Table with Subnets
resource ""aws_route_table_association"" ""first"" {
  subnet_id      = aws_subnet.subnet_az1.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""second"" {
  subnet_id      = aws_subnet.subnet_az2.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""third"" {
  subnet_id      = aws_subnet.subnet_az3.id
  route_table_id = aws_route_table.main.id
}

# Create Security Group
resource ""aws_security_group"" ""sg"" {
  vpc_id = aws_vpc.vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4        = ""0.0.0.0/0""
  ip_protocol      = ""-1""
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4        = ""0.0.0.0/0""
  ip_protocol      = ""-1""
}

# Create Serverless MSK Cluster
resource ""aws_msk_serverless_cluster"" ""example"" {
  cluster_name = ""example-serverless-msk""

  vpc_config {
    subnet_ids         = [aws_subnet.subnet_az1.id, aws_subnet.subnet_az2.id, aws_subnet",,,,,,"I'll help you create a Terraform configuration for a serverless MSK cluster with 3 broker nodes. Based on Example 2 from the reference, I'll create a similar configuration but focused on the serverless MSK cluster setup.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC Configuration
resource ""aws_vpc"" ""msk_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_support   = true
  enable_dns_hostnames = true
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Create 3 subnets in different AZs
resource ""aws_subnet"" ""msk_subnet_1"" {
  vpc_id            = aws_vpc.msk_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = data.aws_availability_zones.available.names[0]
}

resource ""aws_subnet"" ""msk_subnet_2"" {
  vpc_id            = aws_vpc.msk_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = data.aws_availability_zones.available.names[1]
}

resource ""aws_subnet"" ""msk_subnet_3"" {
  vpc_id            = aws_vpc.msk_vpc.id
  cidr_block        = ""10.0.3.0/24""
  availability_zone = data.aws_availability_zones.available.names[2]
}

# Security Group
resource ""aws_security_group"" ""msk_sg"" {
  name        = ""msk-security-group""
  description = ""Security group for MSK cluster""
  vpc_id      = aws_vpc.msk_vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""msk_ingress"" {
  security_group_id = aws_security_group.msk_sg.id
  cidr_ipv4        = ""0.0.0.0/0""
  ip_protocol      = ""-1""
}

resource ""aws_vpc_security_group_egress_rule"" ""msk_egress"" {
  security_group_id = aws_security_group.msk_sg.id
  cidr_ipv4        = ""0.0.0.0/0""
  ip_protocol      = ""-1""
}

# MSK Serverless Cluster
resource ""aws_msk_serverless_cluster"" ""msk_serverless"" {
  cluster_name = ""msk-serverless-cluster""

  vpc_config {
    subnet_ids = [
      aws_subnet.msk_subnet_1.id,
      aws_subnet.msk_subnet_2.id,
      aws_subnet.msk_subnet_3.id
    ]
    security_group_ids = [aws_security_group.msk_sg.id]
  }

  client_authentication {
    sasl {
      iam {
        enabled = true
      }
    }
  }
}

# Output the cluster ARN
output ""msk_cluster_arn"" {
  value = aws_msk_serverless_cluster.msk_serverless.arn
}
```

This configuration will create:
1. A VPC with three subnets across different availability zones
2. A security group with basic ingress/egress rules
3. A serverless MSK cluster spanning the three subnets
4. IAM authentication enabled for the cluster

Key features of this configuration:
- The cluster is serverless, which means AWS automatically manages the broker scaling
- The cluster is distributed across 3 availability zones for high availability
- IAM authentication is enabled for security
- The security group allows all traffic (you should restrict this based on your requirements)

Note that with MSK Serverless:
- You",,,,,,"I'll help you create a Terraform configuration for a serverless MSK cluster with 3 broker nodes. I'll use Example 2 as the primary reference since it specifically deals with serverless MSK.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create VPC
resource ""aws_vpc"" ""vpc"" {
  cidr_block           = ""192.168.0.0/22""
  enable_dns_support   = true
  enable_dns_hostnames = true
}

# Get available AZs
data ""aws_availability_zones"" ""azs"" {
  state = ""available""
}

# Create subnets in 3 AZs
resource ""aws_subnet"" ""subnet_az1"" {
  availability_zone       = data.aws_availability_zones.azs.names[0]
  cidr_block             = ""192.168.0.0/24""
  vpc_id                 = aws_vpc.vpc.id
  map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az2"" {
  availability_zone       = data.aws_availability_zones.azs.names[1]
  cidr_block             = ""192.168.1.0/24""
  vpc_id                 = aws_vpc.vpc.id
  map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az3"" {
  availability_zone       = data.aws_availability_zones.azs.names[2]
  cidr_block             = ""192.168.2.0/24""
  vpc_id                 = aws_vpc.vpc.id
  map_public_ip_on_launch = true
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.vpc.id
}

# Create Route Table
resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }
}

# Associate Route Table with Subnets
resource ""aws_route_table_association"" ""first"" {
  subnet_id      = aws_subnet.subnet_az1.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""second"" {
  subnet_id      = aws_subnet.subnet_az2.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""third"" {
  subnet_id      = aws_subnet.subnet_az3.id
  route_table_id = aws_route_table.main.id
}

# Create Security Group
resource ""aws_security_group"" ""sg"" {
  vpc_id = aws_vpc.vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4        = ""0.0.0.0/0""
  ip_protocol      = ""-1""
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4        = ""0.0.0.0/0""
  ip_protocol      = ""-1""
}

# Create Serverless MSK Cluster
resource ""aws_msk_serverless_cluster"" ""example"" {
  cluster_name = ""example-serverless""

  vpc_config {
    subnet_ids         = [aws_subnet.subnet_az1.id, aws_subnet.subnet_az2.id, aws_subnet.subnet_az3.id]
    security_group_ids",,,,,
"aws_api_gateway_integration, aws_api_gateway_integration, aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_dynamodb_table, aws_iam_role, aws_iam_role, aws_lambda_function, aws_lambda_function, aws_lambda_permission, aws_lambda_permission, aws_s3_bucket, archive_file, archive_file","An API Gateway Rest API named ""caas"" that has a resource endpoint named ""caas_cat"" with a path_part ""cat"" linking to two methods ""caas_get"" and ""caas_put"" with http_method ""GET"" and ""PUT."" Each of the methods should have a corresponding Lambda function that the API Gateway will have permission to access. A S3 bucket that stores cat pictures which will be accessed by a Lambda function, and a DynamoDB table with one keyed attribute, strings ""name"" of the cat.","package caas

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
        api.address in cat.expressions.parent_id.references
        api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
        method.expressions.http_method.constant_value == method_string
        resource.address in method.expressions.resource_id.references
        api.address in method.expressions.rest_api_id.references
}

table_valid(table) if {
        some attribute in table.expressions.attribute
        attribute.name.constant_value == ""name""
        attribute.type.constant_value == ""S""

        table.expressions.hash_key.constant_value == ""name""
}

lambda_valid(lambda, bucket) if {
        some env in lambda.expressions.environment
        bucket.address in env.variables.references
}

permission_valid(permission, lambda, api) if {
        permission.expressions.action.constant_value == ""lambda:InvokeFunction""
        lambda.address in permission.expressions.function_name.references
        permission.expressions.principal.constant_value == ""apigateway.amazonaws.com""
        api.address in permission.expressions.source_arn.references
}

integration_valid(integration, lambda, method, resource, api, integration_method) if {
        method.address in integration.expressions.http_method.references
        resource.address in integration.expressions.resource_id.references
        api.address in integration.expressions.rest_api_id.references
        integration.expressions.integration_http_method.constant_value == integration_method
        integration.expressions.type.constant_value == ""AWS_PROXY""
        lambda.address in integration.expressions.uri.references
}

default valid := false

valid if {
        resources := input.configuration.root_module.resources

        some api in resources
        api.type == ""aws_api_gateway_rest_api""

        some cat in resources
        cat.type == ""aws_api_gateway_resource""

        some method_get in resources
        method_get.type == ""aws_api_gateway_method""

        some method_put in resources
        method_put.type == ""aws_api_gateway_method""

        some table in resources
        table.type == ""aws_dynamodb_table""

        some lambda_get in resources
        lambda_get.type == ""aws_lambda_function""

        some lambda_put in resources
        lambda_put.type == ""aws_lambda_function""

        some bucket in resources
        bucket.type == ""aws_s3_bucket""

        some permission_get in resources
        permission_get.type == ""aws_lambda_permission""

        some permission_put in resources
        permission_put.type == ""aws_lambda_permission""

        some integration_get in resources
        integration_get.type == ""aws_api_gateway_integration""

        some integration_put in resources
        integration_put.type == ""aws_api_gateway_integration""

        api_valid(api)
        cat_valid(cat, api)
        method_valid(method_get, ""GET"", cat, api)
        method_valid(method_put, ""PUT"", cat, api)
        lambda_valid(lambda_get, bucket)
        lambda_valid(lambda_put, bucket)
        permission_valid(permission_get, lambda_get, api)
        permission_valid(permission_put, lambda_put, api)
        integration_valid(integration_get, lambda_get, method_get, cat, api, ""GET"")
        integration_valid(integration_put, lambda_put, method_put, cat, api, ""PUT"")
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument name
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument with value ""cat""
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_api resource id
    with ""resource_id"" linking to aws_api_gateway_resource resource
    with ""authorization"" argument with value ""NONE""
    with ""http_method"" argument with values ""GET"" and ""PUT""
have one ""aws_s3_bucket"" resource
have one ""aws_dynamodb_table"" resource
    with ""name"" argument
    with ""hash_key"" argument with value ""name""
    with ""attribute"" argument block
        with ""name"" argument with value ""name""
        with ""type"" argument with value ""S""
with two ""aws_lambda_function"" resources
    with ""function_name"" argument
    with ""role"" argument linking to aws_iam_role resources
    with ""environment"" argument block
        with ""variables"" argument block linking to the aws_s3_bucket
with two ""aws_lambda_permission"" resources
    with ""action"" argument with value ""lambda:InvokeFunction""
    with ""principal"" argument with value ""apigateway.amazonaws.com""
    with ""function_name"" argument linking to aws_lambda_function resource
    with ""source_arn"" argument linking to aws_api_gateway_rest_api (two different end points)
with two ""aws_api_gateway_integration"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource resource
    with ""http_method"" argument linking to aws_api_gateway_method resource
    with ""type"" argument with value ""AWS_PROXY""
    with ""integration_http_method"" argument with values ""GET"" and ""PUT""","Based on the provided examples and requirements, I'll help you create a Terraform configuration for your cat picture service. This solution will include API Gateway, Lambda functions, S3 bucket, and DynamoDB table configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# DynamoDB Table
resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

# S3 Bucket for cat pictures
resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-pictures""
}

# IAM Role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""caas_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""caas_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem""
        ]
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

# GET Method
resource ""aws_api_gateway_method"" ""caas_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# PUT Method
resource ""aws_api_gateway_method"" ""caas_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

# Lambda Functions
resource ""aws_lambda_function"" ""caas_get"" {
  filename         = ""get_function.zip""
  function_name    = ""caas_get_function""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs18.x""

  environment {
    variables = {
      S3_BUCKET      = aws_s3_bucket.caas.id
      DYNAMODB_TABLE = aws_dynamodb_table.caas.id
    }
  }
}

resource ""aws_lambda_function"" ""caas_put"" {
  filename         = ""put_function.zip""
  function_name    = ""caas_put",,,,,,"I'll help you create a Terraform configuration for your cat picture service using AWS API Gateway, Lambda, S3, and DynamoDB. This solution is based on the provided examples and follows AWS best practices.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# DynamoDB Table
resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

# S3 Bucket
resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-pictures""
}

# IAM Role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Policy for Lambda
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

# API Gateway Methods
resource ""aws_api_gateway_method"" ""caas_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

# Lambda Functions
resource ""aws_lambda_function"" ""caas_get"" {
  filename      = ""get_function.zip""
  function_name = ""caas_get""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs18.x""

  environment {
    variables = {
      S3_BUCKET     = aws_s3_bucket.caas.id
      DYNAMODB_TABLE = aws_dynamodb_table.caas.id
    }
  }
}

resource ""aws_lambda_function"" ""caas_put"" {
  filename      = ""put_function.zip""
  function_name = ""caas_put""
  role          = aws_iam_role.lambda_role.arn
  ",,,,,,"Based on the examples provided and your requirements, I'll help you create a Terraform configuration for your cat picture service. This solution will include an API Gateway, Lambda functions, S3 bucket, and DynamoDB table with proper permissions.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# DynamoDB Table
resource ""aws_dynamodb_table"" ""cat_table"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

# S3 Bucket for cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket_prefix = ""cat-pictures""
}

# IAM Role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_service_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""lambda.amazonaws.com""
      }
    }]
  })
}

# IAM Policy for Lambda
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_service_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Resource = ""${aws_s3_bucket.cat_pictures.arn}/*""
      },
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem""
        ]
        Resource = aws_dynamodb_table.cat_table.arn
      }
    ]
  })
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

# GET Method
resource ""aws_api_gateway_method"" ""caas_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# PUT Method
resource ""aws_api_gateway_method"" ""caas_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

# Lambda Functions
resource ""aws_lambda_function"" ""get_cat"" {
  filename         = ""get_cat.zip""
  function_name    = ""get_cat""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs18.x""

  environment {
    variables = {
      S3_BUCKET      = aws_s3_bucket.cat_pictures.id
      DYNAMODB_TABLE = aws_dynamodb_table.cat_table.id
    }
  }
}

resource ""aws_lambda_function"" ""put_cat"" {
  filename         = ""put_cat.zip""
  function_name    = ""put_cat""
  role            = aws_",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate an Application Load Balancer,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# has valid subnet
is_valid_subnet {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_subnet""
    resource.expressions.vpc_id != null
    have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_listener""
    resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    valid_type := {""instance"", ""ip"", ""alb""}
    type := resources.values.target_type
    valid_type[type]

    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_target_group""
    resource.expressions.port != null
    resource.expressions.protocol != null
    resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
    is_valid_lb
    is_valid_lb_listener
    is_valid_lb_target_group_attachment
    is_valid_lb_target_group
    is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.public_subnets[0]
  security_groups = [ aws_security_group.instance-sg.id ]
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.id
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you generate a Terraform configuration for an Application Load Balancer (ALB) in AWS. Based on the examples provided, I'll create a complete configuration including the necessary networking and security components.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# VPC Configuration
module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""alb-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.available.names
  public_subnets  = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name   = ""alb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""alb_sg_ingress_http"" {
  security_group_id = aws_security_group.alb_sg.id
  from_port         = 80
  to_port           = 80
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
}

resource ""aws_vpc_security_group_egress_rule"" ""alb_sg_egress"" {
  security_group_id = aws_security_group.alb_sg.id
  from_port         = 0
  to_port           = 0
  ip_protocol       = ""-1""
  cidr_ipv4         = ""0.0.0.0/0""
}

# Application Load Balancer
resource ""aws_lb"" ""application_lb"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = module.vpc.public_subnets

  enable_deletion_protection = false
}

# ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.application_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type = ""fixed-response""

    fixed_response {
      content_type = ""text/plain""
      message_body = ""Welcome to ALB""
      status_code  = ""200""
    }
  }
}

# Target Group
resource ""aws_lb_target_group"" ""target_group"" {
  name     = ""alb-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    timeout             = 5
    path                = ""/""
    port                = ""traffic-port""
    protocol            = ""HTTP""
    matcher             = ""200""
    unhealthy_threshold = 2
  }
}
```

This configuration creates:
1. A VPC with public subnets across multiple availability zones
2. A security group for the ALB that allows inbound HTTP traffic
3. An Application Load Balancer deployed across the public subnets
4. A listener on port 80 with a default fixed response
5. A target group for the ALB

The ALB is:
- Public-facing (not internal)
- Deployed across multiple availability zones for high availability
- Configured with basic HTTP listener
- Has a default fixed response
- Includes a target group ready for attaching instances or other resources

You can customize this further by:
- Adding HTTPS listeners
- Mo",,,,,,"I'll help you generate a Terraform configuration for an Application Load Balancer (ALB) in AWS. Based on the examples provided, I'll create a complete configuration including the necessary VPC, security groups, and target group setup.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs            = data.aws_availability_zones.available.names
  public_subnets = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name   = ""alb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""alb_sg_ingress_rule"" {
  from_port         = 80
  to_port           = 80
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.alb_sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""alb_sg_egress_rule"" {
  from_port         = 0
  to_port           = 0
  ip_protocol       = ""-1""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.alb_sg.id
}

# Application Load Balancer
resource ""aws_lb"" ""application_lb"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = module.vpc.public_subnets

  enable_deletion_protection = false
}

# Target Group
resource ""aws_lb_target_group"" ""target_group"" {
  name     = ""my-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.application_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}
```

This configuration creates:
1. A VPC with public subnets across multiple availability zones
2. A security group for the ALB allowing HTTP traffic on port 80
3. An Application Load Balancer that's internet-facing
4. A target group for the ALB with health check configuration
5. A listener that forwards HTTP traffic to the target group

The ALB is configured as internet-facing (public) and will distribute incoming HTTP traffic across the targets that you register with the target group. You can attach EC2 instances, IP addresses, or Lambda functions to this target group depending on your needs.

Note that this is a basic configuration - you might want to add:
- HTTPS listener with SSL certificate
- Additional security group rules
- Custom routing rules
- Access logs configuration
- Tags",,,,,,"I'll help you generate a Terraform configuration for an Application Load Balancer (ALB) in AWS. Based on the examples provided, I'll create a complete configuration including the necessary VPC, security groups, and target group setup.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs            = data.aws_availability_zones.available.names
  public_subnets = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name   = ""alb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""alb_http"" {
  security_group_id = aws_security_group.alb_sg.id
  from_port         = 80
  to_port           = 80
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
}

resource ""aws_vpc_security_group_egress_rule"" ""alb_egress"" {
  security_group_id = aws_security_group.alb_sg.id
  from_port         = 0
  to_port           = 0
  ip_protocol       = ""-1""
  cidr_ipv4         = ""0.0.0.0/0""
}

# Application Load Balancer
resource ""aws_lb"" ""application_lb"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = module.vpc.public_subnets

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
  }
}

# Target Group
resource ""aws_lb_target_group"" ""target_group"" {
  name     = ""my-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 3
  }
}

# Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.application_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}
```

This configuration creates:
1. A VPC with public subnets across multiple availability zones
2. A security group for the ALB allowing HTTP traffic on port 80
3. An Application Load Balancer deployed across the public subnets
4. A target group for the ALB
5. A listener that forwards HTTP traffic to the target group

The ALB is configured as an internet-facing load balancer (external) and includes basic health checks. You can customize the configuration further by:
- Adding HTTPS listeners
- Modifying health check parameters
- Adding additional security group rules
- Configuring access logs
- Adding tags
- Adjusting deletion protection settings

To use this configuration, you'll need to:
1. Initialize Terraform
2.",,,,,
"aws_db_instance, aws_db_instance, aws_elastic_beanstalk_application, aws_elastic_beanstalk_application, aws_elastic_beanstalk_environment, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment, aws_route53_record, aws_route53_record, aws_route53_zone","Configure multi-region application deployment with Route 53 resources to manage a domain and direct traffic to Elastic Beanstalk environments that have a connected RDS database. Provision the respective resources in either ""us-east-1"" or ""eu-west-1"" regions and name the aliases ""us-east-1"" and ""eu-west-1"" respectively. Name the environments ""myenv_us_east"" and ""myenv_eu_west"", the applications ""myapp_us_east"" and ""myapp_eu_west"", the instance profile ""eb_ec2_profile3"", and the databases ""main_db_us_east"" and ""main_db_eu_west""","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_eb_app = false

default is_valid_eb_env = false

default is_valid_db_instance = false

default is_valid_r53_zone = false

default is_valid_r53_record = false



# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.name
}

# Validate aws_route53_record
is_valid_r53_record {
        some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.provider_config_key == ""aws.us_east_1""
    resource.expressions.name
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.set_identifier
    resource.expressions.latency_routing_policy[0].region.constant_value == ""us-east-1""
    resource.expressions.records.references[0] == ""aws_elastic_beanstalk_environment.myenv_us_east.cname""
    resource.expressions.zone_id.references[0]
    
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_route53_record""    
    resource2.provider_config_key == ""aws.eu_west_1""
    resource2.expressions.name
    resource2.expressions.type
    resource2.expressions.ttl
    resource2.expressions.set_identifier
    resource2.expressions.latency_routing_policy[0].region.constant_value == ""eu-west-1""
    resource2.expressions.records.references[0] == ""aws_elastic_beanstalk_environment.myenv_eu_west.cname""
    resource2.expressions.zone_id.references[0]

}


is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0]
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0]
}

# Validate aws_eb_app
is_valid_eb_app {
        some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
    resource.provider_config_key == ""aws.us_east_1""
        
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_elastic_beanstalk_application""
    resource2.expressions.name
    resource2.provider_config_key == ""aws.eu_west_1""
}

# Validate aws_eb_env
is_valid_eb_env {
        some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.provider_config_key == ""aws.us_east_1""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.myapp_us_east.name""
    resource.expressions.solution_stack_name
    
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_elastic_beanstalk_environment""
    resource2.provider_config_key == ""aws.eu_west_1""
    resource2.expressions.name
    resource2.expressions.application.references[0] == ""aws_elastic_beanstalk_application.myapp_eu_west.name""
    resource2.expressions.solution_stack_name
    
            some a, b, c, d, e, f, g, h
    resource.expressions.setting[a].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile3.name""
    resource.expressions.setting[b].value.references[0] == ""aws_db_instance.main_db_us_east.username""
    resource.expressions.setting[c].value.references[0] == ""aws_db_instance.main_db_us_east.password""
    resource.expressions.setting[d].value.references[0] == ""aws_db_instance.main_db_us_east.address""
    
    resource2.expressions.setting[e].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile3.name""
    resource2.expressions.setting[f].value.references[0] == ""aws_db_instance.main_db_eu_west.username""
    resource2.expressions.setting[g].value.references[0] == ""aws_db_instance.main_db_eu_west.password""
    resource2.expressions.setting[h].value.references[0] == ""aws_db_instance.main_db_eu_west.address""

}

is_valid_db_instance {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_db_instance""
    has_required_main_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_main_db_arguments {
                some i, j
    resource := input.configuration.root_module.resources[i]
    resource2 := input.configuration.root_module.resources[j]
    resource.type == ""aws_db_instance""
    resource.provider_config_key == ""aws.us_east_1""
    resource2.type == ""aws_db_instance""
    resource2.provider_config_key == ""aws.eu_west_1""
    # Check for allocated_storage, engine, instance_class, username, password, and skip_final_snapshot
    requirement1(resource.expressions)
    requirement1(resource2.expressions)
    # Check for instance_class validity
    requirement2(resource.expressions)
    requirement2(resource2.expressions)
}



# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
    expressions.allocated_storage
    expressions.engine
    expressions.username
    expressions.password
    is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
    expressions.snapshot_identifier
}

# Check for instance_class validity
requirement2(expressions) {
    expressions.instance_class
    is_valid_instance_class(expressions.instance_class.constant_value)
}


# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
    is_valid_eb_app
    is_valid_eb_env
    is_valid_db_instance
    is_valid_r53_zone
    is_valid_r53_record
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""

      configuration_aliases = [ aws.us_east_1, aws.eu_west_1 ]
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  alias  = ""us_east_1""
  region = ""us-east-1""

  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

provider ""aws"" {
  alias  = ""eu_west_1""
  region = ""eu-west-1""

  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}


resource ""aws_iam_role"" ""eb_ec2_role"" {
  provider = aws.us_east_1
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  provider = aws.us_east_1
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}


# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  provider = aws.us_east_1
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# RDS in US East (N. Virginia)
resource ""aws_db_instance"" ""main_db_us_east"" {
  provider             = aws.us_east_1
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  username             = ""dbadmin""
  password             = ""mysecurepassword""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
  multi_az             = true
}

# Elastic Beanstalk Application in US East
resource ""aws_elastic_beanstalk_application"" ""myapp_us_east"" {
  provider = aws.us_east_1
  name     = ""my-application-us-east""
}

# Elastic Beanstalk Environment in US East
resource ""aws_elastic_beanstalk_environment"" ""myenv_us_east"" {
  provider            = aws.us_east_1
  name                = ""my-application-env-us-east""
  application         = aws_elastic_beanstalk_application.myapp_us_east.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.main_db_us_east.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.main_db_us_east.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.main_db_us_east.address
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# RDS in EU West (Ireland)
resource ""aws_db_instance"" ""main_db_eu_west"" {
  provider             = aws.eu_west_1
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  multi_az             = true
  username             = ""dbadmin""
  password             = ""mysecurepassword""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
}

# Elastic Beanstalk Application in EU West
resource ""aws_elastic_beanstalk_application"" ""myapp_eu_west"" {
  provider = aws.eu_west_1
  name     = ""my-application-eu-west""
}

# Elastic Beanstalk Environment in EU West
resource ""aws_elastic_beanstalk_environment"" ""myenv_eu_west"" {
  provider            = aws.eu_west_1
  name                = ""my-application-env-eu-west""
  application         = aws_elastic_beanstalk_application.myapp_eu_west.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.main_db_eu_west.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.main_db_eu_west.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.main_db_eu_west.address
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Main Route 53 Hosted Zone
resource ""aws_route53_zone"" ""main"" {
  provider = aws.us_east_1
  name = ""myapp.com""
}

# Latency Based Routing in Route 53 for US East Elastic Beanstalk Environment
resource ""aws_route53_record"" ""eb_env_us_east"" {
  provider = aws.us_east_1
  zone_id = aws_route53_zone.main.zone_id
  name    = ""us-east.myapp.com""
  type    = ""CNAME""
  ttl     = ""60""
  records = [aws_elastic_beanstalk_environment.myenv_us_east.cname]
  set_identifier = ""EastEnv""
  latency_routing_policy {
    region = ""us-east-1""
  }
}

resource ""aws_route53_record"" ""eb_env_eu_west"" {
  provider = aws.eu_west_1
  zone_id = aws_route53_zone.main.zone_id
  name    = ""eu-west.myapp.com""
  type    = ""CNAME""
  ttl     = ""60""
  records = [aws_elastic_beanstalk_environment.myenv_eu_west.cname]
  set_identifier = ""WestEnv""
  latency_routing_policy {
    region = ""eu-west-1""
  }
}
","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has two ""aws_elastic_beanstalk_application"" resource provisioned in different regions
    with ""name""

Has two ""aws_elastic_beanstalk_environment"" resources provisioned in different regions
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with a ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource
    with a ""setting"" block that has a ""value""  that references the ""address"" from one of the ""db_instance"" resource
    with a ""setting"" block that has a ""value""  that references the ""username"" from one of the ""db_instance"" resource 
    with a ""setting"" block that has a ""value""  that references the ""password"" from one of the ""db_instance"" resource 

Has two ""aws_db_instance"" resources provisioned in different regions
    with ""instance_class""
    with ""allocated_storage""
    with ""engine""
    with ""username""
    with ""password""
    with ""backup_retention_period"" set to a non-zero number
    with ""skip_final_snapshot""


Has one ""aws_route53_zone"" resource
    with ""name""

Has two ""aws_route53_record"" resources provisioned in different regions
    with ""name""
    with ""type""
    with ""ttl""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
    with ""set_identifier""
    with ""records"" referencing one of the ""aws_elastic_beanstalk_environment"" resource
    with ""latency_routing_policy"" containing ""region"" 


","I'll help you create a Terraform configuration for a multi-region application deployment with Route 53, Elastic Beanstalk, and RDS. Based on the provided examples, I'll combine and adapt the configurations to meet your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
      configuration_aliases = [ aws.us-east-1, aws.eu-west-1 ]
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  alias  = ""us-east-1""
}

provider ""aws"" {
  region = ""eu-west-1""
  alias  = ""eu-west-1""
}

# IAM Role and Instance Profile (shared across regions)
resource ""aws_iam_role"" ""eb_ec2_role3"" {
  provider = aws.us-east-1
  name = ""elastic_beanstalk_ec2_role3""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  provider   = aws.us-east-1
  role       = aws_iam_role.eb_ec2_role3.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile3"" {
  provider = aws.us-east-1
  name = ""elastic_beanstalk_ec2_profile3""
  role = aws_iam_role.eb_ec2_role3.name
}

# US East Resources
resource ""aws_db_instance"" ""main_db_us_east"" {
  provider             = aws.us-east-1
  identifier           = ""main-db-us-east""
  allocated_storage    = 20
  storage_type        = ""gp2""
  engine              = ""mysql""
  engine_version      = ""5.7""
  instance_class      = ""db.t3.micro""
  username            = ""dbadmin""
  password            = ""mypassword123""
  skip_final_snapshot = true
}

resource ""aws_elastic_beanstalk_application"" ""myapp_us_east"" {
  provider    = aws.us-east-1
  name        = ""myapp-us-east""
  description = ""US East Application""
}

resource ""aws_elastic_beanstalk_environment"" ""myenv_us_east"" {
  provider            = aws.us-east-1
  name                = ""myenv-us-east""
  application         = aws_elastic_beanstalk_application.myapp_us_east.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.main_db_us_east.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.main_db_us_east.username
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile3.name
  }
}

# EU West Resources
resource ""aws_db_instance"" ""main_db_eu_west"" {
  provider             = aws.eu-west-1
  identifier           =",,,,,,"I'll help you create a Terraform configuration for a multi-region application deployment with Route 53, Elastic Beanstalk, and RDS. I'll combine elements from both examples and adapt them to your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
      configuration_aliases = [ aws.us-east-1, aws.eu-west-1 ]
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  alias  = ""us-east-1""
}

provider ""aws"" {
  region = ""eu-west-1""
  alias  = ""eu-west-1""
}

# IAM Role and Instance Profile (shared across regions)
resource ""aws_iam_role"" ""eb_ec2_role3"" {
  provider = aws.us-east-1
  name = ""elastic_beanstalk_ec2_role3""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  provider   = aws.us-east-1
  role       = aws_iam_role.eb_ec2_role3.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile3"" {
  provider = aws.us-east-1
  name = ""elastic_beanstalk_ec2_profile3""
  role = aws_iam_role.eb_ec2_role3.name
}

# US East Resources
resource ""aws_db_instance"" ""main_db_us_east"" {
  provider             = aws.us-east-1
  identifier           = ""main-db-us-east""
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine              = ""mysql""
  engine_version      = ""5.7""
  instance_class      = ""db.t3.micro""
  username            = ""dbadmin""
  password            = ""mypassword123""
  skip_final_snapshot = true
}

resource ""aws_elastic_beanstalk_application"" ""myapp_us_east"" {
  provider    = aws.us-east-1
  name        = ""myapp-us-east""
  description = ""US East Application""
}

resource ""aws_elastic_beanstalk_environment"" ""myenv_us_east"" {
  provider            = aws.us-east-1
  name                = ""myenv-us-east""
  application         = aws_elastic_beanstalk_application.myapp_us_east.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile3.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.main_db_us_east.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.main_db_us_east.username
  }
}

# EU West Resources
resource ""aws_db_instance"" ""main_db_eu_west"" {
  provider             = aws.eu-west-1
  identifier           = ""main-db",,,,,,"I'll help you create a Terraform configuration for a multi-region application deployment with Route 53, Elastic Beanstalk, and RDS. Based on the provided examples, I'll combine elements from both while adapting them to your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
      configuration_aliases = [ aws.us-east-1, aws.eu-west-1 ]
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  alias  = ""us-east-1""
}

provider ""aws"" {
  region = ""eu-west-1""
  alias  = ""eu-west-1""
}

# IAM Role and Instance Profile (shared across regions)
resource ""aws_iam_role"" ""eb_ec2_role3"" {
  provider = aws.us-east-1
  name = ""elastic_beanstalk_ec2_role3""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  provider   = aws.us-east-1
  role       = aws_iam_role.eb_ec2_role3.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile3"" {
  provider = aws.us-east-1
  name = ""elastic_beanstalk_ec2_profile3""
  role = aws_iam_role.eb_ec2_role3.name
}

# US East Resources
resource ""aws_db_instance"" ""main_db_us_east"" {
  provider             = aws.us-east-1
  identifier           = ""main-db-us-east""
  allocated_storage    = 20
  storage_type        = ""gp2""
  engine              = ""mysql""
  engine_version      = ""5.7""
  instance_class      = ""db.t3.micro""
  username            = ""dbadmin""
  password            = ""mypassword123""
  skip_final_snapshot = true
}

resource ""aws_elastic_beanstalk_application"" ""myapp_us_east"" {
  provider    = aws.us-east-1
  name        = ""myapp-us-east""
  description = ""US East Application""
}

resource ""aws_elastic_beanstalk_environment"" ""myenv_us_east"" {
  provider            = aws.us-east-1
  name                = ""myenv-us-east""
  application         = aws_elastic_beanstalk_application.myapp_us_east.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile3.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.main_db_us_east.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.main_db_us_east.username
  }
}

# EU West Resources
resource ""aws_db_instance"" ""main_db_eu_west"" {
  provider             = aws.eu-west-1
  identifier",,,,,
aws_lightsail_database,"create AWS Lightsail that creates a mysql database. It should allow daily backups to take place between 16:00 and 16:30 each day and  requires any maintiance tasks (anything that would cause an outage, including changing some attributes) to take place on Tuesdays between 17:00 and 17:30","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.bundle_id
    resource.values.preferred_backup_window == ""16:00-16:30""
    resource.values.preferred_maintenance_window == ""Tue:17:00-Tue:17:30""
    
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name     = ""test""
  availability_zone            = ""us-east-1a""
  master_database_name         = ""testdatabasename""
  master_password              = ""testdatabasepassword""
  master_username              = ""test""
  blueprint_id                 = ""mysql_8_0""
  bundle_id                    = ""micro_1_0""
  preferred_backup_window      = ""16:00-16:30""
  preferred_maintenance_window = ""Tue:17:00-Tue:17:30""
}","have one ""aws_lightsail_database"" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id
    with bundle_id
    with  preferred_backup_window    = ""16:00-16:30""
  preferred_maintenance_window = ""Tue:17:00-Tue:17:30""

    ","I'll help you create a Terraform configuration for an AWS Lightsail MySQL database with the specified backup and maintenance windows. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

resource ""aws_lightsail_database"" ""mysql_db"" {
  relational_database_name = ""my-mysql-database""
  availability_zone       = ""us-east-1a""  # Change to match your region
  master_database_name   = ""main_db""
  master_username       = ""admin""
  master_password       = ""your-secure-password""  # Replace with a secure password
  blueprint_id          = ""mysql_8_0""
  bundle_id            = ""micro_1_0""  # Smallest instance size, adjust as needed
  
  preferred_backup_window      = ""16:00-16:30""
  preferred_maintenance_window = ""tue:17:00-tue:17:30""
  
  backup_retention_enabled = true
  
  # Configure other optional parameters
  skip_final_snapshot     = true
  apply_immediately      = false  # Set to true if you want changes to apply immediately
  
  tags = {
    Name = ""mysql-lightsail-db""
    Environment = ""production""
  }
}

# Optional: Create a static IP for the database
resource ""aws_lightsail_database_public_ip"" ""mysql_db_ip"" {
  relational_database_name = aws_lightsail_database.mysql_db.id
}

# Output the database endpoint
output ""database_endpoint"" {
  value = aws_lightsail_database.mysql_db.master_endpoint_address
}

# Output the database port
output ""database_port"" {
  value = aws_lightsail_database.mysql_db.master_endpoint_port
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a Lightsail MySQL database with:
   - Daily backup window between 16:00-16:30
   - Maintenance window on Tuesdays between 17:00-17:30
   - MySQL 8.0 as the database engine
   - Automatic backups enabled
3. Creates a public IP for the database (optional)
4. Outputs the database endpoint and port for reference

Important notes:
- Replace `your-secure-password` with a strong password
- Adjust the `availability_zone` to match your region
- The `bundle_id` can be changed based on your performance needs
- You may want to modify the `tags` according to your naming convention
- The `skip_final_snapshot` is set to true to allow easier cleanup, but you might want to set it to false in production
- Security best practices would suggest using AWS Secrets Manager or SSM Parameter Store for the password

Remember to ensure your AWS credentials have the necessary permissions to create Lightsail resources.",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail MySQL database with the specified backup and maintenance windows. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

resource ""aws_lightsail_database"" ""mysql_db"" {
  relational_database_name = ""my-mysql-database""
  availability_zone       = ""us-east-1a""  # Change to match your region
  master_database_name   = ""main_db""
  master_username        = ""admin""
  master_password        = ""your-secure-password""  # Replace with a secure password
  blueprint_id           = ""mysql_8_0""
  bundle_id             = ""micro_1_0""  # Smallest instance size, adjust as needed
  
  preferred_backup_window      = ""16:00-16:30""
  preferred_maintenance_window = ""tue:17:00-tue:17:30""
  
  backup_retention_enabled = true
  
  # Optional: Configure public accessibility
  public_access = false
  
  # Optional: Configure backup count (between 1-7)
  backup_retention_count = 7
  
  tags = {
    Name = ""mysql-lightsail-db""
    Environment = ""production""
  }
}

# Optional: Create a static IP for the database
resource ""aws_lightsail_database_public_ip"" ""mysql_db_ip"" {
  relational_database_name = aws_lightsail_database.mysql_db.id
}

# Optional: Output the database endpoint
output ""database_endpoint"" {
  value = aws_lightsail_database.mysql_db.master_endpoint_address
}

# Optional: Output the database username
output ""database_username"" {
  value = aws_lightsail_database.mysql_db.master_username
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a Lightsail MySQL 8.0 database with:
   - Daily backup window between 16:00-16:30
   - Maintenance window on Tuesdays between 17:00-17:30
   - 7-day backup retention
   - Private access by default (can be changed with public_access parameter)
3. Optionally creates a public IP for the database
4. Provides useful outputs for database connectivity

Important notes:
- Replace `your-secure-password` with a strong password
- Adjust the `availability_zone` to match your desired region
- The `bundle_id` can be changed to accommodate different instance sizes
- The backup retention count can be adjusted between 1-7 days
- The configuration includes tags for better resource management

You can customize the configuration further by:
- Adding parameter groups
- Configuring additional security settings
- Adding more outputs as needed
- Adjusting the bundle size based on your requirements

Remember to properly secure the master password in production environments using secure secret management practices like AWS Secrets Manager or using variables with secure storage.",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail MySQL database with the specified backup and maintenance windows. Based on the examples provided and your requirements, here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_database"" ""mysql_db"" {
  relational_database_name = ""mysql-database""
  availability_zone       = ""us-east-1a""
  master_database_name   = ""main_db""
  master_username        = ""admin""
  master_password        = ""your-secure-password-here""
  blueprint_id           = ""mysql_8_0""
  bundle_id             = ""micro_1_0""

  # Backup window: 16:00-16:30 daily
  preferred_backup_window = ""16:00-16:30""

  # Maintenance window: Tuesday 17:00-17:30
  preferred_maintenance_window = ""tue:17:00-tue:17:30""

  # Enable automatic backups
  backup_retention_enabled = true

  # Configure other optional parameters
  skip_final_snapshot     = true

  tags = {
    Name = ""mysql-lightsail-db""
    Environment = ""production""
  }
}

# Output the database endpoint
output ""database_endpoint"" {
  value = aws_lightsail_database.mysql_db.master_endpoint_address
}

# Output the database port
output ""database_port"" {
  value = aws_lightsail_database.mysql_db.master_endpoint_port
}
```

This configuration:

1. Sets up the required AWS provider

2. Creates a Lightsail MySQL database with:
   - MySQL 8.0 as the database engine
   - Micro bundle size (suitable for development/testing)
   - Daily backup window between 16:00 and 16:30
   - Maintenance window on Tuesdays between 17:00 and 17:30
   - Automatic backups enabled
   - Skip final snapshot when the database is deleted

3. Includes outputs for the database endpoint and port

Important notes:
- Replace `your-secure-password-here` with a secure password
- The `availability_zone` might need to be adjusted based on your region
- The `bundle_id` can be adjusted based on your performance needs
- You may want to store sensitive information like the master password in a secure manner (e.g., using AWS Secrets Manager or as a variable)

This configuration follows AWS best practices and provides the backup and maintenance windows you specified while ensuring the database is properly configured and accessible.",,,,,
"aws_cloudwatch_log_group, aws_msk_cluster, aws_security_group, aws_subnet, aws_subnet, aws_subnet, aws_vpc, aws_availability_zones",Create a managed MSK cluster in us-east1 logging to a CloudWatch group.,"package main

import future.keywords.in

default allow = false

# Check if any MSK cluster is being created
msk_cluster_created(resources) {
some resource in resources
resource.type == ""aws_msk_cluster""
resource.change.actions[_] == ""create""
}

# Check if CloudWatch logging is enabled for broker logs
cloudwatch_logging_enabled(resource) {
resource.type == ""aws_msk_cluster""
resource.change.after.logging_info[_].broker_logs[_].cloudwatch_logs[_].enabled == true
}

# Aggregate all checks
allow {
msk_cluster_created(input.resource_changes)
some resource in input.resource_changes
cloudwatch_logging_enabled(resource)
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""vpc"" {
    cidr_block = ""192.168.0.0/22""
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

resource ""aws_subnet"" ""subnet_az1"" {
    availability_zone = data.aws_availability_zones.azs.names[0]
    cidr_block = ""192.168.0.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az2"" {
    availability_zone = data.aws_availability_zones.azs.names[1]
    cidr_block = ""192.168.1.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az3"" {
    availability_zone = data.aws_availability_zones.azs.names[2]
    cidr_block = ""192.168.2.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.vpc.id
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }
}

resource ""aws_route_table_association"" ""first"" {
  subnet_id      = aws_subnet.subnet_az1.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""second"" {
  subnet_id      = aws_subnet.subnet_az2.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""third"" {
  subnet_id      = aws_subnet.subnet_az3.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_security_group"" ""sg"" {
    vpc_id = aws_vpc.vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4 = ""0.0.0.0/0""
  ip_protocol = ""-1""
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  ip_protocol       = ""-1""
}

resource ""aws_cloudwatch_log_group"" ""test"" {
    name = ""msk_broker_logs""
}

resource ""aws_msk_cluster"" ""example"" {
    cluster_name = ""example""
    kafka_version = ""3.2.0""
    number_of_broker_nodes = 3

    broker_node_group_info {
        instance_type = ""kafka.t3.small""
        client_subnets = [
            aws_subnet.subnet_az1.id,
            aws_subnet.subnet_az2.id,
            aws_subnet.subnet_az3.id,
        ]

        storage_info {
            ebs_storage_info {
                volume_size = 1000
            }
        }

        security_groups = [aws_security_group.sg.id]
    }

    open_monitoring {
        prometheus {
            jmx_exporter {
                enabled_in_broker = true
            }
            node_exporter {
                enabled_in_broker = true
            }
        }
    }

    logging_info {
        broker_logs {
            cloudwatch_logs {
                enabled = true
                log_group = aws_cloudwatch_log_group.test.name
            }
            firehose {
                enabled = false
            }
            s3 {
                enabled = false
            }
        }
    }
}

output ""zookeeper_connect_string"" {
    value = aws_msk_cluster.example.zookeeper_connect_string
}

output ""bootstrap_brokers_tls"" {
    description = ""TLS connection host:port pairs""
    value = aws_msk_cluster.example.bootstrap_brokers_tls
}","Has one ""aws_msk_cluster"" resource
with one ""logging_info""
with one ""broker_logs""
with one ""cloudwatch_logs""
with one ""enabled"" = true","I'll help you create a Terraform configuration for an MSK cluster with CloudWatch logging. This will be based on the provided examples but modified to use CloudWatch logs instead of S3 or Firehose.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
    cidr_block = ""192.168.0.0/22""
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

resource ""aws_subnet"" ""subnet_az1"" {
    availability_zone = data.aws_availability_zones.azs.names[0]
    cidr_block = ""192.168.0.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az2"" {
    availability_zone = data.aws_availability_zones.azs.names[1]
    cidr_block = ""192.168.1.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az3"" {
    availability_zone = data.aws_availability_zones.azs.names[2]
    cidr_block = ""192.168.2.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.vpc.id
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }
}

resource ""aws_route_table_association"" ""first"" {
  subnet_id      = aws_subnet.subnet_az1.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""second"" {
  subnet_id      = aws_subnet.subnet_az2.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""third"" {
  subnet_id      = aws_subnet.subnet_az3.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_security_group"" ""sg"" {
    vpc_id = aws_vpc.vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4 = ""0.0.0.0/0""
  ip_protocol = ""-1""
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  ip_protocol       = ""-1""
}

resource ""aws_cloudwatch_log_group"" ""msk_log_group"" {
    name              = ""/aws/msk/brokers""
    retention_in_days = 7
}

resource ""aws_msk_cluster"" ""example"" {
    cluster_name = ""example""
    kafka_version = ""3.2.0""
    number_of_broker_nodes = 3

    broker_node_group_info {
        instance_type = ""kafka.t3.small""
        client_subnets = [
            aws_subnet.subnet_az1.id,
            aws_subnet.subnet_az2.id,
            aws_subnet.subnet_az3.id,
        ]
        storage_info {",,,,,,"I'll help you create a Terraform configuration for an MSK cluster with CloudWatch logging. Based on the provided examples, I'll modify the configuration to use CloudWatch logs instead of S3 or Firehose.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
    cidr_block = ""192.168.0.0/22""
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

resource ""aws_subnet"" ""subnet_az1"" {
    availability_zone = data.aws_availability_zones.azs.names[0]
    cidr_block = ""192.168.0.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az2"" {
    availability_zone = data.aws_availability_zones.azs.names[1]
    cidr_block = ""192.168.1.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az3"" {
    availability_zone = data.aws_availability_zones.azs.names[2]
    cidr_block = ""192.168.2.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.vpc.id
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }
}

resource ""aws_route_table_association"" ""first"" {
  subnet_id      = aws_subnet.subnet_az1.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""second"" {
  subnet_id      = aws_subnet.subnet_az2.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""third"" {
  subnet_id      = aws_subnet.subnet_az3.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_security_group"" ""sg"" {
    vpc_id = aws_vpc.vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4 = ""0.0.0.0/0""
  ip_protocol = ""-1""
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  ip_protocol       = ""-1""
}

resource ""aws_cloudwatch_log_group"" ""msk_log_group"" {
    name              = ""/aws/msk/broker""
    retention_in_days = 7
}

resource ""aws_msk_cluster"" ""example"" {
    cluster_name = ""example""
    kafka_version = ""3.2.0""
    number_of_broker_nodes = 3

    broker_node_group_info {
        instance_type = ""kafka.t3.small""
        client_subnets = [
            aws_subnet.subnet_az1.id,
            aws_subnet.subnet_az2.id,
            aws_subnet.subnet_az3.id,
        ]
        storage_info {",,,,,,"I'll help you create a Terraform configuration for an MSK cluster with CloudWatch logging. Based on the provided examples, I'll modify the configuration to use CloudWatch logs instead of S3 or Firehose.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
    cidr_block = ""192.168.0.0/22""
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

resource ""aws_subnet"" ""subnet_az1"" {
    availability_zone = data.aws_availability_zones.azs.names[0]
    cidr_block = ""192.168.0.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az2"" {
    availability_zone = data.aws_availability_zones.azs.names[1]
    cidr_block = ""192.168.1.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az3"" {
    availability_zone = data.aws_availability_zones.azs.names[2]
    cidr_block = ""192.168.2.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.vpc.id
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }
}

resource ""aws_route_table_association"" ""first"" {
  subnet_id      = aws_subnet.subnet_az1.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""second"" {
  subnet_id      = aws_subnet.subnet_az2.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""third"" {
  subnet_id      = aws_subnet.subnet_az3.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_security_group"" ""sg"" {
    vpc_id = aws_vpc.vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4 = ""0.0.0.0/0""
  ip_protocol = ""-1""
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  ip_protocol       = ""-1""
}

resource ""aws_cloudwatch_log_group"" ""msk_log_group"" {
    name = ""msk_broker_logs""
    retention_in_days = 7
}

resource ""aws_msk_cluster"" ""example"" {
    cluster_name = ""example""
    kafka_version = ""3.2.0""
    number_of_broker_nodes = 3

    broker_node_group_info {
        instance_type = ""kafka.t3.small""
        client_subnets = [
            aws_subnet.subnet_az1.id,
            aws_subnet.subnet_az2.id,
            aws_subnet.subnet_az3.id,
        ]
        storage_info {",,,,,
"aws_backup_plan, aws_backup_selection, aws_backup_vault, aws_iam_role, aws_iam_role_policy_attachment, aws_instance, aws_iam_policy_document","An AWS service that provisions an EC2 instance and backs it up every day at midnight. Accomplish this using the following services: AWS EC2, AWS Backup. Add any necessary services.","package cloud_desktop_high

import rego.v1

default valid := false

instance_valid(instance, ami) if {
        instance.expressions.instance_type
        ami.address in instance.expressions.ami.references
}

backup_plan_valid(backup_plan, backup_vault) if {
        # advanced_backup_setting
        some backup_setting in backup_plan.expressions.advanced_backup_setting
        backup_setting.backup_options

        backup_setting.resource_type.constant_value == ""EC2""

        # rule
        some rule in backup_plan.expressions.rule
        backup_vault.address in rule.target_vault_name.references
        rule.schedule.constant_value == ""cron(0 0 * * ? *)""
}

backup_selection_valid(backup_selection, backup_plan, instance) if {
        backup_plan.address in backup_selection.expressions.plan_id.references
        instance.address in backup_selection.expressions.resources.references
}

valid if {
        resources := input.configuration.root_module.resources
        some backup_plan in resources
        backup_plan.type == ""aws_backup_plan""
        some backup_vault in resources
        backup_vault.type == ""aws_backup_vault""
        some backup_selection in resources
        backup_selection.type == ""aws_backup_selection""
        some instance in resources
        instance.type == ""aws_instance""
        some ami in resources
        ami.type == ""aws_ami""

        instance_valid(instance, ami)
        backup_plan_valid(backup_plan, backup_vault)
        backup_selection_valid(backup_selection, backup_plan, instance)
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_ami"" ""amzn2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}


resource ""aws_instance"" ""cloud_desktop"" {
  instance_type = ""t2.micro""
  ami = data.aws_ami.amzn2.id
}

resource ""aws_backup_plan"" ""cloud_desktop_backup"" {
  name = ""cloud_desktop_backup_plan""
  rule {
    rule_name         = ""cloud_desktop_backup_rule"" # required, not specified
    target_vault_name = aws_backup_vault.valut.name # required, not specified
    schedule          = ""cron(0 0 * * ? *)""
  }

  advanced_backup_setting {
    backup_options = {
      WindowsVSS = ""enabled""
    }
    resource_type = ""EC2""
  }
}

resource ""aws_backup_vault"" ""valut"" {
  name = ""cloud_desktop_backup_valut""
}

resource ""aws_backup_selection"" ""backup_ec2"" {
  iam_role_arn = aws_iam_role.backup.arn

  name    = ""backup_ec2""
  plan_id = aws_backup_plan.cloud_desktop_backup.id

  resources = [
    aws_instance.cloud_desktop.arn,
  ]
}

data ""aws_iam_policy_document"" ""backup_assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""backup.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""backup"" {
  name               = ""backup""
  assume_role_policy = data.aws_iam_policy_document.backup_assume_role.json
}

resource ""aws_iam_role_policy_attachment"" ""backup"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup""
  role       = aws_iam_role.backup.name
}","have one ""aws_ami"" data resource
have one ""aws_instance"" resource
  with ""ami"" argument linking to the aws_ami data resource
  with ""instance_type"" argument
have one ""aws_backup_plan"" resource
  with ""name"" argument
  with ""rule"" argument block
    with ""rule_name"" argument
    with ""target_vault_name"" argument linking to backup_vault
    with ""schedule"" argument with value ""cron(0 0 * * ? *)""
  with ""advanced_backup_setting"" argument block
    with ""backup_options"" argument block
      with ""WindowsVSS"" argument
    with ""resource_type"" argument with value ""EC2""
have one ""aws_backup_vault"" resource
  with ""name"" argument
have one ""aws_backup_selection"" resource
  with ""iam_role_arn"" argument
  with ""name"" argument
  with ""plan_id"" argument linking to aws_backup_plan resource
  with ""resources"" argument including the aws_instance resource","Based on the provided examples and the current question, I'll help you create a Terraform configuration that provisions an EC2 instance with daily backups at midnight using AWS EC2 and AWS Backup services. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Get the latest Amazon Linux 2 AMI
data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }
}

# EC2 Instance
resource ""aws_instance"" ""backup_example"" {
  ami           = data.aws_ami.amazon_linux_2.id
  instance_type = ""t2.micro""

  tags = {
    Name = ""backup-example-instance""
  }
}

# AWS Backup Vault
resource ""aws_backup_vault"" ""example"" {
  name = ""example_backup_vault""
}

# AWS Backup Plan
resource ""aws_backup_plan"" ""example"" {
  name = ""example_backup_plan""

  rule {
    rule_name         = ""daily_backup_rule""
    target_vault_name = aws_backup_vault.example.name
    schedule          = ""cron(0 0 * * ? *)""  # Midnight every day

    lifecycle {
      delete_after = 14  # Keep backups for 14 days
    }
  }
}

# IAM Role for AWS Backup
data ""aws_iam_policy_document"" ""backup_assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""backup.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""backup_role"" {
  name               = ""backup-role""
  assume_role_policy = data.aws_iam_policy_document.backup_assume_role.json
}

resource ""aws_iam_role_policy_attachment"" ""backup_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup""
  role       = aws_iam_role.backup_role.name
}

# AWS Backup Selection
resource ""aws_backup_selection"" ""example"" {
  name         = ""example_backup_selection""
  iam_role_arn = aws_iam_role.backup_role.arn
  plan_id      = aws_backup_plan.example.id

  resources = [
    aws_instance.backup_example.arn
  ]
}
```

This configuration:
1. Sets up the AWS provider and required versions
2. Creates an EC2 instance using the latest Amazon Linux 2 AMI
3. Creates a backup vault to store the backups
4. Sets up a backup plan that runs daily at midnight
5. Creates the necessary IAM role and attachments for AWS Backup to function
6. Creates a backup selection that associates the EC2 instance with the backup plan

The backup will run automatically every day at midnight and keep the backups for 14 days. The configuration includes all necessary services and permissions for the backup system to work properly.",,,,,,"I'll help you create a Terraform configuration for provisioning an EC2 instance with daily backups at midnight using AWS EC2 and AWS Backup services. Based on the examples provided, here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Get the latest Amazon Linux 2 AMI
data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }
}

# EC2 Instance
resource ""aws_instance"" ""server"" {
  ami           = data.aws_ami.amazon_linux_2.id
  instance_type = ""t2.micro""

  tags = {
    Name = ""backup-example-server""
  }
}

# AWS Backup Vault
resource ""aws_backup_vault"" ""backup_vault"" {
  name = ""daily-backup-vault""
}

# AWS Backup Plan
resource ""aws_backup_plan"" ""daily_backup"" {
  name = ""daily_backup_plan""

  rule {
    rule_name         = ""daily_backup_rule""
    target_vault_name = aws_backup_vault.backup_vault.name
    schedule          = ""cron(0 0 * * ? *)""

    lifecycle {
      delete_after = 14
    }
  }
}

# IAM Role for AWS Backup
data ""aws_iam_policy_document"" ""backup_assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""backup.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""backup_role"" {
  name               = ""aws-backup-service-role""
  assume_role_policy = data.aws_iam_policy_document.backup_assume_role.json
}

resource ""aws_iam_role_policy_attachment"" ""backup_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup""
  role       = aws_iam_role.backup_role.name
}

# AWS Backup Selection
resource ""aws_backup_selection"" ""ec2_backup"" {
  name         = ""ec2_backup_selection""
  iam_role_arn = aws_iam_role.backup_role.arn
  plan_id      = aws_backup_plan.daily_backup.id

  resources = [
    aws_instance.server.arn
  ]
}
```

This configuration:
1. Sets up the AWS provider and required versions
2. Creates an EC2 instance using the latest Amazon Linux 2 AMI
3. Creates an AWS Backup vault to store the backups
4. Sets up a backup plan that runs daily at midnight
5. Creates the necessary IAM role and attachments for AWS Backup
6. Creates a backup selection that targets the EC2 instance

The backup will run automatically every day at midnight (UTC) and keep backups for 14 days. The configuration includes all necessary permissions and roles for AWS Backup to function properly.",,,,,,"I'll help you create a Terraform configuration that provisions an EC2 instance and sets up AWS Backup to create daily backups at midnight. Based on the provided examples, I'll include all necessary resources and IAM roles.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Get the latest Amazon Linux 2 AMI
data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }
}

# EC2 Instance
resource ""aws_instance"" ""backup_example"" {
  ami           = data.aws_ami.amazon_linux_2.id
  instance_type = ""t2.micro""
  
  tags = {
    Name = ""backup-example-instance""
  }
}

# AWS Backup Vault
resource ""aws_backup_vault"" ""example"" {
  name = ""example_backup_vault""
}

# AWS Backup Plan
resource ""aws_backup_plan"" ""example"" {
  name = ""example_backup_plan""

  rule {
    rule_name         = ""daily_backup_rule""
    target_vault_name = aws_backup_vault.example.name
    schedule          = ""cron(0 0 * * ? *)""  # Midnight every day

    lifecycle {
      delete_after = 14  # Keep backups for 14 days
    }
  }
}

# IAM Role for AWS Backup
data ""aws_iam_policy_document"" ""backup_assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""backup.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""backup_role"" {
  name               = ""aws_backup_service_role""
  assume_role_policy = data.aws_iam_policy_document.backup_assume_role.json
}

# Attach necessary policy to the IAM role
resource ""aws_iam_role_policy_attachment"" ""backup_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup""
  role       = aws_iam_role.backup_role.name
}

# AWS Backup Selection
resource ""aws_backup_selection"" ""example"" {
  name         = ""example_backup_selection""
  iam_role_arn = aws_iam_role.backup_role.arn
  plan_id      = aws_backup_plan.example.id

  resources = [
    aws_instance.backup_example.arn
  ]
}
```

This configuration:
1. Sets up the AWS provider and required versions
2. Creates an EC2 instance using the latest Amazon Linux 2 AMI
3. Creates an AWS Backup vault to store the backups
4. Sets up a backup plan that runs daily at midnight
5. Creates necessary IAM roles and policies for AWS Backup to function
6. Creates a backup selection that targets the EC2 instance

The backup plan is configured to:
- Run every day at midnight (using cron expression)
- Keep backups for 14 days
- Store backups in a dedicated backup vault

All necessary permissions are handled through the IAM role and policy attachments, allowing AWS Backup to perform its operations on the EC2 instance.",,,,,
"aws_lex_bot, aws_lex_intent",create an aws_lex_bot,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for multiple aws_lex_intent resources
has_valid_lex_intents {
    count([intent | intent := input.planned_values.root_module.resources[_]; intent.type == ""aws_lex_intent""; intent.values.name; intent.values.fulfillment_activity])
}

# Rule for aws_lex_bot resource with specific arguments
has_valid_lex_bot_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lex_bot""
    is_boolean(resource.values.child_directed)
    resource.values.clarification_prompt
    resource.values.abort_statement
    resource.values.name
    # is_boolean(resource.values.create_version)
    # resource.values.description
    # is_boolean(resource.values.detect_sentiment)
    # resource.values.idle_session_ttl_in_seconds
    # resource.values.process_behavior
    # resource.values.voice_id
    # resource.values.locale
    # is_boolean(resource.values.enable_model_improvements)
    # resource.values.nlu_intent_confidence_threshold
}

# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lex_intents
    has_valid_lex_bot_instance
}
",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_lex_intent"" ""order_flowers"" {
  name = ""OrderFlowers""
  description = ""Intent to order a bouquet of flowers for pick up""

  sample_utterances = [
    ""I would like to pick up flowers"",
    ""I would like to order some flowers"",
  ]

  slot {
    name                     = ""FlowerType""
    description              = ""Type of flower to order""
    slot_constraint          = ""Required"" 
    slot_type                = ""AMAZON.AlphaNumeric""
    priority                 = 1

    sample_utterances = [""I would like to get {FlowerType} flower.""]

    value_elicitation_prompt {
      message {
        content             = ""What type of flower would you like?""
        content_type        = ""PlainText""
      }
      max_attempts         = 2
    }
  }

  slot {
    name                     = ""PickupDate""
    description              = ""Date of flower pickup""
    slot_constraint          = ""Required"" 
    slot_type                = ""AMAZON.DATE""
    priority                 = 2

    value_elicitation_prompt {
      message {
        content             = ""When would you like to pickup your flower?""
        content_type        = ""PlainText""
      }
      max_attempts         = 2
    }
  }

  confirmation_prompt {
    max_attempts = 2

    message {
      content      = ""Okay, your {FlowerType} will be ready for pickup on {PickupDate}.  Does this sound okay?""
      content_type = ""PlainText""
    }
  }

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  rejection_statement {
    message {
      content      = ""Okay, I will not place your order.""
      content_type = ""PlainText""
    }
  }
}

resource ""aws_lex_bot"" ""order_flowers"" {
  name     = ""OrderFlowers""
  description = ""Bot to order flowers on the behalf of a user""
  locale   = ""en-US""
  process_behavior = ""BUILD""
  voice_id = ""Salli""
  child_directed = false
  create_version = false
  detect_sentiment            = false
  idle_session_ttl_in_seconds = 600

  abort_statement {
    message {
      content_type = ""PlainText""
      content      = ""Sorry, I am not able to assist at this time""
    }
  }

  clarification_prompt {
    max_attempts = 2

    message {
      content_type = ""PlainText""
      content      = ""I didn't understand you, what would you like to do?""
    }
  }
  
  intent {
    intent_name    = aws_lex_intent.order_flowers.name
    intent_version = aws_lex_intent.order_flowers.version
  }
}","Has one resource ""aws_lex_bot"" and one resource ""aws_lex_intent""","I'll help you create an AWS Lex bot using Terraform. Based on the examples provided, I'll create a simple bot configuration with basic settings and an intent.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create the intent first
resource ""aws_lex_intent"" ""example_intent"" {
  name        = ""ExampleIntent""
  description = ""Example intent for demonstration""
  
  sample_utterances = [
    ""Hello"",
    ""Hi there"",
    ""Greetings""
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  conclusion_statement {
    message {
      content      = ""Thanks for using our service!""
      content_type = ""PlainText""
    }
  }
}

# Create the bot
resource ""aws_lex_bot"" ""example_bot"" {
  name        = ""ExampleBot""
  description = ""An example bot""
  
  voice_id    = ""Joanna""
  idle_session_ttl_in_seconds = 300
  child_directed = false
  locale        = ""en-US""
  process_behavior = ""BUILD""

  abort_statement {
    message {
      content      = ""Sorry, I could not understand. Please try again.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    message {
      content      = ""I didn't understand you. Could you please rephrase that?""
      content_type = ""PlainText""
    }
    max_attempts = 3
  }

  intent {
    intent_name    = aws_lex_intent.example_intent.name
    intent_version = aws_lex_intent.example_intent.version
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an intent with basic sample utterances and a conclusion statement
3. Creates a bot that:
   - Uses the Joanna voice
   - Has a 5-minute session timeout
   - Includes clarification and abort statements
   - Links to the created intent
   - Uses English (US) locale
   - Is not directed at children

The bot will be built automatically when deployed. You can customize the various parameters (name, description, utterances, etc.) according to your specific needs.",,,,,,"I'll help you create an AWS Lex bot using Terraform. Based on the provided examples, I'll create a simple bot configuration with essential components.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# First, create an intent for the bot
resource ""aws_lex_intent"" ""simple_intent"" {
  name        = ""SimpleIntent""
  description = ""A simple intent for demonstration""
  
  sample_utterances = [
    ""Hello"",
    ""Hi there"",
    ""Greetings""
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  conclusion_statement {
    message {
      content      = ""Hello! How can I help you today?""
      content_type = ""PlainText""
    }
  }
}

# Create the Lex bot
resource ""aws_lex_bot"" ""simple_bot"" {
  name                         = ""SimpleBot""
  description                  = ""A simple demonstration bot""
  voice_id                     = ""Joanna""
  idle_session_ttl_in_seconds = 300
  child_directed              = false
  locale                      = ""en-US""
  process_behavior            = ""BUILD""

  clarification_prompt {
    message {
      content      = ""I didn't understand you, could you please repeat that?""
      content_type = ""PlainText""
    }
    max_attempts = 3
  }

  abort_statement {
    message {
      content      = ""Sorry, I cannot help you at this time.""
      content_type = ""PlainText""
    }
  }

  intent {
    intent_name    = aws_lex_intent.simple_intent.name
    intent_version = aws_lex_intent.simple_intent.version
  }
}
```

This configuration creates:

1. A simple intent with basic greeting utterances and a conclusion statement
2. A Lex bot that:
   - Uses the Joanna voice
   - Has a 5-minute session timeout
   - Includes clarification and abort statements
   - Links to the created intent
   - Is set to English (US) locale
   - Has child-directed features disabled

The bot will be able to respond to basic greetings and can be extended with more intents and functionality as needed.",,,,,,"I'll help you create an AWS Lex bot using Terraform. Based on the provided examples, I'll create a simple bot configuration that follows best practices.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# First, create an intent that the bot will use
resource ""aws_lex_intent"" ""example_intent"" {
  name        = ""ExampleIntent""
  description = ""An example intent for demonstration""
  
  sample_utterances = [
    ""Hello"",
    ""Hi there"",
    ""Greetings""
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  conclusion_statement {
    message {
      content      = ""Thanks for using our service!""
      content_type = ""PlainText""
    }
  }
}

# Create the Lex bot
resource ""aws_lex_bot"" ""example_bot"" {
  name                     = ""ExampleBot""
  description              = ""An example bot created with Terraform""
  voice_id                 = ""Joanna""
  idle_session_ttl_in_seconds = 300
  child_directed           = false
  locale                  = ""en-US""
  process_behavior        = ""BUILD""

  abort_statement {
    message {
      content      = ""Sorry, I am not able to assist at this time.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    message {
      content      = ""I didn't understand you. Could you please repeat that?""
      content_type = ""PlainText""
    }
    max_attempts = 3
  }

  intent {
    intent_name    = aws_lex_intent.example_intent.name
    intent_version = aws_lex_intent.example_intent.version
  }

  tags = {
    Environment = ""development""
    Terraform   = ""true""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a basic intent with sample utterances
3. Creates a Lex bot that:
   - Uses the created intent
   - Has a clarification prompt for unclear inputs
   - Has an abort statement
   - Uses the Joanna voice
   - Sets a 5-minute idle session timeout
   - Includes basic tagging

The bot is configured with English (US) locale and includes basic error handling through the clarification prompt and abort statement. You can customize the various parameters like `name`, `description`, utterances, and messages according to your specific needs.",,,,,
"aws_iam_role, aws_sagemaker_code_repository, aws_sagemaker_notebook_instance, aws_iam_policy_document, aws_sagemaker_prebuilt_ecr_image","Create an aws_sagemaker_notebook instance from ""https://github.com/hashicorp/terraform-provider-aws.git""","package main

import future.keywords.in

default allow = false

aws_iam_role_valid(resources) {
    some resource in resources
    resource.type == ""aws_iam_role""
}

aws_sagemaker_code_repository_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_code_repository""
    resource.change.after.git_config[0].repository_url ==  ""https://github.com/hashicorp/terraform-provider-aws.git""
}

aws_sagemaker_notebook_instance_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_notebook_instance""
    resource.change.after.default_code_repository != null
}

# Aggregate all checks
allow {
    aws_iam_role_valid(input.resource_changes)
    aws_sagemaker_code_repository_valid(input.resource_changes)
    aws_sagemaker_notebook_instance_valid(input.resource_changes)
}",3,"resource ""aws_sagemaker_code_repository"" ""example"" {
  code_repository_name = ""my-notebook-instance-code-repo""

  git_config {
    repository_url = ""https://github.com/hashicorp/terraform-provider-aws.git""
  }
}

resource ""aws_sagemaker_notebook_instance"" ""ni"" {
  name                    = ""my-notebook-instance""
  role_arn                = aws_iam_role.example.arn
  instance_type           = ""ml.t2.medium""
  default_code_repository = aws_sagemaker_code_repository.example.code_repository_name
}

resource ""aws_iam_role"" ""example"" {
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""sagemaker.amazonaws.com""]
    }
  }
}

data ""aws_sagemaker_prebuilt_ecr_image"" ""test"" {
  repository_name = ""kmeans""
}",,"I'll help you create an AWS SageMaker notebook instance that's connected to the specified Git repository. Here's the Terraform configuration that combines both a notebook instance and a code repository:

```hcl
# First, create the code repository
resource ""aws_sagemaker_code_repository"" ""example"" {
  code_repository_name = ""terraform-provider-aws-repo""

  git_config {
    repository_url = ""https://github.com/hashicorp/terraform-provider-aws.git""
  }
}

# Create IAM role for SageMaker
resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-notebook-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Create the notebook instance
resource ""aws_sagemaker_notebook_instance"" ""notebook"" {
  name                  = ""my-notebook-instance""
  role_arn              = aws_iam_role.sagemaker_role.arn
  instance_type         = ""ml.t2.medium""
  default_code_repository = aws_sagemaker_code_repository.example.code_repository_name

  tags = {
    Name = ""sagemaker-notebook""
  }
}
```

This configuration:
1. Creates a SageMaker code repository linked to the specified GitHub repository
2. Creates an IAM role that SageMaker can assume
3. Creates a notebook instance that uses the code repository and IAM role
4. Uses the default ml.t2.medium instance type as shown in the example
5. Links the notebook instance to the code repository using the `default_code_repository` parameter

The notebook instance will have access to the specified GitHub repository when it launches.",,,,,,"I'll help you create an AWS SageMaker notebook instance that's connected to the specified Git repository. This will combine elements from both example cases to create a complete solution.

```hcl
# Create the IAM role for SageMaker
resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-notebook-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Create the code repository
resource ""aws_sagemaker_code_repository"" ""repo"" {
  code_repository_name = ""terraform-provider-aws-repo""

  git_config {
    repository_url = ""https://github.com/hashicorp/terraform-provider-aws.git""
  }
}

# Create the notebook instance
resource ""aws_sagemaker_notebook_instance"" ""notebook"" {
  name                    = ""terraform-notebook-instance""
  role_arn               = aws_iam_role.sagemaker_role.arn
  instance_type          = ""ml.t2.medium""
  default_code_repository = aws_sagemaker_code_repository.repo.code_repository_name

  tags = {
    Name = ""terraform-notebook-instance""
  }
}

# Attach AmazonSageMakerFullAccess policy to the role
resource ""aws_iam_role_policy_attachment"" ""sagemaker_policy"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}
```

This configuration:
1. Creates an IAM role with the necessary permissions for SageMaker
2. Creates a code repository linked to the specified GitHub repository
3. Creates a notebook instance that uses the code repository
4. Attaches the necessary SageMaker policy to the IAM role

The notebook instance will be created with the specified Git repository as its default code repository, allowing you to access the code directly from the notebook instance.",,,,,,"I'll help you create a Terraform configuration for an AWS SageMaker notebook instance that uses the specified Git repository. This will combine elements from both examples, creating both the notebook instance and connecting it to the code repository.

```hcl
# Create the code repository
resource ""aws_sagemaker_code_repository"" ""example"" {
  code_repository_name = ""terraform-provider-aws-repo""

  git_config {
    repository_url = ""https://github.com/hashicorp/terraform-provider-aws.git""
  }
}

# Create IAM role for SageMaker
resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-notebook-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Create the notebook instance
resource ""aws_sagemaker_notebook_instance"" ""notebook"" {
  name                  = ""my-notebook-instance""
  role_arn              = aws_iam_role.sagemaker_role.arn
  instance_type         = ""ml.t2.medium""
  default_code_repository = aws_sagemaker_code_repository.example.code_repository_name

  tags = {
    Name = ""SageMaker-Notebook-Instance""
  }
}

# Attach AmazonSageMakerFullAccess policy to the role
resource ""aws_iam_role_policy_attachment"" ""sagemaker_policy"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}
```

This configuration:
1. Creates a SageMaker code repository linked to the specified GitHub repository
2. Creates an IAM role with the necessary permissions for SageMaker
3. Creates a notebook instance that uses the code repository and IAM role
4. Attaches the required SageMaker policy to the IAM role

The notebook instance will be created with the specified Git repository as its default code repository, allowing you to access the repository content directly from the notebook instance.",,,,,
aws_nat_gateway,"creates a NAT Gateway associated with a specified subnet and Elastic IP allocation ID. The NAT Gateway is configured for public connectivity and tagged with a key-value pair (pike = ""permissions""), indicating its purpose or ownership.","package terraform.validation

# Set default validation state
default is_valid_nat_gateway = false

# Validate aws_nat_gateway resource
is_valid_nat_gateway {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_nat_gateway""

        # Ensure it is associated with a specified subnet
        resource.expressions.subnet_id != null

        # Ensure it uses a specific Elastic IP allocation ID
        resource.expressions.allocation_id != null

        # Check for the specific tag indicating its purpose or ownership
        resource.expressions.tags.constant_value.pike == ""permissions""
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_nat_gateway"" ""pike"" {
  subnet_id         = ""subnet-0562ef1d304b968f4""
  allocation_id     = ""eipalloc-0047fa56c40637c3b""
  connectivity_type = ""public""
  tags = {
    pike = ""permissions""
  }
}","Has one ""aws_nat_gateway"" resource:
Associated with a specified ""aws_subnet"" resource for hosting the NAT Gateway.
Utilizes an ""Elastic IP allocation ID"" to provide the NAT Gateway with a public IP address.
Configured for public connectivity, allowing resources within the private subnet to access the internet securely.
Tagged with a key-value pair: ""pike"" set to ""permissions"", which categorizes its role, purpose, or ownership within the infrastructure.","I'll help you create a Terraform configuration for a NAT Gateway with public connectivity and the specified tag. Based on the examples provided, I'll combine the relevant elements to meet your requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""  # Set your desired AWS region
}

resource ""aws_nat_gateway"" ""pike"" {
  subnet_id         = ""subnet-0562ef1d304b968f4""
  allocation_id     = ""eipalloc-0047fa56c40637c3b""
  connectivity_type = ""public""
  
  tags = {
    pike = ""permissions""
  }
}
```

This configuration:
1. Sets up the required AWS provider configuration
2. Creates a NAT Gateway with public connectivity
3. Associates it with the specified subnet and Elastic IP allocation ID
4. Adds the requested tag with key ""pike"" and value ""permissions""

The configuration combines the public connectivity type from Example 2 with the tagging functionality from Example 1 to meet your specific requirements.",,,,,,"I'll help you create a Terraform configuration for a NAT Gateway with public connectivity and the specified tag. Based on the examples provided, I'll combine the relevant elements to meet your requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

resource ""aws_nat_gateway"" ""pike"" {
  subnet_id         = ""subnet-0562ef1d304b968f4""
  allocation_id     = ""eipalloc-0047fa56c40637c3b""
  connectivity_type = ""public""
  
  tags = {
    pike = ""permissions""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a NAT Gateway with public connectivity
3. Associates it with the specified subnet and Elastic IP
4. Includes the requested tag with key ""pike"" and value ""permissions""

The configuration combines the public connectivity type from Example 2 with the tagging from Example 1 to meet your specific requirements.",,,,,,"I'll help you create a Terraform configuration for a NAT Gateway with public connectivity and the specified tag. Based on the examples provided, I'll combine elements from both examples to meet your requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_nat_gateway"" ""pike"" {
  subnet_id         = ""subnet-0562ef1d304b968f4""
  allocation_id     = ""eipalloc-0047fa56c40637c3b""
  connectivity_type = ""public""
  
  tags = {
    pike = ""permissions""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a NAT Gateway with public connectivity
3. Associates it with the specified subnet and Elastic IP
4. Includes the requested tag with key ""pike"" and value ""permissions""
5. Uses the same subnet and allocation IDs as in the examples

The configuration combines the public connectivity type from Example 2 with the tagging functionality from Example 1 to meet your specific requirements.",,,,,
"aws_s3_bucket, aws_s3_bucket_acl, aws_s3_bucket_policy","Create a Terraform configuration that defines an AWS S3 bucket, an ACL for the bucket, and a bucket policy allowing s3:GetObject actions with a condition based on the AWS Referer header. The bucket should have a unique name like ""my_unique_bucket_name,"" and the policy should grant access to all principals. ","package terraform.validation

default has_aws_s3_bucket = false
default has_aws_s3_bucket_acl = false
default has_aws_s3_bucket_policy = false
has_aws_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""my_bucket""
    bucket.values.bucket == ""my_unique_bucket_name""
}

has_aws_s3_bucket_acl {
    acl := input.planned_values.root_module.resources[_]
    acl.type == ""aws_s3_bucket_acl""
    acl.name == ""my_bucket_acl""
    acl.values.bucket == input.planned_values.root_module.resources[_].values.id  # Ensure correct bucket reference
    acl.values.acl == ""private""
}

has_aws_s3_bucket_policy {
    policy := input.planned_values.root_module.resources[_]
    policy.type == ""aws_s3_bucket_policy""
    policy.name == ""my_bucket_policy""
    policy.values.bucket == input.planned_values.root_module.resources[_].values.id  # Ensure correct bucket reference
}

valid_configuration {
    has_aws_s3_bucket
    has_aws_s3_bucket_acl
    has_aws_s3_bucket_policy
}
",3,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""my_bucket"" {
  bucket = ""my_unique_bucket_name""
}

resource ""aws_s3_bucket_acl"" ""my_bucket_acl"" {
  bucket = aws_s3_bucket.my_bucket.id
  acl    = ""private""
}

resource ""aws_s3_bucket_policy"" ""my_bucket_policy"" {
  bucket = aws_s3_bucket.my_bucket.id
  policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Sid       = ""ExampleStatement1"",
        Effect    = ""Allow"",
        Principal = ""*"",
        Action    = [
          ""s3:GetObject""
        ],
        Resource = [
          ""${aws_s3_bucket.my_bucket.arn}/*""
        ],
        Condition = {
          StringEquals = {
            ""aws:Referer"" = ""http://www.example.com""
          }
        }
      }
    ]
  })
}","Resource ""aws_s3_bucket""
has one ""bucket""

Resource ""aws_s3_bucket_acl""
has one ""bucket""
has one ""acl""

Resource ""aws_s3_bucket_policy""
has one ""bucket""
has one ""policy""","I'll help you create a Terraform configuration for an S3 bucket with ACL and a bucket policy that includes a Referer condition. Based on the examples provided and best practices, here's the solution:

```hcl
provider ""aws"" {
  region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-unique-bucket-name""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id
  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

resource ""aws_s3_bucket_acl"" ""example"" {
  depends_on = [aws_s3_bucket_ownership_controls.example]
  bucket     = aws_s3_bucket.example.id
  acl        = ""private""
}

resource ""aws_s3_bucket_policy"" ""allow_get_with_referer"" {
  bucket = aws_s3_bucket.example.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowGetObjectWithReferer""
        Effect    = ""Allow""
        Principal = ""*""
        Action    = ""s3:GetObject""
        Resource  = ""${aws_s3_bucket.example.arn}/*""
        Condition = {
          StringLike = {
            ""aws:Referer"" = [
              ""http://example.com/*"",
              ""https://example.com/*""
            ]
          }
        }
      }
    ]
  })
}
```

This configuration:
1. Creates an S3 bucket with a unique name
2. Sets up bucket ownership controls
3. Configures a private ACL
4. Adds a bucket policy that:
   - Allows s3:GetObject actions
   - Applies to all principals (*)
   - Includes a condition based on the Referer header
   - Only allows access from specified domains

The resources are properly linked using references, and the configuration includes necessary dependencies. The bucket policy uses the jsonencode function to properly format the IAM policy document.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with ACL and a bucket policy that includes a referer-based condition. Based on the examples provided and best practices, here's the solution:

```hcl
provider ""aws"" {
  region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-unique-bucket-name""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id
  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

resource ""aws_s3_bucket_acl"" ""example"" {
  depends_on = [aws_s3_bucket_ownership_controls.example]
  bucket     = aws_s3_bucket.example.id
  acl        = ""private""
}

resource ""aws_s3_bucket_policy"" ""allow_access_with_referer"" {
  bucket = aws_s3_bucket.example.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowGetObjectWithReferer""
        Effect    = ""Allow""
        Principal = ""*""
        Action    = ""s3:GetObject""
        Resource  = ""${aws_s3_bucket.example.arn}/*""
        Condition = {
          StringLike = {
            ""aws:Referer"" = [""http://allowed-domain.com/*""]
          }
        }
      }
    ]
  })
}
```

This configuration:
1. Creates an S3 bucket with a unique name
2. Sets up bucket ownership controls
3. Configures a private ACL
4. Adds a bucket policy that:
   - Allows s3:GetObject actions
   - Applies to all principals
   - Includes a condition based on the AWS Referer header
   - Only allows access from the specified domain

Note that you should replace ""http://allowed-domain.com/*"" with your actual allowed domain pattern.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with the specified ACL and bucket policy. Based on the examples provided and best practices, here's the solution:

```hcl
provider ""aws"" {
  region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my_unique_bucket_name""
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.example.id
  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

resource ""aws_s3_bucket_acl"" ""example"" {
  depends_on = [aws_s3_bucket_ownership_controls.example]
  bucket     = aws_s3_bucket.example.id
  acl        = ""private""
}

resource ""aws_s3_bucket_policy"" ""allow_access_with_referer"" {
  bucket = aws_s3_bucket.example.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowGetObjectWithReferer""
        Effect    = ""Allow""
        Principal = ""*""
        Action    = ""s3:GetObject""
        Resource  = ""${aws_s3_bucket.example.arn}/*""
        Condition = {
          StringLike = {
            ""aws:Referer"" = [""http://example.com/*""]
          }
        }
      }
    ]
  })
}
```

This configuration:
1. Creates an S3 bucket with a specified name
2. Sets up bucket ownership controls
3. Configures a private ACL for the bucket
4. Adds a bucket policy that:
   - Allows s3:GetObject actions
   - Applies to all principals (*)
   - Includes a condition based on the Referer header
   - Applies to all objects in the bucket (using the `/*` suffix in the Resource)

The resources are properly linked using references, and the configuration includes all necessary dependencies.",,,,,
aws_lightsail_instance,create AWS Lightsail with WordPress blueprint,"package terraform.validation

default has_valid_lightsail_instance = false

# Main rule to check for a valid aws_lightsail_instance
has_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id == ""wordpress""
    resource.values.bundle_id
}",1,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""wordpress""
  bundle_id         = ""nano_2_0""
 }","have one ""aws_lightsail_instance"" resource
with ""name"" argument
with ""availability_zone"" argument
with ""blueprint_id"" = wordpress
with ""bundle_id"" argument","I'll help you create an AWS Lightsail instance with a WordPress blueprint. Based on the examples provided, I'll modify the configuration to use the WordPress blueprint specifically.

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""wordpress_instance"" {
  name              = ""wordpress-site""
  availability_zone = ""us-east-1a""
  blueprint_id      = ""wordpress""
  bundle_id         = ""small_2_0""  # Slightly larger instance for WordPress

  tags = {
    Name        = ""WordPress Instance""
    Environment = ""Production""
  }
}

# Optional: Add static IP
resource ""aws_lightsail_static_ip"" ""wordpress_static_ip"" {
  name = ""wordpress-static-ip""
}

resource ""aws_lightsail_static_ip_attachment"" ""wordpress_static_ip_attachment"" {
  static_ip_name = aws_lightsail_static_ip.wordpress_static_ip.name
  instance_name  = aws_lightsail_instance.wordpress_instance.name
}
```

This configuration:
1. Creates a Lightsail instance using the WordPress blueprint
2. Uses a 'small' bundle size which is more suitable for WordPress
3. Includes a static IP attachment for better availability
4. Adds relevant tags for resource management

The WordPress blueprint comes pre-configured with WordPress installed and ready to use. After deployment, you can access the WordPress installation through the instance's IP address or domain name.",,,,,,"I'll help you create an AWS Lightsail instance with a WordPress blueprint. Here's the Terraform configuration:

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""wordpress_instance"" {
  name              = ""wordpress-site""
  availability_zone = ""us-east-1a""
  blueprint_id      = ""wordpress""  # WordPress blueprint
  bundle_id         = ""small_2_0"" # Slightly larger instance for WordPress

  tags = {
    Name = ""WordPress Instance""
    Environment = ""Production""
  }
}

# Optional: Create a static IP for the WordPress instance
resource ""aws_lightsail_static_ip"" ""wordpress_static_ip"" {
  name = ""wordpress-static-ip""
}

# Attach the static IP to the instance
resource ""aws_lightsail_static_ip_attachment"" ""wordpress_static_ip_attachment"" {
  static_ip_name = aws_lightsail_static_ip.wordpress_static_ip.name
  instance_name  = aws_lightsail_instance.wordpress_instance.name
}
```

This configuration:
1. Creates a Lightsail instance using the WordPress blueprint
2. Uses a 'small' bundle size which is recommended for WordPress
3. Creates and attaches a static IP (optional but recommended for production WordPress sites)
4. Includes basic tags for better resource management

The WordPress blueprint comes pre-configured with:
- WordPress installation
- Apache web server
- PHP
- MySQL database

You can access your WordPress site using either the instance's IP address or the static IP once the deployment is complete.",,,,,,"I'll help you create an AWS Lightsail instance with a WordPress blueprint. Here's the Terraform configuration:

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""wordpress_instance"" {
  name              = ""wordpress-site""
  availability_zone = ""us-east-1a""
  blueprint_id      = ""wordpress""  # WordPress blueprint
  bundle_id         = ""small_2_0""  # Using a slightly larger instance for WordPress

  tags = {
    Name = ""WordPress Instance""
    Environment = ""Production""
  }
}

# Optional: Add static IP
resource ""aws_lightsail_static_ip"" ""wordpress_static_ip"" {
  name = ""wordpress-static-ip""
}

resource ""aws_lightsail_static_ip_attachment"" ""wordpress_static_ip_attachment"" {
  static_ip_name = aws_lightsail_static_ip.wordpress_static_ip.name
  instance_name  = aws_lightsail_instance.wordpress_instance.name
}
```

This configuration:
1. Creates a Lightsail instance using the WordPress blueprint
2. Uses a small_2_0 bundle which is suitable for WordPress hosting
3. Adds a static IP and attaches it to the instance
4. Includes basic tags for better resource management

You can modify the region, availability zone, and bundle_id based on your specific needs. The WordPress blueprint comes pre-configured with WordPress installed and ready to use.",,,,,
"aws_db_instance, aws_db_parameter_group, aws_db_subnet_group, aws_kms_key, aws_security_group, aws_subnet, aws_subnet","Provision a secure PostgreSQL database (version is 15 and storage size is 50GB) instance within a specified AWS VPC, leveraging AWS services like RDS, subnets, and KMS for encryption. It sets up a custom parameter group for PostgreSQL settings, and a security group to manage access. The database instance is configured with specifications like storage size, engine version, multi-AZ deployment for high availability, and encryption using a KMS key.","package terraform.validation

default is_valid_db_instance = false

default is_valid_security_group = false

default is_valid_db_subnet_group = false

default is_valid_db_parameter_group = false

default is_valid_kms_key = false

# Validate aws_db_instance resource
is_valid_db_instance {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_instance""
        resource.expressions.allocated_storage.constant_value == 20
        resource.expressions.engine.constant_value == ""postgres""
    resource.expressions.engine_version.constant_value == ""15""
        resource.expressions.instance_class != null
        resource.expressions.username != null
        resource.expressions.password != null
        resource.expressions.multi_az.constant_value == true
        resource.expressions.storage_encrypted.constant_value == true
        resource.expressions.kms_key_id != null
}

# Validate aws_security_group resource
is_valid_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""
        count(resource.expressions.ingress) > 0
        count(resource.expressions.egress) > 0
}

# Validate aws_db_subnet_group resource
is_valid_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}

# Validate aws_db_parameter_group resource
is_valid_db_parameter_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_parameter_group""
        # Additional checks for specific parameter values can be added here if needed
}

# Validate aws_kms_key resource
is_valid_kms_key {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_kms_key""
        # Additional checks for KMS key attributes can be added here if needed
}
",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

resource ""aws_db_subnet_group"" ""db-subnet-group"" {
  name       = ""db-subnet-group""
  subnet_ids = module.vpc.public_subnets
}

resource ""aws_security_group"" ""db-sg"" {
  name = ""db-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-sg-ingress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.db-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-sg-egress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.db-sg.id
}

resource ""aws_db_parameter_group"" ""postgre-param-group"" {
  name   = ""pgparamgrp15""
  family = ""postgres15""

  parameter {
    name  = ""password_encryption""
    value = ""scram-sha-256""
  }

  parameter {
    name  = ""rds.force_ssl""
    value = ""0""
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource ""aws_kms_key"" ""rds-key"" {
  description             = ""kmsrds""
  deletion_window_in_days = 14
}

resource ""aws_db_instance"" ""postgres"" {
  identifier                      = ""pg""
  skip_final_snapshot             = true
  allocated_storage               = 50
  backup_retention_period         = 7
  db_subnet_group_name            = aws_db_subnet_group.db-subnet-group.id
  parameter_group_name            = aws_db_parameter_group.postgre-param-group.name
  enabled_cloudwatch_logs_exports = [""postgresql"", ""upgrade""]
  engine                          = ""postgres""
  engine_version                  = ""15""
  instance_class                  = ""db.t3.micro""
  db_name                         = ""postgres"" # Initial database name
  username                        = ""myusername""
  password                        = ""mypassword""
  vpc_security_group_ids          = [aws_security_group.db-sg.id]
  # Other security settings
  publicly_accessible             = true
  multi_az                        = true
  storage_encrypted               = true
  kms_key_id                      = aws_kms_key.rds-key.arn
  # Default daily backup window
  # https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage""
with one ""engine"" and a valid engine value, such as ""postgres""
with one ""instance_class"" and a valid instance class type, such as ""db.t3.micro""
with one ""password""
with one ""username""
with ""multi_az"" deployment set to true for high availability
with ""storage_encrypted"" set to true
with one ""kms_key_id"" referencing a valid AWS KMS key

Has one ""aws_security_group"" resource
with rules to manage access to the PostgreSQL database instance

Has one ""aws_db_subnet_group"" resource
with ""subnet_ids"" including at least two subnets within the VPC

Has one ""aws_db_parameter_group"" resource
with customized parameters for PostgreSQL settings, such as ""password_encryption"" and ""rds.force_ssl""

Has one ""aws_kms_key"" resource
for encrypting the PostgreSQL database storage","Based on the provided examples and your specific requirements, I'll help you create a Terraform configuration for a PostgreSQL database with version 15 and 50GB storage. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs            = data.aws_availability_zones.available.names
  public_subnets = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

resource ""aws_db_subnet_group"" ""db-subnet-group"" {
  name       = ""db-subnet-group""
  subnet_ids = module.vpc.public_subnets
}

resource ""aws_security_group"" ""db-sg"" {
  name   = ""db-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-sg-ingress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.db-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-sg-egress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.db-sg.id
}

resource ""aws_db_parameter_group"" ""postgre-param-group"" {
  name   = ""pgparamgrp15""
  family = ""postgres15""

  parameter {
    name  = ""password_encryption""
    value = ""scram-sha-256""
  }

  parameter {
    name  = ""rds.force_ssl""
    value = ""0""
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource ""aws_kms_key"" ""rds-key"" {
  description             = ""kmsrds""
  deletion_window_in_days = 14
}

resource ""aws_db_instance"" ""postgres"" {
  identifier                      = ""pg""
  skip_final_snapshot            = true
  allocated_storage              = 50  # Set to 50GB as requested
  db_subnet_group_name           = aws_db_subnet_group.db-subnet-group.id
  parameter_group_name           = aws_db_parameter_group.postgre-param-group.name
  enabled_cloudwatch_logs_exports = [""postgresql"", ""upgrade""]
  engine                         = ""postgres""
  engine_version                 = ""15""  # PostgreSQL version 15 as requested
  instance_class                 = ""db.t3.micro""
  db_name                        = ""postgres""
  username                       = ""myusername""
  password                       = ""mypassword""
  vpc_security_group_ids         = [aws_security_group.db-sg.id]
  publicly_accessible            = true
  multi_az                       = true
  storage_encrypted              = true
  kms_key_id                     = aws_kms_key.rds-key.arn
  backup_retention_period        = 7
}
```

This configuration:
1. Sets up a PostgreSQL database with version 15
2. Allocates 50GB of storage as requested
3. Implements encryption using KMS
4",,,,,,"Based on the provided examples and your specific requirements, I'll help you create a Terraform configuration for a PostgreSQL database with version 15 and 50GB storage. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs            = data.aws_availability_zones.available.names
  public_subnets = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

resource ""aws_db_subnet_group"" ""db-subnet-group"" {
  name       = ""db-subnet-group""
  subnet_ids = module.vpc.public_subnets
}

resource ""aws_security_group"" ""db-sg"" {
  name   = ""db-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-sg-ingress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.db-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-sg-egress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.db-sg.id
}

resource ""aws_db_parameter_group"" ""postgre-param-group"" {
  name   = ""pgparamgrp15""
  family = ""postgres15""

  parameter {
    name  = ""password_encryption""
    value = ""scram-sha-256""
  }

  parameter {
    name  = ""rds.force_ssl""
    value = ""0""
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource ""aws_kms_key"" ""rds-key"" {
  description             = ""kmsrds""
  deletion_window_in_days = 14
}

resource ""aws_db_instance"" ""postgres"" {
  identifier                      = ""pg""
  skip_final_snapshot            = true
  allocated_storage              = 50  # Setting storage to 50GB as requested
  db_subnet_group_name           = aws_db_subnet_group.db-subnet-group.id
  parameter_group_name           = aws_db_parameter_group.postgre-param-group.name
  enabled_cloudwatch_logs_exports = [""postgresql"", ""upgrade""]
  engine                         = ""postgres""
  engine_version                 = ""15""  # Setting PostgreSQL version to 15 as requested
  instance_class                 = ""db.t3.micro""
  db_name                        = ""postgres""
  username                       = ""myusername""
  password                       = ""mypassword""
  vpc_security_group_ids         = [aws_security_group.db-sg.id]
  publicly_accessible            = true
  multi_az                       = true
  storage_encrypted              = true
  kms_key_id                     = aws_kms_key.rds-key.arn
  backup_retention_period        = 7
}
```

This configuration includes:
1. PostgreSQL version 15 as specified
2. 50GB storage allocation as requested
3. Multi-AZ deployment for high availability",,,,,,"Based on the examples provided and your specific requirements, I'll help you create a Terraform configuration for a PostgreSQL database with version 15 and 50GB storage. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs            = data.aws_availability_zones.available.names
  public_subnets = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

resource ""aws_db_subnet_group"" ""db-subnet-group"" {
  name       = ""db-subnet-group""
  subnet_ids = module.vpc.public_subnets
}

resource ""aws_security_group"" ""db-sg"" {
  name   = ""db-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-sg-ingress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.db-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-sg-egress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.db-sg.id
}

resource ""aws_db_parameter_group"" ""postgre-param-group"" {
  name   = ""pgparamgrp15""
  family = ""postgres15""

  parameter {
    name  = ""password_encryption""
    value = ""scram-sha-256""
  }

  parameter {
    name  = ""rds.force_ssl""
    value = ""0""
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource ""aws_kms_key"" ""rds-key"" {
  description             = ""kmsrds""
  deletion_window_in_days = 14
}

resource ""aws_db_instance"" ""postgres"" {
  identifier                      = ""pg""
  skip_final_snapshot            = true
  allocated_storage              = 50  # Setting storage to 50GB as requested
  db_subnet_group_name           = aws_db_subnet_group.db-subnet-group.id
  parameter_group_name           = aws_db_parameter_group.postgre-param-group.name
  enabled_cloudwatch_logs_exports = [""postgresql"", ""upgrade""]
  engine                         = ""postgres""
  engine_version                 = ""15""  # Setting PostgreSQL version to 15 as requested
  instance_class                 = ""db.t3.micro""
  db_name                        = ""postgres""
  username                       = ""myusername""
  password                       = ""mypassword""
  vpc_security_group_ids         = [aws_security_group.db-sg.id]
  
  # Security settings
  publicly_accessible    = false  # Enhanced security by making it private
  multi_az              = true    # High availability with multi-AZ deployment
  storage_encrypted     = true
  kms_key_id            = aws_kms_key.rds-key.arn
  
  backup_retention_period = 7     # Retain backups for 7 days
}
```

This configuration",,,,,
"aws_codebuild_project, aws_iam_policy, aws_iam_role, aws_iam_role_policy_attachment, aws_s3_bucket, aws_iam_policy_document, aws_iam_policy_document","Infrastructure for an autograder for a CS class. The autograder needs to run students' code stored on GitHub in a specific environment and store the result. Accomplish this using the following resources: AWS S3 Bucket, AWS CodeBuild Project. Add any necessary resources.","package autograder_middle

import rego.v1

codebuild_project_valid(codebuild_project, s3_bucket) if {
        some artifact in codebuild_project.expressions.artifacts
        s3_bucket.address in artifact.location.references
        artifact.name
        artifact.type.constant_value == ""S3""

        some environment in codebuild_project.expressions.environment
        environment.compute_type
        environment.image
        environment.type

        some source in codebuild_project.expressions.source
        source.type.constant_value == ""GITHUB""
        source.location
}

default valid := false

valid if {
        resources := input.configuration.root_module.resources
        some codebuild_project in resources
        codebuild_project.type == ""aws_codebuild_project""
        some s3_bucket in resources
        s3_bucket.type == ""aws_s3_bucket""
        some security_group in resources
        codebuild_project_valid(codebuild_project, s3_bucket)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""artifact-bucket-""
}

resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder_build""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip"" # include this
  }

  environment { # arguments required, exact value not specified
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1 # good to have, not required
    location        = ""github.com/source-location""
  }
}

resource ""aws_iam_role"" ""autograder_build_role"" {
  assume_role_policy = data.aws_iam_policy_document.autograder_build_policy_assume.json
}

data ""aws_iam_policy_document"" ""autograder_build_policy_assume"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }
  }

}

data ""aws_iam_policy_document"" ""autograder_build_policy"" {
  statement {
    effect = ""Allow""
    actions = [
      ""ec2:DescribeSubnets"",
      ""ec2:DescribeSecurityGroups"",
      ""ec2:DescribeVpcs""
    ]
    resources = [
      ""*""
    ]
  }
}

resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name        = ""lambda_policy""
  description = ""Grants permissions to Lambda to describe vpc, subnet, security group""

  policy = data.aws_iam_policy_document.autograder_build_policy.json
}

resource ""aws_iam_role_policy_attachment"" ""lambda_policy_attach"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}","have one ""aws_s3_bucket"" resource
have one ""aws_codebuild_project"" resource
  with ""artifacts"" argument block
    with ""type"" argument with value ""S3""
    with ""location"" argument
    with ""name"" argument
  with ""environment"" argument block
    with ""computer_type"" argument
    with ""image"" argument
    with ""type"" argument
  with ""source"" argument block
    with ""type"" argument with value ""GITHUB""
    with ""location"" argument","Based on the provided examples and requirements, I'll help create a Terraform configuration for an autograder infrastructure using AWS S3 and CodeBuild. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket to store build artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""artifact-bucket-""
}

# CodeBuild project for the autograder
resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder_build""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip""
  }

  environment {
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1
    location        = ""github.com/source-location""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""autograder_build_role"" {
  name = ""autograder_build_role""

  assume_role_policy = data.aws_iam_policy_document.autograder_build_policy_assume.json
}

# IAM assume role policy document
data ""aws_iam_policy_document"" ""autograder_build_policy_assume"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }
  }
}

# IAM policy document for CodeBuild permissions
data ""aws_iam_policy_document"" ""autograder_build_policy"" {
  statement {
    effect = ""Allow""
    actions = [
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:PutLogEvents""
    ]
    resources = [""*""]
  }

  statement {
    effect = ""Allow""
    actions = [""s3:*""]
    resources = [
      aws_s3_bucket.artifact_bucket.arn,
      ""${aws_s3_bucket.artifact_bucket.arn}/*""
    ]
  }
}

# IAM policy for CodeBuild
resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name        = ""autograder_build_policy""
  description = ""Policy for CodeBuild autograder project""
  policy      = data.aws_iam_policy_document.autograder_build_policy.json
}

# Attach the policy to the role
resource ""aws_iam_role_policy_attachment"" ""autograder_policy_attachment"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}
```

This configuration creates:
1. An S3 bucket to store the autograder results
2. A CodeBuild project that will run the student code from GitHub
3. Necessary IAM roles and policies for CodeBuild to access S3 and create logs
4. Appropriate permissions and trust relationships

The CodeBuild project will:
- Pull code from the specified GitHub repository
- Run in a Linux container environment
- Store results in the S3 bucket
- Have necessary permissions to write logs and access S3

The configuration includes all required components for the autograder to function while maintaining AWS best practices for permissions and security.",,,,,,"Based on the provided examples and the current question, I'll help you create a Terraform configuration for an autograder infrastructure using AWS S3 and CodeBuild. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket to store build artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""artifact-bucket-""
}

# CodeBuild project for autograder
resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder_build""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip""
  }

  environment {
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1
    location        = ""github.com/source-location""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""autograder_build_role"" {
  name = ""autograder_build_role""

  assume_role_policy = data.aws_iam_policy_document.autograder_build_policy_assume.json
}

# IAM assume role policy
data ""aws_iam_policy_document"" ""autograder_build_policy_assume"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }
  }
}

# IAM policy document for CodeBuild
data ""aws_iam_policy_document"" ""autograder_build_policy"" {
  statement {
    effect = ""Allow""
    actions = [
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:PutLogEvents""
    ]
    resources = [""*""]
  }

  statement {
    effect = ""Allow""
    actions = [""s3:*""]
    resources = [
      aws_s3_bucket.artifact_bucket.arn,
      ""${aws_s3_bucket.artifact_bucket.arn}/*""
    ]
  }
}

# IAM policy for CodeBuild
resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name        = ""autograder_build_policy""
  description = ""Policy for CodeBuild autograder project""
  policy      = data.aws_iam_policy_document.autograder_build_policy.json
}

# Attach the policy to the role
resource ""aws_iam_role_policy_attachment"" ""autograder_policy_attachment"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}
```

This configuration creates:
1. An S3 bucket to store the build artifacts and results
2. A CodeBuild project that will run the autograder
3. Necessary IAM roles and policies for CodeBuild to access S3 and CloudWatch logs
4. The CodeBuild project is configured to pull code from GitHub and store results in S3

The configuration includes all required permissions and settings for the autograder to function properly. The CodeBuild project will use a standard Linux container environment to run the tests, and results will be stored in the S3 bucket.",,,,,,"Based on the provided examples and the current question, I'll help create a Terraform configuration for an autograder infrastructure using AWS S3 and AWS CodeBuild. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket to store build artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""artifact-bucket-""
}

# CodeBuild project for the autograder
resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder_build""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip""
  }

  environment {
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1
    location        = ""github.com/source-location""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""autograder_build_role"" {
  name = ""autograder_build_role""

  assume_role_policy = data.aws_iam_policy_document.autograder_build_policy_assume.json
}

# IAM assume role policy
data ""aws_iam_policy_document"" ""autograder_build_policy_assume"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }
  }
}

# IAM policy document for CodeBuild
data ""aws_iam_policy_document"" ""autograder_build_policy"" {
  statement {
    effect = ""Allow""
    actions = [
      ""logs:CreateLogGroup"",
      ""logs:CreateLogStream"",
      ""logs:PutLogEvents""
    ]
    resources = [""*""]
  }

  statement {
    effect = ""Allow""
    actions = [
      ""s3:GetObject"",
      ""s3:PutObject"",
      ""s3:GetObjectVersion""
    ]
    resources = [
      aws_s3_bucket.artifact_bucket.arn,
      ""${aws_s3_bucket.artifact_bucket.arn}/*""
    ]
  }
}

# IAM policy for CodeBuild
resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name        = ""autograder_build_policy""
  description = ""Policy for autograder CodeBuild project""
  policy      = data.aws_iam_policy_document.autograder_build_policy.json
}

# Attach the policy to the role
resource ""aws_iam_role_policy_attachment"" ""autograder_policy_attachment"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}
```

This configuration:
1. Sets up an S3 bucket to store the build artifacts
2. Creates a CodeBuild project that pulls code from GitHub and runs it in a controlled environment
3. Establishes the necessary IAM roles and policies for CodeBuild to:
   - Assume the required role
   - Access the S3 bucket
   - Create and manage CloudWatch logs
4. Configures the CodeBuild environment with appropriate compute resources

The configuration includes all necessary permissions and resources to run the autograder while maintaining security best practices. The CodeBuild project will pull code from GitHub, run it in the specified environment, and store",,,,,
"aws_route53_record, aws_route53_zone","Set up a Pointer record for reverse DNS using Route 53 resources. The domain name should be ""host.example53.com"" and name the zone ""reverse_zone""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""2.0.192.in-addr.arpa""
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type.constant_value == ""PTR""
    resource.expressions.ttl
    resource.expressions.records.constant_value[0] == ""host.example.com""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.reverse_zone.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

# Create Reverse DNS Hosted Zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""2.0.192.in-addr.arpa""
}

# Create a PTR Record for a specific IP address within that zone
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""53.2.0.192.in-addr.arpa""
  type    = ""PTR""
  ttl     = ""3600""
  records = [""host.example.com""]
}","Has one ""aws_route53_zone"" resource
    with ""name"" ending in "".in-addr.arpa""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type"" set to ""PTR""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
","I'll help you create a Terraform configuration for a PTR (Pointer) record for reverse DNS using Route 53. Based on the examples provided, I'll adapt the configuration to create a reverse DNS zone and PTR record.

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

# Create the reverse DNS zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""1.0.10.in-addr.arpa""  # Example reverse DNS zone for 10.0.1.x network
}

# Create the PTR record
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""1.1.0.10.in-addr.arpa""  # PTR record for IP 10.0.1.1
  type    = ""PTR""
  ttl     = ""300""
  records = [""host.example53.com""]
}
```

This configuration:
1. Sets up an AWS provider in the us-east-1 region
2. Creates a reverse DNS zone using `aws_route53_zone` for the 10.0.1.x network
3. Creates a PTR record that points the IP address 10.0.1.1 to ""host.example53.com""

Note: The reverse DNS zone name follows the standard in-addr.arpa format, and the PTR record name is the reverse of the IP address you want to map. Adjust these values according to your actual IP addressing scheme.

The TTL (Time To Live) is set to 300 seconds (5 minutes), which is a common value but can be adjusted based on your needs.",,,,,,"I'll help you create a Terraform configuration for a PTR (Pointer) record for reverse DNS using Route 53. PTR records are used for reverse DNS lookups, mapping an IP address to a domain name.

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

# Create the reverse DNS zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""1.0.10.in-addr.arpa""  # Example for IP range 10.0.1.x
}

# Create the PTR record
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""45.1.0.10.in-addr.arpa""  # Example for IP 10.0.1.45
  type    = ""PTR""
  ttl     = ""300""
  records = [""host.example53.com""]
}
```

This configuration:
1. Sets up a reverse DNS zone for the IP range 10.0.1.x
2. Creates a PTR record that points from the IP address 10.0.1.45 to the hostname ""host.example53.com""
3. Uses a standard TTL of 300 seconds

Note: The reverse DNS zone name and PTR record name should be adjusted based on your actual IP address. The format follows the in-addr.arpa convention where the IP address is reversed and appended with "".in-addr.arpa"".

Make sure you have the necessary permissions in your AWS account to create Route 53 zones and records.",,,,,,"I'll help you create a Terraform configuration for setting up a PTR (Pointer) record for reverse DNS using Route 53 resources.

```hcl
provider ""aws"" {
  region = ""us-east-1""
}

# Create the reverse DNS zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""1.0.10.in-addr.arpa""  # Example reverse DNS zone for 10.0.1.x network
}

# Create the PTR record
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""45.1.0.10.in-addr.arpa""  # Example for IP 10.0.1.45
  type    = ""PTR""
  ttl     = ""300""
  records = [""host.example53.com""]
}
```

This configuration:
1. Sets up a reverse DNS zone for the 10.0.1.x network
2. Creates a PTR record that points an IP address (in this example, 10.0.1.45) to the hostname ""host.example53.com""
3. Uses a standard TTL of 300 seconds

Note: 
- The reverse DNS zone name follows the standard format: reverse the octets of the network address and append ""in-addr.arpa""
- The PTR record name follows the same format but includes the specific IP address you want to map
- You may need to adjust the zone name and PTR record name based on your actual IP addressing scheme",,,,,
"aws_lex_bot, aws_lex_intent",create an aws_lex_bot for booking a trip with all arguments filled,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for multiple aws_lex_intent resources
has_valid_lex_intents {
    count([intent | intent := input.planned_values.root_module.resources[_]; intent.type == ""aws_lex_intent""; intent.values.name; intent.values.fulfillment_activity])
}

# Rule for aws_lex_bot resource with specific arguments
has_valid_lex_bot_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lex_bot""
    is_boolean(resource.values.child_directed)
    resource.values.clarification_prompt
    resource.values.abort_statement
    resource.values.name
    is_boolean(resource.values.create_version)
    resource.values.description
    is_boolean(resource.values.detect_sentiment)
    resource.values.idle_session_ttl_in_seconds
    resource.values.process_behavior
    resource.values.voice_id
    resource.values.locale
    is_boolean(resource.values.enable_model_improvements)
    resource.values.nlu_intent_confidence_threshold
}

# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lex_intents
    has_valid_lex_bot_instance
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_lex_intent"" ""BookTrip"" {
  description    = ""Book a trip""
  name           = ""BookTrip""

  sample_utterances = [
    ""I want to book a trip"",
    ""I would like to book a trip"",
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

resource ""aws_lex_bot"" ""BookTripBot"" {
  description                   = ""Bot for booking trips""
  idle_session_ttl_in_seconds   = 600
  locale                        = ""en-US""
  name                          = ""BookTripBot""
  process_behavior              = ""BUILD""
  voice_id                      = ""Salli""
  detect_sentiment = false
  enable_model_improvements = true
  nlu_intent_confidence_threshold = 0
  child_directed                = false

  abort_statement {
    message {
      content      = ""Sorry, I cannot assist you to book the trip right now.""
      content_type = ""PlainText""
    }
  }
  
  clarification_prompt {
    max_attempts = 3

    message {
      content      = ""I'm sorry, I didn't understand. Could you please repeat that?""
      content_type = ""PlainText""
    }
  }

  intent {
    intent_name    = aws_lex_intent.BookTrip.name
    intent_version = aws_lex_intent.BookTrip.version
  }
}","Has one resource ""aws_lex_bot"" with all arguments initialized and at least one ""aws_lex_intent"" resource ","I'll help you create an AWS Lex bot for booking a trip with all necessary arguments. This example will include both the bot and its associated intent with multiple slots for trip booking details.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lex_intent"" ""book_trip_intent"" {
  name        = ""BookTripIntent""
  description = ""Intent to help users book a trip""
  
  sample_utterances = [
    ""I want to book a trip"",
    ""I need to plan a vacation"",
    ""Help me book travel arrangements"",
    ""I want to make travel plans""
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  slot {
    name             = ""Destination""
    description      = ""Destination city for the trip""
    slot_constraint  = ""Required""
    slot_type        = ""AMAZON.City""
    priority         = 1
    value_elicitation_prompt {
      message {
        content      = ""Which city would you like to visit?""
        content_type = ""PlainText""
      }
      max_attempts = 3
    }
  }

  slot {
    name             = ""DepartureDate""
    description      = ""Departure date for the trip""
    slot_constraint  = ""Required""
    slot_type        = ""AMAZON.DATE""
    priority         = 2
    value_elicitation_prompt {
      message {
        content      = ""When would you like to depart?""
        content_type = ""PlainText""
      }
      max_attempts = 3
    }
  }

  slot {
    name             = ""ReturnDate""
    description      = ""Return date for the trip""
    slot_constraint  = ""Required""
    slot_type        = ""AMAZON.DATE""
    priority         = 3
    value_elicitation_prompt {
      message {
        content      = ""When would you like to return?""
        content_type = ""PlainText""
      }
      max_attempts = 3
    }
  }

  slot {
    name             = ""RoomType""
    description      = ""Type of room needed""
    slot_constraint  = ""Required""
    slot_type        = ""AMAZON.AlphaNumeric""
    priority         = 4
    value_elicitation_prompt {
      message {
        content      = ""What type of room would you like (standard, deluxe, suite)?""
        content_type = ""PlainText""
      }
      max_attempts = 3
    }
  }

  confirmation_prompt {
    message {
      content      = ""Should I book a trip to {Destination} from {DepartureDate} to {ReturnDate} with a {RoomType} room?""
      content_type = ""PlainText""
    }
    max_attempts = 3
  }

  rejection_statement {
    message {
      content      = ""Okay, I'll not book the trip at this time.""
      content_type = ""PlainText""
    }
  }

  conclusion_statement {
    message {
      content      = ""Thank you! I've booked your trip to {Destination}. Your confirmation will be sent via email.""
      content_type = ""PlainText""
    }
  }
}

resource ""aws_lex_bot"" ""trip_booking_bot"" {
  name                         = ""TripBookingBot""
  description                  = ""Bot to help users book their trips""
  child_directed              = false
  create_version              = true
  process_behavior            = ""BUILD""
  voice_id                    = ""Joanna""
  idle_session_ttl_in_seconds = 600
  locale                      = ""en-US""
  enable_model_improvements   = true
  nlu_intent_confidence_threshold = 0.40

  abort_statement {
    message {
      content      = ""Sorry, I'm having trouble understanding you. Please try",,,,,,"I'll help you create an AWS Lex bot for booking a trip with all necessary slots and arguments. Here's a complete Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lex_intent"" ""book_trip_intent"" {
  name        = ""BookTripIntent""
  description = ""Intent to book a travel trip""
  
  sample_utterances = [
    ""I want to book a trip"",
    ""I need to plan a vacation"",
    ""Help me book travel arrangements"",
    ""I want to make travel plans""
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  slot {
    name            = ""Destination""
    description     = ""Destination city for the trip""
    slot_constraint = ""Required""
    slot_type       = ""AMAZON.City""
    priority        = 1

    value_elicitation_prompt {
      message {
        content      = ""Which city would you like to visit?""
        content_type = ""PlainText""
      }
      max_attempts = 3
    }
  }

  slot {
    name            = ""DepartureDate""
    description     = ""Departure date for the trip""
    slot_constraint = ""Required""
    slot_type       = ""AMAZON.DATE""
    priority        = 2

    value_elicitation_prompt {
      message {
        content      = ""When would you like to depart?""
        content_type = ""PlainText""
      }
      max_attempts = 3
    }
  }

  slot {
    name            = ""ReturnDate""
    description     = ""Return date for the trip""
    slot_constraint = ""Required""
    slot_type       = ""AMAZON.DATE""
    priority        = 3

    value_elicitation_prompt {
      message {
        content      = ""When would you like to return?""
        content_type = ""PlainText""
      }
      max_attempts = 3
    }
  }

  slot {
    name            = ""TravellerCount""
    description     = ""Number of travelers""
    slot_constraint = ""Required""
    slot_type       = ""AMAZON.NUMBER""
    priority        = 4

    value_elicitation_prompt {
      message {
        content      = ""How many people will be traveling?""
        content_type = ""PlainText""
      }
      max_attempts = 3
    }
  }

  confirmation_prompt {
    message {
      content      = ""Should I book a trip to {Destination} for {TravellerCount} people, departing on {DepartureDate} and returning on {ReturnDate}?""
      content_type = ""PlainText""
    }
    max_attempts = 3
  }

  rejection_statement {
    message {
      content      = ""Okay, I will not book the trip. Is there anything else I can help you with?""
      content_type = ""PlainText""
    }
  }

  conclusion_statement {
    message {
      content      = ""Thank you! I've booked your trip to {Destination}. Your departure is set for {DepartureDate} and you'll return on {ReturnDate}.""
      content_type = ""PlainText""
    }
  }
}

resource ""aws_lex_bot"" ""travel_booking_bot"" {
  name                         = ""TravelBookingBot""
  description                  = ""Bot to help users book travel arrangements""
  voice_id                     = ""Salli""
  idle_session_ttl_in_seconds = 600
  child_directed              = false
  detect_sentiment            = true
  locale                      = ""en-US""
  process_behavior            = ""BUILD""

  abort_statement {
    message {
      content      = ""Sorry, I'm having trouble understanding you. Please try again later.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {",,,,,,"I'll help you create an AWS Lex bot for booking a trip with all necessary slots and arguments. Here's a complete Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_lex_intent"" ""book_trip_intent"" {
  name        = ""BookTripIntent""
  description = ""Intent to book a trip""
  
  sample_utterances = [
    ""I want to book a trip"",
    ""I need to plan a vacation"",
    ""Help me book travel arrangements"",
    ""Book a trip for me""
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  slot {
    name            = ""Destination""
    description     = ""Destination city for the trip""
    slot_constraint = ""Required""
    slot_type       = ""AMAZON.City""
    priority        = 1
    
    value_elicitation_prompt {
      message {
        content      = ""Which city would you like to visit?""
        content_type = ""PlainText""
      }
      max_attempts = 3
    }
  }

  slot {
    name            = ""DepartureDate""
    description     = ""Departure date for the trip""
    slot_constraint = ""Required""
    slot_type       = ""AMAZON.DATE""
    priority        = 2
    
    value_elicitation_prompt {
      message {
        content      = ""When would you like to depart?""
        content_type = ""PlainText""
      }
      max_attempts = 3
    }
  }

  slot {
    name            = ""ReturnDate""
    description     = ""Return date for the trip""
    slot_constraint = ""Required""
    slot_type       = ""AMAZON.DATE""
    priority        = 3
    
    value_elicitation_prompt {
      message {
        content      = ""When would you like to return?""
        content_type = ""PlainText""
      }
      max_attempts = 3
    }
  }

  slot {
    name            = ""NumberOfTravelers""
    description     = ""Number of people traveling""
    slot_constraint = ""Required""
    slot_type       = ""AMAZON.NUMBER""
    priority        = 4
    
    value_elicitation_prompt {
      message {
        content      = ""How many people will be traveling?""
        content_type = ""PlainText""
      }
      max_attempts = 3
    }
  }

  confirmation_prompt {
    message {
      content      = ""Should I proceed with booking a trip to {Destination} for {NumberOfTravelers} people, departing on {DepartureDate} and returning on {ReturnDate}?""
      content_type = ""PlainText""
    }
    max_attempts = 3
  }

  rejection_statement {
    message {
      content      = ""Okay, I'll cancel this booking request.""
      content_type = ""PlainText""
    }
  }

  conclusion_statement {
    message {
      content      = ""Great! I've booked your trip to {Destination}. You will receive a confirmation email shortly.""
      content_type = ""PlainText""
    }
  }
}

resource ""aws_lex_bot"" ""travel_booking_bot"" {
  name        = ""TravelBookingBot""
  description = ""Bot to help users book their travel arrangements""
  
  abort_statement {
    message {
      content      = ""Sorry, I'm having trouble understanding you. Please try again later.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    message {
      content      = ""I didn't understand that. Could you please rephrase your request?""
      content_type = ""PlainText""
    }
    max_attempts = 5
  }

  child_directed = false

  idle_session_ttl_in_seconds = 600

  locale           = ""en",,,,,
"aws_vpc_dhcp_options, aws_vpc_dhcp_options_association","creates DHCP options named ""pike"" for an AWS VPC, specifying Google's DNS servers, and then associates these DHCP options with a specified VPC, tagging both the DHCP options and their association with ""permissions"".","package terraform.validation

# Set default validation states
default is_valid_dhcp_options = false

default is_valid_dhcp_options_association = false

# Validate aws_vpc_dhcp_options resource
is_valid_dhcp_options {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options""
        resource.expressions.tags.constant_value != null
}

# Validate aws_vpc_dhcp_options_association resource
is_valid_dhcp_options_association {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options_association""
        resource.expressions.dhcp_options_id.references[0] == ""aws_vpc_dhcp_options.pike.id""
        resource.expressions.vpc_id != null
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}


resource ""aws_vpc_dhcp_options_association"" ""pike"" {
  dhcp_options_id = aws_vpc_dhcp_options.pike.id
  vpc_id          = ""vpc-0c33dc8cd64f408c4""
}

resource ""aws_vpc_dhcp_options"" ""pike"" {
  domain_name_servers = [""8.8.8.8"", ""8.8.4.4""]
  tags = {
    pike = ""permissions""
  }
}","Has one ""aws_vpc_dhcp_options"" resource:
Specifies Google's DNS servers
Tagged with ""permissions"" to categorize or specify its role or access levels within the infrastructure

Has one 
""aws_vpc_dhcp_options_association"" resource:
Associates the ""aws_vpc_dhcp_options"" with a specified ""aws_vpc"" resource
Tagged with ""permissions"" to ensure consistent labeling and access management across related resources","Based on the provided examples and the current question, I'll create a Terraform configuration that creates DHCP options with Google's DNS servers and associates them with a VPC, including the requested tagging.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

resource ""aws_vpc_dhcp_options"" ""pike"" {
  domain_name_servers = [""8.8.8.8"", ""8.8.4.4""]
  
  tags = {
    pike = ""permissions""
  }
}

resource ""aws_vpc_dhcp_options_association"" ""pike"" {
  vpc_id          = ""vpc-0c33dc8cd64f408c4""
  dhcp_options_id = aws_vpc_dhcp_options.pike.id
  
  tags = {
    pike = ""permissions""
  }
}
```

This configuration:
1. Sets up the required AWS provider configuration
2. Creates DHCP options named ""pike"" with Google's DNS servers (8.8.8.8 and 8.8.4.4)
3. Associates the DHCP options with a specified VPC
4. Tags both the DHCP options and the association with ""pike = permissions""
5. Uses proper referencing between resources using the `aws_vpc_dhcp_options.pike.id`

Note that you'll need to replace ""vpc-0c33dc8cd64f408c4"" with your actual VPC ID.",,,,,,"Based on the provided examples and your requirement, I'll help you create a Terraform configuration that creates DHCP options with Google's DNS servers and associates them with a VPC, including the requested tagging.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

resource ""aws_vpc_dhcp_options"" ""pike"" {
  domain_name_servers = [""8.8.8.8"", ""8.8.4.4""]
  
  tags = {
    pike = ""permissions""
  }
}

resource ""aws_vpc_dhcp_options_association"" ""pike"" {
  vpc_id          = ""vpc-0c33dc8cd64f408c4""
  dhcp_options_id = aws_vpc_dhcp_options.pike.id
  
  tags = {
    pike = ""permissions""
  }
}
```

This configuration:
1. Sets up the required AWS provider configuration
2. Creates DHCP options named ""pike"" with Google's DNS servers (8.8.8.8 and 8.8.4.4)
3. Associates the DHCP options with the specified VPC
4. Tags both resources with the ""pike = permissions"" tag
5. Uses proper reference syntax to link the DHCP options to the association

Note: You'll need to replace ""vpc-0c33dc8cd64f408c4"" with your actual VPC ID.",,,,,,"Based on the provided examples and your requirement, I'll help you create a Terraform configuration that creates DHCP options with Google's DNS servers and associates them with a VPC, including the requested tagging. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

resource ""aws_vpc_dhcp_options"" ""pike"" {
  domain_name_servers = [""8.8.8.8"", ""8.8.4.4""]
  
  tags = {
    pike = ""permissions""
  }
}

resource ""aws_vpc_dhcp_options_association"" ""pike"" {
  vpc_id          = ""vpc-0c33dc8cd64f408c4""
  dhcp_options_id = aws_vpc_dhcp_options.pike.id
  
  tags = {
    pike = ""permissions""
  }
}
```

This configuration:
1. Sets up the required AWS provider configuration
2. Creates DHCP options named ""pike"" with Google's DNS servers (8.8.8.8 and 8.8.4.4)
3. Associates these DHCP options with the specified VPC
4. Applies the requested ""permissions"" tag to both the DHCP options and the association
5. Uses proper resource referencing with aws_vpc_dhcp_options.pike.id

Note that you'll need to replace ""vpc-0c33dc8cd64f408c4"" with your actual VPC ID.",,,,,
"aws_vpc, aws_vpc, aws_vpc_peering_connection","establishes a VPC peering connection between two AWS Virtual Private Clouds (VPCs). The first VPC, named ""peer,"" is configured with a CIDR block of 10.0.0.0/24, and the second VPC, named ""base,"" has a CIDR block of 10.1.0.0/24. The peering connection, named ""pike,"" connects these two VPCs, allowing them to communicate with each other as if they were in the same network","package terraform.validation

# Set default validation states
default is_valid_vpcs = false
default is_valid_vpc_peering_connection = false

# Validate aws_vpc resources
is_valid_vpcs {
    # Validate the first VPC named ""peer""
    peer_vpc := input.configuration.root_module.resources[_]
    peer_vpc.type == ""aws_vpc""
    peer_vpc.name == ""peer""
    peer_vpc.expressions.cidr_block.constant_value == ""10.0.0.0/24""

    # Validate the second VPC named ""base""
    base_vpc := input.configuration.root_module.resources[_]
    base_vpc.type == ""aws_vpc""
    base_vpc.name == ""base""
    base_vpc.expressions.cidr_block.constant_value == ""10.1.0.0/24""

    # Ensure different VPCs
    peer_vpc != base_vpc
}

# Validate aws_vpc_peering_connection resource
is_valid_vpc_peering_connection {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_vpc_peering_connection""
    resource.name == ""pike""
    # Ensure connection between ""peer"" and ""base"" VPCs
    resource.expressions.peer_vpc_id.references[0] == ""aws_vpc.peer.id""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.base.id""
    
}",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id

}

resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
}

resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
}","Has two ""aws_vpc"" resources:
The first VPC is named ""peer"" and is configured with a ""cidr_block"" of ""10.0.0.0/24"".
The second VPC is named ""base"" and is configured with a ""cidr_block"" of ""10.1.0.0/24"".

Has one ""aws_vpc_peering_connection"" resource:
Named ""pike"" to connect the two specified VPCs (""peer"" and ""base"").
Configured to allow the VPCs to communicate with each other as if they were in the same network.","Based on the provided examples and the current question, I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs. This solution combines elements from Example 1 and includes the specific CIDR blocks requested.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create the first VPC named ""peer""
resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
  
  tags = {
    Name = ""peer""
  }
}

# Create the second VPC named ""base""
resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
  
  tags = {
    Name = ""base""
  }
}

# Create the VPC peering connection named ""pike""
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    Name = ""pike""
  }
}
```

This configuration:
1. Sets up the required provider and version constraints
2. Configures the AWS provider for the us-east-2 region
3. Creates two VPCs with their specified CIDR blocks and names
4. Establishes a VPC peering connection between them named ""pike""
5. Uses proper resource references and includes appropriate tags
6. Enables auto-accept for the peering connection

Note that you might need to add route table entries to enable actual traffic flow between the VPCs, but this configuration establishes the basic peering connection as requested.",,,,,,"Based on your request, I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs. This solution closely aligns with Example 1 from the provided examples.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create the first VPC named ""peer""
resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
  
  tags = {
    Name = ""peer""
  }
}

# Create the second VPC named ""base""
resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
  
  tags = {
    Name = ""base""
  }
}

# Establish the VPC peering connection named ""pike""
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    Name = ""pike""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates two VPCs with specified CIDR blocks and names
3. Establishes a VPC peering connection between them
4. Includes appropriate tagging for resource identification
5. Uses auto_accept for automatic peering connection acceptance

Note: For this to work in a production environment, you might also need to configure route tables for both VPCs to route traffic between the peered VPCs. Additionally, ensure that the CIDR blocks don't overlap to avoid networking conflicts.",,,,,,"Based on the examples provided and the current question, I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs with the specified requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create the first VPC named ""peer""
resource ""aws_vpc"" ""peer"" {
  cidr_block           = ""10.0.0.0/24""
  enable_dns_hostnames = true

  tags = {
    Name = ""peer""
  }
}

# Create the second VPC named ""base""
resource ""aws_vpc"" ""base"" {
  cidr_block           = ""10.1.0.0/24""
  enable_dns_hostnames = true

  tags = {
    Name = ""base""
  }
}

# Create the VPC peering connection named ""pike""
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    Name = ""pike""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates two VPCs (""peer"" and ""base"") with their specified CIDR blocks
3. Establishes a VPC peering connection named ""pike"" between the two VPCs
4. Includes appropriate tagging for resource identification
5. Enables DNS hostnames for both VPCs
6. Uses auto_accept for automatic peering connection acceptance

Note: You'll need to configure route tables separately to enable actual traffic flow between the VPCs through this peering connection.",,,,,
"aws_elasticache_user, aws_elasticache_user_group",Provide an ElastiCache user group resource.,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_group_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user_group""
}

# Aggregate all checks
allow {
    aws_elasticache_user_group_valid(input.resource_changes)
}",2,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""default""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}

resource ""aws_elasticache_user_group"" ""test"" {
  engine        = ""REDIS""
  user_group_id = ""userGroupId""
  user_ids      = [aws_elasticache_user.test.user_id]
}","creates elasticache user resources(requires access string, engine = REDIS, user id and user name) and sets engine , creates group resource using the user ids","I'll help you create a Terraform configuration for an ElastiCache user group resource. Based on the examples provided, I'll create a basic configuration that includes both the user and user group resources:

```hcl
# Create an ElastiCache user
resource ""aws_elasticache_user"" ""default"" {
  user_id       = ""defaultUserId""
  user_name     = ""default""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}

# Create the ElastiCache user group
resource ""aws_elasticache_user_group"" ""example"" {
  engine        = ""REDIS""
  user_group_id = ""userGroupId""
  user_ids      = [aws_elasticache_user.default.user_id]
}
```

This configuration:
1. Creates an ElastiCache user with specified permissions and access string
2. Creates an ElastiCache user group that references the created user
3. Uses REDIS as the engine
4. Associates the user with the user group using the `user_ids` parameter

The user group can be modified to include additional users by adding more user IDs to the `user_ids` list. The configuration follows AWS best practices and maintains proper security controls through the access string specification.",,,,,,"I'll help you create a basic ElastiCache user group resource based on the examples provided. Here's a Terraform configuration for creating an ElastiCache user group:

```hcl
# Create an ElastiCache user first
resource ""aws_elasticache_user"" ""default"" {
  user_id       = ""default-user""
  user_name     = ""default""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}

# Create the ElastiCache user group
resource ""aws_elasticache_user_group"" ""example"" {
  engine        = ""REDIS""
  user_group_id = ""example-user-group""
  user_ids      = [aws_elasticache_user.default.user_id]
}
```

This configuration:
1. Creates an ElastiCache user with basic read permissions
2. Creates an ElastiCache user group that:
   - Uses Redis as the engine
   - Has a specified user group ID
   - Associates the created user with the group
   - Automatically manages user IDs through the user_ids attribute

The user group can be used to manage access control for your ElastiCache Redis instance. Make sure to replace the placeholder values (like passwords and IDs) with your desired values in a production environment.",,,,,,"I'll help you create a simple ElastiCache user group resource based on the provided examples. Here's a basic configuration for an ElastiCache user group:

```hcl
resource ""aws_elasticache_user"" ""default"" {
  user_id       = ""defaultUserID""
  user_name     = ""default""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}

resource ""aws_elasticache_user_group"" ""example"" {
  engine        = ""REDIS""
  user_group_id = ""userGroupId""
  user_ids      = [aws_elasticache_user.default.user_id]
}
```

This configuration:
1. Creates a default ElastiCache user
2. Creates an ElastiCache user group
3. Associates the default user with the user group
4. Uses REDIS as the engine
5. Sets appropriate access permissions through the access_string

The user group can be referenced by other resources using `aws_elasticache_user_group.example.user_group_id`.",,,,,
"aws_eks_addon, aws_eks_cluster, aws_iam_role, aws_subnet, aws_vpc","Generate a Terraform script to add an addon with name ""example"", to an AWS EKS cluster. Define a resource that specifies the cluster's name and sets the addon name to 'vpc-cni'.","package terraform.validation

default has_aws_eks_addon_example = false

has_aws_eks_addon_example {
    addon := input.planned_values.root_module.resources[_]
    addon.type == ""aws_eks_addon""
    addon.name == ""example""
    addon.values.cluster_name == input.planned_values.root_module.resources[_].values.name  # Ensures it references aws_eks_cluster.example correctly
    addon.values.addon_name == ""vpc-cni""
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_iam_role"" ""example"" {
  name = ""eks-cluster-1""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""eks.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_eks_cluster"" ""example"" {
  name     = ""test""
  version  = ""test-version""
  role_arn = aws_iam_role.example.arn

  vpc_config {
    subnet_ids = [
      aws_subnet.private-us-east-1a.id,
    ]
  }
}

resource ""aws_eks_addon"" ""example"" {
  cluster_name = aws_eks_cluster.example.name
  addon_name   = ""vpc-cni""
}","has one ""aws_eks_addon"" resource
with one ""cluster_name""
with one ""addon_name""","I'll help you create a Terraform script to add a VPC CNI addon to an AWS EKS cluster. Based on the provided examples and requirements, here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""
}

# Reference to existing EKS cluster
resource ""aws_eks_cluster"" ""example"" {
  name     = ""example""
  role_arn = aws_iam_role.example.arn

  vpc_config {
    subnet_ids = [aws_subnet.example1.id, aws_subnet.example2.id]
  }
}

# Add VPC CNI addon to the EKS cluster
resource ""aws_eks_addon"" ""example"" {
  cluster_name = aws_eks_cluster.example.name
  addon_name   = ""vpc-cni""

  # Ensure the addon is created after the cluster
  depends_on = [
    aws_eks_cluster.example
  ]
}

# Output the addon status
output ""addon_status"" {
  value = aws_eks_addon.example.status
}
```

This Terraform configuration:
1. Sets up the required AWS provider configuration
2. References an existing EKS cluster named ""example""
3. Creates an EKS addon resource for the VPC CNI
4. Adds a dependency to ensure proper creation order
5. Includes an output to monitor the addon status

The VPC CNI addon will be installed and managed by EKS, providing networking capabilities for your cluster's pods.",,,,,,"I'll help you create a Terraform script to add a VPC CNI addon to an AWS EKS cluster. Based on the provided context and best practices, here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# Reference to existing EKS cluster
resource ""aws_eks_cluster"" ""example"" {
  name     = ""example""
  # ... other cluster configurations ...
}

# Add VPC CNI addon to the EKS cluster
resource ""aws_eks_addon"" ""example"" {
  cluster_name = aws_eks_cluster.example.name
  addon_name   = ""vpc-cni""

  # Optional: Specify addon version if needed
  # addon_version = ""v1.12.0-eksbuild.1""

  # Optional: Configure resolve conflicts
  resolve_conflicts = ""OVERWRITE""

  # Ensure the cluster exists before creating the addon
  depends_on = [aws_eks_cluster.example]
}

# Optional: Output the addon version
output ""vpc_cni_version"" {
  description = ""The version of the VPC CNI addon""
  value       = aws_eks_addon.example.addon_version
}
```

This Terraform configuration:
1. Sets up the required AWS provider
2. References an existing EKS cluster named ""example""
3. Creates a VPC CNI addon using the `aws_eks_addon` resource
4. Includes a dependency to ensure the cluster exists before creating the addon
5. Provides an optional output for the addon version

The configuration follows AWS best practices and ensures proper resource creation order through dependencies. The VPC CNI addon is essential for pod networking in EKS clusters.",,,,,,"I'll help you create a Terraform script to add an addon to an AWS EKS cluster based on the provided examples and requirements.

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
}

# Reference to existing EKS cluster
resource ""aws_eks_cluster"" ""example"" {
  name     = ""example""
  # ... other cluster configurations ...
}

# EKS Addon resource
resource ""aws_eks_addon"" ""example"" {
  cluster_name = aws_eks_cluster.example.name
  addon_name   = ""vpc-cni""

  # Ensure the addon is created after the cluster
  depends_on = [
    aws_eks_cluster.example
  ]
}

# Optional: Output the addon version
output ""addon_version"" {
  value = aws_eks_addon.example.addon_version
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an EKS addon resource that:
   - Links to the specified EKS cluster using `cluster_name`
   - Sets the addon name to ""vpc-cni""
3. Includes a dependency to ensure proper creation order
4. Provides an output for the addon version

Note: This assumes you have an existing EKS cluster. If you need to create both the cluster and the addon, you would need to include the full cluster configuration as shown in the example documentation.",,,,,
"aws_iam_policy, aws_iam_role, aws_iam_role_policy_attachment, aws_kinesis_analytics_application, aws_kinesis_stream, aws_iam_policy_document, aws_iam_policy_document",Generate a basic Amazon Kinesis Analytics Application with an input kinesis stream,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kinesisanalytics.amazonaws.com"") 
}

has_valid_kinesis_stream {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_stream""
    resource.values.name
}

has_valid_kinesis_analytics_application {
        some i
        resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_analytics_application""
    resource.values.name
    resource.values.inputs[_].kinesis_stream
}

has_valid_resources {
        has_valid_iam_role(input.resource_changes)
        has_valid_kinesis_stream
    has_valid_kinesis_analytics_application
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}


data ""aws_iam_policy_document"" ""StreamReaderWriter"" {
  statement {
    effect = ""Allow""
    actions = [""kinesis:DescribeStream"",
               ""kinesis:GetShardIterator"",
               ""kinesis:GetRecords""]
    resources = [aws_kinesis_stream.test_stream.arn]
  }
}

data ""aws_iam_policy_document"" ""KinesisAssume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}

resource ""aws_iam_policy"" ""StreamReaderWriter"" {
  name   = ""KinesisStreamReaderWriter""
  policy = data.aws_iam_policy_document.StreamReaderWriter.json
}

resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_role""
  assume_role_policy = data.aws_iam_policy_document.KinesisAssume.json
}

resource ""aws_iam_role_policy_attachment"" ""attach_kinesis_role"" {
  role       = aws_iam_role.kinesis_role.name
  policy_arn = aws_iam_policy.StreamReaderWriter.arn
}

resource ""aws_kinesis_stream"" ""test_stream"" {
  name             = ""events""
  shard_count      = 1
  retention_period = 168
}


resource ""aws_kinesis_analytics_application"" ""test_application"" {
  name = ""kinesis-analytics-application-test""

  inputs {
    name_prefix = ""test_prefix""

    kinesis_stream {
      resource_arn = aws_kinesis_stream.test_stream.arn
      role_arn     = aws_iam_role.kinesis_role.arn
    }

    parallelism {
      count = 1
    }

    schema {
      record_columns {
        mapping  = ""$.test""
        name     = ""test""
        sql_type = ""VARCHAR(8)""
      }

      record_encoding = ""UTF-8""

      record_format {
        mapping_parameters {
          json {
            record_row_path = ""$""
          }
        }
      }
    }
  }
}","Has one ""aws_kinesis_analytics_application"" resource with one ""aws_kinesis_stream"" resource as an input","I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with an input Kinesis stream. Based on the examples provided, I'll expand the configuration to include the input stream and necessary configurations.

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

# Create a Kinesis stream as input
resource ""aws_kinesis_stream"" ""input_stream"" {
  name             = ""analytics-input-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""test""
  }
}

# Create IAM role for Kinesis Analytics
data ""aws_iam_policy_document"" ""kinesis_analytics_assume"" {
  statement {
    effect = ""Allow""
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name               = ""kinesis-analytics-role""
  assume_role_policy = data.aws_iam_policy_document.kinesis_analytics_assume.json
}

# Attach policy to allow reading from input stream
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kinesis:GetShardIterator"",
          ""kinesis:GetRecords"",
          ""kinesis:DescribeStream"",
          ""kinesis:ListShards""
        ]
        Resource = aws_kinesis_stream.input_stream.arn
      }
    ]
  })
}

# Create Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""example-analytics-application""

  inputs {
    name_prefix = ""INPUT_""
    
    kinesis_stream {
      resource_arn = aws_kinesis_stream.input_stream.arn
      role_arn    = aws_iam_role.kinesis_analytics_role.arn
    }

    schema_version = ""1""

    schema {
      record_columns {
        name     = ""example_column""
        sql_type = ""VARCHAR(8)""
        mapping  = ""$.example""
      }

      record_format {
        mapping_parameters {
          json {
            record_row_path = ""$""
          }
        }
      }
    }
  }
}
```

This configuration:
1. Creates a Kinesis input stream
2. Sets up the necessary IAM role and policy for Kinesis Analytics to read from the input stream
3. Creates a Kinesis Analytics application with the input stream configuration
4. Includes a basic schema definition for the input data (assumes JSON format)

Key features:
- The input stream has 1 shard and 24-hour retention
- The IAM role has permissions to read from the Kinesis stream
- The analytics application is configured with JSON input format
- Includes a sample schema with one column (you can modify the schema based on your actual data structure)

Remember to:
- Adjust the region as needed
- Modify the shard count based on your throughput requirements
- Update the schema definition to match your actual data structure
- Adjust resource names and tags according to your naming conventions",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with an input Kinesis stream. Here's the solution:

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

# Create IAM role for Kinesis Analytics
data ""aws_iam_policy_document"" ""kinesis_analytics_assume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}

resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name               = ""kinesis_analytics_role""
  assume_role_policy = data.aws_iam_policy_document.kinesis_analytics_assume.json
}

# Create Kinesis Stream for input
resource ""aws_kinesis_stream"" ""input_stream"" {
  name             = ""input-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""test""
  }
}

# Create Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""example-kinesis-analytics-app""

  inputs {
    name_prefix = ""SOURCE_SQL_STREAM""

    kinesis_stream {
      resource_arn = aws_kinesis_stream.input_stream.arn
      role_arn     = aws_iam_role.kinesis_analytics_role.arn
    }

    schema_version = ""1""

    schema {
      record_columns {
        name     = ""example_column""
        sql_type = ""VARCHAR(8)""
        mapping  = ""$.example""
      }

      record_format {
        mapping_parameters {
          json {
            record_row_path = ""$""
          }
        }
      }
    }
  }
}

# Add necessary permissions to the IAM role
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis_analytics_policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kinesis:GetShardIterator"",
          ""kinesis:GetRecords"",
          ""kinesis:DescribeStream"",
          ""kinesis:ListShards""
        ]
        Resource = aws_kinesis_stream.input_stream.arn
      }
    ]
  })
}
```

This configuration:
1. Creates a Kinesis Stream that will serve as the input
2. Sets up the necessary IAM role and policies for Kinesis Analytics to access the input stream
3. Creates a Kinesis Analytics Application with the input stream configuration
4. Configures a basic schema for the input data (assuming JSON format)

The application expects JSON input data and creates a simple schema with one example column. You can modify the schema section according to your specific data structure needs.

Key features:
- Input stream with 1 shard and 24-hour retention
- JSON record format
- Basic IAM permissions for stream access
- Single column schema (expandable based on needs)

Remember to adjust the region, names, and schema according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with an input Kinesis stream. Based on the examples provided, I'll expand the configuration to include the input stream configuration.

```hcl
provider ""aws"" {
  region = ""us-west-2""
}

# Create an IAM role for Kinesis Analytics
data ""aws_iam_policy_document"" ""kinesis_analytics_assume"" {
  statement {
    effect = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}

resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name               = ""kinesis-analytics-role""
  assume_role_policy = data.aws_iam_policy_document.kinesis_analytics_assume.json
}

# Create a Kinesis stream as input
resource ""aws_kinesis_stream"" ""input_stream"" {
  name             = ""input-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""test""
  }
}

# Create the Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""example-kinesis-analytics-app""

  inputs {
    name_prefix = ""SOURCE_SQL_STREAM""
    
    kinesis_stream {
      resource_arn = aws_kinesis_stream.input_stream.arn
      role_arn     = aws_iam_role.kinesis_analytics_role.arn
    }

    schema_version = ""1""

    schema {
      record_columns {
        name     = ""example_column""
        sql_type = ""VARCHAR(8)""
        mapping  = ""$.example""
      }

      record_format {
        mapping_parameters {
          json {
            record_row_path = ""$""
          }
        }
      }
    }
  }
}

# Add necessary permissions to the IAM role
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kinesis:GetShardIterator"",
          ""kinesis:GetRecords"",
          ""kinesis:DescribeStream"",
          ""kinesis:ListShards""
        ]
        Resource = aws_kinesis_stream.input_stream.arn
      }
    ]
  })
}
```

This configuration:
1. Creates a Kinesis stream that will serve as the input
2. Sets up the necessary IAM role and policies for Kinesis Analytics to access the input stream
3. Creates a Kinesis Analytics application with the input stream configuration
4. Includes a basic schema definition for the input data (which you should modify according to your actual data structure)

The configuration assumes JSON input data and includes one example column. You should modify the schema section according to your actual data structure and requirements.

Key features:
- Input stream with 1 shard and 24-hour retention
- JSON record format
- Necessary IAM permissions for Kinesis Analytics to read from the input stream
- Basic schema definition that you can customize

Remember to adjust the region, names, and schema according to your specific needs.",,,,,
aws_lambda_layer_version,"Create a Lambda Layer Version resource from ""lambda_layer_payload.zip""","package main

import future.keywords.in

default allow = false

# Check if the AWS Lambda Layer Version is valid
aws_lambda_layer_version_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_layer_version""
    resource.change.after.filename == ""lambda_layer_payload.zip""
}

# Aggregate all checks
allow {
    aws_lambda_layer_version_valid(input.resource_changes)
}",1,"resource ""aws_lambda_layer_version"" ""lambda_layer"" {
  filename   = ""lambda_layer_payload.zip""
  layer_name = ""lambda_layer_name""

  compatible_runtimes = [""nodejs16.x""]
}","Create a aws_lambda_layer_version by using file_name = ""lambda_layer_payload.zip"". Layer name can be any.","I'll help you create a Lambda Layer Version resource in Terraform. Based on the examples provided and AWS Lambda Layer best practices, here's the solution:

```hcl
# Create Lambda Layer Version
resource ""aws_lambda_layer_version"" ""example_layer"" {
  filename            = ""lambda_layer_payload.zip""
  layer_name          = ""example_layer""
  description         = ""Example Lambda Layer""
  compatible_runtimes = [""nodejs18.x""]

  # Optional: Specify compatible architectures
  compatible_architectures = [""x86_64""]
}

# Output the layer ARN for reference
output ""lambda_layer_arn"" {
  value = aws_lambda_layer_version.example_layer.arn
}
```

This configuration:
1. Creates a Lambda Layer Version using the specified zip file
2. Sets a name and description for the layer
3. Specifies compatible runtimes (in this case Node.js 18.x)
4. Defines compatible architectures
5. Includes an output to reference the layer ARN

You can then reference this layer in your Lambda functions using the layer ARN. This follows AWS best practices for Lambda layers and maintains proper HCL syntax.

To use this layer in a Lambda function, you would reference it like this:

```hcl
resource ""aws_lambda_function"" ""example_lambda"" {
  filename      = ""lambda_function_payload.zip""
  function_name = ""example_function""
  role          = aws_iam_role.iam_for_lambda.arn
  handler       = ""index.handler""
  runtime       = ""nodejs18.x""

  layers = [aws_lambda_layer_version.example_layer.arn]
}
```",,,,,,"I'll help you create a Lambda Layer Version resource using Terraform. Based on the examples provided, I'll create a solution that adds a Lambda Layer from a ZIP file.

```hcl
# Archive the Lambda Layer files
data ""archive_file"" ""lambda_layer"" {
  type        = ""zip""
  source_dir  = ""${path.module}/layer""  # Directory containing your layer files
  output_path = ""lambda_layer_payload.zip""
}

# Create the Lambda Layer Version
resource ""aws_lambda_layer_version"" ""example_layer"" {
  filename            = ""lambda_layer_payload.zip""
  layer_name         = ""example_layer""
  description        = ""Example Lambda Layer""
  compatible_runtimes = [""nodejs18.x""]  # Specify compatible runtimes

  # Optional: Skip destroying older versions when a new layer version is created
  skip_destroy = true
}
```

This configuration:
1. Creates an archive of your Lambda Layer files (if you need to package them)
2. Creates a Lambda Layer Version resource with:
   - The specified ZIP file as the layer content
   - A name for the layer
   - A description
   - Compatible runtimes
   - Option to preserve older versions

You can reference this layer in your Lambda functions using the layer ARN output from this resource.

To use this layer in a Lambda function, you would add it to the layers argument like this:

```hcl
resource ""aws_lambda_function"" ""example"" {
  # ... other configuration ...
  layers = [aws_lambda_layer_version.example_layer.arn]
}
```",,,,,,"I'll help you create a Lambda Layer Version resource in Terraform. Based on the examples provided, I'll create a solution that follows AWS best practices.

Here's the Terraform configuration to create a Lambda Layer Version from a ZIP file:

```hcl
resource ""aws_lambda_layer_version"" ""example_layer"" {
  filename            = ""lambda_layer_payload.zip""
  layer_name         = ""example_lambda_layer""
  description        = ""Example Lambda Layer""
  compatible_runtimes = [""nodejs18.x""]

  # Optional: Specify compatible architectures
  compatible_architectures = [""x86_64""]
}

# Optional: If you need to create the zip file as part of the Terraform configuration
data ""archive_file"" ""layer"" {
  type        = ""zip""
  source_dir  = ""${path.module}/layer""  # Directory containing your layer code
  output_path = ""lambda_layer_payload.zip""
}
```

This configuration:
1. Creates a Lambda Layer Version using the specified ZIP file
2. Sets a name and description for the layer
3. Specifies compatible runtimes (in this case Node.js 18.x)
4. Optionally includes compatible architectures
5. Includes an optional archive_file data source if you need to create the ZIP file during Terraform execution

You can reference this layer in your Lambda functions using:
```hcl
resource ""aws_lambda_function"" ""example"" {
  # ... other configuration ...
  layers = [aws_lambda_layer_version.example_layer.arn]
}
```

The layer will be available to any Lambda function that uses a compatible runtime and architecture.",,,,,
