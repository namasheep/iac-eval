Resource,Prompt,Rego intent,Difficulty,Reference output,Intent,LLM Output #0,LLM Plannable? #0,LLM Correct? #0,LLM Plan Phase Error #0,LLM OPA match phase Error #0,LLM Notes #0,LLM Output #1,LLM Plannable? #1,LLM Correct? #1,LLM Plan Phase Error #1,LLM OPA match phase Error #1,LLM Notes #1,LLM Output #2,LLM Plannable? #2,LLM Correct? #2,LLM Plan Phase Error #2,LLM OPA match phase Error #2,LLM Notes #2
"aws_api_gateway_integration, aws_api_gateway_integration, aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_dynamodb_table, aws_iam_role, aws_iam_role, aws_lambda_function, aws_lambda_function, aws_lambda_permission, aws_lambda_permission, aws_s3_bucket","An AWS service that holds a web server which allows you to upload cat pictures and provides random cat pictures on demand. Accomplish this using the following resources: AWS DynamoDB table, AWS S3 bucket, AWS Lambda function, AWS Lambda permission, AWS API Gateway rest API, AWS API Gateway resource, AWS API Gateway method. Add any necessary resources.","package caas_middle

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
	api.address in cat.expressions.parent_id.references
	api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
	method.expressions.http_method.constant_value == method_string
	resource.address in method.expressions.resource_id.references
	api.address in method.expressions.rest_api_id.references
}

table_valid(table) if {
	some attribute in table.expressions.attribute
	attribute.name
	attribute.type

	table.expressions.hash_key
}

lambda_valid(lambda, bucket) if {
	some env in lambda.expressions.environment
	bucket.address in env.variables.references
}

permission_valid(permission, lambda, api) if {
	permission.expressions.action.constant_value == ""lambda:InvokeFunction""
	lambda.address in permission.expressions.function_name.references
	permission.expressions.principal.constant_value == ""apigateway.amazonaws.com""
	api.address in permission.expressions.source_arn.references
}

integration_valid(integration, lambda, method, resource, api, integration_method) if {
	method.address in integration.expressions.http_method.references
	resource.address in integration.expressions.resource_id.references
	api.address in integration.expressions.rest_api_id.references
	integration.expressions.integration_http_method.constant_value == integration_method
	integration.expressions.type.constant_value == ""AWS_PROXY""
	lambda.address in integration.expressions.uri.references
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some api in resources
	api.type == ""aws_api_gateway_rest_api""

	some cat in resources
	cat.type == ""aws_api_gateway_resource""

	some method_get in resources
	method_get.type == ""aws_api_gateway_method""

	some method_put in resources
	method_put.type == ""aws_api_gateway_method""

	some table in resources
	table.type == ""aws_dynamodb_table""

	some lambda_get in resources
	lambda_get.type == ""aws_lambda_function""

	some lambda_put in resources
	lambda_put.type == ""aws_lambda_function""

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	some permission_get in resources
	permission_get.type == ""aws_lambda_permission""

	some permission_put in resources
	permission_put.type == ""aws_lambda_permission""

	some integration_get in resources
	integration_get.type == ""aws_api_gateway_integration""

	some integration_put in resources
	integration_put.type == ""aws_api_gateway_integration""

	api_valid(api)
	cat_valid(cat, api)
	method_valid(method_get, ""GET"", cat, api)
	method_valid(method_put, ""PUT"", cat, api)
	lambda_valid(lambda_get, bucket, archive_get)
	lambda_valid(lambda_put, bucket, archive_put)
	permission_valid(permission_get, lambda_get, api)
	permission_valid(permission_put, lambda_put, api)
	integration_valid(integration_get, lambda_get, method_get, cat, api, ""GET"")
	integration_valid(integration_put, lambda_put, method_put, cat, api, ""PUT"")
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_api resource id
    with ""resource_id"" linking to aws_api_gateway_resource resource
    with ""authorization"" argument
    with ""http_method"" argument with values ""GET"" and ""PUT""
have one ""aws_s3_bucket"" resource
have one ""aws_dynamodb_table"" resource
    with ""name"" argument
    with ""hash_key"" argument
    with ""attribute"" argument block
        with ""name"" argument
        with ""type"" argument
with two ""aws_lambda_function"" resources
    with ""function_name"" argument
    with ""role"" argument
    with ""environment"" argument block
        with ""variables"" argument block linking to the aws_s3_bucket
with two ""aws_lambda_permission"" resources
    with ""action"" argument with value ""lambda:InvokeFunction""
    with ""principal"" argument with value ""apigateway.amazonaws.com""
    with ""function_name"" argument linking to aws_lambda_function resource
    with ""source_arn"" argument linking to aws_api_gateway_rest_api (two different end points)
with two ""aws_api_gateway_integration"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource resource
    with ""http_method"" argument linking to aws_api_gateway_method resource
    with ""type"" argument with value ""AWS_PROXY""
    with ""integration_http_method"" argument with values ""GET"" and ""PUT""","I'll help you create a Terraform configuration for a cat picture service using AWS services. Here's a complete solution that includes all the necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-store""
}

resource ""aws_s3_bucket_public_access_block"" ""cat_pictures"" {
  bucket = aws_s3_bucket.cat_pictures.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# DynamoDB table to store picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures"" {
  name           = ""cat-pictures""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  attribute {
    name = ""id""
    type = ""S""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for the Lambda role
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket"",
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*"",
          aws_dynamodb_table.cat_pictures.arn
        ]
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_pictures"" {
  filename      = ""lambda_function.zip""
  function_name = ""cat_pictures_handler""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME    = aws_s3_bucket.cat_pictures.id
      DYNAMODB_TABLE = aws_dynamodb_table.cat_pictures.name
    }
  }
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name        = ""cat-pictures-api""
  description = ""API for cat pictures service""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway methods
resource ""aws_api_gateway_method"" ""get_cats"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""post_cats"" {",,,,,,"I'll help you create a Terraform configuration for a cat picture service using AWS services. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-bucket""
}

# S3 bucket policy to allow Lambda access
resource ""aws_s3_bucket_policy"" ""allow_lambda_access"" {
  bucket = aws_s3_bucket.cat_pictures.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          AWS = aws_iam_role.lambda_role.arn
        }
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*""
        ]
      }
    ]
  })
}

# DynamoDB table for storing picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures_metadata"" {
  name           = ""cat-pictures-metadata""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""picture_id""

  attribute {
    name = ""picture_id""
    type = ""S""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan"",
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_dynamodb_table.cat_pictures_metadata.arn,
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*""
        ]
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_pictures_lambda"" {
  filename         = ""lambda_function.zip""
  function_name    = ""cat_pictures_handler""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      S3_BUCKET     = aws_s3_bucket.cat_pictures.id
      DYNAMODB_TABLE = aws_dynamodb_table.cat_pictures_metadata.name
    }
  }
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat-pictures-api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id",,,,,,"I'll help you create a Terraform configuration for a cat picture service using AWS services. This will include an S3 bucket for storage, DynamoDB for metadata, Lambda functions for processing, and API Gateway for the interface.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 Bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-storage-${random_string.suffix.result}""
}

resource ""random_string"" ""suffix"" {
  length  = 8
  special = false
  upper   = false
}

# DynamoDB table for metadata
resource ""aws_dynamodb_table"" ""cat_metadata"" {
  name           = ""cat-pictures-metadata""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  attribute {
    name = ""id""
    type = ""S""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket"",
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*"",
          aws_dynamodb_table.cat_metadata.arn
        ]
      }
    ]
  })
}

# Lambda function for getting random cat picture
resource ""aws_lambda_function"" ""get_cat"" {
  filename         = ""get_cat.zip""  # You'll need to provide this
  function_name    = ""get-random-cat""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""
}

# Lambda function for uploading cat picture
resource ""aws_lambda_function"" ""upload_cat"" {
  filename         = ""upload_cat.zip""  # You'll need to provide this
  function_name    = ""upload-cat""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name        = ""cat-pictures-api""
  description = ""API for cat pictures service""
}

# API Gateway resource for /cats
resource ""aws_api_gateway_resource"" ""cats"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# GET method for random cat
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cats",,,,,
"aws_codebuild_project, aws_iam_role, aws_s3_bucket, aws_s3_bucket",create a AWS codebuild project resource with example iam role and secondary artifiacts,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for aws_iam_role resource
has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""codebuild.amazonaws.com"") 
} 

has_valid_codebuild {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_codebuild_project""
        resource.values.artifacts
    resource.values.environment
    resource.values.name
    resource.values.source[_].type
    resource.values.secondary_artifacts[_].type
    role := input.configuration.root_module.resources[i]
    role.expressions.service_role
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    
}


has_valid_resources {
        has_valid_iam_role(input.resource_changes)
    has_valid_codebuild
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""test_role8"" {
  name = ""test_role8""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Sid    = """"
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      },
    ]
  })
}

resource ""aws_s3_bucket"" ""apriltwentyninth"" {
  bucket = ""apriltwentyninth""
}

resource ""aws_s3_bucket"" ""apriltwentyninth2"" {
  bucket = ""apriltwentyninth2""
}


resource ""aws_codebuild_project"" ""example7"" {
  name          = ""test-project8""
  service_role  = aws_iam_role.test_role8.arn

  artifacts {
    location  = aws_s3_bucket.apriltwentyninth.bucket
    type      = ""S3""
    name     = ""results.zip""
    path      = ""/""
    packaging = ""ZIP""
  }

  secondary_artifacts {
    artifact_identifier =  ""SecondaryArtifact""
    type = ""S3""
    location  = aws_s3_bucket.apriltwentyninth2.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:7.0-24.10.29""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  source_version = ""master""

  secondary_sources {
    source_identifier = ""source2""
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  secondary_source_version {
    source_identifier = ""source2""
    source_version = ""master""
  }
}","Has one ""aws_iam_role"" resource, at least one ""aws_s3_bucket"" resource, and one ""aws_codebuild_project"" resource with a ""secondary_artifacts"" block","I'll help you create a Terraform configuration for an AWS CodeBuild project with an IAM role and secondary artifacts. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-project-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""codebuild-project-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
        Resource = [
          ""arn:aws:s3:::*""
        ]
      },
      {
        Effect = ""Allow""
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = [""*""]
      }
    ]
  })
}

# S3 bucket for artifacts
resource ""aws_s3_bucket"" ""artifacts"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""project"" {
  name          = ""my-codebuild-project""
  description   = ""My CodeBuild project""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.artifacts.bucket
  }

  secondary_artifacts {
    type                = ""S3""
    location            = ""${aws_s3_bucket.artifacts.bucket}/secondary""
    artifact_identifier = ""secondary_artifact""
  }

  cache {
    type = ""NO_CACHE""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/my-repo/my-project.git""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""codebuild-project-log-group""
      stream_name = ""codebuild-project-log-stream""
      status      = ""ENABLED""
    }

    s3_logs {
      status   = ""ENABLED""
      location = ""${aws_s3_bucket.artifacts.bucket}/build-logs/""
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration includes:

1. An IAM role and policy for CodeBuild with necessary permissions
2. An S3 bucket for storing artifacts
3. A CodeBuild project with:
   - Primary artifacts stored in S3
   - Secondary artifacts configuration
   - GitHub source configuration
   - Environment settings
   - Logging configuration
   - Cache",,,,,,"I'll help you create an AWS CodeBuild project with an IAM role and secondary artifacts. Here's a comprehensive Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-service-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""codebuild-service-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:*"",
          ""codecommit:*""
        ]
      }
    ]
  })
}

# S3 Bucket for artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""example"" {
  name          = ""example-project""
  description   = ""Example CodeBuild project""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  secondary_artifacts {
    artifact_identifier = ""secondary_artifact1""
    type               = ""S3""
    location           = aws_s3_bucket.artifact_bucket.id
    path              = ""secondary1/""
    packaging         = ""ZIP""
    namespace_type    = ""BUILD_ID""
  }

  secondary_artifacts {
    artifact_identifier = ""secondary_artifact2""
    type               = ""S3""
    location           = aws_s3_bucket.artifact_bucket.id
    path              = ""secondary2/""
    packaging         = ""ZIP""
    namespace_type    = ""BUILD_ID""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""ENVIRONMENT""
      value = ""development""
    }
  }

  source {
    type            = ""CODECOMMIT""
    location        = ""https://git-codecommit.region.amazonaws.com/v1/repos/example""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""example-codebuild-log-group""
      stream_name = ""example-codebuild-log-stream""
      status      = ""ENABLED""
    }

    s3_logs {
      status   = ""ENABLED""
      location = ""${aws_s3_bucket.artifact_bucket.id}/build-logs/""
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration includes:

1. An IAM role and policy for CodeBuild with basic permissions
2. An S3 bucket for storing artifacts
3. A CodeBuild project with:
   - Two secondary artifacts configurations
   - Environment settings
   - Source configuration (using CodeComm",,,,,,"I'll help you create an AWS CodeBuild project with an IAM role and secondary artifacts. Here's a complete Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-project-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""codebuild-project-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
      }
    ]
  })
}

# S3 Bucket for artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""project"" {
  name           = ""example-project""
  description    = ""Example CodeBuild project""
  build_timeout  = ""60""
  service_role   = aws_iam_role.codebuild_role.arn
  queued_timeout = 480

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.artifact_bucket.bucket
  }

  secondary_artifacts {
    type                = ""S3""
    location            = ""${aws_s3_bucket.artifact_bucket.bucket}/secondary""
    artifact_identifier = ""secondary_artifact""
    encryption_disabled = false
    name               = ""secondary_artifact_name""
    namespace_type     = ""BUILD_ID""
    packaging          = ""ZIP""
  }

  cache {
    type = ""NO_CACHE""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""example-project-log-group""
      stream_name = ""example-project-log-stream""
      status      = ""ENABLED""
    }

    s3_logs {
      status   = ""ENABLED""
      location = ""${aws_s3_bucket.artifact_bucket.bucket}/build-logs/""
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration includes:

1. Required AWS provider configuration
2. An IAM role and policy for CodeBuild with basic permissions
3. An S3 bucket for artifacts
4. A CodeBuild project with:
   - Primary artifacts configuration
   - Secondary artifacts configuration
   - Environment settings
   - Source configuration (GitHub in this example)
   - Logs configuration
   - Cache settings

The secondary artifacts are configured to:
- Store in a specific S3",,,,,
"aws_db_instance, aws_db_subnet_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc","sets up an AWS environment with a main VPC, two subnets across different availability zones, and a security group to allow database access. It also configures a 20 GB 15.3 PostgresSQL RDS instance with public accessibility, linked to the defined subnets and security group, outputting the database endpoint for connectivity.","package terraform.validation

default is_valid_db_instance = false

default is_valid_vpc = false

default is_valid_subnet = false

default is_valid_security_group = false

default is_valid_db_subnet_group = false

# Validate aws_db_instance resource
is_valid_db_instance {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_instance""
        resource.expressions.allocated_storage.constant_value == 20
        resource.expressions.engine.constant_value == ""postgres""
        resource.expressions.engine_version.constant_value == ""15.3""
        resource.expressions.instance_class.constant_value != null
        resource.expressions.password.constant_value != null
        resource.expressions.username.constant_value != null
        resource.expressions.publicly_accessible.constant_value == true
        resource.expressions.db_subnet_group_name.references != null
        resource.expressions.vpc_security_group_ids.references != null
}

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# Validate aws_subnet resource (more than one)
is_valid_subnet {
        count(subnets) > 1
}

subnets[resource] {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
}

# Validate aws_security_group resource
is_valid_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""
        # Additional checks for specific ingress/egress rules can be added here
}

# Validate aws_db_subnet_group resource
is_valid_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}
",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type           = ""gp2""
  engine                 = ""postgres""
  engine_version         = ""17.2""
  instance_class         = ""db.t4g.micro""
  skip_final_snapshot    = true
  db_name                = ""mydbname""
  username               = ""username""
  password               = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name   = aws_db_subnet_group.main.id
  publicly_accessible    = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""
    with one publicly_accessible = true

has one aws_vpc

has more than one aws_subnet

has one security_group

has one db_subnet_group","I'll help you create a Terraform configuration that sets up an AWS environment with VPC, subnets, security group, and a PostgreSQL RDS instance. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""subnet-2""
  }
}

# Security Group
resource ""aws_security_group"" ""db_sg"" {
  name        = ""database-sg""
  description = ""Security group for PostgreSQL database""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""database-sg""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""main-db-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgresql-db""
  engine              = ""postgres""
  engine_version      = ""15.3""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""your-secure-password""
  
  publicly_accessible = true
  
  db_subnet_group_name   = aws_db_subnet_group.db_subnet_group.name
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  
  skip_final_snapshot = true

  tags = {
    Name = ""postgresql-db""
  }
}

# Output
output ""db_endpoint"" {
  description = ""The endpoint of the database""
  value       = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with CIDR block 10.0.0.0/16
2. Creates two subnets in different availability zones
3. Configures a security group allowing PostgreSQL access (port 5432)
4. Creates a DB subnet group with both subnets
5. Deploys a PostgreSQL 15.3 RDS instance with:
   - 20GB storage
   - Public accessibility
   - Connected to the created VPC, subnets, and security group
6. Outputs the database endpoint for connection

Note: Remember to replace ""your-secure-password"" with a strong password in a production environment, preferably using variables or secrets management.",,,,,,"I'll help you create a Terraform configuration that sets up an AWS environment with VPC, subnets, security group, and a PostgreSQL RDS instance. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""subnet-2""
  }
}

# Security Group
resource ""aws_security_group"" ""db_sg"" {
  name        = ""database-sg""
  description = ""Security group for PostgreSQL database""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""database-sg""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""main-db-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgres-db""
  engine              = ""postgres""
  engine_version      = ""15.3""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""yourpasswordhere""
  
  publicly_accessible = true
  
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  db_subnet_group_name   = aws_db_subnet_group.db_subnet_group.name
  
  skip_final_snapshot = true

  tags = {
    Name = ""postgres-db""
  }
}

# Output
output ""db_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with CIDR block 10.0.0.0/16
2. Creates two subnets in different availability zones
3. Configures a security group allowing PostgreSQL access (port 5432)
4. Creates a DB subnet group with both subnets
5. Deploys a PostgreSQL 15.3 RDS instance with:
   - 20GB storage
   - Public accessibility
   - Connected to the created security group and subnet group
6. Outputs the database endpoint for connection

Remember to:
- Replace ""yourpasswordhere"" with a secure password
- Adjust the region if needed
- Consider your security requirements before allowing public access (0.0.0.0/0)
- Review the instance class base",,,,,,"I'll help you create a Terraform configuration that sets up an AWS environment with VPC, subnets, security group, and a PostgreSQL RDS instance. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""subnet-2""
  }
}

# Security Group
resource ""aws_security_group"" ""db_sg"" {
  name        = ""database-sg""
  description = ""Security group for PostgreSQL database""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""main-db-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""Main DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgresql-db""
  engine              = ""postgres""
  engine_version      = ""15.3""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  username            = ""dbadmin""
  password            = ""your-secure-password""
  publicly_accessible = true

  db_subnet_group_name   = aws_db_subnet_group.db_subnet_group.name
  vpc_security_group_ids = [aws_security_group.db_sg.id]

  skip_final_snapshot = true

  tags = {
    Name = ""PostgreSQL-DB""
  }
}

# Output the database endpoint
output ""db_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with CIDR block 10.0.0.0/16
2. Creates two subnets in different availability zones
3. Creates a security group allowing PostgreSQL access (port 5432)
4. Sets up a DB subnet group with both subnets
5. Creates a PostgreSQL 15.3 RDS instance with:
   - 20GB storage
   - Public accessibility
   - Connected to the created subnets and security group
6. Outputs the database endpoint for connection

Remember to:
- Replace ""your-secure-password"" with a strong password
- Consider your security requirements before allowing public access (0.0.0.0/0)
- Adjust the region and availability zones as needed
- Modify the instance class based on your performance requirements",,,,,
"aws_rds_cluster, aws_vpc","Create an mySQL cluster with 100 GB allocated to us-east-1a, us-east-1b, us-east-1c.","package main

import future.keywords.in

default allow = false

# Required AZs
required_azs := [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]

# Check for RDS cluster creation
rds_cluster_created(resources) {
some resource in resources
resource.type == ""aws_rds_cluster""
resource.change.actions[_] == ""create""
}

# Check if engine is MySQL
rds_engine_mysql(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.engine == ""mysql""
}

# Check if AZs are correct and storage is 100GB
azs_and_storage_valid(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.availability_zones == required_azs
resource.change.after.allocated_storage == 100
}

# Aggregate checks for RDS cluster
allow {
rds_cluster_created(input.resource_changes)
some resource in input.resource_changes
rds_engine_mysql(resource)
azs_and_storage_valid(resource)
}",3,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

# Configure the AWS Provider
provider ""aws"" {
region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
cidr_block = ""192.168.0.0/22""
}

resource ""aws_rds_cluster"" ""example"" {
cluster_identifier = ""example""
availability_zones = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
engine = ""mysql""
db_cluster_instance_class = ""db.r6gd.large""
storage_type = ""io1""
allocated_storage = 100
iops = 1000
master_username = ""test""
master_password = ""mustbeeightcharaters""
skip_final_snapshot = true
}
","Has one ""aws_rds_cluster"" resource
with one ""allocated_storage"" = 100
with one ""availability_zones"" = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
with one ""engine"" = ""mysql""","I'll help you create a Terraform configuration for an Aurora MySQL cluster with storage allocated across multiple availability zones in us-east-1. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier     = ""mysql-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.11.2""
  database_name         = ""mydb""
  master_username       = ""admin""
  master_password       = ""your-secure-password-here""
  skip_final_snapshot   = true
  
  availability_zones = [
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ]
  
  storage_encrypted = true
}

resource ""aws_rds_cluster_instance"" ""cluster_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r5.large""
  engine              = aws_rds_cluster.mysql_cluster.engine
  engine_version      = aws_rds_cluster.mysql_cluster.engine_version
  
  availability_zone   = element([""us-east-1a"", ""us-east-1b"", ""us-east-1c""], count.index)
  
  allocated_storage   = 100
}

resource ""aws_db_subnet_group"" ""mysql_subnet_group"" {
  name       = ""mysql-subnet-group""
  subnet_ids = [""subnet-xxxxxxxx"", ""subnet-yyyyyyyy"", ""subnet-zzzzzzzz""] # Replace with your subnet IDs
}
```

This configuration:
1. Creates an Aurora MySQL cluster across three availability zones
2. Allocates 100GB of storage per instance
3. Creates three instances, one in each specified availability zone (us-east-1a, us-east-1b, us-east-1c)
4. Includes encryption at rest
5. Sets up a subnet group for the cluster

Note: You'll need to:
- Replace the master password with a secure password
- Replace the subnet IDs with your actual subnet IDs
- Adjust the instance class if needed
- Consider adding security groups and other network configurations based on your requirements

Make sure you have the appropriate VPC, subnets, and network access configured before deploying this cluster.",,,,,,"I'll help you create a Terraform configuration for an Aurora MySQL cluster with storage allocated across three availability zones in us-east-1. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier     = ""my-mysql-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.10.2""
  database_name         = ""mydb""
  master_username       = ""admin""
  master_password       = ""your-secure-password-here""
  storage_encrypted     = true
  
  availability_zones = [
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ]

  backup_retention_period = 7
  preferred_backup_window = ""03:00-04:00""
  skip_final_snapshot     = true
}

resource ""aws_rds_cluster_instance"" ""cluster_instances"" {
  count               = 3
  identifier          = ""my-mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r5.large""
  engine              = aws_rds_cluster.mysql_cluster.engine
  engine_version      = aws_rds_cluster.mysql_cluster.engine_version
  availability_zone   = element([""us-east-1a"", ""us-east-1b"", ""us-east-1c""], count.index)

  allocated_storage   = 100
}

resource ""aws_db_subnet_group"" ""mysql_subnet_group"" {
  name       = ""mysql-subnet-group""
  subnet_ids = [""subnet-xxxxx"", ""subnet-yyyyy"", ""subnet-zzzzz""] # Replace with your subnet IDs
}
```

This configuration creates:

1. An Aurora MySQL cluster spread across three availability zones
2. Three DB instances (one in each AZ) with 100GB storage each
3. A subnet group for the cluster
4. Necessary provider configuration

Key features:
- Storage encryption enabled
- 7-day backup retention
- Instances distributed across us-east-1a, us-east-1b, and us-east-1c
- Each instance allocated with 100GB storage
- Using Aurora MySQL 5.7 compatible version

Note: You'll need to:
1. Replace the master password with a secure password
2. Replace the subnet IDs with your actual subnet IDs
3. Adjust the instance class if needed
4. Consider adding security groups and other network configurations based on your requirements",,,,,,"I'll help you create a Terraform configuration for an Amazon RDS MySQL cluster with multi-availability zone deployment in us-east-1a, us-east-1b, and us-east-1c with 100GB storage for each instance.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_subnet_group"" ""mysql_subnet_group"" {
  name       = ""mysql-subnet-group""
  subnet_ids = [""subnet-xxxxx"", ""subnet-yyyyy"", ""subnet-zzzzz""] # Replace with your subnet IDs

  tags = {
    Name = ""MySQL DB subnet group""
  }
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier     = ""mysql-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.10.2""
  database_name         = ""mydb""
  master_username       = ""admin""
  master_password       = ""your-secure-password""
  storage_encrypted     = true
  skip_final_snapshot   = true
  availability_zones    = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
  db_subnet_group_name  = aws_db_subnet_group.mysql_subnet_group.name

  tags = {
    Name = ""mysql-cluster""
  }
}

resource ""aws_rds_cluster_instance"" ""mysql_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r5.large""
  engine              = ""aurora-mysql""
  engine_version      = ""5.7.mysql_aurora.2.10.2""
  publicly_accessible = false
  
  availability_zone = element([""us-east-1a"", ""us-east-1b"", ""us-east-1c""], count.index)

  tags = {
    Name = ""mysql-instance-${count.index}""
  }
}

resource ""aws_security_group"" ""mysql_sg"" {
  name        = ""mysql-security-group""
  description = ""Security group for MySQL cluster""
  vpc_id      = ""vpc-xxxxx"" # Replace with your VPC ID

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""] # Replace with your CIDR block
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""mysql-security-group""
  }
}
```

This configuration:
1. Creates a DB subnet group spanning the three availability zones
2. Sets up an Aurora MySQL cluster with 100GB allocated storage (Aurora storage automatically scales)
3. Creates three DB instances, one in each availability zone
4. Sets up necessary security group for MySQL access
5. Uses encryption for enhanced security

Note: You'll need to:
1. Replace the subnet IDs with your actual subnet IDs
2. Replace the VPC ID with your actual VPC ID
3. Update the CIDR blocks in the security group to match your network
4. Change the master password to a secure password
5. Adjust the instance class if needed based on your performance requirements

The storage in Aurora MySQL is automatically managed and will scale up to 128TB as needed, starting with your initial 100GB allocation.",,,,,
"aws_cloudwatch_event_rule, aws_cloudwatch_event_target, aws_iam_role, aws_lambda_function, aws_lambda_permission, archive_file, aws_iam_policy_document",Create a Lambda function and invoke it every 15 minutes,"package main

import future.keywords.in

default allow = false

# Check if AWS Lambda function is being created
aws_lambda_function_created(resources) {
    some resource in resources
    resource.type == ""aws_lambda_function""
}

# Check if CloudWatch Event Rule is set to invoke Lambda every 15 minutes
cloudwatch_event_rule_for_lambda_valid(resources) {
    some resource in resources
    resource.type == ""aws_cloudwatch_event_rule""
    resource.change.after.schedule_expression == ""rate(15 minutes)""
}

# Aggregate all checks
allow {
    aws_lambda_function_created(input.resource_changes)
    cloudwatch_event_rule_for_lambda_valid(input.resource_changes)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_cloudwatch_event_rule"" ""lambda_schedule"" {
  name        = ""lambda-schedule-rule""
  description = ""Invoke Lambda function every 15 minutes""
  schedule_expression = ""rate(15 minutes)""
  role_arn = aws_iam_role.cron.arn
}

# Create a CloudWatch Events Target to associate with the Lambda function
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule = aws_cloudwatch_event_rule.lambda_schedule.name
  arn = aws_lambda_function.cron.arn
}

data ""archive_file"" ""lambda-func"" {
  type        = ""zip""
  source_file = ""./supplement/lambda_func.py""
  output_path = ""./supplement/lambda_func.zip""
}

resource ""aws_lambda_function"" ""cron"" {
  function_name = ""cron-lambda-function""
  role          = aws_iam_role.cron.arn
  filename      = data.archive_file.lambda-func.output_path
  source_code_hash = data.archive_file.lambda-func.output_base64sha256
  handler       = ""lambda_func.handler""
  runtime       = ""python3.12""
}

resource ""aws_lambda_permission"" ""cron"" {
  function_name = aws_lambda_function.cron.function_name
  action        = ""lambda:InvokeFunction""
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.lambda_schedule.arn
}

data ""aws_iam_policy_document"" ""cron_assume_role"" {
  statement {
    actions = [""sts:AssumeRole""]
    principals {
      type = ""Service""
      identifiers = [
        ""lambda.amazonaws.com"",
        ""events.amazonaws.com""
      ]
    }
  }
}

resource ""aws_iam_role"" ""cron"" {
  name               = ""cron_assume_role""
  assume_role_policy = data.aws_iam_policy_document.cron_assume_role.json
}

resource ""aws_iam_role_policy_attachment"" ""cron"" {
  role       = aws_iam_role.cron.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}
",Needs to have an aws_lambda_function defined. Create a aws_cloudwatch_event_rule to trigger every 15 minutes and aws_lambda_permission to have permission to run lambda function. Create a target to trigger the lambda function.,"I'll help you create a Terraform configuration for an AWS Lambda function that gets invoked every 15 minutes using CloudWatch Events (EventBridge). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""scheduled_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Basic Lambda execution policy
resource ""aws_iam_role_policy_attachment"" ""lambda_policy"" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}

# Lambda function
resource ""aws_lambda_function"" ""scheduled_lambda"" {
  filename         = ""lambda_function.zip""  # Your Lambda function code
  function_name    = ""scheduled_function""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""  # Change to your desired runtime

  environment {
    variables = {
      EXAMPLE_VAR = ""example_value""
    }
  }
}

# CloudWatch Event Rule (runs every 15 minutes)
resource ""aws_cloudwatch_event_rule"" ""schedule"" {
  name                = ""every-fifteen-minutes""
  description         = ""Triggers every 15 minutes""
  schedule_expression = ""rate(15 minutes)""
}

# CloudWatch Event Target
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule      = aws_cloudwatch_event_rule.schedule.name
  target_id = ""SendToLambda""
  arn       = aws_lambda_function.scheduled_lambda.arn
}

# Lambda permission to allow CloudWatch Events to invoke the function
resource ""aws_lambda_permission"" ""allow_cloudwatch"" {
  statement_id  = ""AllowExecutionFromCloudWatch""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.scheduled_lambda.function_name
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.schedule.arn
}
```

This configuration:
1. Creates an IAM role for the Lambda function
2. Attaches the basic Lambda execution policy to the role
3. Creates a Lambda function (note: you need to provide the actual function code in a ZIP file)
4. Creates a CloudWatch Event Rule that triggers every 15 minutes
5. Sets up the Lambda function as a target for the CloudWatch Event
6. Adds the necessary permission for CloudWatch Events to invoke the Lambda function

Remember to:
- Replace the region with your desired AWS region
- Provide your Lambda function code in a ZIP file named ""lambda_function.zip""
- Adjust the runtime if you're using a different programming language
- Modify the environment variables as needed for your specific use case",,,,,,"I'll help you create a Terraform configuration for an AWS Lambda function that gets invoked every 15 minutes using CloudWatch Events (EventBridge). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""scheduled_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Basic Lambda execution policy
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Lambda function
resource ""aws_lambda_function"" ""scheduled_function"" {
  filename      = ""lambda_function.zip""  # Make sure this file exists
  function_name = ""scheduled_lambda""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""  # Change according to your needs

  environment {
    variables = {
      EXAMPLE_VAR = ""example_value""
    }
  }
}

# CloudWatch Event Rule (runs every 15 minutes)
resource ""aws_cloudwatch_event_rule"" ""every_fifteen_minutes"" {
  name                = ""every-fifteen-minutes""
  description         = ""Fires every fifteen minutes""
  schedule_expression = ""rate(15 minutes)""
}

# CloudWatch Event Target
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule      = aws_cloudwatch_event_rule.every_fifteen_minutes.name
  target_id = ""SendToLambda""
  arn       = aws_lambda_function.scheduled_function.arn
}

# Lambda permission to allow CloudWatch Events to invoke the function
resource ""aws_lambda_permission"" ""allow_cloudwatch"" {
  statement_id  = ""AllowExecutionFromCloudWatch""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.scheduled_function.function_name
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.every_fifteen_minutes.arn
}
```

This configuration:
1. Creates an IAM role for the Lambda function
2. Attaches basic Lambda execution permissions to the role
3. Creates a Lambda function (note: you need to provide the actual Lambda function code in a ZIP file)
4. Creates a CloudWatch Event Rule that triggers every 15 minutes
5. Sets up the Lambda function as a target for the CloudWatch Event
6. Adds necessary permissions for CloudWatch Events to invoke the Lambda function

Remember to:
- Replace the region with your desired AWS region
- Provide the actual Lambda function code in a ZIP file named ""lambda_function.zip""
- Adjust the runtime if you're using a different programming language
- Modify the environment variables according to your needs

The Lambda function will be automatically invoked every 15 minutes after applying this configuration.",,,,,,"I'll help you create a Terraform configuration for an AWS Lambda function that gets invoked every 15 minutes using CloudWatch Events (EventBridge). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""scheduled_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Basic Lambda execution policy
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Lambda function
resource ""aws_lambda_function"" ""scheduled_lambda"" {
  filename         = ""lambda_function.zip""  # Make sure this file exists
  function_name    = ""scheduled_function""
  role             = aws_iam_role.lambda_role.arn
  handler          = ""index.handler""
  runtime          = ""nodejs14.x""  # Change as needed

  environment {
    variables = {
      EXAMPLE_VAR = ""example_value""
    }
  }
}

# CloudWatch Event Rule
resource ""aws_cloudwatch_event_rule"" ""schedule"" {
  name                = ""every-15-minutes""
  description         = ""Trigger Lambda function every 15 minutes""
  schedule_expression = ""rate(15 minutes)""
}

# CloudWatch Event Target
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule      = aws_cloudwatch_event_rule.schedule.name
  target_id = ""SendToLambda""
  arn       = aws_lambda_function.scheduled_lambda.arn
}

# Lambda permission to allow CloudWatch Events to invoke the function
resource ""aws_lambda_permission"" ""allow_cloudwatch"" {
  statement_id  = ""AllowExecutionFromCloudWatch""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.scheduled_lambda.function_name
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.schedule.arn
}
```

This configuration:
1. Creates an IAM role for the Lambda function
2. Attaches basic Lambda execution policy to the role
3. Creates a Lambda function (note: you need to provide the actual Lambda function code in a ZIP file)
4. Creates a CloudWatch Event Rule that triggers every 15 minutes
5. Creates a CloudWatch Event Target that points to the Lambda function
6. Adds necessary permissions for CloudWatch Events to invoke the Lambda function

Remember to:
- Replace the region with your desired AWS region
- Provide the actual Lambda function code in a ZIP file named ""lambda_function.zip""
- Adjust the runtime if you're using a different programming language
- Modify the environment variables as needed for your specific use case",,,,,
"aws_dax_cluster, aws_iam_role",Create a cluster with dax.r4.large and replication factor = 1.,"package main

import future.keywords.in

default allow = false

# Check if the AWS DynamoDB table is being created in us-east-1
aws_dax_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_dax_cluster""
    resource.change.after.node_type == ""dax.r4.large""
    resource.change.after.replication_factor == 1
}

# Aggregate all checks
allow {
    aws_dax_cluster_valid(input.resource_changes)
}",3,"resource ""aws_iam_role"" ""example"" {
  name = ""redshift_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""redshift.amazonaws.com""
        },
      },
    ],
  })
}

resource ""aws_dax_cluster"" ""bar"" {
  cluster_name       = ""cluster-example""
  iam_role_arn       = aws_iam_role.example.arn
  node_type          = ""dax.r4.large""
  replication_factor = 1
}","require a cluster name, iam_role_arn, node_type, replication factor","I'll help you create a DAX cluster using Terraform with the specified instance type (dax.r4.large) and replication factor of 1.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""dax-example-cluster""
  node_type         = ""dax.r4.large""
  replication_factor = 1

  iam_role_arn      = aws_iam_role.dax_role.arn
  subnet_group_name = aws_dax_subnet_group.example.name
  security_group_ids = [aws_security_group.dax.id]

  server_side_encryption {
    enabled = true
  }

  tags = {
    Environment = ""production""
  }
}

resource ""aws_dax_subnet_group"" ""example"" {
  name       = ""dax-subnet-group""
  subnet_ids = [aws_subnet.example.id]  # Replace with your subnet IDs
}

resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""dax_policy"" {
  role       = aws_iam_role.dax_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSDAXFullAccess""
}

resource ""aws_security_group"" ""dax"" {
  name        = ""dax-security-group""
  description = ""Security group for DAX cluster""
  vpc_id      = aws_vpc.example.id  # Replace with your VPC ID

  ingress {
    from_port   = 8111
    to_port     = 8111
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]  # Modify according to your security requirements
  }
}
```

This configuration creates:
1. A DAX cluster with dax.r4.large instance type and replication factor of 1
2. Required subnet group for the DAX cluster
3. IAM role with necessary permissions
4. Security group for the cluster

Make sure to:
1. Replace the region with your desired AWS region
2. Update the subnet_ids with your actual subnet IDs
3. Update the vpc_id with your actual VPC ID
4. Adjust the security group ingress rules according to your security requirements
5. Modify the tags as needed

The cluster will be created with server-side encryption enabled for additional security.",,,,,,"I'll help you create a Terraform configuration for an AWS DAX cluster using dax.r4.large node type and a replication factor of 1. Here's the code:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""dax-example-cluster""
  node_type         = ""dax.r4.large""
  replication_factor = 1

  # IAM role for DAX cluster
  iam_role_arn      = aws_iam_role.dax_role.arn

  # Basic cluster settings
  maintenance_window = ""sun:05:00-sun:09:00""
  port              = 8111

  # Network configuration
  subnet_group_name = aws_dax_subnet_group.example.name
  security_group_ids = [aws_security_group.dax.id]

  # Tags
  tags = {
    Environment = ""production""
    Name        = ""dax-cluster""
  }
}

# Required IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax-cluster-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Attach the required DAX policy to the role
resource ""aws_iam_role_policy_attachment"" ""dax_policy"" {
  role       = aws_iam_role.dax_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSDAXServiceRole""
}

# DAX subnet group
resource ""aws_dax_subnet_group"" ""example"" {
  name       = ""dax-subnet-group""
  subnet_ids = [""subnet-xxxxxxxx"", ""subnet-yyyyyyyy""] # Replace with your subnet IDs
}

# Security group for DAX
resource ""aws_security_group"" ""dax"" {
  name        = ""dax-security-group""
  description = ""Security group for DAX cluster""
  vpc_id      = ""vpc-xxxxxxxx"" # Replace with your VPC ID

  ingress {
    from_port   = 8111
    to_port     = 8111
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""] # Adjust CIDR blocks as needed
  }
}
```

This configuration creates:
1. A DAX cluster with dax.r4.large node type and replication factor of 1
2. The necessary IAM role and policy attachment for the DAX cluster
3. A subnet group for the DAX cluster
4. A security group with basic ingress rules

Remember to:
- Replace the subnet IDs with your actual subnet IDs
- Replace the VPC ID with your actual VPC ID
- Adjust the CIDR blocks in the security group as per your network requirements
- Modify the maintenance window if needed
- Adjust the tags according to your naming convention

The cluster will be created with minimal configuration while maintaining security best practices. The node type is set to dax.r4.large as requested, and the replication factor is set to 1.",,,,,,"I'll help you create a DAX cluster using Terraform with the specified node type (dax.r4.large) and replication factor of 1.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust region as needed
}

resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""dax-example-cluster""
  node_type         = ""dax.r4.large""
  replication_factor = 1

  # IAM role for DAX cluster
  iam_role_arn = aws_iam_role.dax_role.arn

  # Recommended parameters
  server_side_encryption {
    enabled = true
  }

  tags = {
    Environment = ""production""
    Name        = ""dax-cluster""
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax-cluster-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Attach AWS managed policy for DAX
resource ""aws_iam_role_policy_attachment"" ""dax_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSDAXServiceRole""
  role       = aws_iam_role.dax_role.name
}
```

This configuration:
1. Creates a DAX cluster with the specified node type (dax.r4.large)
2. Sets replication factor to 1
3. Enables server-side encryption
4. Creates necessary IAM role and attaches the required AWS managed policy
5. Includes basic tagging

Make sure to:
- Adjust the region according to your needs
- Review and adjust the cluster name
- Consider adding additional parameters like maintenance_window or subnet_group_name if needed
- Verify that the node type is available in your chosen region

The configuration includes the required AWS provider block and creates all necessary components for a functional DAX cluster.",,,,,
"aws_iam_role, aws_sagemaker_domain, aws_subnet, aws_vpc, aws_iam_policy_document",Create a SageMaker Domain resource.,"package main

import future.keywords.in

default allow = false


aws_sagemaker_domain_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_domain""
}

aws_iam_role_valid(resources) {
    some resource in resources
    resource.type == ""aws_iam_role""
}

aws_vpc_valid(resources) {
    some resource in resources
    resource.type == ""aws_vpc""
}

aws_subnet_valid(resources) {
    some resource in resources
    resource.type == ""aws_subnet""
}

# Aggregate all checks
allow {
    aws_sagemaker_domain_valid(input.resource_changes)
    aws_iam_role_valid(input.resource_changes)
    aws_vpc_valid(input.resource_changes)
    aws_subnet_valid(input.resource_changes)
}
",3,"resource ""aws_vpc"" ""example"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""example"" {
  vpc_id     = aws_vpc.example.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_sagemaker_domain"" ""example"" {
  domain_name = ""example""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.example.id
  subnet_ids  = [aws_subnet.example.id]

  default_user_settings {
    execution_role = aws_iam_role.example.arn
  }
}

resource ""aws_iam_role"" ""example"" {
  name               = ""example""
  path               = ""/""
  assume_role_policy = data.aws_iam_policy_document.example.json
}

data ""aws_iam_policy_document"" ""example"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""sagemaker.amazonaws.com""]
    }
  }
}",Has a sagemaker domain resources which requires a an iam role and a subnet,"I'll help you create a Terraform configuration for an AWS SageMaker Domain resource. Here's a complete example with the required provider and necessary configurations:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_iam_role"" ""sagemaker_execution_role"" {
  name = ""sagemaker-domain-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_vpc"" ""sagemaker_vpc"" {
  cidr_block = ""10.0.0.0/16""

  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""sagemaker-vpc""
  }
}

resource ""aws_subnet"" ""sagemaker_subnet"" {
  vpc_id     = aws_vpc.sagemaker_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-subnet""
  }
}

resource ""aws_sagemaker_domain"" ""example"" {
  domain_name = ""example-sagemaker-domain""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.sagemaker_vpc.id
  subnet_ids  = [aws_subnet.sagemaker_subnet.id]

  default_user_settings {
    execution_role = aws_iam_role.sagemaker_execution_role.arn
  }

  default_space_settings {
    execution_role = aws_iam_role.sagemaker_execution_role.arn
  }

  domain_settings {
    security_group_ids = []
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration creates:
1. The required AWS provider
2. An IAM role for SageMaker execution
3. A VPC and subnet for the SageMaker domain
4. The SageMaker domain itself with basic settings

The SageMaker domain is configured with:
- IAM authentication mode
- VPC and subnet association
- Default user and space settings with execution roles
- Domain settings with security group configuration
- Basic tagging

You can customize this further by:
- Adding more subnet IDs
- Configuring additional security groups
- Modifying the authentication mode
- Adding more specific user settings
- Adjusting the domain settings

Remember to adjust the CIDR blocks, domain name, and other parameters according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS SageMaker Domain resource. This will include the required provider and necessary configurations for setting up a basic SageMaker Domain.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create VPC for SageMaker Domain
resource ""aws_vpc"" ""sagemaker_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""sagemaker-vpc""
  }
}

# Create subnet for SageMaker Domain
resource ""aws_subnet"" ""sagemaker_subnet"" {
  vpc_id     = aws_vpc.sagemaker_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-subnet""
  }
}

# Create IAM role for SageMaker
resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-domain-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Attach AmazonSageMakerFullAccess policy to the role
resource ""aws_iam_role_policy_attachment"" ""sagemaker_policy"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

# Create SageMaker Domain
resource ""aws_sagemaker_domain"" ""domain"" {
  domain_name = ""my-sagemaker-domain""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.sagemaker_vpc.id
  subnet_ids  = [aws_subnet.sagemaker_subnet.id]

  default_user_settings {
    execution_role = aws_iam_role.sagemaker_role.arn
  }

  default_space_settings {
    execution_role = aws_iam_role.sagemaker_role.arn
  }

  tags = {
    Environment = ""Development""
    Name        = ""SageMaker-Domain""
  }
}
```

This configuration creates:
1. A required AWS provider block
2. A VPC and subnet for the SageMaker Domain
3. An IAM role with the necessary permissions for SageMaker
4. A SageMaker Domain with basic settings

The domain uses IAM authentication mode and is configured with default user and space settings. The domain is created within the specified VPC and subnet, and uses the created IAM role for execution permissions.

Make sure to:
- Adjust the CIDR blocks according to your network requirements
- Modify the domain name and tags as needed
- Add additional security groups or network configurations if required
- Consider adding more specific IAM permissions based on your use case",,,,,,"I'll help you create a Terraform configuration for an AWS SageMaker Domain resource. This will include the required provider and necessary configurations for setting up a SageMaker Domain.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_iam_role"" ""sagemaker_execution_role"" {
  name = ""sagemaker-domain-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""sagemaker_policy"" {
  role       = aws_iam_role.sagemaker_execution_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

resource ""aws_vpc"" ""sagemaker_vpc"" {
  cidr_block = ""10.0.0.0/16""

  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""sagemaker-domain-vpc""
  }
}

resource ""aws_subnet"" ""sagemaker_subnet"" {
  vpc_id     = aws_vpc.sagemaker_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-domain-subnet""
  }
}

resource ""aws_sagemaker_domain"" ""domain"" {
  domain_name = ""example-sagemaker-domain""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.sagemaker_vpc.id
  subnet_ids  = [aws_subnet.sagemaker_subnet.id]

  default_user_settings {
    execution_role = aws_iam_role.sagemaker_execution_role.arn
  }

  retention_policy {
    home_efs_file_system = ""Delete""
  }

  tags = {
    Environment = ""Development""
    Project     = ""ML-Platform""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role and policy attachment for SageMaker execution
3. Creates a VPC and subnet for the SageMaker domain
4. Creates the SageMaker domain with basic settings including:
   - Domain name
   - Authentication mode (IAM)
   - VPC and subnet configurations
   - Default user settings with execution role
   - Retention policy for the EFS file system
   - Tags for resource management

The domain will be accessible through the AWS SageMaker console and can be used to launch notebook instances and other SageMaker resources.

Remember to adjust the values (like CIDR blocks, domain name, etc.) according to your specific requirements and ensure they don't conflict with existing resources in your AWS account.",,,,,
"aws_eks_cluster, aws_iam_role, aws_iam_role_policy_attachment, aws_subnet, aws_subnet, aws_vpc","Generate a Terraform configuration to create an AWS IAM role called ""demo"" with attribute name set to ""eks-cluster-demo"", for an Amazon EKS cluster. The role should allow assuming roles for EKS services and have the AmazonEKSClusterPolicy attached. Additionally, create an AWS EKS cluster with the name ""demo"" and configure its VPC with specified subnet IDs from in the us-east-1 region. Ensure dependency handling for the IAM role's policy attachment during EKS cluster management.","package terraform.analysis

default has_aws_iam_role = false
default has_aws_iam_role_policy_attachment = false
default has_aws_eks_cluster = false

find_resource(rtype, rname) = resource {
    resource = input.configuration.root_module.resources[_]
    resource.type == rtype
    resource.name == rname
}

has_aws_iam_role {
    r := find_resource(""aws_iam_role"", ""demo"")
    r.expressions.name.constant_value == ""eks-cluster-demo""
    r.expressions.assume_role_policy
}

has_aws_iam_role_policy_attachment {
    r := input.configuration.root_module.resources[_]
    r.type == ""aws_iam_role_policy_attachment""
    r.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
    r.expressions.role.references[_] == ""aws_iam_role.demo.name""
}

has_aws_eks_cluster {
    r := find_resource(""aws_eks_cluster"", ""demo"")
    r.expressions.role_arn.references[_] == ""aws_iam_role.demo.arn""
    count(r.expressions.vpc_config[_].subnet_ids.references) >= 4
    count(r.depends_on) == 1
}

valid_config {
    has_aws_iam_role
    has_aws_iam_role_policy_attachment
    has_aws_eks_cluster
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_subnet"" ""public-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
}

resource ""aws_iam_role"" ""example"" {
  name = ""eks-cluster-demo""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""eks.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_eks_cluster"" ""demo"" {
  name     = ""cluster-name""
  role_arn = aws_iam_role.example.arn

  vpc_config {
    subnet_ids = [
      aws_subnet.private-us-east-1a.id,
      aws_subnet.public-us-east-1a.id,
    ]
  }

  depends_on = [aws_iam_role_policy_attachment.eks_cluster_policy,
                aws_iam_role_policy_attachment.eks_service_policy]
}","has one ""aws_iam_role""
with one ""name""
with one ""assume_role_policy"" 

has one ""aws_iam_role_policy_attachment"" 
with one ""policy_arn""
with one ""role"" referencing the ""aws_iam_role.demo.name""

has one ""variable"" block for ""cluster_name""
with one ""default""
with one ""type""
with one ""description""
with one ""nullable""

has one ""aws_eks_cluster"" 
with one ""name""
with one ""role_arn"" referencing the ""aws_iam_role.demo.arn""
with one ""vpc_config"" containing ""subnet_ids""
with dependencies on one IAM role policy attachment","I'll help you create a Terraform configuration for an EKS cluster with the required IAM role and policy attachments. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM role for EKS cluster
resource ""aws_iam_role"" ""eks_cluster_role"" {
  name = ""demo""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""eks.amazonaws.com""
        }
      }
    ]
  })

  tags = {
    Name = ""eks-cluster-demo""
  }
}

# Attach AmazonEKSClusterPolicy to the role
resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.eks_cluster_role.name
}

# Create EKS cluster
resource ""aws_eks_cluster"" ""demo"" {
  name     = ""demo""
  role_arn = aws_iam_role.eks_cluster_role.arn

  vpc_config {
    subnet_ids = [
      ""subnet-xxxxxxxxxxxx"",  # Replace with actual subnet IDs
      ""subnet-xxxxxxxxxxxx""
    ]
  }

  # Ensure that IAM Role permissions are created before and deleted after
  # the EKS Cluster. Otherwise, EKS will not be able to properly delete
  # EKS managed EC2 infrastructure such as Security Groups.
  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy
  ]
}

output ""endpoint"" {
  value = aws_eks_cluster.demo.endpoint
}

output ""kubeconfig-certificate-authority-data"" {
  value = aws_eks_cluster.demo.certificate_authority[0].data
}
```

This configuration:
1. Sets up the AWS provider for us-east-1 region
2. Creates an IAM role named ""demo"" with the required assume role policy for EKS
3. Attaches the AmazonEKSClusterPolicy to the IAM role
4. Creates an EKS cluster named ""demo"" using the IAM role
5. Uses depends_on to ensure proper dependency handling
6. Includes useful outputs for cluster information

Note: Replace the subnet IDs in the vpc_config block with your actual subnet IDs. The subnets should be in different availability zones for high availability.

The configuration includes proper dependency handling through the depends_on block, ensuring that the IAM role and policy attachment are created before the EKS cluster and deleted after the cluster is destroyed.",,,,,,"I'll help you create a Terraform configuration for an AWS EKS cluster with the required IAM role and policy attachments. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM role for EKS cluster
resource ""aws_iam_role"" ""eks_cluster_role"" {
  name = ""demo""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""eks.amazonaws.com""
        }
      }
    ]
  })

  tags = {
    Name = ""eks-cluster-demo""
  }
}

# Attach AmazonEKSClusterPolicy to the role
resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.eks_cluster_role.name
}

# Create EKS cluster
resource ""aws_eks_cluster"" ""demo"" {
  name     = ""demo""
  role_arn = aws_iam_role.eks_cluster_role.arn

  vpc_config {
    subnet_ids = [
      ""subnet-12345678"", # Replace with actual subnet IDs
      ""subnet-87654321""  # Replace with actual subnet IDs
    ]
  }

  # Ensure that IAM Role permissions are created before and deleted after
  # the EKS Cluster. Otherwise, EKS will not be able to properly delete
  # EKS managed EC2 infrastructure such as Security Groups.
  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy
  ]
}

# Output the endpoint of the cluster
output ""endpoint"" {
  value = aws_eks_cluster.demo.endpoint
}

# Output the kubeconfig certificate authority data
output ""kubeconfig-certificate-authority-data"" {
  value = aws_eks_cluster.demo.certificate_authority[0].data
}
```

This configuration:
1. Sets up the AWS provider for us-east-1 region
2. Creates an IAM role named ""demo"" with the required assume role policy for EKS
3. Attaches the AmazonEKSClusterPolicy to the IAM role
4. Creates an EKS cluster named ""demo"" using the created IAM role
5. Configures the VPC settings with subnet IDs (you'll need to replace the example subnet IDs with your actual subnet IDs)
6. Handles dependencies properly using depends_on
7. Provides useful outputs for the cluster endpoint and certificate authority data

Make sure to:
- Replace the subnet IDs with your actual subnet IDs
- Ensure your AWS credentials are properly configured
- Verify that you have the necessary permissions to create these resources
- Consider adding additional security groups or VPC configurations as needed for your specific use case",,,,,,"I'll help you create a Terraform configuration for an AWS EKS cluster with the required IAM role and policy attachments. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM role for EKS cluster
resource ""aws_iam_role"" ""eks_cluster_role"" {
  name = ""demo""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""eks.amazonaws.com""
        }
      }
    ]
  })

  tags = {
    Name = ""eks-cluster-demo""
  }
}

# Attach AmazonEKSClusterPolicy to the role
resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.eks_cluster_role.name
}

# Create EKS cluster
resource ""aws_eks_cluster"" ""demo"" {
  name     = ""demo""
  role_arn = aws_iam_role.eks_cluster_role.arn

  vpc_config {
    subnet_ids = [
      ""subnet-12345678"",  # Replace with actual subnet IDs
      ""subnet-87654321""   # Replace with actual subnet IDs
    ]
  }

  # Ensure that IAM Role permissions are created before and deleted after EKS Cluster handling
  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy
  ]
}

output ""endpoint"" {
  value = aws_eks_cluster.demo.endpoint
}

output ""kubeconfig-certificate-authority-data"" {
  value = aws_eks_cluster.demo.certificate_authority[0].data
}
```

This configuration:
1. Creates an IAM role named ""demo"" with the necessary assume role policy for EKS
2. Attaches the AmazonEKSClusterPolicy to the IAM role
3. Creates an EKS cluster named ""demo"" in the specified VPC subnets
4. Handles dependencies properly using depends_on
5. Includes useful outputs for cluster endpoint and certificate authority data

Note: Remember to replace the subnet IDs with your actual subnet IDs from your VPC in us-east-1 region.",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer with ALB Target Group,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.values.subnet_mapping != null
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.ipv4_ipam_pool_id.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.ipv6_cider_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value != ""forward""
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    resources.values.target_type == ""alb""
    
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group""
        resource.expressions.port != null
        resource.expressions.protocol != null
        resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
        is_valid_lb
        is_valid_lb_listener
        is_valid_lb_target_group_attachment
        is_valid_lb_target_group
        is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.public_subnets[0]
  security_groups = [ aws_security_group.instance-sg.id ]
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.id
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id
    with target_type = ""alb""

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Target Group. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create ALB
resource ""aws_lb"" ""example"" {
  name               = ""example-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [""subnet-12345678"", ""subnet-87654321""]  # Replace with your subnet IDs

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
  }
}

# Create Target Group
resource ""aws_lb_target_group"" ""example"" {
  name        = ""example-target-group""
  port        = 80
  protocol    = ""HTTP""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID
  target_type = ""instance""

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.example.arn
  }
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""allow_http""
  description = ""Allow HTTP inbound traffic""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID

  ingress {
    description = ""HTTP from anywhere""
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""allow_http""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. A Target Group for the ALB
3. A Listener that forwards traffic to the Target Group
4. A Security Group for the ALB

Note: You'll need to replace the following placeholder values with your actual values:
- subnet IDs in the ALB resource
- VPC ID in the Target Group and Security Group resources
- Adjust the region in the provider block
- Modify any tags, names, or other parameters according to your needs

The ALB is configured as internet-facing (internal = false) and listens on port 80. The Target Group is set up with basic HTTP health checks, and the Security Group allows inbound HTTP traffic from anywhere.",,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Target Group. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create ALB
resource ""aws_lb"" ""main"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [""subnet-xxxxxxxx"", ""subnet-yyyyyyyy""]  # Replace with your subnet IDs

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
    Name        = ""my-alb""
  }
}

# Create Target Group
resource ""aws_lb_target_group"" ""main"" {
  name        = ""my-target-group""
  port        = 80
  protocol    = ""HTTP""
  vpc_id      = ""vpc-xxxxxxxx""  # Replace with your VPC ID
  target_type = ""instance""

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }

  tags = {
    Name = ""my-target-group""
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.main.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.main.arn
  }
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = ""vpc-xxxxxxxx""  # Replace with your VPC ID

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""alb-security-group""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. A Target Group
3. A Listener that forwards traffic to the Target Group
4. A Security Group for the ALB

Note: You'll need to replace the following placeholder values with your actual values:
- subnet IDs in the ALB resource
- VPC ID in the Target Group and Security Group resources
- Adjust the region in the provider block
- Modify the security group rules according to your needs
- Adjust health check settings in the target group if needed

The ALB is set up as an internet-facing load balancer (internal = false) and listens on port 80. The target group is configured for HTTP traffic and includes basic health check settings.",,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Target Group. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create the ALB
resource ""aws_lb"" ""example"" {
  name               = ""example-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [""subnet-12345678"", ""subnet-87654321""]  # Replace with your subnet IDs

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
    Name        = ""example-alb""
  }
}

# Create Target Group
resource ""aws_lb_target_group"" ""example"" {
  name        = ""example-target-group""
  port        = 80
  protocol    = ""HTTP""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID
  target_type = ""instance""

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }

  tags = {
    Name = ""example-target-group""
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.example.arn
  }
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""allow_http""
  description = ""Allow HTTP inbound traffic""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID

  ingress {
    description = ""HTTP from anywhere""
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""allow_http""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. A Target Group
3. A Listener that forwards traffic to the Target Group
4. A Security Group for the ALB

Key points to note:
- Replace the subnet IDs and VPC ID with your actual values
- The ALB is set up as internet-facing (internal = false)
- The Target Group is configured for HTTP traffic on port 80
- Health checks are configured with standard parameters
- The Security Group allows inbound HTTP traffic from anywhere
- The Listener is configured to forward traffic to the Target Group

Remember to adjust the configuration according to your specific requirements, such as:
- Changing the ports if you need HTTPS
- Modifying the health check parameters
- Adjusting the security group rules
- Adding additional listeners if needed",,,,,
aws_elastic_beanstalk_application,Provision a resource to deploy and scale a web application that was developed with supported programming languages.,"package terraform.validation

default is_configuration_valid = false

default is_valid_eb_app = false


is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}


is_configuration_valid {
    is_valid_eb_app
}",1,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_elastic_beanstalk_application"" ""tftest"" {
  name        = ""tf-test-name""
  description = ""tf-test-desc""
}","Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""","I'll help you create a Terraform configuration to deploy an AWS Elastic Beanstalk application, which is an ideal service for deploying and scaling web applications. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""web_app"" {
  name        = ""my-web-application""
  description = ""Web application deployed using Elastic Beanstalk""
}

# Create Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""web_app_env"" {
  name                = ""my-web-app-environment""
  application         = aws_elastic_beanstalk_application.web_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.4 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }

  setting {
    namespace = ""aws:ec2:instances""
    name      = ""InstanceTypes""
    value     = ""t3.micro""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""ENVIRONMENT""
    value     = ""production""
  }
}

# Output the environment URL
output ""environment_url"" {
  value = aws_elastic_beanstalk_environment.web_app_env.endpoint_url
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an Elastic Beanstalk application
3. Creates an Elastic Beanstalk environment with:
   - Auto-scaling configuration (min 1, max 4 instances)
   - T3.micro instance type
   - Load balanced environment
   - Node.js 18 platform (you can change this to other supported platforms)
4. Outputs the environment URL

You can customize the solution stack name based on your application's programming language (Java, Python, Ruby, Go, .NET, etc.) and adjust the auto-scaling and instance settings according to your needs.",,,,,,"I'll help you create a Terraform configuration to deploy a web application using AWS Elastic Beanstalk, which is an ideal service for deploying and scaling web applications. Here's a solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""web_app"" {
  name        = ""my-web-application""
  description = ""Web application deployed using Elastic Beanstalk""
}

# Create Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""web_app_env"" {
  name                = ""web-app-production""
  application         = aws_elastic_beanstalk_application.web_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.1 running Node.js 18""  # Example for Node.js

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""3""
  }

  setting {
    namespace = ""aws:ec2:instances""
    name      = ""InstanceTypes""
    value     = ""t3.micro""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""ENVIRONMENT""
    value     = ""production""
  }
}
```

This configuration:
1. Sets up an AWS Elastic Beanstalk application and environment
2. Configures auto-scaling with minimum 1 and maximum 3 instances
3. Uses t3.micro instances for cost-effectiveness
4. Sets up a load-balanced environment
5. Includes basic environment variables
6. Uses Node.js as an example platform (you can change the solution_stack_name for other supported languages like Java, Python, Ruby, etc.)

The environment will automatically handle deployment, capacity provisioning, load balancing, auto-scaling, and application health monitoring.",,,,,,"I'll help you create a Terraform configuration to deploy an AWS Elastic Beanstalk application and environment, which is ideal for deploying and scaling web applications. Here's a solution that includes the necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""web_app"" {
  name        = ""my-web-application""
  description = ""Web application deployed with Elastic Beanstalk""
}

# Create Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""web_app_env"" {
  name                = ""my-web-app-environment""
  application         = aws_elastic_beanstalk_application.web_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.1 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }

  setting {
    namespace = ""aws:ec2:instances""
    name      = ""InstanceTypes""
    value     = ""t3.micro""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment:process:default""
    name      = ""HealthyThresholdCount""
    value     = ""3""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment:process:default""
    name      = ""UnhealthyThresholdCount""
    value     = ""5""
  }
}

# Output the environment URL
output ""environment_url"" {
  value = aws_elastic_beanstalk_environment.web_app_env.endpoint_url
}
```

This configuration:

1. Creates an Elastic Beanstalk application
2. Sets up an environment with:
   - Auto-scaling configuration (min 1, max 4 instances)
   - Uses t3.micro instances
   - Load balanced environment
   - Health check configuration
3. Uses Node.js 18 platform (you can change the solution_stack_name for other supported languages)
4. Outputs the environment URL for easy access

The environment will automatically handle scaling, load balancing, and deployment of your web application. You can modify the settings according to your specific needs, such as instance types, scaling parameters, and platform version.",,,,,
"aws_iam_role, aws_kinesisanalyticsv2_application, aws_iam_policy_document",Generate a basic Amazon Kinesis V2 Analytics Application to manage a SQL application,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
	some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kinesisanalytics.amazonaws.com"") 
}


has_valid_kinesisanalyticsv2_application {
	some i
	resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesisanalyticsv2_application""
    resource.values.name
    resource.values.runtime_environment == ""SQL-1_0""
   	role := input.configuration.root_module.resources[i]
    role.expressions.service_execution_role
}

has_valid_resources {
	has_valid_iam_role(input.resource_changes)
    has_valid_kinesisanalyticsv2_application
}",3,"provider ""aws"" {
  region = ""us-west-2""
}

data ""aws_iam_policy_document"" ""KinesisAssume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}


resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_role""
  assume_role_policy = data.aws_iam_policy_document.KinesisAssume.json
}

resource ""aws_kinesisanalyticsv2_application"" ""flink_app"" {
  name                 = ""example-application1""
  runtime_environment  = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_role.arn
}","Has one ""aws_iam_role"" resource, and one ""aws_kinesisanalyticsv2_application"" with argument runtime_environment == ""SQL-1_0""","I'll help you create a Terraform configuration for an Amazon Kinesis Analytics V2 Application for SQL processing. I'll include the required provider and create a basic configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_kinesisanalyticsv2_application"" ""example"" {
  name                   = ""example-kinesis-analytics-sql-app""
  application_configuration {
    sql_application_configuration {
      input {
        name_prefix = ""PREFIX_""
        
        kinesis_streams_input {
          resource_arn = aws_kinesis_stream.example.arn
          role_arn     = aws_iam_role.example.arn
        }

        input_schema {
          record_format {
            record_format_type = ""JSON""
            
            mapping_parameters {
              json_mapping_parameters {
                record_row_path = ""$""
              }
            }
          }

          record_column {
            name     = ""example_column""
            sql_type = ""VARCHAR(8)""
            mapping  = ""$.example_field""
          }
        }
      }
    }
  }

  runtime_environment = ""SQL-1_0""
  service_execution_role = aws_iam_role.example.arn

  tags = {
    Environment = ""dev""
    Project     = ""example""
  }
}

# Example IAM role for Kinesis Analytics
resource ""aws_iam_role"" ""example"" {
  name = ""kinesis-analytics-example-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Example Kinesis stream as input
resource ""aws_kinesis_stream"" ""example"" {
  name             = ""example-input-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""dev""
  }
}
```

This configuration creates:
1. A Kinesis Analytics V2 application configured for SQL processing
2. A basic IAM role for the application
3. A sample Kinesis stream as input

Key features of this configuration:
- Uses SQL-1_0 runtime environment
- Configures JSON input format
- Includes basic input schema configuration
- Sets up necessary IAM roles and permissions
- Includes tags for resource management

Remember to:
1. Adjust the region in the provider block
2. Modify the IAM role permissions according to your needs
3. Adjust the input schema to match your actual data structure
4. Modify the Kinesis stream configuration based on your requirements

You may need to add additional permissions to the IAM role and configure additional settings depending on your specific use case.",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics V2 Application that manages a SQL application. Here's a basic configuration that includes the required provider and the Kinesis Analytics application:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_kinesisanalyticsv2_application"" ""example"" {
  name                   = ""example-kinesis-analytics-sql-app""
  application_configuration {
    sql_application_configuration {
      input {
        name_prefix = ""PREFIX_""
        
        kinesis_streams_input {
          resource_arn = aws_kinesis_stream.example.arn
          role_arn     = aws_iam_role.kinesis_analytics_role.arn
        }

        input_schema {
          record_format {
            record_format_type = ""JSON""
            
            mapping_parameters {
              json_mapping_parameters {
                record_row_path = ""$""
              }
            }
          }

          record_column {
            name     = ""example_column""
            sql_type = ""VARCHAR(8)""
            mapping  = ""$.example_field""
          }
        }
      }
    }
  }

  runtime_environment = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_analytics_role.arn

  tags = {
    Environment = ""Production""
    Purpose     = ""DataAnalytics""
  }
}

# Example IAM role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Example Kinesis stream as input
resource ""aws_kinesis_stream"" ""example"" {
  name             = ""example-kinesis-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""Production""
  }
}
```

This configuration includes:

1. The required AWS provider configuration
2. A Kinesis Analytics V2 Application resource configured for SQL processing
3. Basic input configuration with a Kinesis stream as the source
4. A simple input schema definition with JSON format
5. The necessary IAM role for the application
6. An example Kinesis stream as the input source

The application is configured with:
- SQL runtime environment
- JSON input format
- Basic input schema with one example column
- Tags for resource management
- Associated IAM role for execution permissions
- A linked Kinesis stream as the input source

You can customize this configuration by:
- Modifying the region
- Adjusting the input schema to match your data structure
- Adding more record columns
- Modifying the IAM role permissions
- Adjusting the Kinesis stream configuration
- Adding output configurations as needed

Remember to add appropriate IAM policies to the role based on your specific requirements and to configure any additional settings specific to your use case.",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics V2 Application that manages a SQL application. I'll include the required provider and necessary resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_kinesisanalyticsv2_application"" ""example"" {
  name                   = ""example-kinesis-analytics-sql-app""
  application_configuration {
    sql_application_configuration {
      input {
        name_prefix = ""PREFIX_""
        
        kinesis_streams_input {
          resource_arn = aws_kinesis_stream.example.arn
          
          input_schema {
            record_format {
              record_format_type = ""JSON""
              
              mapping_parameters {
                json_mapping_parameters {
                  record_row_path = ""$""
                }
              }
            }
            
            record_encoding = ""UTF-8""
            
            record_column {
              name     = ""example_column_1""
              sql_type = ""VARCHAR(8)""
              mapping  = ""$.example_field_1""
            }
            
            record_column {
              name     = ""example_column_2""
              sql_type = ""INTEGER""
              mapping  = ""$.example_field_2""
            }
          }
        }
      }
    }
  }

  cloudwatch_logging_options {
    log_stream_arn = aws_cloudwatch_log_stream.example.arn
  }

  service_execution_role = aws_iam_role.example.arn
}

# Supporting resources
resource ""aws_kinesis_stream"" ""example"" {
  name             = ""example-input-stream""
  shard_count      = 1
  retention_period = 24
}

resource ""aws_cloudwatch_log_group"" ""example"" {
  name              = ""/aws/kinesis-analytics/example-app""
  retention_in_days = 14
}

resource ""aws_cloudwatch_log_stream"" ""example"" {
  name           = ""example-stream""
  log_group_name = aws_cloudwatch_log_group.example.name
}

resource ""aws_iam_role"" ""example"" {
  name = ""kinesis-analytics-example-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""example"" {
  name = ""kinesis-analytics-example-policy""
  role = aws_iam_role.example.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kinesis:GetShardIterator"",
          ""kinesis:GetRecords"",
          ""kinesis:DescribeStream"",
          ""kinesis:ListShards""
        ]
        Resource = aws_kinesis_stream.example.arn
      },
      {
        Effect = ""Allow""
        Action = [
          ""logs:PutLogEvents""
        ]
        Resource = aws_cloudwatch_log_stream.example.arn
      }
    ]
  })
}
```

This configuration creates:
1. A Kinesis Analytics V2 Application configured for SQL processing
2. A Kinesis stream as the input source
3. CloudWatch log group and stream for logging
4. IAM role and policy for the application to access required resources

The application is configured to:
- Process JSON-formatted data
- Define two example columns with their respective mappings
- Use CloudWatch for logging
- Have appropriate IAM permissions

You can customize the configuration by:
- Modifying the region
-",,,,,
"aws_dax_cluster, aws_iam_role",Create a DAX cluster with node type = dax.r4.large and replication factor = 1.,"package main

import future.keywords.in

default allow = false

# Check if the AWS DynamoDB table is being created in us-east-1
aws_dax_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_dax_cluster""
    resource.change.after.node_type == ""dax.r4.large""
    resource.change.after.replication_factor == 1
}

# Aggregate all checks
allow {
    aws_dax_cluster_valid(input.resource_changes)
}",3,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""example"" {
  name = ""test""

  assume_role_policy = jsonencode({
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""eks.amazonaws.com""
      }
    }]
    Version = ""2012-10-17""
  })
}


resource ""aws_dax_cluster"" ""bar"" {
  cluster_name       = ""cluster-example""
  iam_role_arn       = aws_iam_role.example.arn
  node_type          = ""dax.r4.large""
  replication_factor = 1
}","require a cluster name, iam_role_arn, node_type, replication factor","I'll help you create a Terraform configuration for an AWS DAX (DynamoDB Accelerator) cluster with the specified requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

# Create IAM Role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax-cluster-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Attach DAX policy to the role
resource ""aws_iam_role_policy_attachment"" ""dax_policy"" {
  role       = aws_iam_role.dax_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSDAXFullAccess""
}

# Create DAX Subnet Group
resource ""aws_dax_subnet_group"" ""example"" {
  name       = ""dax-subnet-group""
  subnet_ids = [""subnet-xxxxxxxx""]  # Replace with your subnet IDs
}

# Create DAX Cluster
resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
  subnet_group_name = aws_dax_subnet_group.example.name

  server_side_encryption {
    enabled = true
  }

  tags = {
    Environment = ""production""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role for DAX with necessary permissions
3. Creates a DAX subnet group (you'll need to specify your subnet IDs)
4. Creates a DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - Server-side encryption enabled
   - Appropriate IAM role and subnet group associations

Note: Make sure to:
- Replace the subnet IDs with your actual subnet IDs
- Adjust the region as needed
- Modify the cluster name and other parameters according to your requirements
- Ensure your AWS account has the necessary permissions to create these resources",,,,,,"I'll help you create a Terraform configuration for an AWS DAX (DynamoDB Accelerator) cluster with the specified requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_dax_cluster"" ""dax_cluster"" {
  cluster_name       = ""dax-cluster""
  node_type         = ""dax.r4.large""
  replication_factor = 1

  iam_role_arn      = aws_iam_role.dax_role.arn

  maintenance_window = ""sun:05:00-sun:09:00""
  security_group_ids = [aws_security_group.dax_sg.id]

  server_side_encryption {
    enabled = true
  }

  tags = {
    Name = ""dax-cluster""
  }
}

resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax-service-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""dax_policy"" {
  role       = aws_iam_role.dax_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSDAXServiceRole""
}

resource ""aws_security_group"" ""dax_sg"" {
  name        = ""dax-security-group""
  description = ""Security group for DAX cluster""
  vpc_id      = ""vpc-xxxxx"" # Replace with your VPC ID

  ingress {
    from_port   = 8111
    to_port     = 8111
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""] # Replace with your VPC CIDR
  }
}
```

This configuration creates:
1. A DAX cluster with the specified node type (dax.r4.large) and replication factor (1)
2. The necessary IAM role and policy attachment for the DAX cluster
3. A security group to control access to the DAX cluster

Note: 
- Replace the VPC ID and CIDR blocks in the security group with your actual values
- The maintenance window can be adjusted according to your needs
- Server-side encryption is enabled by default for security best practices
- The configuration includes basic tags for better resource management

Make sure you have the appropriate permissions to create these resources in your AWS account.",,,,,,"I'll help you create a DAX (DynamoDB Accelerator) cluster using Terraform. Here's a configuration that creates a DAX cluster with the specified node type and replication factor:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create an IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax-service-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Attach the required policy to the IAM role
resource ""aws_iam_role_policy_attachment"" ""dax_policy"" {
  role       = aws_iam_role.dax_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSDAXServiceRole""
}

# Create a subnet group for DAX
resource ""aws_dax_subnet_group"" ""example"" {
  name       = ""dax-subnet-group""
  subnet_ids = [""subnet-12345678"", ""subnet-87654321""]  # Replace with your subnet IDs
}

# Create the DAX cluster
resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""my-dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
  subnet_group_name = aws_dax_subnet_group.example.name

  server_side_encryption {
    enabled = true
  }

  tags = {
    Environment = ""production""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role and attaches the necessary policy for DAX
3. Creates a subnet group for the DAX cluster
4. Creates the DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - Server-side encryption enabled
   - The specified subnet group and IAM role

Note: Make sure to:
- Replace the subnet IDs with your actual subnet IDs
- Adjust the region as needed
- Update the cluster name and other tags according to your requirements
- Ensure your VPC and security group settings allow DAX traffic

The configuration includes all necessary components for a working DAX cluster while following AWS best practices for security and deployment.",,,,,
"aws_redshift_cluster, aws_redshift_usage_limit",Create a 2 node RedShift cluster and limit the concurrency scaling to 60 minutes,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 2
}

aws_redshift_usage_limit_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_usage_limit""
    resource.change.after.feature_type == ""concurrency-scaling""
    resource.change.after.limit_type == ""time""
    resource.change.after.amount == 60
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
    aws_redshift_usage_limit_valid(input.resource_changes)
}
",2,"resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type          = ""dc2.large""
  number_of_nodes    = 2

  database_name = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  skip_final_snapshot = true
}

resource ""aws_redshift_usage_limit"" ""example"" {
  cluster_identifier = aws_redshift_cluster.example.id
  feature_type       = ""concurrency-scaling""
  limit_type         = ""time""
  amount             = 60
}","Has an aws_redshift_cluster resource and check cluster_type is ""single_node"" or number of nodes is 2, check there is a aws_redshift_usage_limit resouce where the feature type is concurrency_scaling, limit_type is ""time"", and amount is 60","I'll help you create a Terraform configuration for a 2-node Amazon RedShift cluster with concurrency scaling limited to 60 minutes. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier  = ""example-redshift-cluster""
  database_name      = ""exampledb""
  master_username    = ""admin""
  master_password    = ""YourStrongPassword123!""  # Change this to a secure password
  node_type         = ""dc2.large""  # Choose appropriate node type
  cluster_type      = ""multi-node""
  number_of_nodes   = 2
  
  # Concurrency scaling configuration
  cluster_parameter_group_name = aws_redshift_parameter_group.example.name

  # Additional recommended settings
  encrypted         = true
  skip_final_snapshot = true
  
  tags = {
    Environment = ""production""
    Name        = ""example-redshift-cluster""
  }
}

resource ""aws_redshift_parameter_group"" ""example"" {
  name   = ""example-redshift-params""
  family = ""redshift-1.0""

  parameter {
    name  = ""max_concurrency_scaling_clusters""
    value = ""1""
  }

  parameter {
    name  = ""concurrency_scaling_max_time""
    value = ""60""  # 60 minutes limit
  }

  tags = {
    Environment = ""production""
    Name        = ""example-redshift-params""
  }
}
```

This configuration:
1. Creates a 2-node RedShift cluster
2. Sets up a parameter group to limit concurrency scaling to 60 minutes
3. Uses encryption for security
4. Includes basic tags for resource management
5. Configures the cluster with a parameter group that limits concurrency scaling

Make sure to:
- Replace the password with a secure value
- Adjust the region as needed
- Choose an appropriate node type based on your workload requirements
- Review and adjust any other parameters according to your specific needs

The concurrency scaling is controlled through the parameter group settings, specifically the `concurrency_scaling_max_time` parameter which is set to 60 minutes as requested.",,,,,,"I'll help you create a Terraform configuration for a 2-node Amazon RedShift cluster with concurrency scaling limited to 60 minutes. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier     = ""redshift-cluster-example""
  database_name         = ""mydb""
  master_username      = ""admin""
  master_password      = ""YourStrongPassword123!""  # Change this to a secure password
  node_type            = ""dc2.large""  # Choose appropriate node type
  cluster_type         = ""multi-node""
  number_of_nodes      = 2
  
  # Concurrency scaling configuration
  concurrency_scaling {
    max_concurrency_scaling_clusters = 1
    auto_concurrency_scaling_enabled = true
    max_minutes_allowed             = 60
  }

  # Additional recommended settings
  skip_final_snapshot   = true
  publicly_accessible   = false
  port                 = 5439
  encrypted            = true
  
  tags = {
    Environment = ""production""
    Name        = ""redshift-cluster""
  }
}

# Security group for the RedShift cluster
resource ""aws_security_group"" ""redshift"" {
  name        = ""redshift-security-group""
  description = ""Security group for RedShift cluster""
  
  ingress {
    from_port   = 5439
    to_port     = 5439
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]  # Modify according to your security requirements
  }
}

# Associate the security group with the cluster
resource ""aws_redshift_cluster_security_group_association"" ""example"" {
  cluster_identifier  = aws_redshift_cluster.example.id
  security_group_id  = aws_security_group.redshift.id
}
```

This configuration:
1. Creates a 2-node RedShift cluster
2. Sets concurrency scaling to 60 minutes maximum
3. Includes basic security settings (encryption, security group)
4. Uses the recommended port 5439
5. Includes proper security group configuration

Make sure to:
1. Replace the master password with a secure value
2. Adjust the region to your needs
3. Modify the security group ingress rules according to your security requirements
4. Choose an appropriate node type based on your workload requirements

The concurrency scaling configuration limits the scaling to 60 minutes as requested, while still allowing for automatic scaling when needed.",,,,,,"I'll help you create a Terraform configuration for a 2-node Amazon RedShift cluster with concurrency scaling limited to 60 minutes. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""example-redshift-cluster""
  node_type         = ""dc2.large""  # Choose appropriate node type
  number_of_nodes   = 2
  
  # Database configuration
  database_name   = ""exampledb""
  master_username = ""admin""
  master_password = ""YourStrongPasswordHere123!""  # Change this

  # Networking
  publicly_accessible = false
  
  # Concurrency scaling configuration
  concurrency_scaling {
    max_concurrency_scaling_clusters = 1
    max_concurrency_scaling_time_in_minutes = 60
  }

  # Additional recommended settings
  encrypted                   = true
  enhanced_vpc_routing       = true
  skip_final_snapshot       = true
  
  # Tags
  tags = {
    Environment = ""production""
    Name        = ""example-redshift-cluster""
  }
}
```

This configuration:
1. Creates a 2-node RedShift cluster
2. Sets concurrency scaling limit to 60 minutes
3. Includes basic security features like encryption
4. Uses dc2.large node type (you can adjust based on your needs)
5. Sets up basic database configuration

Remember to:
- Change the region to your desired AWS region
- Set a strong master password
- Adjust the node type based on your workload requirements
- Consider adding additional security groups and VPC configurations based on your network requirements

The `concurrency_scaling` block specifically addresses your requirement to limit concurrency scaling to 60 minutes.",,,,,
"aws_db_proxy, aws_db_subnet_group, aws_iam_role, aws_rds_cluster, aws_secretsmanager_secret, aws_secretsmanager_secret_version, aws_security_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc","Set up an AWS RDS Aurora MySQL cluster with a proxy for enhanced connection management, including VPC, subnets, security groups, IAM roles, and Secrets Manager for secure credential storage. The backup retention period for rds cluster should be 5, and the preffered backup window is 7 am to 9 am.","package terraform.validation

default is_valid_aws_db_proxy = false

default is_valid_aws_rds_cluster = false

default is_valid_aws_vpc = false

default is_valid_aws_subnet = false

default is_valid_aws_security_group = false

default is_valid_aws_db_subnet_group = false

default is_valid_aws_secretsmanager_secret = false

default is_valid_aws_iam_role = false

# Validate aws_db_proxy resource
is_valid_aws_db_proxy {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_proxy""
        resource.expressions.engine_family.constant_value == ""MYSQL""
        resource.expressions.require_tls.constant_value == true
        auth := resource.expressions.auth[_]
        auth.auth_scheme.constant_value == ""SECRETS""
}

# Validate aws_rds_cluster resource
is_valid_aws_rds_cluster {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_rds_cluster""
        resource.expressions.engine.constant_value == ""aurora-mysql""
        resource.expressions.master_username != null
        resource.expressions.master_password != null
    resource.expressions.preferred_backup_window.constant_value == ""07:00-09:00""
    resource.expressions.backup_retention_period.constant_value == 5
}

# Validate aws_vpc resource
is_valid_aws_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block != null
}

# Validate aws_subnet resource
is_valid_aws_subnet {
        count([x |
                resource := input.configuration.root_module.resources[x]
                resource.type == ""aws_subnet""
        ]) == 2 # Ensure there are exactly two subnet instances
}

# Validate aws_security_group resource
is_valid_aws_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""

        # Checks for at least one ingress and one egress rule, more specific validation can be added
        count(resource.expressions.ingress) > 0
        count(resource.expressions.egress) > 0
}

# Validate aws_db_subnet_group resource
is_valid_aws_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}

# Validate aws_secretsmanager_secret resource
is_valid_aws_secretsmanager_secret {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_secretsmanager_secret""
        resource.expressions.name != null
}

# Validate aws_iam_role resource
is_valid_aws_iam_role {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_iam_role""
        resource.expressions.assume_role_policy != null
}

",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port       = 3306
  to_port         = 3306
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port       = 3306
  to_port         = 3306
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

resource ""aws_rds_cluster"" ""example"" {
  cluster_identifier      = ""example-cluster""
  engine                  = ""aurora-mysql""
  engine_version          = ""8.0.mysql_aurora.3.08.0""
  master_username         = ""myusername""
  master_password         = ""password123""
  database_name           = ""exampledb""
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot     = true
  vpc_security_group_ids  = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name    = aws_db_subnet_group.main.name
}

resource ""aws_secretsmanager_secret"" ""db-credentials"" {
  name_prefix = ""db-credentials-""
}

resource ""aws_secretsmanager_secret_version"" ""db-credentials-version"" {
  secret_id     = aws_secretsmanager_secret.db-credentials.id
  secret_string = jsonencode({
    username = aws_rds_cluster.example.master_username
    password = aws_rds_cluster.example.master_password
  })
}

resource ""aws_iam_role"" ""rds-proxy-role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy"" ""rds-proxy-policy"" {
  name        = ""rds-proxy-policy""

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""secretsmanager:GetSecretValue"",
        ]
        Resource = [
          ""${aws_secretsmanager_secret_version.db-credentials-version.arn}"",
        ]
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""attach-proxy-policy"" {
  role       = aws_iam_role.rds-proxy-role.name
  policy_arn = aws_iam_policy.rds-proxy-policy.arn
}

resource ""aws_db_proxy"" ""example"" {
  name                   = ""example-proxy""
  debug_logging          = false
  engine_family          = ""MYSQL""
  idle_client_timeout    = 1800
  require_tls            = true
  role_arn               = aws_iam_role.rds-proxy-role.arn
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  vpc_subnet_ids         = module.vpc.private_subnets

  auth {
    auth_scheme  = ""SECRETS""
    secret_arn   = aws_secretsmanager_secret.db-credentials.arn
    iam_auth     = ""DISABLED""
  }
}

resource ""aws_db_proxy_default_target_group"" ""example"" {
  db_proxy_name = aws_db_proxy.example.name

  connection_pool_config {
    connection_borrow_timeout    = 120
    init_query                   = ""SET x=1, y=2""
    max_connections_percent      = 100
    max_idle_connections_percent = 50
    session_pinning_filters      = [""EXCLUDE_VARIABLE_SETS""]
  }
}

resource ""aws_db_proxy_target"" ""example"" {
  db_cluster_identifier = aws_rds_cluster.example.cluster_identifier
  db_proxy_name          = aws_db_proxy.example.name
  target_group_name      = aws_db_proxy_default_target_group.example.name
}
","has one ""aws_db_proxy"" resource
with ""engine_family"" = ""MYSQL""
with ""require_tls"" = true
with ""auth_scheme"" = ""SECRETS""

has one ""aws_rds_cluster"" resource
with ""engine"" = ""aurora-mysql""
with ""master_username"" and ""master_password"" from variables

has one ""aws_vpc"" resource
with a ""cidr_block""

has one ""aws_subnet"" resource
with two instances for different availability zones

has one ""aws_security_group"" resource
with ingress and egress rules for RDS and EC2 instances

has one ""aws_db_subnet_group"" resource
with subnet IDs

has one ""aws_secretsmanager_secret"" resource
with DB credentials

has one ""aws_iam_role"" resource
for the RDS proxy with an assume role policy","I'll help you create a Terraform configuration for an AWS RDS Aurora MySQL cluster with a proxy. This solution will include all the necessary components you mentioned.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""aurora_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""aurora-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""aurora_subnet_1"" {
  vpc_id            = aws_vpc.aurora_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""aurora-subnet-1""
  }
}

resource ""aws_subnet"" ""aurora_subnet_2"" {
  vpc_id            = aws_vpc.aurora_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""aurora-subnet-2""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""aurora_subnet_group"" {
  name       = ""aurora-subnet-group""
  subnet_ids = [aws_subnet.aurora_subnet_1.id, aws_subnet.aurora_subnet_2.id]

  tags = {
    Name = ""Aurora DB subnet group""
  }
}

# Security Group
resource ""aws_security_group"" ""aurora_sg"" {
  name        = ""aurora-security-group""
  description = ""Security group for Aurora cluster""
  vpc_id      = aws_vpc.aurora_vpc.id

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}

# Secrets Manager for DB credentials
resource ""aws_secretsmanager_secret"" ""aurora_secret"" {
  name = ""aurora-credentials""
}

resource ""aws_secretsmanager_secret_version"" ""aurora_secret_version"" {
  secret_id = aws_secretsmanager_secret.aurora_secret.id
  secret_string = jsonencode({
    username = ""admin""
    password = ""yourpasswordhere""
  })
}

# IAM Role for RDS Proxy
resource ""aws_iam_role"" ""rds_proxy_role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

# Aurora Cluster
resource ""aws_rds_cluster"" ""aurora_cluster"" {
  cluster_identifier      = ""aurora-cluster""
  engine                  = ""aurora-mysql""
  engine_version         = ""5.7.mysql_aurora.2.10.2""
  database_name          = ""mydb""
  master_username        = jsondecode(aws_secretsmanager_secret_version.aurora_secret_version.secret_string)[""username""]
  master_password        = jsondecode(aws_secretsmanager_secret_version.aurora_secret_version.secret_string)[""password""]
  db_subnet_group_name   = aws_db_subnet_group.aurora_subnet_group.name
  vpc_security_group_ids = [aws_security_group.aurora_sg.id]
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot    = true",,,,,,"I'll help you create a Terraform configuration for an AWS RDS Aurora MySQL cluster with a proxy setup. This will include all the necessary networking components and security configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""aurora_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""aurora-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""aurora_subnet_1"" {
  vpc_id            = aws_vpc.aurora_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""aurora-subnet-1""
  }
}

resource ""aws_subnet"" ""aurora_subnet_2"" {
  vpc_id            = aws_vpc.aurora_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""aurora-subnet-2""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""aurora_subnet_group"" {
  name       = ""aurora-subnet-group""
  subnet_ids = [aws_subnet.aurora_subnet_1.id, aws_subnet.aurora_subnet_2.id]

  tags = {
    Name = ""Aurora DB subnet group""
  }
}

# Security Group
resource ""aws_security_group"" ""aurora_sg"" {
  name        = ""aurora-security-group""
  description = ""Security group for Aurora cluster""
  vpc_id      = aws_vpc.aurora_vpc.id

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}

# Secrets Manager for DB credentials
resource ""aws_secretsmanager_secret"" ""aurora_secret"" {
  name = ""aurora-credentials""
}

resource ""aws_secretsmanager_secret_version"" ""aurora_secret_version"" {
  secret_id = aws_secretsmanager_secret.aurora_secret.id
  secret_string = jsonencode({
    username = ""admin""
    password = ""yourpasswordhere""
  })
}

# IAM Role for RDS Proxy
resource ""aws_iam_role"" ""rds_proxy_role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

# Aurora Cluster
resource ""aws_rds_cluster"" ""aurora_cluster"" {
  cluster_identifier     = ""aurora-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.10.2""
  database_name         = ""mydb""
  master_username       = jsondecode(aws_secretsmanager_secret_version.aurora_secret_version.secret_string)[""username""]
  master_password       = jsondecode(aws_secretsmanager_secret_version.aurora_secret_version.secret_string)[""password""]
  db_subnet_group_name  = aws_db_subnet_group.aurora_subnet_group.name
  vpc_security_group_ids = [aws_security_group.aurora_sg.id]
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot    =",,,,,,"I'll help you create a Terraform configuration for an AWS RDS Aurora MySQL cluster with a proxy setup. Here's a comprehensive solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC and Networking
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""aurora-vpc""
  }
}

resource ""aws_subnet"" ""private"" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.${count.index + 1}.0/24""
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = ""aurora-private-subnet-${count.index + 1}""
  }
}

# Security Groups
resource ""aws_security_group"" ""aurora"" {
  name        = ""aurora-security-group""
  description = ""Security group for Aurora cluster""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}

# Secrets Manager for DB credentials
resource ""aws_secretsmanager_secret"" ""aurora_credentials"" {
  name = ""aurora-credentials""
}

resource ""aws_secretsmanager_secret_version"" ""aurora_credentials"" {
  secret_id = aws_secretsmanager_secret.aurora_credentials.id
  secret_string = jsonencode({
    username = ""admin""
    password = random_password.master_password.result
  })
}

resource ""random_password"" ""master_password"" {
  length  = 16
  special = true
}

# IAM Role for RDS Proxy
resource ""aws_iam_role"" ""rds_proxy"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

# Aurora DB Cluster
resource ""aws_rds_cluster"" ""aurora"" {
  cluster_identifier     = ""aurora-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.10.2""
  database_name         = ""mydb""
  master_username       = jsondecode(aws_secretsmanager_secret_version.aurora_credentials.secret_string)[""username""]
  master_password       = jsondecode(aws_secretsmanager_secret_version.aurora_credentials.secret_string)[""password""]
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot   = true
  
  vpc_security_group_ids = [aws_security_group.aurora.id]
  db_subnet_group_name   = aws_db_subnet_group.aurora.name
}

resource ""aws_rds_cluster_instance"" ""aurora_instances"" {
  count               = 2
  identifier          = ""aurora-instance-${count.index}""
  cluster_identifier  = aws_rds_cluster.aurora.id
  instance_class      = ""db.r5.large""
  engine              = aws_rds_cluster.aurora.engine
  engine_version      = aws_rds_cluster.aurora.engine_version
}

resource ""aws_db_subnet_group"" ""aurora"" {
  name       = ""aurora-subnet-group""
  subnet_ids = aws_subnet.private",,,,,
"aws_dlm_lifecycle_policy, aws_glacier_vault",generate an S3 Glacier vault and use it with AWS Data Lifecycle Manager,"package terraform.validation

default is_valid_terraform_plan = false

# Rule to check for an AWS Glacier Vault resource with a 'name' attribute
is_valid_glacier_vault {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_glacier_vault""
        not is_null(resource.values.name)
}

# Rule to check for an AWS DLM Lifecycle Policy with required attributes
is_valid_dlm_lifecycle_policy {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_dlm_lifecycle_policy""
        not is_null(resource.values.description)
#         not is_null(resource.values.execution_role_arn)
        not is_null(resource.values.policy_details)
    
    execution_role_arn_configured(resource)
}

execution_role_arn_configured(_) {
    role_resource := input.configuration.root_module.resources[_]
    role_resource.type == ""aws_dlm_lifecycle_policy""
#     role_resource.name == ""dlm_lifecycle_role""
    not is_null(role_resource.expressions.execution_role_arn)
}

# Combined rule to validate the entire Terraform plan
is_valid_terraform_plan {
        is_valid_glacier_vault
        is_valid_dlm_lifecycle_policy
}",3,"resource ""aws_glacier_vault"" ""example"" {
  name = ""my-glacier-vault""
}

resource ""aws_dlm_lifecycle_policy"" ""example"" {
  description = ""Automated archiving policy""

  execution_role_arn = ""arn:aws:iam::123456789012:role/DLMServiceRole""  # Replace with your DLM execution role ARN

  policy_details {
    resource_types = [""VOLUME""]
    target_tags = {
      ""archive"" = ""true""
    }
    schedule {
      name = ""2 weeks of daily snapshots""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times         = [""23:45""]
      }

      retain_rule {
        count = 14
      }

      tags_to_add = {
        SnapshotCreator = ""DLM""
      }

      copy_tags = false
    }
  }
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute

has one ""aws_dlm_lifecycle_policy"" resource
    with ""description"" attribute
    with ""execution_role_arn""
    with ""policy_details""","I'll help you create a Terraform configuration for an S3 Glacier vault with AWS Data Lifecycle Manager. Here's a solution that implements this:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  tags = {
    Environment = ""Production""
  }
}

# Create SNS Topic for notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-vault-notifications""
}

# Create DLM Lifecycle Policy
resource ""aws_dlm_lifecycle_policy"" ""glacier_lifecycle_policy"" {
  description        = ""Glacier vault lifecycle policy""
  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn
  state             = ""ENABLED""

  policy_details {
    resource_types = [""VOLUME""]

    schedule {
      name = ""Move to Glacier after 90 days""
      
      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times        = [""23:45""]
      }

      retain_rule {
        count = 90
      }

      transition_to_cold {
        days = 90
      }

      tags_to_add = {
        LifecyclePolicy = ""ToGlacier""
      }
    }

    target_tags = {
      Backup = ""true""
    }
  }
}

# Create IAM Role for DLM
resource ""aws_iam_role"" ""dlm_lifecycle_role"" {
  name = ""dlm-lifecycle-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dlm.amazonaws.com""
        }
      }
    ]
  })
}

# Attach required policy to DLM role
resource ""aws_iam_role_policy"" ""dlm_lifecycle"" {
  name = ""dlm-lifecycle-policy""
  role = aws_iam_role.dlm_lifecycle_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""ec2:CreateSnapshot"",
          ""ec2:DeleteSnapshot"",
          ""ec2:DescribeVolumes"",
          ""ec2:DescribeSnapshots""
        ]
        Resource = ""*""
      },
      {
        Effect = ""Allow""
        Action = [
          ""glacier:CreateVaultLock"",
          ""glacier:InitiateJob"",
          ""glacier:AbortMultipartUpload"",
          ""glacier:CreateVault"",
          ""glacier:DeleteArchive"",
          ""glacier:DeleteVault"",
          ""glacier:DescribeJob"",
          ""glacier:DescribeVault"",
          ""glacier:GetJobOutput"",
          ""glacier:InitiateMultipartUpload"",
          ""glacier:ListJobs"",
          ""glacier:ListMultipartUploads"",
          ""glacier:ListParts"",
          ""glacier:ListVaults"",
          ""glacier:UploadArchive"",
          ""glacier:UploadMultipartPart""
        ]
        Resource = aws_glacier_vault.example_vault.arn
      }
    ]
  })
}
```

This configuration:

1. Creates a Glacier vault with SNS notifications
2. Sets up an SNS topic for vault notifications
3. Creates a DLM lifecycle policy that transitions data to Glacier after 90",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault and AWS Data Lifecycle Manager. Here's a solution that includes both resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create S3 Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  tags = {
    Environment = ""production""
  }

  notification {
    sns_topic = aws_sns_topic.glacier_updates.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  access_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""add-read-only-perm""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""glacier:InitiateJob"",
          ""glacier:GetJobOutput""
        ]
        Resource = ""arn:aws:glacier:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:vaults/${var.vault_name}""
      }
    ]
  })
}

# Create SNS Topic for Glacier notifications
resource ""aws_sns_topic"" ""glacier_updates"" {
  name = ""glacier-vault-updates""
}

# Create DLM Lifecycle Policy
resource ""aws_dlm_lifecycle_policy"" ""example"" {
  description        = ""example DLM lifecycle policy""
  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn
  state             = ""ENABLED""

  policy_details {
    resource_types = [""VOLUME""]

    schedule {
      name = ""2 weeks of daily snapshots""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times        = [""23:45""]
      }

      retain_rule {
        count = 14
      }

      copy_tags = true
    }

    target_tags = {
      Snapshot = ""true""
    }
  }
}

# Create IAM role for DLM
resource ""aws_iam_role"" ""dlm_lifecycle_role"" {
  name = ""dlm-lifecycle-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dlm.amazonaws.com""
        }
      }
    ]
  })
}

# Attach required policy to DLM role
resource ""aws_iam_role_policy"" ""dlm_lifecycle"" {
  name = ""dlm-lifecycle-policy""
  role = aws_iam_role.dlm_lifecycle_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""ec2:CreateSnapshot"",
          ""ec2:DeleteSnapshot"",
          ""ec2:DescribeVolumes"",
          ""ec2:DescribeSnapshots""
        ]
        Resource = ""*""
      },
      {
        Effect = ""Allow""
        Action = [
          ""ec2:CreateTags""
        ]
        Resource = ""arn:aws:ec2:*::snapshot/*""
      }
    ]
  })
}

# Data sources for current region and account ID
data ""aws_region"" ""current"" {}
data ""aws_caller_identity"" ""current"" {}

# Variable for vault name
variable ""vault_name"" {
  type        = string
  description = ""Name of the Glacier vault""
  default     = ""example-glacier-vault""
}
```

This configuration:

1. Creates an S3 ",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault with AWS Data Lifecycle Manager. This configuration will include the necessary resources for both the Glacier vault and the lifecycle policy.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  tags = {
    Environment = ""Production""
  }

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  access_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""add-read-only-permissions""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""glacier:InitiateJob"",
          ""glacier:GetJobOutput""
        ]
        Resource = ""arn:aws:glacier:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:vaults/${aws_glacier_vault.example_vault.name}""
      }
    ]
  })
}

# Create SNS Topic for Glacier notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-vault-notifications""
}

# Create DLM Lifecycle Policy
resource ""aws_dlm_lifecycle_policy"" ""example"" {
  description        = ""example DLM lifecycle policy""
  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn
  state             = ""ENABLED""

  policy_details {
    resource_types = [""VOLUME""]

    schedule {
      name = ""2 weeks of daily snapshots""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times        = [""23:45""]
      }

      retain_rule {
        count = 14
      }

      copy_tags = true
    }

    target_tags = {
      Snapshot = ""true""
    }
  }
}

# IAM Role for DLM
resource ""aws_iam_role"" ""dlm_lifecycle_role"" {
  name = ""dlm-lifecycle-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dlm.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Policy for DLM Role
resource ""aws_iam_role_policy"" ""dlm_lifecycle"" {
  name = ""dlm-lifecycle-policy""
  role = aws_iam_role.dlm_lifecycle_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""ec2:CreateSnapshot"",
          ""ec2:DeleteSnapshot"",
          ""ec2:DescribeVolumes"",
          ""ec2:DescribeSnapshots""
        ]
        Resource = ""*""
      },
      {
        Effect = ""Allow""
        Action = [
          ""ec2:CreateTags""
        ]
        Resource = ""arn:aws:ec2:*::snapshot/*""
      }
    ]
  })
}

# Data sources for current region and account ID
data ""aws_region"" ""current"" {}
data ""aws_caller_identity"" ""current"" {}
```

This configuration:

1. Creates a Glacier vault with:
   - Notification configuration using SNS
   - Access policy for basic operations
   - Tags for identification

2. Sets up Data Lifecycle Manager with:",,,,,
"aws_connect_bot_association, aws_connect_instance, aws_lex_bot, aws_lex_intent, aws_region",create an aws_connect_bot_association resource with a SAML aws_connect_instance,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for multiple aws_lex_intent resources
has_valid_lex_intents {
    count([intent | intent := input.planned_values.root_module.resources[_]; intent.type == ""aws_lex_intent""; intent.values.name; intent.values.fulfillment_activity])
}

has_valid_connect_instance {
	count([instance | instance := input.planned_values.root_module.resources[_]; instance.type == ""aws_connect_instance""; instance.values.identity_management_type == ""SAML""; instance.values.inbound_calls_enabled; instance.values.outbound_calls_enabled])
}

# Rule for aws_lex_bot resource with specific arguments
has_valid_lex_bot_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lex_bot""
    resource.values.child_directed == true
    resource.values.clarification_prompt
    resource.values.abort_statement
    resource.values.name
    # is_boolean(resource.values.create_version)
    # resource.values.description
    # is_boolean(resource.values.detect_sentiment)
    # resource.values.idle_session_ttl_in_seconds
    # resource.values.process_behavior
    # resource.values.voice_id
    # resource.values.locale
    # is_boolean(resource.values.enable_model_improvements)
    # resource.values.nlu_intent_confidence_threshold
}

has_valid_connect_bot_association {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_connect_bot_association""
    resource.values.lex_bot[_].name
    instance := input.configuration.root_module.resources[i]
    instance.expressions.instance_id
    
    
}



# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lex_intents
    has_valid_lex_bot_instance
    has_valid_connect_instance
    has_valid_connect_bot_association
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_region"" ""current"" {}

resource ""aws_connect_instance"" ""example"" {
  identity_management_type = ""CONNECT_MANAGED""
  inbound_calls_enabled    = true
  instance_alias           = ""friendly-name-connect-13""
  outbound_calls_enabled   = true
}

resource ""aws_lex_intent"" ""example"" {
  create_version = true
  name           = ""connect_lex_intent""
  
  sample_utterances = [
    ""I would like to pick up flowers."",
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

resource ""aws_lex_bot"" ""example13"" {
  name             = ""connect_lex_bot""
  process_behavior = ""BUILD""
  child_directed   = false

  abort_statement {
    message {
      content      = ""Sorry, I am not able to assist at this time.""
      content_type = ""PlainText""
    }
  }
  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, what would you like to do?""
      content_type = ""PlainText""
    }
  }
  intent {
    intent_name    = aws_lex_intent.example.name
    intent_version = ""1""
  }  
}

resource ""aws_connect_bot_association"" ""example"" {
  instance_id = aws_connect_instance.example.id
  lex_bot {
    lex_region = data.aws_region.current.name
    name       = aws_lex_bot.example13.name
  }
}

","Has one resource ""aws_connect_instance"" with argument ""identity_management_type"" = SAML, at least one ""aws_lex_intent"" resource, one ""aws_lex_bot"" resource, and one ""aws_connnect_bot_association"" resource","I'll help you create a Terraform configuration for an AWS Connect bot association with a SAML-based Connect instance. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create the SAML-based Connect Instance
resource ""aws_connect_instance"" ""example"" {
  identity_management_type = ""SAML""
  instance_alias          = ""example-connect-instance""
  inbound_calls_enabled   = true
  outbound_calls_enabled  = true
}

# Create the Connect Bot Association
resource ""aws_connect_bot_association"" ""example"" {
  instance_id = aws_connect_instance.example.id
  lex_bot {
    lex_region = ""us-west-2""  # Specify the region where your Lex bot is located
    name       = ""ExampleBot"" # Name of your Lex bot
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a SAML-based Connect instance
3. Creates a bot association linking a Lex bot to the Connect instance

Note that you'll need to:
- Ensure you have the appropriate permissions to create these resources
- Replace the `lex_region` with the actual region where your Lex bot is deployed
- Replace the `name` with your actual Lex bot name
- Adjust the `instance_alias` to match your desired Connect instance name

The bot association will be created after the Connect instance is successfully provisioned due to the implicit dependency created by referencing the instance ID.",,,,,,"I'll help you create a Terraform configuration for an AWS Connect bot association with a SAML-based Connect instance. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create the SAML-based Connect Instance
resource ""aws_connect_instance"" ""example"" {
  identity_management_type = ""SAML""
  instance_alias          = ""example-connect-instance""
  inbound_calls_enabled   = true
  outbound_calls_enabled  = true
}

# Create the Connect Bot Association
resource ""aws_connect_bot_association"" ""example"" {
  instance_id = aws_connect_instance.example.id
  
  lex_bot {
    lex_region = ""us-west-2""  # Specify the region where your Lex bot is located
    name       = ""ExampleBot"" # Name of your Lex bot
  }
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates a SAML-based Connect instance
3. Creates a bot association linking a Lex bot to the Connect instance

Note that you'll need to:
- Replace ""us-west-2"" with the actual region where your Lex bot is deployed
- Replace ""ExampleBot"" with your actual Lex bot name
- Ensure you have the necessary permissions to create these resources
- Make sure the Lex bot exists in the specified region

The bot association will be created after the Connect instance is successfully created due to the implicit dependency created by referencing the instance ID.",,,,,,"I'll help you create an AWS Connect bot association with a SAML-based Connect instance using Terraform. Here's a complete configuration that includes both the Connect instance with SAML and the bot association:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust region as needed
}

# Create the Connect Instance with SAML
resource ""aws_connect_instance"" ""example"" {
  identity_management_type = ""SAML""
  instance_alias          = ""my-connect-instance""
  inbound_calls_enabled   = true
  outbound_calls_enabled  = true
}

# Create the Connect Bot Association
resource ""aws_connect_bot_association"" ""example"" {
  instance_id = aws_connect_instance.example.id
  
  lex_bot {
    lex_region = ""us-west-2""  # Adjust region as needed
    name       = ""MyLexBot""
  }
}

# Optional: Create IAM role for the bot if needed
resource ""aws_iam_role"" ""connect_lex_role"" {
  name = ""connect-lex-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""connect.amazonaws.com""
        }
      }
    ]
  })
}

# Optional: Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""connect_lex_policy"" {
  role       = aws_iam_role.connect_lex_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AmazonLexRunBotsPolicy""
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Connect instance with SAML authentication
3. Creates a bot association linking a Lex bot to the Connect instance
4. Includes optional IAM role and policy configuration for the bot integration

Make sure to:
- Adjust the region as needed
- Replace ""MyLexBot"" with your actual Lex bot name
- Modify the instance alias to match your needs
- Customize the IAM role and policies according to your security requirements

You can also add additional configurations to the Connect instance and bot association based on your specific requirements.",,,,,
aws_lightsail_database,"AWS Lightsail that creates a postgres database, which enable creating a final snapshot of your database on deletion","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.blueprint_id == ""postgres_12""
    resource.values.bundle_id
    resource.values.final_snapshot_name
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name     = ""test""
  availability_zone            = ""us-east-1a""
  master_database_name         = ""testdatabasename""
  master_password              = ""testdatabasepassword""
  master_username              = ""test""
  blueprint_id                 = ""postgres_12""
  bundle_id                    = ""micro_1_0""
  final_snapshot_name          = ""MyFinalSnapshot""
}","""have one """"aws_lightsail_database"""" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id = postgres_12
    with bundle_id
    with final_snapshot_name","I'll help you create a Terraform configuration for an AWS Lightsail PostgreSQL database with final snapshot enabled. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

resource ""aws_lightsail_database"" ""postgres_db"" {
  relational_database_name = ""my-postgres-db""
  availability_zone       = ""us-east-1a""  # Change to match your region
  master_database_name   = ""main_database""
  master_username       = ""dbadmin""
  master_password       = ""YourStrongPassword123!""  # Replace with secure password
  blueprint_id          = ""postgres_12""
  bundle_id            = ""micro_2_0""  # Choose appropriate bundle size
  
  # Enable final snapshot
  final_snapshot_name = ""final-snapshot-${aws_lightsail_database.postgres_db.relational_database_name}""
  skip_final_snapshot = false
  
  # Additional optional configurations
  preferred_backup_window      = ""03:00-04:00""
  preferred_maintenance_window = ""tue:04:00-tue:05:00""
  
  tags = {
    Environment = ""production""
    Managed_by  = ""terraform""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Lightsail PostgreSQL database
3. Enables final snapshot creation upon deletion with a dynamic name
4. Includes basic security and maintenance configurations

Key features:
- `skip_final_snapshot = false` ensures a final snapshot is created when the database is deleted
- `final_snapshot_name` defines the name pattern for the final snapshot
- The configuration includes backup and maintenance windows
- Uses tags for better resource management

Remember to:
- Replace the password with a secure value
- Adjust the region and availability zone according to your needs
- Choose an appropriate bundle size for your workload
- Modify the backup and maintenance windows as needed",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail PostgreSQL database with a final snapshot enabled. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1"" # Change to your desired region
}

resource ""aws_lightsail_database"" ""postgres_db"" {
  relational_database_name = ""my-postgres-db""
  availability_zone       = ""us-east-1a""
  master_database_name   = ""main_database""
  master_username        = ""dbadmin""
  master_password        = ""your-secure-password"" # Replace with a secure password
  blueprint_id           = ""postgres_12""
  bundle_id             = ""micro_2_0"" # Smallest instance, adjust as needed
  
  # Enable final snapshot on deletion
  skip_final_snapshot    = false
  final_snapshot_name    = ""final-snapshot-${aws_lightsail_database.postgres_db.relational_database_name}""
  
  # Additional optional configurations
  preferred_backup_window = ""03:00-04:00""
  preferred_maintenance_window = ""tue:04:00-tue:05:00""
  
  # Enable automatic backups
  backup_retention_enabled = true
  
  tags = {
    Environment = ""production""
    Name        = ""postgres-database""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail PostgreSQL database
3. Enables final snapshot creation when the database is deleted
4. Configures basic database settings like name, credentials, and instance size
5. Sets up backup and maintenance windows
6. Includes basic tagging

Key features:
- `skip_final_snapshot = false` ensures a final snapshot is created on deletion
- `final_snapshot_name` defines the name pattern for the final snapshot
- Uses PostgreSQL 12 as the database engine
- Includes backup retention and maintenance window configurations
- Uses the micro_2_0 bundle (smallest instance size, adjust according to your needs)

Remember to:
- Replace the password with a secure value
- Adjust the region and availability zone as needed
- Modify the bundle_id based on your performance requirements
- Update the backup and maintenance windows according to your needs",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail PostgreSQL database with final snapshot enabled. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

resource ""aws_lightsail_database"" ""postgres_db"" {
  relational_database_name = ""my-postgres-db""
  availability_zone       = ""us-east-1a""  # Change to match your region
  master_database_name   = ""main_database""
  master_username       = ""dbadmin""
  master_password       = ""YourStrongPassword123!""  # Replace with secure password
  blueprint_id          = ""postgres_12""
  bundle_id            = ""micro_2_0""  # Choose appropriate bundle size
  
  # Enable final snapshot on deletion
  skip_final_snapshot  = false
  final_snapshot_name = ""final-snapshot-${aws_lightsail_database.postgres_db.relational_database_name}""
  
  # Additional optional configurations
  preferred_backup_window      = ""03:00-04:00""
  preferred_maintenance_window = ""sun:05:00-sun:06:00""
  
  tags = {
    Environment = ""production""
    Name        = ""postgres-database""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail PostgreSQL database
3. Enables final snapshot creation on deletion with `skip_final_snapshot = false`
4. Specifies a name for the final snapshot
5. Includes basic backup and maintenance windows
6. Adds some example tags

Key features:
- The database will create a final snapshot before deletion
- Uses PostgreSQL 12 as the database engine
- Sets up basic security with master username and password
- Configures backup and maintenance windows
- Allows for easy customization of the instance size through the bundle_id

Remember to:
- Replace the password with a secure value
- Adjust the region and availability zone as needed
- Choose an appropriate bundle size for your needs
- Consider storing sensitive information like passwords in variables or secrets management",,,,,
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket and an example payment configuration resource for the S3 bucket.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""

        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""Requester""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name","I'll help you create a Terraform configuration for an S3 bucket with a payment configuration. Here's a solution that includes both resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Replace with your desired region
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""my-payment-configuration-bucket""
}

# Create payment configuration for the S3 bucket
resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example_bucket.id
  payer  = ""Requester""  # Can be either ""BucketOwner"" or ""Requester""
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an S3 bucket named ""my-payment-configuration-bucket""
3. Configures the request payment settings for the bucket to charge the requester for data transfer costs

The payment configuration allows you to specify who pays for the data transfer and request costs associated with the bucket. In this example, we set it to ""Requester"", meaning the person requesting the data pays for the transfer costs. You can change it to ""BucketOwner"" if you want the bucket owner to pay for the costs instead.

Make sure to replace the bucket name with a globally unique name as S3 bucket names must be unique across all AWS accounts.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a payment configuration. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""my-example-payment-bucket""
}

# Configure payment settings for the bucket
resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example_bucket.id
  payer  = ""Requester""  # Can be either ""BucketOwner"" or ""Requester""
}

# Optional: Add bucket versioning
resource ""aws_s3_bucket_versioning"" ""versioning"" {
  bucket = aws_s3_bucket.example_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Optional: Add server-side encryption
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""encryption"" {
  bucket = aws_s3_bucket.example_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = ""AES256""
    }
  }
}
```

This configuration:
1. Creates an S3 bucket with a specified name
2. Sets up request payment configuration where the requester pays for data transfer and request costs
3. Includes optional versioning and encryption configurations for better security
4. Uses proper resource dependencies through references

Make sure to:
- Change the bucket name to something unique (S3 bucket names must be globally unique)
- Adjust the region as needed
- Consider whether you want the optional versioning and encryption configurations

The payment configuration allows you to specify who pays for the data transfer and request costs associated with the bucket. The `payer` field can be either ""BucketOwner"" (default) or ""Requester"".",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a payment configuration. Here's a solution that includes both resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""my-example-payment-bucket""
}

# Create payment configuration for the S3 bucket
resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example_bucket.id
  payer  = ""Requester""  # Can be either ""BucketOwner"" or ""Requester""
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an S3 bucket named ""my-example-payment-bucket""
3. Configures the request payment settings for the bucket, setting the payer to ""Requester"" (meaning the person requesting the data pays for the transfer costs)

The payment configuration allows you to specify who pays for data transfer costs associated with the bucket. You can set the `payer` to either:
- ""BucketOwner"" (default) - The bucket owner pays for all transfer costs
- ""Requester"" - The requester pays for the data transfer costs

Make sure to replace the bucket name with your desired unique name, as S3 bucket names must be globally unique across all AWS accounts.",,,,,
"aws_internet_gateway, aws_vpc","creates a VPC with a CIDR block, and an internet gateway in the AWS us-east-2 region, both tagged with names ""vpc"" and ""ig"" respectively.","package terraform.validation

# Set default validation states
default is_valid_vpc = false

default is_valid_internet_gateway = false

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
        resource.expressions.enable_dns_hostnames.constant_value == true
        resource.expressions.tags != null
}

# Validate aws_internet_gateway resource
is_valid_internet_gateway {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_internet_gateway""
        resource.expressions.vpc_id.references[0] == ""aws_vpc._.id""
        resource.expressions.tags != null
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""_"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  tags = merge({
    ""Name"" = ""vpc""
  })
}

resource ""aws_internet_gateway"" ""_"" {
  vpc_id = aws_vpc._.id
  tags = merge({
    ""Name"" = ""ig""
  })
}","Has one ""aws_vpc"" resource:
with a specified ""cidr_block""
with ""enable_dns_hostnames"" set to true

Has one ""aws_internet_gateway"" resource:
with ""vpc_id"" referencing the ""aws_vpc"" resource by its ID
with a ""tags"" map that is not null, including at least a ""Name"" key with a value","I'll help you create a Terraform configuration that creates a VPC with a CIDR block and an Internet Gateway in the AWS us-east-2 region, including the required tags.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""vpc""
  }
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""ig""
  }
}
```

This configuration:
1. Sets up the AWS provider for the us-east-2 region
2. Creates a VPC with CIDR block 10.0.0.0/16
3. Creates an Internet Gateway and attaches it to the VPC
4. Tags both resources with their respective names as requested
5. Enables DNS support and DNS hostnames for the VPC (best practice)

The resources will be created in the specified order, with the Internet Gateway being attached to the VPC automatically through the vpc_id reference.",,,,,,"I'll help you create a Terraform configuration that creates a VPC with a CIDR block and an Internet Gateway in the AWS us-east-2 region, including the specified tags.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""vpc""
  }
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""ig""
  }
}
```

This configuration:
1. Sets up the AWS provider for us-east-2 region
2. Creates a VPC with CIDR block 10.0.0.0/16
3. Creates an Internet Gateway and attaches it to the VPC
4. Applies the requested tags to both resources
5. Enables DNS support and hostnames for the VPC

The resources will be created in the specified order, with the Internet Gateway being attached to the VPC automatically through the `vpc_id` reference.",,,,,,"I'll help you create a Terraform configuration that creates a VPC with a CIDR block and an Internet Gateway in the AWS us-east-2 region, with appropriate tags.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""vpc""
  }
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""ig""
  }
}
```

This configuration:
1. Sets up the AWS provider for the us-east-2 region
2. Creates a VPC with CIDR block 10.0.0.0/16 and tags it with name ""vpc""
3. Creates an Internet Gateway attached to the VPC and tags it with name ""ig""
4. Includes the required AWS provider block
5. Enables DNS hostnames and support for the VPC

The resources will be created in the specified order and the Internet Gateway will be automatically attached to the VPC using the vpc_id reference.",,,,,
"aws_route53_health_check, aws_route53_health_check, aws_route53_record, aws_route53_record, aws_route53_zone","Configure Active-Active Failover policies using Route 53 resources. Name the health check resources ""primary_health_check"" and ""secondary_health_check"" and use a fqdn for both. Call the zone ""main""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record_primary = false

default is_valid_r53_record_secondary = false

default is_valid_r53_health_check_primary = false

default is_valid_r53_health_check_secondary = false



# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""

}

# Validate aws_route53_record
is_valid_r53_record_primary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.failover_routing_policy[0].type.constant_value == ""PRIMARY""
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.set_identifier
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.main.zone_id""
    resource.expressions.health_check_id.references[0] == ""aws_route53_health_check.primary_health_check.id""
}

is_valid_r53_record_secondary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.failover_routing_policy[0].type.constant_value == ""SECONDARY""
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.set_identifier
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.main.zone_id""
    resource.expressions.health_check_id.references[0] == ""aws_route53_health_check.secondary_health_check.id""

}

# Validate aws_route53_health_check
is_valid_r53_health_check_primary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_health_check""
    resource.name == ""primary_health_check""
    resource.expressions.fqdn
    resource.expressions.request_interval
    resource.expressions.type
}

is_valid_r53_health_check_secondary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_health_check""
    resource.name == ""secondary_health_check""
    resource.expressions.fqdn
    resource.expressions.request_interval
    resource.expressions.type
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_health_check_primary
    is_valid_r53_health_check_secondary
    is_valid_r53_zone
    is_valid_r53_record_primary
    is_valid_r53_record_secondary
}




",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
}

resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example53.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
}

resource ""aws_route53_record"" ""primary_record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""60""
  records = [""192.0.2.101""]
  set_identifier = ""primary-endpoint""

  failover_routing_policy {
    type = ""PRIMARY""
  }

  health_check_id = aws_route53_health_check.primary_health_check.id
}

resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example53.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
}

resource ""aws_route53_record"" ""secondary_record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""60""
  records = [""192.0.2.102""] 
  set_identifier = ""secondary-endpoint""
  
  failover_routing_policy {
    type = ""SECONDARY""
  }

  health_check_id = aws_route53_health_check.secondary_health_check.id
}","Has one ""aws_route53_zone"" resource
    with ""name""

Has two ""aws_route53_record"" resources
    The first ""aws_route53_record"" 
        with ""name""
        with ""type""
        with ""ttl""
        with ""records""
        with ""zone_id"" referencing the ""aws_route53_zone"" resource
        with ""health_check_id"" referencing one of the ""aws_health_check"" resource
        with ""set_identifier""
        with ""failover_routing_policy"" block that contains ""type"" = ""PRIMARY""
    The second ""aws_route53_record""
        with ""name""
        with ""type""
        with ""ttl""
        with ""records""
        with ""zone_id"" referencing the ""aws_route53_zone"" resource
        with ""health_check_id"" referencing the other ""aws_health_check"" resource
        with ""set_identifier""
        with ""failover_routing_policy"" block that contains ""type"" = ""SECONDARY""

Has two ""aws_health_check"" resources
    with ""fqdn"" 
    with ""request_interval""
    with ""type""

","I'll help you create a Terraform configuration for Active-Active Failover using Route 53 with health checks. Here's a solution that implements two health checks and DNS records for active-active failover:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Primary Health Check
resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/health""
  failure_threshold = ""3""
  request_interval  = ""30""

  tags = {
    Name = ""primary-health-check""
  }
}

# Secondary Health Check
resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/health""
  failure_threshold = ""3""
  request_interval  = ""30""

  tags = {
    Name = ""secondary-health-check""
  }
}

# Route 53 Record Set for Active-Active Configuration
resource ""aws_route53_record"" ""www"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""
  type    = ""A""

  failover_routing_policy {
    type = ""PRIMARY""
  }

  set_identifier = ""primary""
  health_check_id = aws_route53_health_check.primary_health_check.id

  alias {
    name                   = ""primary.example.com""
    zone_id                = aws_route53_zone.main.zone_id
    evaluate_target_health = true
  }
}

resource ""aws_route53_record"" ""www_secondary"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""
  type    = ""A""

  failover_routing_policy {
    type = ""SECONDARY""
  }

  set_identifier = ""secondary""
  health_check_id = aws_route53_health_check.secondary_health_check.id

  alias {
    name                   = ""secondary.example.com""
    zone_id                = aws_route53_zone.main.zone_id
    evaluate_target_health = true
  }
}
```

This configuration:
1. Creates a Route 53 zone named ""main""
2. Sets up two health checks (primary and secondary) using FQDNs
3. Creates two Route 53 record sets for active-active failover
4. Implements health check monitoring on port 80 with HTTP checks
5. Uses alias records for the actual endpoints
6. Includes proper failover routing policies for both primary and secondary records

Remember to replace ""example.com"" with your actual domain name and adjust the health check parameters (ports, paths, intervals) according to your specific needs.",,,,,,"I'll help you create a Terraform configuration for Active-Active Failover using Route 53 with health checks. Here's a solution with primary and secondary health checks and DNS routing:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Primary Health Check
resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = ""3""
  request_interval  = ""30""

  tags = {
    Name = ""Primary Health Check""
  }
}

# Secondary Health Check
resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = ""3""
  request_interval  = ""30""

  tags = {
    Name = ""Secondary Health Check""
  }
}

# Route 53 Record Set for Active-Active Configuration
resource ""aws_route53_record"" ""active_active"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""
  type    = ""A""

  set_identifier = ""primary""
  
  failover_routing_policy {
    type = ""PRIMARY""
  }

  alias {
    name                   = ""primary.example.com""
    zone_id               = aws_route53_zone.main.zone_id
    evaluate_target_health = true
  }

  health_check_id = aws_route53_health_check.primary_health_check.id
}

resource ""aws_route53_record"" ""active_active_secondary"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""
  type    = ""A""

  set_identifier = ""secondary""

  failover_routing_policy {
    type = ""SECONDARY""
  }

  alias {
    name                   = ""secondary.example.com""
    zone_id               = aws_route53_zone.main.zone_id
    evaluate_target_health = true
  }

  health_check_id = aws_route53_health_check.secondary_health_check.id
}
```

This configuration creates:
1. A Route 53 hosted zone named ""main""
2. Two health checks monitoring primary and secondary endpoints
3. Two Route 53 record sets implementing active-active failover
4. The health checks are configured to monitor HTTP endpoints on port 80
5. The failover policy is set up with primary and secondary routing targets

Remember to replace ""example.com"" with your actual domain name and adjust the health check parameters (ports, paths, intervals) according to your specific needs.",,,,,,"I'll help you create a Terraform configuration for Active-Active Failover using Route 53 with health checks and DNS records. Here's a solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Health Check for Primary Endpoint
resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = ""3""
  request_interval  = ""30""

  tags = {
    Name = ""primary-health-check""
  }
}

# Health Check for Secondary Endpoint
resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = ""3""
  request_interval  = ""30""

  tags = {
    Name = ""secondary-health-check""
  }
}

# Route 53 Record Set for Active-Active Configuration
resource ""aws_route53_record"" ""www"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""
  type    = ""A""

  failover_routing_policy {
    type = ""PRIMARY""
  }

  set_identifier = ""primary""
  health_check_id = aws_route53_health_check.primary_health_check.id

  alias {
    name                   = ""primary.example.com""
    zone_id                = aws_route53_zone.main.zone_id
    evaluate_target_health = true
  }
}

resource ""aws_route53_record"" ""www_secondary"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""
  type    = ""A""

  failover_routing_policy {
    type = ""SECONDARY""
  }

  set_identifier = ""secondary""
  health_check_id = aws_route53_health_check.secondary_health_check.id

  alias {
    name                   = ""secondary.example.com""
    zone_id                = aws_route53_zone.main.zone_id
    evaluate_target_health = true
  }
}
```

This configuration:
1. Creates a Route 53 zone named ""main""
2. Sets up two health checks (primary_health_check and secondary_health_check) using FQDNs
3. Creates two Route 53 records with failover routing policies
4. Configures active-active failover between the primary and secondary endpoints
5. Uses alias records pointing to the respective endpoints
6. Includes health check monitoring for both endpoints

The health checks will monitor the specified FQDNs, and Route 53 will route traffic based on the health check results, providing high availability through the active-active configuration.",,,,,
"aws_iam_role, aws_iam_role_policy, aws_kendra_index",Generate a basic Kendra index resource With user group resolution configuration,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
    some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kendra.amazonaws.com"") 
}

has_valid_kendra_index {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_index""
    resource.values.name
    resource.values.user_group_resolution_configuration
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
}


has_valid_resources {
    has_valid_iam_role(input.resource_changes)
    has_valid_kendra_index
}
",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""role"" {
  name = ""Kendra-Role""
  assume_role_policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": ""sts:AssumeRole"",
      ""Principal"": {
        ""Service"": ""kendra.amazonaws.com""
      },
      ""Effect"": ""Allow"",
      ""Sid"": """"
    }
  ]
}
EOF
}

resource ""aws_iam_role_policy"" ""policy"" {
  name = ""Kendra-Policy""
  role = aws_iam_role.role.id

  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:GetObject"",
        ""s3:ListBucket""
      ],
      ""Resource"": ""*""
    }
  ]
}
EOF
}

resource ""aws_kendra_index"" ""example"" {
  name     = ""example""
  role_arn = aws_iam_role.role.arn

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""
  }
}","Has one ""aws_iam_role"" resource and one ""aws_kendra_index"" resource with the ""user_group_resolution_configuration"" block","I'll help you create a Terraform configuration for an AWS Kendra index resource with user group resolution configuration. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust region as needed
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra index with user group resolution""
  role_arn    = aws_iam_role.kendra_role.arn

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""  # Can also be ""NONE"" if you don't want to use group resolution
  }

  edition = ""ENTERPRISE_EDITION""  # Required for user group resolution
  
  tags = {
    Environment = ""production""
    Project     = ""search""
  }
}

# Required IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra-index-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""kendra_policy"" {
  role       = aws_iam_role.kendra_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSKendraServiceRole""
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Kendra index with user group resolution configuration
3. Configures it to use AWS SSO for user group resolution
4. Creates the necessary IAM role with appropriate permissions
5. Uses the Enterprise Edition which is required for user group resolution
6. Includes basic tagging for resource management

The index is configured with AWS SSO as the user group resolution mode, but you can change it to ""NONE"" if you don't need group resolution. Make sure to adjust the region and other parameters according to your needs.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra index resource with user group resolution configuration. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra Index with user group resolution""
  role_arn    = aws_iam_role.kendra_index_role.arn

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""
  }

  edition       = ""ENTERPRISE_EDITION""
  tags = {
    Environment = ""production""
    Purpose     = ""search""
  }
}

# IAM role for Kendra Index
resource ""aws_iam_role"" ""kendra_index_role"" {
  name = ""kendra_index_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Kendra Index role
resource ""aws_iam_role_policy"" ""kendra_index_policy"" {
  name = ""kendra_index_policy""
  role = aws_iam_role.kendra_index_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kendra:*"",
          ""sso:*""
        ]
        Resource = ""*""
      }
    ]
  })
}
```

This configuration creates:
1. A Kendra index with user group resolution configured for AWS SSO
2. The necessary IAM role and policy for the Kendra index
3. Uses Enterprise Edition of Kendra
4. Includes basic tagging

Key features:
- User group resolution is set to AWS SSO mode
- Proper IAM permissions for Kendra service
- Enterprise edition specification
- Basic tags for resource management
- Required AWS provider configuration

Make sure to adjust the names, descriptions, and permissions according to your specific needs. The IAM policy provided is basic and should be refined based on your security requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra index resource with user group resolution configuration. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust region as needed
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra Index with user group resolution""
  edition     = ""ENTERPRISE_EDITION""  # ENTERPRISE_EDITION supports user group resolution
  role_arn    = aws_iam_role.kendra_role.arn

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""  # Can be AWS_SSO or AWS_CLOUD_WATCH_LOGS
  }

  tags = {
    Environment = ""production""
    Purpose     = ""search""
  }
}

# IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra_index_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Kendra role
resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra_index_policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""cloudwatch:PutMetricData""
        ]
        Resource = ""*""
        Condition = {
          StringEquals = {
            ""cloudwatch:namespace"" = ""AWS/Kendra""
          }
        }
      },
      {
        Effect = ""Allow""
        Action = [
          ""logs:DescribeLogStreams"",
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = ""*""
      }
    ]
  })
}
```

This configuration creates:
1. A Kendra index with user group resolution configured for AWS SSO
2. The necessary IAM role and policy for the Kendra index to function
3. Basic tags for resource management

Key features:
- Uses ENTERPRISE_EDITION which is required for user group resolution
- Configures user group resolution for AWS SSO
- Includes necessary IAM permissions for CloudWatch logging and metrics
- Implements proper tagging for resource management

Make sure to:
- Adjust the region as needed
- Modify the tags according to your organization's standards
- Review and adjust the IAM permissions based on your specific requirements
- Consider adding additional Kendra configurations as needed for your use case",,,,,
aws_elasticache_cluster,Provide a Memcached Cluster,"package main

import future.keywords.in

default allow = false

aws_elasticache_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_cluster""
    resource.change.after.engine == ""memcached""
}

# Aggregate all checks
allow {
    aws_elasticache_cluster_valid(input.resource_changes)
}",2,"provider ""aws"" {
  region=""us-east-1""
}
resource ""aws_elasticache_cluster"" ""example"" {
  cluster_id           = ""cluster-example""
  engine               = ""memcached""
  node_type            = ""cache.m4.large""
  num_cache_nodes      = 2
  parameter_group_name = ""default.memcached1.4""
  port                 = 11211
}","requires cluster id, engine=memcached, node_type, num_cache_nodes, parameter_group_name","I'll help you create a Terraform configuration for an AWS ElastiCache Memcached cluster. Here's a solution that includes the required provider and sets up a basic Memcached cluster with essential configurations:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create ElastiCache subnet group
resource ""aws_elasticache_subnet_group"" ""memcached_subnet_group"" {
  name       = ""memcached-subnet-group""
  subnet_ids = [""subnet-xxxxx"", ""subnet-yyyyy""]  # Replace with your subnet IDs
}

# Create ElastiCache parameter group
resource ""aws_elasticache_parameter_group"" ""memcached_params"" {
  family = ""memcached1.6""
  name   = ""memcached-parameters""

  parameter {
    name  = ""max_item_size""
    value = ""10485760""  # 10MB
  }
}

# Create ElastiCache security group
resource ""aws_security_group"" ""memcached_sg"" {
  name        = ""memcached-security-group""
  description = ""Security group for Memcached cluster""
  vpc_id      = ""vpc-xxxxx""  # Replace with your VPC ID

  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]  # Adjust CIDR based on your needs
  }
}

# Create Memcached cluster
resource ""aws_elasticache_cluster"" ""memcached_cluster"" {
  cluster_id           = ""memcached-cluster""
  engine              = ""memcached""
  node_type           = ""cache.t3.micro""
  num_cache_nodes     = 2
  port                = 11211
  
  parameter_group_name = aws_elasticache_parameter_group.memcached_params.name
  subnet_group_name    = aws_elasticache_subnet_group.memcached_subnet_group.name
  security_group_ids   = [aws_security_group.memcached_sg.id]
  
  az_mode             = ""cross-az""
  maintenance_window  = ""sun:05:00-sun:06:00""
  
  tags = {
    Environment = ""production""
    Name        = ""memcached-cluster""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a subnet group for the Memcached cluster
3. Creates a parameter group with custom settings
4. Sets up a security group for the cluster
5. Creates a Memcached cluster with:
   - 2 nodes
   - Cross-AZ deployment
   - t3.micro instance type
   - Maintenance window
   - Security group and subnet group associations

Remember to:
- Replace the subnet IDs with your actual subnet IDs
- Replace the VPC ID with your actual VPC ID
- Adjust the CIDR blocks in the security group to match your network requirements
- Modify the node type, number of nodes, and other parameters according to your needs

The cluster will be accessible on port 11211, which is the default Memcached port.",,,,,,"I'll help you create a Terraform configuration for an AWS ElastiCache Memcached cluster. Here's a complete configuration that includes the required provider and a basic Memcached cluster setup:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_elasticache_cluster"" ""memcached_cluster"" {
  cluster_id           = ""memcached-cluster""
  engine              = ""memcached""
  node_type           = ""cache.t3.micro""
  num_cache_nodes     = 2
  port                = 11211
  
  parameter_group_name = ""default.memcached1.6""
  
  maintenance_window   = ""sun:05:00-sun:06:00""
  az_mode             = ""cross-az""
  
  tags = {
    Environment = ""production""
    Name        = ""memcached-cluster""
  }
}

resource ""aws_elasticache_subnet_group"" ""memcached_subnet"" {
  name       = ""memcached-subnet-group""
  subnet_ids = [""subnet-12345678"", ""subnet-87654321""]  # Replace with your subnet IDs
}

resource ""aws_security_group"" ""memcached_sg"" {
  name        = ""memcached-security-group""
  description = ""Security group for Memcached cluster""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID

  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]  # Replace with your CIDR block
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""memcached-security-group""
  }
}
```

This configuration creates:
1. A Memcached cluster with 2 nodes
2. A subnet group for the cluster
3. A security group to control access to the cluster

Key features of this configuration:
- Uses t3.micro nodes (adjust based on your needs)
- Sets up cross-AZ deployment for high availability
- Configures maintenance window
- Uses the default Memcached 1.6 parameter group
- Sets up necessary security group rules for Memcached port (11211)

Remember to:
1. Replace the subnet IDs with your actual subnet IDs
2. Replace the VPC ID with your actual VPC ID
3. Adjust the CIDR blocks in the security group to match your network
4. Modify the node type and number of nodes based on your requirements
5. Adjust the region to match your deployment needs

You can customize the configuration further by adding additional parameters such as:
- apply_immediately
- notification_topic_arn
- snapshot_retention_limit
- snapshot_window
- additional tags",,,,,,"I'll help you create a Terraform configuration for an AWS ElastiCache Memcached cluster. Here's a solution that includes the required provider and creates a basic Memcached cluster with its necessary components:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_elasticache_cluster"" ""memcached_cluster"" {
  cluster_id           = ""memcached-cluster""
  engine              = ""memcached""
  node_type           = ""cache.t3.micro""
  num_cache_nodes     = 2
  parameter_group_name = aws_elasticache_parameter_group.memcached_params.name
  port                = 11211
  az_mode             = ""cross-az""
  security_group_ids  = [aws_security_group.memcached_sg.id]

  tags = {
    Name = ""memcached-cluster""
    Environment = ""production""
  }
}

resource ""aws_elasticache_parameter_group"" ""memcached_params"" {
  family = ""memcached1.6""
  name   = ""memcached-parameters""

  parameter {
    name  = ""max_item_size""
    value = ""10485760""
  }
}

resource ""aws_security_group"" ""memcached_sg"" {
  name        = ""memcached-security-group""
  description = ""Security group for Memcached cluster""
  vpc_id      = ""vpc-12345678"" # Replace with your VPC ID

  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""] # Replace with your CIDR block
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""memcached-sg""
  }
}
```

This configuration:
1. Creates a Memcached cluster with 2 nodes
2. Uses t3.micro instances
3. Sets up a custom parameter group
4. Creates a security group for the cluster
5. Configures the standard Memcached port (11211)
6. Enables cross-AZ deployment

Remember to:
- Replace the VPC ID with your actual VPC ID
- Adjust the CIDR blocks in the security group to match your network
- Modify the node type and number of nodes based on your requirements
- Update the tags according to your naming convention

The cluster will be accessible through the endpoint provided in the cluster's attributes after creation.",,,,,
aws_dax_parameter_group,Create a DAX parameter group.,"package main

import future.keywords.in

default allow = false

# Check for DAX parameter group creation
dax_parameter_group_created(resources) {
some resource in resources
resource.type == ""aws_dax_parameter_group""
resource.change.actions[_] == ""create""
}

# Allow DAX parameter group creation with specific parameters
allow {
dax_parameter_group_created(input.resource_changes)
}",2,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

resource ""aws_dax_parameter_group"" ""example"" {
name = ""example""

parameters {
name = ""query-ttl-millis""
value = ""100000""
}

parameters {
name = ""record-ttl-millis""
value = ""100000""
}
}","Has one ""aws_dax_parameter_group""","I'll help you create a Terraform configuration for an AWS DAX parameter group. DAX (DynamoDB Accelerator) parameter groups allow you to specify runtime parameters for DAX clusters.

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_dax_parameter_group"" ""example"" {
  name        = ""example-dax-parameter-group""
  description = ""Example DAX parameter group""

  parameters {
    name  = ""query-ttl-millis""
    value = ""100000""
  }

  parameters {
    name  = ""record-ttl-millis""
    value = ""100000""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a DAX parameter group with the name ""example-dax-parameter-group""
3. Includes two common parameters:
   - query-ttl-millis: Time-to-live for query results
   - record-ttl-millis: Time-to-live for individual records

You can modify the parameter values according to your needs. The parameter group can then be associated with a DAX cluster when you create one.

Remember to adjust the region and parameter values according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS DAX parameter group. A DAX parameter group allows you to specify runtime parameters for DAX clusters.

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_dax_parameter_group"" ""example"" {
  name        = ""example-dax-parameter-group""
  description = ""Example DAX parameter group""

  parameters {
    name  = ""query-ttl-millis""
    value = ""100000""
  }

  parameters {
    name  = ""record-ttl-millis""
    value = ""200000""
  }
}
```

This configuration:

1. Declares the AWS provider requirement
2. Creates a DAX parameter group with:
   - A name for the parameter group
   - A description
   - Two common parameters as examples:
     - query-ttl-millis: Time-to-live for query results
     - record-ttl-millis: Time-to-live for individual records

You can modify the parameter values according to your needs. The parameter group can then be associated with a DAX cluster when you create one.

Remember that DAX parameter groups help you manage the configuration of your DAX clusters in a consistent way across your infrastructure.",,,,,,"I'll help you create a DAX (DynamoDB Accelerator) parameter group using Terraform. Here's a code solution that demonstrates how to create a DAX parameter group with some basic configurations:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_dax_parameter_group"" ""example"" {
  name        = ""example-dax-parameter-group""
  description = ""Example DAX parameter group""

  parameters {
    name  = ""query-ttl-millis""
    value = ""100000""
  }

  parameters {
    name  = ""record-ttl-millis""
    value = ""100000""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a DAX parameter group with a name and description
3. Configures two common DAX parameters:
   - query-ttl-millis: Time-to-live for query results
   - record-ttl-millis: Time-to-live for individual records

You can modify the parameter values according to your specific requirements. The parameter group can then be associated with a DAX cluster when you create one.

Note that DAX parameter groups are similar to DynamoDB parameter groups but are specifically for DAX clusters. The parameters allow you to tune the caching behavior of your DAX cluster.",,,,,
"aws_s3_bucket, aws_s3_bucket_notification, aws_sns_topic, aws_iam_policy_document",Create an IAM policy document that allows the 's3.amazonaws.com' service to publish to an SNS topic for S3 event notifications. Specify the actions as 'SNS:Publish' and set the resource to the ARN of the SNS topic 's3-event-notification-topic'. Apply a condition to allow this action only if the source ARN is similar to the ARN of the specified S3 bucket. Create an SNS topic named 's3-event-notification-topic' with the generated IAM policy. Set up an S3 bucket named 'your-bucket-name' and configure it to send notifications to the SNS topic for 's3:ObjectCreated:*' events with a filter for '.log' suffix.,"package terraform.validation

default has_aws_iam_policy_document_topic = false
default has_aws_sns_topic = false
default has_aws_s3_bucket = false
default has_aws_s3_bucket_notification = false

has_aws_iam_policy_document_topic {
    policy := input.planned_values.root_module.resources[_]
    policy.type == ""aws_iam_policy_document""
}

has_aws_sns_topic {
    topic := input.planned_values.root_module.resources[_]
    topic.type == ""aws_sns_topic""
    topic.name == ""topic""
    topic.values.name == ""s3-event-notification-topic""
    topic.values.policy == input.planned_values.root_module.resources[_].values.json  # Validate policy is correctly used from data source
}

has_aws_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""bucket""
    bucket.values.bucket == ""your-bucket-name""
}

has_aws_s3_bucket_notification {
    notification := input.planned_values.root_module.resources[_]
    notification.type == ""aws_s3_bucket_notification""
    notification.name == ""bucket_notification""
    notification.values.bucket == input.planned_values.root_module.resources[_].values.id  # Correct bucket ID reference
    notification.values.topic.topic_arn == input.planned_values.root_module.resources[_].values.arn  # Correct topic ARN reference
    notification.values.topic.events[0] == ""s3:ObjectCreated:*""
    notification.values.topic.filter_suffix == "".log""
}

valid_configuration {
    has_aws_iam_policy_document_topic
    has_aws_sns_topic
    has_aws_s3_bucket
    has_aws_s3_bucket_notification
}
",3,"provider ""aws"" {
    region = ""us-west-1""
}

data ""aws_iam_policy_document"" ""topic"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""s3.amazonaws.com""]
    }

    actions   = [""SNS:Publish""]
    resources = [""arn:aws:sns:*:*:s3-event-notification-topic""]

    condition {
      test     = ""ArnLike""
      variable = ""aws:SourceArn""
      values   = [aws_s3_bucket.bucket.arn]
    }
  }
}
resource ""aws_sns_topic"" ""topic"" {
  name   = ""s3-event-notification-topic""
  policy = data.aws_iam_policy_document.topic.json
}

resource ""aws_s3_bucket"" ""bucket"" {
  bucket = ""your-bucket-name""
}

resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.bucket.id

  topic {
    topic_arn     = aws_sns_topic.topic.arn
    events        = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }
}","Data ""aws_iam_policy_document""
has one ""effect""
has one ""principals"" 
has one ""actions""
has one ""resources""

Resource ""aws_sns_topic"" 
has one ""name""
uses the JSON output

Resource ""aws_s3_bucket"" 
has one ""bucket""

Resource ""aws_s3_bucket_notification"" 
has one ""bucket""
configures a ""topic"" notification with:
""topic_arn""
""events""
""filter_suffix""","I'll help you create a Terraform configuration that sets up an S3 bucket with SNS notifications, including the necessary IAM policy and SNS topic. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

# Create the SNS Topic
resource ""aws_sns_topic"" ""s3_notification_topic"" {
  name = ""s3-event-notification-topic""
}

# Create IAM policy document for S3 to SNS permissions
data ""aws_iam_policy_document"" ""s3_sns_policy"" {
  statement {
    effect = ""Allow""
    principals {
      type        = ""Service""
      identifiers = [""s3.amazonaws.com""]
    }
    actions   = [""SNS:Publish""]
    resources = [aws_sns_topic.s3_notification_topic.arn]

    condition {
      test     = ""ArnLike""
      variable = ""aws:SourceArn""
      values   = [""arn:aws:s3:*:*:your-bucket-name""]
    }
  }
}

# Attach the policy to the SNS topic
resource ""aws_sns_topic_policy"" ""default"" {
  arn    = aws_sns_topic.s3_notification_topic.arn
  policy = data.aws_iam_policy_document.s3_sns_policy.json
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""notification_bucket"" {
  bucket = ""your-bucket-name""
}

# Enable versioning (recommended for production use)
resource ""aws_s3_bucket_versioning"" ""bucket_versioning"" {
  bucket = aws_s3_bucket.notification_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Configure S3 bucket notification
resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.notification_bucket.id

  topic {
    topic_arn = aws_sns_topic.s3_notification_topic.arn
    events    = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }

  depends_on = [aws_sns_topic_policy.default]
}
```

This configuration:
1. Creates an SNS topic named 's3-event-notification-topic'
2. Creates an IAM policy document that allows S3 to publish to the SNS topic
3. Attaches the policy to the SNS topic
4. Creates an S3 bucket named 'your-bucket-name'
5. Enables versioning on the bucket (best practice)
6. Configures the bucket to send notifications to the SNS topic for object creation events with .log suffix

Make sure to replace ""your-bucket-name"" with your desired bucket name and adjust the region if needed. The configuration includes proper dependency management and follows AWS best practices for S3 bucket notifications.",,,,,,"I'll help you create a Terraform configuration that sets up an S3 bucket with SNS notifications and the necessary IAM policies. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create SNS Topic
resource ""aws_sns_topic"" ""s3_notification_topic"" {
  name = ""s3-event-notification-topic""
}

# Create IAM Policy Document
data ""aws_iam_policy_document"" ""sns_topic_policy"" {
  statement {
    effect = ""Allow""
    
    principals {
      type = ""Service""
      identifiers = [""s3.amazonaws.com""]
    }

    actions = [""SNS:Publish""]
    
    resources = [aws_sns_topic.s3_notification_topic.arn]

    condition {
      test     = ""ArnLike""
      variable = ""aws:SourceArn""
      values   = [""arn:aws:s3:*:*:your-bucket-name""]
    }
  }
}

# Attach Policy to SNS Topic
resource ""aws_sns_topic_policy"" ""default"" {
  arn    = aws_sns_topic.s3_notification_topic.arn
  policy = data.aws_iam_policy_document.sns_topic_policy.json
}

# Create S3 Bucket
resource ""aws_s3_bucket"" ""notification_bucket"" {
  bucket = ""your-bucket-name""
}

# Create S3 Bucket Notification
resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.notification_bucket.id

  topic {
    topic_arn = aws_sns_topic.s3_notification_topic.arn
    events    = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }

  depends_on = [aws_sns_topic_policy.default]
}
```

This configuration:
1. Creates an SNS topic named ""s3-event-notification-topic""
2. Creates an IAM policy document that allows S3 to publish to the SNS topic
3. Attaches the policy to the SNS topic
4. Creates an S3 bucket
5. Sets up bucket notifications for object creation events with a .log suffix

The policy includes a condition to ensure the action is only allowed from the specified S3 bucket, and all resources are properly linked together using Terraform references.

Important notes:
- Replace ""your-bucket-name"" with your desired bucket name
- The `depends_on` block ensures the SNS topic policy is created before the bucket notification
- The configuration uses the AWS provider for us-east-1 region (modify as needed)
- The filter is set up to only trigger for files ending in "".log""",,,,,,"I'll help you create a Terraform configuration that sets up an S3 bucket with SNS notifications and the necessary IAM policies. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create SNS Topic
resource ""aws_sns_topic"" ""s3_notification_topic"" {
  name = ""s3-event-notification-topic""
}

# Create IAM Policy Document for SNS
data ""aws_iam_policy_document"" ""sns_topic_policy"" {
  statement {
    effect = ""Allow""
    
    principals {
      type        = ""Service""
      identifiers = [""s3.amazonaws.com""]
    }

    actions = [""SNS:Publish""]
    
    resources = [aws_sns_topic.s3_notification_topic.arn]

    condition {
      test     = ""ArnLike""
      variable = ""aws:SourceArn""
      values   = [""arn:aws:s3:::your-bucket-name""]
    }
  }
}

# Attach policy to SNS topic
resource ""aws_sns_topic_policy"" ""default"" {
  arn    = aws_sns_topic.s3_notification_topic.arn
  policy = data.aws_iam_policy_document.sns_topic_policy.json
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""notification_bucket"" {
  bucket = ""your-bucket-name""
}

# Enable versioning for the bucket
resource ""aws_s3_bucket_versioning"" ""bucket_versioning"" {
  bucket = aws_s3_bucket.notification_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Configure S3 bucket notification
resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.notification_bucket.id

  topic {
    topic_arn = aws_sns_topic.s3_notification_topic.arn
    events    = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }

  depends_on = [aws_sns_topic_policy.default]
}
```

This configuration:
1. Creates an SNS topic named 's3-event-notification-topic'
2. Creates an IAM policy document that allows S3 to publish to the SNS topic
3. Attaches the policy to the SNS topic
4. Creates an S3 bucket named 'your-bucket-name'
5. Enables versioning on the bucket
6. Configures the bucket to send notifications to the SNS topic for object creation events with '.log' suffix

The configuration includes all necessary dependencies and follows AWS best practices for S3 bucket notifications. Make sure to replace 'your-bucket-name' with your desired bucket name before applying the configuration.",,,,,
aws_db_instance,Create a PostgresQL instance.,"package main

import future.keywords.in

default allow = false

# Check for DB instance creation
db_instance_created(resources) {
some resource in resources
resource.type == ""aws_db_instance""
resource.change.actions[_] == ""create""
}

# Check if engine is MySQL
db_engine_mysql(resource) {
resource.type == ""aws_db_instance""
resource.change.after.engine == ""postgres""
}

# Aggregate checks for DB instance
allow {
db_instance_created(input.resource_changes)
some resource in input.resource_changes
db_engine_mysql(resource)
}",3,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

# Configure the AWS Provider
provider ""aws"" {
region = ""us-east-1""
}

resource ""aws_db_instance"" ""default"" {
allocated_storage = 10
db_name = ""mydb""
engine = ""postgres""
instance_class = ""db.t3.micro""
username = ""USERNAME""
password = ""PASSWORD""
skip_final_snapshot = true
}","Has one ""aws_db_instance"" resource
with one ""engine"" = ""postgres""","I'll help you create a PostgreSQL RDS instance using Terraform. Here's a configuration that sets up a basic PostgreSQL RDS instance with common best practices:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_db_instance"" ""postgresql"" {
  identifier           = ""my-postgresql-db""
  engine              = ""postgres""
  engine_version      = ""13.7""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username           = ""dbadmin""
  password           = ""YourSecurePassword123!"" # In production, use secrets management

  # Network settings
  publicly_accessible    = false
  multi_az              = false
  
  # Backup settings
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  # Enhanced monitoring
  monitoring_interval = 0
  
  # Security settings
  storage_encrypted     = true
  skip_final_snapshot  = true

  # Performance insights
  performance_insights_enabled = false

  tags = {
    Environment = ""production""
    Name        = ""postgresql-instance""
  }
}
```

This configuration includes:
1. Required AWS provider configuration
2. A PostgreSQL RDS instance with:
   - Basic instance settings (size, storage, engine version)
   - Security settings (encryption)
   - Backup configuration
   - Maintenance windows
   - Basic monitoring settings
   - Tags for resource management

Remember to:
- Change the password to a secure value and preferably use AWS Secrets Manager
- Adjust the instance class based on your workload requirements
- Consider enabling Multi-AZ deployment for production workloads
- Modify the backup retention period based on your needs
- Add a security group and subnet group if needed
- Adjust other parameters like storage size based on your requirements

Note: This is a basic configuration. For production use, you might want to add additional security measures, networking configurations, and parameter groups.",,,,,,"I'll help you create a PostgreSQL RDS instance using Terraform. Here's a configuration that sets up a basic PostgreSQL RDS instance with common security settings:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create a security group for the RDS instance
resource ""aws_security_group"" ""postgres_sg"" {
  name        = ""postgres-security-group""
  description = ""Security group for PostgreSQL RDS instance""

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]  # Note: In production, restrict this to specific IPs
  }
}

# Create a subnet group for RDS
resource ""aws_db_subnet_group"" ""postgres_subnet_group"" {
  name       = ""postgres-subnet-group""
  subnet_ids = [""subnet-xxxxxx"", ""subnet-yyyyyy""]  # Replace with your subnet IDs
}

# Create the RDS instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgresql-instance""
  engine              = ""postgres""
  engine_version      = ""13.7""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mypostgresdb""
  username           = ""dbadmin""
  password           = ""your-secure-password""  # Use variables or secrets manager in production
  
  vpc_security_group_ids = [aws_security_group.postgres_sg.id]
  db_subnet_group_name   = aws_db_subnet_group.postgres_subnet_group.name
  
  publicly_accessible    = false
  skip_final_snapshot   = true
  
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""
  
  tags = {
    Name        = ""PostgreSQL Instance""
    Environment = ""Production""
  }
}

# Output the endpoint
output ""db_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Creates a security group allowing PostgreSQL traffic (port 5432)
2. Sets up a subnet group for the RDS instance
3. Creates a PostgreSQL RDS instance with:
   - 20GB GP2 storage
   - db.t3.micro instance type (modify for production needs)
   - Basic backup and maintenance windows
   - Tags for better resource management
4. Outputs the database endpoint

Remember to:
- Replace the subnet IDs with your actual subnet IDs
- Use a stronger password in production
- Consider using AWS Secrets Manager for credentials
- Adjust the security group ingress rules according to your security requirements
- Modify the instance class and storage based on your workload needs",,,,,,"I'll help you create a PostgreSQL RDS instance using Terraform. Here's a configuration that sets up a basic PostgreSQL RDS instance with common security settings:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_db_instance"" ""postgresql"" {
  identifier           = ""my-postgresql-db""
  engine              = ""postgres""
  engine_version      = ""13.7""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""YourSecurePassword123!""  # Consider using variables or secrets manager
  
  skip_final_snapshot = true
  
  # Network settings
  publicly_accessible    = false
  vpc_security_group_ids = [aws_security_group.postgresql_sg.id]
  
  # Backup settings
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  # Enhanced monitoring
  monitoring_interval = 60
  
  tags = {
    Name        = ""postgresql-instance""
    Environment = ""production""
  }
}

resource ""aws_security_group"" ""postgresql_sg"" {
  name        = ""postgresql-security-group""
  description = ""Security group for PostgreSQL RDS instance""

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]  # Replace with your VPC CIDR
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""postgresql-sg""
  }
}
```

This configuration:
1. Creates a PostgreSQL RDS instance with 20GB storage
2. Uses a t3.micro instance type (suitable for dev/test, adjust for production)
3. Sets up basic backup and maintenance windows
4. Creates a security group allowing PostgreSQL traffic (port 5432)
5. Includes basic monitoring settings
6. Sets up tags for better resource management

Remember to:
- Change the password to something secure
- Adjust the CIDR blocks in the security group to match your network
- Modify the instance class based on your workload requirements
- Consider using AWS Secrets Manager for database credentials
- Adjust the storage size based on your needs
- Change the region if needed",,,,,
"aws_glacier_vault, aws_sns_topic, aws_iam_policy_document",generate an S3 Glacier vault with notification service,"package terraform.validation

default is_valid_terraform_plan = false

# Rule to check for an AWS Glacier Vault resource with 'name' and 'notification' attributes
is_valid_glacier_vault {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_glacier_vault""
	not is_null(resource.name)
	not is_null(resource.expressions)
	not is_null(resource.expressions.notification[0].sns_topic)
	not is_null(resource.expressions.notification[0].events)
}


# Rule to check for the existence of an AWS SNS Topic resource
is_valid_sns_topic {
	sns_topic_resource := input.planned_values.root_module.resources[_]
	sns_topic_resource.type == ""aws_sns_topic""
}

# Combined rule to validate the entire Terraform plan
is_valid_terraform_plan {
	is_valid_glacier_vault
	is_valid_sns_topic
}",3,"resource ""aws_sns_topic"" ""aws_sns_topic"" {
  name = ""glacier-sns-topic""
}

data ""aws_iam_policy_document"" ""my_archive"" {
  statement {
    sid    = ""add-read-only-perm""
    effect = ""Allow""

    principals {
      type        = ""*""
      identifiers = [""*""]
    }

    actions = [
      ""glacier:InitiateJob"",
      ""glacier:GetJobOutput"",
    ]

    resources = [""arn:aws:glacier:eu-west-1:432981146916:vaults/MyArchive""]
  }
}

resource ""aws_glacier_vault"" ""my_archive"" {
  name = ""MyArchive""

  notification {
    sns_topic = aws_sns_topic.aws_sns_topic.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  access_policy = data.aws_iam_policy_document.my_archive.json

  tags = {
    Test = ""MyArchive""
  }
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute
     with ""notification"" attribute
          with ""event"" attribute
          with ""sns_topic"" attribute

with ""aws_sns_topic"" resource","I'll help you create a Terraform configuration for an S3 Glacier vault with notification service. This will include the AWS Glacier vault and its notification configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Configure the AWS Provider
provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  tags = {
    Environment = ""Production""
    Purpose     = ""Long-term-storage""
  }
}

# Create SNS Topic for notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-vault-notifications""
}

# SNS Topic Policy
resource ""aws_sns_topic_policy"" ""glacier_notification_policy"" {
  arn = aws_sns_topic.glacier_notifications.arn

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""GlacierNotifications""
        Effect = ""Allow""
        Principal = {
          Service = ""glacier.amazonaws.com""
        }
        Action   = ""SNS:Publish""
        Resource = aws_sns_topic.glacier_notifications.arn
      }
    ]
  })
}

# Optional: Create an SNS Topic subscription (e.g., to receive email notifications)
resource ""aws_sns_topic_subscription"" ""glacier_notification_subscription"" {
  topic_arn = aws_sns_topic.glacier_notifications.arn
  protocol  = ""email""  # Can be: sqs, sms, lambda, email, etc.
  endpoint  = ""your-email@example.com""  # Replace with your endpoint
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a Glacier vault with notification configuration
3. Creates an SNS topic to receive notifications
4. Sets up the necessary SNS topic policy to allow Glacier to publish notifications
5. Optionally creates a subscription to receive notifications

Key features:
- The vault will send notifications for archive and inventory retrieval completions
- The SNS topic policy allows Glacier to publish notifications
- You can customize the notification events based on your needs
- Tags are included for better resource management
- The subscription can be modified to use different protocols (SMS, SQS, Lambda, etc.)

Remember to:
1. Replace the region with your desired AWS region
2. Update the email endpoint in the subscription resource
3. Modify the vault name and tags according to your requirements
4. Adjust the notification events based on your needs

The supported notification events for Glacier include:
- ArchiveRetrievalCompleted
- InventoryRetrievalCompleted
- VaultLockComplete",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault with notifications. Here's a complete solution that includes the Glacier vault creation and SNS notification configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create an SNS topic for notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-vault-notifications""
}

# Create the Glacier vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  tags = {
    Environment = ""Production""
    Purpose     = ""Long-term-storage""
  }
}

# SNS topic policy to allow Glacier to publish notifications
resource ""aws_sns_topic_policy"" ""glacier_notification_policy"" {
  arn = aws_sns_topic.glacier_notifications.arn

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""GlacierNotifications""
        Effect = ""Allow""
        Principal = {
          Service = ""glacier.amazonaws.com""
        }
        Action   = ""SNS:Publish""
        Resource = aws_sns_topic.glacier_notifications.arn
      }
    ]
  })
}

# Optional: Create a vault access policy
resource ""aws_glacier_vault_lock"" ""vault_lock"" {
  vault_name = aws_glacier_vault.example_vault.name
  complete_lock = false  # Set to true for immutable policy

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""enforce-secure-transport""
        Effect = ""Deny""
        Principal = {
          AWS = ""*""
        }
        Action   = ""glacier:*""
        Resource = aws_glacier_vault.example_vault.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"": ""false""
          }
        }
      }
    ]
  })
}
```

This configuration:

1. Creates an S3 Glacier vault with notifications enabled
2. Sets up an SNS topic to receive the notifications
3. Configures the necessary SNS topic policy to allow Glacier to publish notifications
4. Includes an optional vault lock policy that enforces secure transport
5. Sets up notifications for archive and inventory retrieval events

Key features:
- The vault will send notifications when archive retrievals and inventory retrievals are completed
- The SNS topic policy allows Glacier to publish notifications
- The vault lock policy ensures that all access to the vault must use secure transport (HTTPS)
- Tags are included for better resource management
- The configuration includes all necessary permissions and policies

You can customize this further by:
- Adding more event types to the notification configuration
- Modifying the vault lock policy
- Adding additional tags
- Adjusting the region in the provider block
- Adding more specific access policies

Remember to replace the name values with your desired names and adjust the policies according to your security requirements.",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault with notification service. This will include the required AWS provider and the necessary resources for the Glacier vault and its notifications.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2"" # Change this to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  tags = {
    Environment = ""Production""
    Purpose     = ""Long-term-storage""
  }
}

# Create SNS Topic for notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-vault-notifications""
}

# SNS Topic Policy
resource ""aws_sns_topic_policy"" ""glacier_notification_policy"" {
  arn = aws_sns_topic.glacier_notifications.arn

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""GlacierNotificationPolicy""
        Effect = ""Allow""
        Principal = {
          Service = ""glacier.amazonaws.com""
        }
        Action   = ""SNS:Publish""
        Resource = aws_sns_topic.glacier_notifications.arn
      }
    ]
  })
}

# Optional: Create an SNS Topic subscription (e.g., to receive email notifications)
resource ""aws_sns_topic_subscription"" ""glacier_notification_subscription"" {
  topic_arn = aws_sns_topic.glacier_notifications.arn
  protocol  = ""email""
  endpoint  = ""your-email@example.com""  # Replace with your email address
}
```

This configuration creates:
1. A Glacier vault with notification settings
2. An SNS topic to receive the notifications
3. An SNS topic policy that allows Glacier to publish notifications
4. An optional SNS topic subscription for email notifications

Key features:
- The Glacier vault is configured to send notifications for archive and inventory retrieval completions
- The SNS topic is properly configured with the necessary permissions for Glacier
- Tags are added to the vault for better resource management
- Email notifications can be enabled by updating the endpoint in the SNS subscription

Remember to:
1. Replace the region with your desired AWS region
2. Update the email address in the SNS subscription if you want to receive email notifications
3. Modify the vault name and tags according to your needs
4. Adjust the notification events based on your requirements (available events include: ArchiveRetrievalCompleted, InventoryRetrievalCompleted)",,,,,
"aws_route53_record, aws_route53_zone","Set up a non-alias aws_route_53 record that is linked to an aws_route53_zone resource named ""example53.com""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
	some i
	resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}

# Validate aws_route53_record
is_valid_r53_record {
	some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.example53.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""example53_A"" {
  zone_id = aws_route53_zone.example53.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""300""
  records = [""192.0.2.1""]  
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
","I'll help you create a Terraform configuration for setting up an AWS Route 53 record that is linked to a Route 53 zone for ""example53.com"". This will include both the zone and record resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create the Route 53 zone
resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""
}

# Create the Route 53 record
resource ""aws_route53_record"" ""example"" {
  zone_id = aws_route53_zone.primary.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = 300
  records = [""10.0.0.1""]  # Replace with your actual IP address
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates a Route 53 hosted zone for ""example53.com""
3. Creates an A record in the zone (you can adjust the record type, TTL, and IP address as needed)
4. Links the record to the zone using the zone_id reference

Note: You'll need to replace the IP address (""10.0.0.1"") with your actual target IP address. Also, you can modify the record type (A, CNAME, MX, etc.) based on your specific needs.",,,,,,"I'll help you create a Terraform configuration for setting up an AWS Route 53 record that's linked to a Route 53 zone for ""example53.com"". I'll include both the zone and record resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create the Route 53 Zone
resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""
}

# Create the Route 53 Record
resource ""aws_route53_record"" ""primary"" {
  zone_id = aws_route53_zone.primary.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = 300
  records = [""10.0.0.1""]  # Replace with your actual IP address
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates a Route 53 hosted zone for ""example53.com""
3. Creates an A record in that zone pointing to a specified IP address
4. Uses the zone_id from the created zone to link the record to the correct zone

Note: You can modify the record type (currently ""A""), TTL (currently 300 seconds), and records array based on your specific needs. The example uses a simple A record pointing to an IP address, but you can adjust these values according to your requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS Route 53 record that links to a Route 53 zone for ""example53.com"". I'll include both the zone and record resources, along with the required provider block.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

# Create the Route 53 zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
}

# Create the Route 53 record
resource ""aws_route53_record"" ""main"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = 300
  records = [""1.2.3.4""]  # Replace with your actual IP address
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Route 53 hosted zone for ""example53.com""
3. Creates an A record in that zone pointing to a specified IP address
4. Links the record to the zone using the zone_id reference

You can modify the record type, TTL, and records values according to your specific needs. This example uses an A record, but you can change the `type` field to other valid DNS record types like CNAME, MX, TXT, etc.",,,,,
"aws_vpc, aws_vpc_dhcp_options, aws_vpc_dhcp_options_association","sets up a basic AWS network infrastructure consisting of a Virtual Private Cloud (VPC) with a custom DHCP options set. The VPC is configured with a CIDR block of 192.168.0.0/16. The DHCP options include a domain name (windomain.local), a combination of custom and Google's DNS servers (192.168.56.102 and 8.8.8.8), and a NetBIOS name server. These DHCP options are then associated with the created VPC, customizing its DHCP settings.






","package terraform.validation

# Set default validation states
default is_valid_vpc = false

default is_valid_dhcp_options = false

default is_valid_dhcp_options_association = false

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value == ""192.168.0.0/16""
}

# Validate aws_vpc_dhcp_options resource
is_valid_dhcp_options {
        some i
        resource := input.planned_values.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options""
        resource.values.domain_name == ""windomain.local""
    resource.values.domain_name_servers == [""192.168.56.102"", ""8.8.8.8""]
        resource.values.netbios_name_servers != null
}

# Validate aws_vpc_dhcp_options_association resource
is_valid_dhcp_options_association {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options_association""
        resource.expressions.dhcp_options_id.references[0] == ""aws_vpc_dhcp_options.default.id""
        resource.expressions.vpc_id.references[0] == ""aws_vpc.default.id""
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""default"" {
  cidr_block = ""192.168.0.0/16""
}

resource ""aws_vpc_dhcp_options"" ""default"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = concat([""192.168.56.102""], [""8.8.8.8""])
  netbios_name_servers = [""192.168.56.102""]
}

resource ""aws_vpc_dhcp_options_association"" ""default"" {
  vpc_id          = aws_vpc.default.id
  dhcp_options_id = aws_vpc_dhcp_options.default.id
}","Has one ""aws_vpc"" resource:
Configured with a ""cidr_block"" of ""192.168.0.0/16""
Includes associated custom DHCP options

Has one ""aws_vpc_dhcp_options"" resource:
Specifies a ""domain_name"" of ""windomain.local""
Includes DNS servers with a combination of a custom DNS server (""192.168.56.102"") and Google's DNS server (""8.8.8.8"")
Configures a ""NetBIOS"" name server

Has one ""aws_vpc_dhcp_options_association"" resource:
Associates the ""aws_vpc_dhcp_options"" with the created ""aws_vpc""
Customizes DHCP settings specifically for the created VPC environment","I'll help you create a Terraform configuration that sets up a VPC with custom DHCP options as specified. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # You can change this to your desired region
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""dhcp_association"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a custom DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The configuration enables both DNS hostnames and DNS support in the VPC for proper name resolution. All resources are properly tagged for easy identification. The DHCP options will be applied to all instances launched in this VPC.",,,,,,"I'll help you create a Terraform configuration that sets up a VPC with custom DHCP options based on your requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""dhcp_association"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with CIDR block 192.168.0.0/16 and enables DNS support
3. Creates a custom DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The configuration enables proper name resolution and NetBIOS services within your VPC using both custom and public DNS servers. All resources are properly tagged for better resource management.",,,,,,"I'll help you create a Terraform configuration for setting up a VPC with custom DHCP options as specified. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""dns_resolver"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a custom DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The configuration enables DNS hostnames and DNS support in the VPC for proper name resolution. All resources are properly tagged for better resource management.",,,,,
"aws_elastic_beanstalk_application, aws_elastic_beanstalk_application_version, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment, aws_s3_bucket, aws_s3_object, aws_sqs_queue","Create an elastic beanstalk worker environment for running batch processing jobs. Name the iam_role ""eb_ec2_role"", the instance profile ""eb_ec2_profile"", the elastic beanstalk application ""batch_job_app"", the bucket ""sampleapril26426"", the object ""examplebucket_object"", the sqs queue ""batch_job_queue"", and the application version ""version"".","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_s3_bucket = false

default is_valid_s3_object = false

default is_valid_eb_app = false

default is_valid_eb_env = false

is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0] == ""aws_iam_role.eb_ec2_role.name""
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0] == ""aws_iam_role.eb_ec2_role.name""
}

is_valid_s3_bucket {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    resource.name
    resource.expressions.bucket.constant_value == ""sampleapril26426""
}

is_valid_s3_object {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_s3_object""
    resource.expressions.bucket.references[0] == ""aws_s3_bucket.sampleapril26426.id""
    resource.expressions.key
    resource.expressions.source
    
}

is_valid_sqs_queue {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_sqs_queue""
    resource.expressions.name
}

# Validate aws_eb_app
is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}

is_valid_eb_app_version {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application_version""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.batch_job_app.name""
    resource.expressions.bucket.references[0] == ""aws_s3_object.examplebucket_object.bucket""
    resource.expressions.key.references[0] == ""aws_s3_object.examplebucket_object.key""
}
# Validate aws_eb_env
is_valid_eb_env {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.batch_job_app.name""
    resource.expressions.solution_stack_name
    resource.expressions.tier.constant_value == ""Worker""
    resource.expressions.version_label.references[0] == ""aws_elastic_beanstalk_application_version.version.name""
            some a, b
    resource.expressions.setting[a].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource.expressions.setting[b].value.references[0] == ""aws_sqs_queue.batch_job_queue.id""
}


# Combine all checks into a final rule
is_configuration_valid {
        is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
    is_valid_s3_bucket
    is_valid_s3_object 
    is_valid_sqs_queue
    is_valid_eb_app_version
    is_valid_eb_app
    is_valid_eb_env
}
",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region = ""us-east-1"" 
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}


resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}


# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}


resource ""aws_s3_bucket"" ""sample_bucket"" {
  bucket_prefix = ""sample-""
}

resource ""aws_s3_object"" ""examplebucket_object"" {
  key    = ""randofile""
  bucket = aws_s3_bucket.sample_bucket.id
  source = ""./supplement/app.zip""
}

# Define the Elastic Beanstalk application
resource ""aws_elastic_beanstalk_application"" ""batch_job_app"" {
  name        = ""batch-job-application""
}

# Define the Elastic Beanstalk application version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""batch-job-app-version""
  application = aws_elastic_beanstalk_application.batch_job_app.name
  bucket = aws_s3_object.examplebucket_object.bucket
  key    = aws_s3_object.examplebucket_object.key
}

# Create an Elastic Beanstalk environment in the worker tier
resource ""aws_elastic_beanstalk_environment"" ""batch_job_env"" {
  name                = ""batch-job-environment""
  application         = aws_elastic_beanstalk_application.batch_job_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  tier                = ""Worker""
  version_label       = aws_elastic_beanstalk_application_version.version.name

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""SingleInstance"" 
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""WorkerQueueURL""
    value     = aws_sqs_queue.batch_job_queue.id 
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""HttpPath""
    value     = ""/""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""MimeType""
    value     = ""application/json"" 
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Define the SQS queue
resource ""aws_sqs_queue"" ""batch_job_queue"" {
  name = ""batch-job-queue""
}
","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has one ""aws_elastic_beanstalk_application"" resource
    with ""name"" 

Has one ""aws_s3_bucket"" resource
    with ""bucket""

Has one ""aws_s3_object"" resource
    with ""key""
    with ""source""
    with ""bucket"" referencing the ""aws_s3_bucket"" resource

Has one ""aws_elastic_beanstalk_application_version"" resource
    with ""name
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""bucket"" referencing the ""bucket"" from the ""aws_s3_object"" resource
    with ""key"" referencing the ""key"" from the ""aws_s3_object"" resource

Has one ""aws_sqs_queue"" resource
    with ""name""

Has one ""aws_elastic_beanstalk_environment"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with ""tier"" set to ""Worker""
    with ""version_label"" referencing the ""aws_elastic_beanstalk_application_version"" resource
    with several ""setting"" blocks that have ""namespaces"" set to either ""aws:elasticbeanstalk:environment"" or ""aws:elasticbeanstalk:sqsd"", 
    with one ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource
","I'll help you create a Terraform configuration for an Elastic Beanstalk worker environment with the specified components. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Instance Profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# SQS Queue
resource ""aws_sqs_queue"" ""batch_job_queue"" {
  name = ""batch_job_queue""
}

# S3 Bucket
resource ""aws_s3_bucket"" ""application_bucket"" {
  bucket = ""sampleapril26426""
}

# S3 Bucket Object
resource ""aws_s3_object"" ""app_version"" {
  bucket = aws_s3_bucket.application_bucket.id
  key    = ""examplebucket_object""
  source = ""path/to/your/application.zip""  # Replace with your application source
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""batch_app"" {
  name        = ""batch_job_app""
  description = ""Batch processing application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""batch_app_version"" {
  name        = ""version""
  application = aws_elastic_beanstalk_application.batch_app.name
  description = ""Application version created by terraform""
  bucket      = aws_s3_bucket.application_bucket.id
  key         = aws_s3_object.app_version.key
}

# Elastic Beanstalk Worker Environment
resource ""aws_elastic_beanstalk_environment"" ""worker_environment"" {
  name                = ""worker-environment""
  application         = aws_elastic_beanstalk_application.batch_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v3.5.4 running Docker""  # Update with desired solution stack
  tier                = ""Worker""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""WorkerQueueURL""
    value     = aws_sqs_queue.batch_job_queue.id
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }
}
```

This configuration creates:
1. An IAM role and instance profile for EC2 instances
2. An SQS queue for batch job processing
3. An S3 bucket and object for application versions
4. An Elastic Beanstalk application and version
5. A worker environment configured to process messages from the SQS queue

Note: 
- Replace `""path/to/your/application.zip""` with the actual path to your application source code
- Update the `solution_stack_name` to match your desired platform version
- You may need to add additional",,,,,,"I'll help you create a Terraform configuration for an Elastic Beanstalk worker environment with the specified components. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM role for EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# Instance profile for EC2 instances
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# S3 bucket for application versions
resource ""aws_s3_bucket"" ""app_bucket"" {
  bucket = ""sampleapril26426""
}

# S3 bucket object for application code
resource ""aws_s3_object"" ""app_code"" {
  bucket = aws_s3_bucket.app_bucket.id
  key    = ""examplebucket_object""
  source = ""path/to/your/application.zip""  # Replace with actual application code path
}

# SQS queue for batch jobs
resource ""aws_sqs_queue"" ""batch_queue"" {
  name = ""batch_job_queue""
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""batch_app"" {
  name        = ""batch_job_app""
  description = ""Batch processing application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""batch_app_version"" {
  name        = ""version""
  application = aws_elastic_beanstalk_application.batch_app.name
  description = ""application version created by terraform""
  bucket      = aws_s3_bucket.app_bucket.id
  key         = aws_s3_object.app_code.key
}

# Elastic Beanstalk Worker Environment
resource ""aws_elastic_beanstalk_environment"" ""worker_environment"" {
  name                = ""worker-environment""
  application         = aws_elastic_beanstalk_application.batch_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v3.5.3 running Docker""  # Update with desired solution stack
  tier                = ""Worker""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""WorkerQueueURL""
    value     = aws_sqs_queue.batch_queue.id
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }
}
```

This configuration creates:
1. An IAM role and instance profile for EC2 instances
2. An S3 bucket and object for application code
3. An SQS queue for batch jobs
4. An Elastic Beanstalk application and version
5. A worker environment configured with the specified components

Note: You'll need to:
1. Replace the region if you want to use a different one
2. Update the solution_stack_name to match your desired platform
3. Specify the actual path to your application code for the S3 ",,,,,,"I'll help you create a Terraform configuration for an Elastic Beanstalk worker environment with the specified components. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Instance Profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# S3 Bucket
resource ""aws_s3_bucket"" ""app_bucket"" {
  bucket = ""sampleapril26426""
}

# S3 Bucket Object
resource ""aws_s3_object"" ""app_version"" {
  bucket = aws_s3_bucket.app_bucket.id
  key    = ""examplebucket_object""
  source = ""application.zip""  # Make sure this file exists
}

# SQS Queue
resource ""aws_sqs_queue"" ""batch_queue"" {
  name = ""batch_job_queue""
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""batch_app"" {
  name        = ""batch_job_app""
  description = ""Batch processing application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""batch_app_version"" {
  name        = ""version""
  application = aws_elastic_beanstalk_application.batch_app.name
  description = ""application version created by terraform""
  bucket      = aws_s3_bucket.app_bucket.id
  key         = aws_s3_object.app_version.key
}

# Elastic Beanstalk Worker Environment
resource ""aws_elastic_beanstalk_environment"" ""worker_environment"" {
  name                = ""worker-environment""
  application         = aws_elastic_beanstalk_application.batch_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v3.5.3 running Docker""
  tier                = ""Worker""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""WorkerQueueURL""
    value     = aws_sqs_queue.batch_queue.id
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }
}
```

This configuration creates:
1. An IAM role and instance profile for EC2 instances
2. An S3 bucket and object for the application version
3. An SQS queue for worker tasks
4. An Elastic Beanstalk application and version
5. A worker environment configured with the specified components

Note: Make sure to:
- Replace the region if needed
- Have an application.zip file in your working directory
- Adjust the solution stack name according to your needs
- Add any additional environment-specific settings as required
- Add necessary IAM role policies depending on your application's requirements",,,,,
"aws_s3_bucket, aws_s3_bucket_metric","Create a S3 bucket with bucket name = ""mybucket"" and a bucket metric resource that adds metrics configuration for the entire bucket.","package s3_bucket_metric
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some metrics in input.configuration.root_module.resources
    metrics.type == ""aws_s3_bucket_metric""
    bucket.address in metrics.expressions.bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some metrics in input.configuration.root_module.resources
    metrics.type == ""aws_s3_bucket_metric""
    metrics.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example""
}

resource ""aws_s3_bucket_metric"" ""example-entire-bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""
}","Has an aws_s3_bucket resource

Has an aws_s3_bucket_metric
with bucket = bucked id OR name","I'll help you create a Terraform configuration for an S3 bucket with bucket metrics configuration. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

# Create S3 bucket metrics configuration
resource ""aws_s3_bucket_metric"" ""bucket_metrics"" {
  bucket = aws_s3_bucket.mybucket.id
  name   = ""EntireBucketMetrics""

  filter {
    prefix = """" # Empty prefix means entire bucket
  }
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates an S3 bucket named ""mybucket""
3. Creates a bucket metric configuration that covers the entire bucket (empty prefix means it applies to all objects)

The metric configuration will allow you to track metrics for the entire bucket through CloudWatch. The metrics will include things like:
- Number of objects
- Total size of objects
- Number of downloads
- Bytes downloaded
- Other standard S3 metrics",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a bucket metric configuration. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_metrics"" ""bucket_metrics"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucketMetrics""

  filter {
    prefix = """"  # Empty prefix means entire bucket
  }
}
```

This configuration:
1. Sets up the AWS provider requirement
2. Creates an S3 bucket named ""mybucket""
3. Creates a bucket metrics configuration that monitors the entire bucket
   - The empty prefix in the filter block indicates that metrics will be collected for the entire bucket
   - The metrics configuration is named ""EntireBucketMetrics""

The metrics will be available in CloudWatch after the configuration is applied. This allows you to monitor various S3 bucket metrics like number of objects, total bytes, etc.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a bucket metric configuration. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_metrics"" ""entire_bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""

  filter {
    prefix = """"
  }
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates an S3 bucket named ""mybucket""
3. Adds a metrics configuration for the entire bucket using `aws_s3_bucket_metrics`
   - The empty prefix in the filter indicates that metrics will be collected for the entire bucket
   - The metrics configuration is named ""EntireBucket""

The metrics will help you monitor the bucket's usage, performance, and request patterns through CloudWatch metrics.",,,,,
"aws_glacier_vault, aws_iam_policy_document",generage an S3 Glacier vault with a access policy,"package terraform.validation

default is_valid_terraform_plan = false

# Rule to check for an AWS Glacier Vault resource with 'name' and 'access_policy' attributes
is_valid_glacier_vault {
	resource := input.planned_values.root_module.resources[_]
	resource.type == ""aws_glacier_vault""
	not is_null(resource.values.name)
	not is_null(resource.values.access_policy)
}

# Combined rule to validate the entire Terraform plan
is_valid_terraform_plan {
	is_valid_glacier_vault
}",3,"provider ""aws"" {
  region     = ""us-east-1""
}

data ""aws_iam_policy_document"" ""my_archive"" {
  statement {
    sid    = ""add-read-only-perm""
    effect = ""Allow""

    principals {
      type        = ""*""
      identifiers = [""*""]
    }

    actions = [
      ""glacier:InitiateJob"",
      ""glacier:GetJobOutput"",
    ]

    resources = [""arn:aws:glacier:eu-west-1:432981146916:vaults/MyArchive""]
  }
}

resource ""aws_glacier_vault"" ""my_archive"" {
  name = ""MyArchive""
  access_policy = data.aws_iam_policy_document.my_archive.json

  tags = {
    Test = ""MyArchive""
  }
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute
     with ""access_policy"" attribute","I'll help you create a Terraform configuration for an S3 Glacier vault with an access policy. Here's a solution that includes the required provider and creates a Glacier vault with an associated access policy:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  tags = {
    Environment = ""Production""
    Purpose     = ""Long-term-storage""
  }
}

# Create Glacier Vault Access Policy
resource ""aws_glacier_vault_lock"" ""vault_lock"" {
  vault_name = aws_glacier_vault.example_vault.name
  complete_lock = false  # Set to true for immutable policy

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""allow-glacier-access""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""glacier:DescribeVault"",
          ""glacier:ListVaults"",
          ""glacier:InitiateJob"",
          ""glacier:GetJobOutput"",
          ""glacier:UploadArchive"",
          ""glacier:InitiateMultipartUpload"",
          ""glacier:CompleteMultipartUpload"",
          ""glacier:AbortMultipartUpload""
        ]
        Resource = aws_glacier_vault.example_vault.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"": ""true""
          }
        }
      }
    ]
  })
}

# Optional: Add notifications for the vault
resource ""aws_glacier_vault_notification"" ""vault_notification"" {
  vault_name = aws_glacier_vault.example_vault.name
  sns_topic = ""arn:aws:sns:region:account-id:topic-name""  # Replace with your SNS topic ARN

  events = [
    ""ArchiveRetrievalCompleted"",
    ""InventoryRetrievalCompleted""
  ]
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a Glacier vault with tags
3. Implements a vault access policy that:
   - Allows specific Glacier operations
   - Requires secure transport (HTTPS)
   - Can be modified (complete_lock = false)
4. Optionally sets up notifications for vault events

Key features of this configuration:
- The vault policy enforces secure transport
- The policy allows common Glacier operations
- The vault lock is not complete (can be modified)
- Includes optional notifications for vault events

Remember to:
- Replace the region with your desired AWS region
- Update the SNS topic ARN if you're using notifications
- Adjust the policy permissions according to your security requirements
- Consider whether you want the policy to be immutable (complete_lock = true)

The policy can be customized further based on your specific access requirements and security needs.",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault with an access policy. Here's a proper configuration that includes the required AWS provider and creates a Glacier vault with an associated access policy:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  tags = {
    Environment = ""Production""
    Purpose     = ""Long-term-storage""
  }
}

# Create Glacier Vault Access Policy
resource ""aws_glacier_vault_lock"" ""example_vault_lock"" {
  vault_name = aws_glacier_vault.example_vault.name
  complete_lock = false  # Set to true if you want to permanently lock the policy

  policy = jsonencode({
    Version = ""2012-10-17""
    Id      = ""example-vault-policy""
    Statement = [
      {
        Sid    = ""allow-glacier-access""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""glacier:DescribeVault"",
          ""glacier:ListVaults"",
          ""glacier:InitiateJob"",
          ""glacier:GetJobOutput"",
          ""glacier:UploadArchive"",
          ""glacier:InitiateMultipartUpload"",
          ""glacier:UploadMultipartPart"",
          ""glacier:CompleteMultipartUpload""
        ]
        Resource = aws_glacier_vault.example_vault.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"": ""true""
          }
        }
      }
    ]
  })
}

# Optional: Add notifications for the vault
resource ""aws_glacier_vault_notification"" ""example_notification"" {
  vault_name = aws_glacier_vault.example_vault.name
  sns_topic = ""arn:aws:sns:region:account-id:topic-name""  # Replace with your SNS topic ARN

  events = [
    ""ArchiveRetrievalCompleted"",
    ""InventoryRetrievalCompleted""
  ]
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a Glacier vault with some basic tags
3. Applies an access policy to the vault that:
   - Allows specific Glacier operations
   - Requires secure transport (HTTPS)
   - Can be modified (complete_lock = false)
4. Optionally sets up notifications for vault events

You can customize the policy, vault name, and other parameters according to your specific needs. Remember to:
- Update the region in the provider block
- Modify the vault name as needed
- Adjust the policy permissions based on your security requirements
- Update the SNS topic ARN if you want to use notifications
- Set complete_lock to true only when you're sure about the policy, as it cannot be changed afterward",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault with an access policy. Here's a secure configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  tags = {
    Environment = ""Production""
    Purpose     = ""LongTermBackup""
  }
}

# Create Glacier Vault Access Policy
resource ""aws_glacier_vault_lock"" ""example_vault_lock"" {
  vault_name = aws_glacier_vault.example_vault.name
  complete_lock = true

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""enforce-secure-transport""
        Effect = ""Deny""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""glacier:*""
        ]
        Resource = [
          aws_glacier_vault.example_vault.arn
        ]
        Condition = {
          Bool = {
            ""aws:SecureTransport"": ""false""
          }
        }
      },
      {
        Sid    = ""allow-trusted-principals""
        Effect = ""Allow""
        Principal = {
          AWS = ""arn:aws:iam::${data.aws_caller_identity.current.account_id}:root""
        }
        Action = [
          ""glacier:DescribeVault"",
          ""glacier:ListVaults"",
          ""glacier:InitiateJob"",
          ""glacier:GetJobOutput"",
          ""glacier:UploadArchive"",
          ""glacier:InitiateMultipartUpload""
        ]
        Resource = [
          aws_glacier_vault.example_vault.arn
        ]
      }
    ]
  })
}

# Get current AWS account ID
data ""aws_caller_identity"" ""current"" {}

# Output the vault ARN
output ""glacier_vault_arn"" {
  value = aws_glacier_vault.example_vault.arn
}
```

This configuration:

1. Sets up the AWS provider
2. Creates a Glacier vault
3. Implements a vault access policy that:
   - Denies access when not using secure transport (HTTPS)
   - Allows specific Glacier operations for trusted principals
4. Includes tags for better resource management
5. Outputs the vault ARN for reference

The policy enforces secure access patterns while allowing necessary operations from trusted sources. You can modify the policy statements to match your specific security requirements.

Note: Be careful with `complete_lock = true` as it makes the policy immutable (cannot be changed once applied). Set to `false` if you need to modify the policy later.

Remember to replace the region and customize the vault name and policy according to your needs.",,,,,
"aws_chime_voice_connector, aws_chime_voice_connector, aws_chime_voice_connector_group","Create two AWS Chime Voice Connectors, both with encryption. Then create an aws_chime_voice_connector_group resource with both of these connectors. One connector should have priority=1 and the other should have priority=3","package aws_chime_voice_connector
import future.keywords.in

default valid := false
default connectors_valid := false

valid {
    connectors_valid
}

connectors_valid {
    some vc1 in input.configuration.root_module.resources
    vc1.type == ""aws_chime_voice_connector""
    vc1.expressions.require_encryption.constant_value == true

    some vc2 in input.configuration.root_module.resources
    vc2.type == ""aws_chime_voice_connector""
    vc2.expressions.require_encryption.constant_value == true

    not vc1 == vc2
    
    some vcgroup in input.configuration.root_module.resources
    vcgroup.type == ""aws_chime_voice_connector_group""

    some connector1 in vcgroup.expressions.connector
    vc1.address in connector1.voice_connector_id.references
    some connector2 in vcgroup.expressions.connector
    vc2.address in connector2.voice_connector_id.references
    
    not connector1 == connector2
    
    connector1.priority.constant_value == 1
    connector2.priority.constant_value == 3
}

connectors_valid {
        some vc1 in input.configuration.root_module.resources
    vc1.type == ""aws_chime_voice_connector""
    vc1.expressions.require_encryption.constant_value == true

    some vc2 in input.configuration.root_module.resources
    vc2.type == ""aws_chime_voice_connector""
    vc2.expressions.require_encryption.constant_value == true

    not vc1 == vc2
    
    some vcgroup in input.configuration.root_module.resources
    vcgroup.type == ""aws_chime_voice_connector_group""

    some connector1 in vcgroup.expressions.connector
    vc1.address in connector1.voice_connector_id.references
    some connector2 in vcgroup.expressions.connector
    vc2.address in connector2.voice_connector_id.references
    
    not connector1 == connector2
    
    connector1.priority.constant_value == 3
    connector2.priority.constant_value == 1
}",3,"resource ""aws_chime_voice_connector"" ""vc1"" {
  name               = ""connector-test-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""vc2"" {
  name               = ""connector-test-2""
  require_encryption = true
  aws_region         = ""us-west-2""
}

resource ""aws_chime_voice_connector_group"" ""group"" {
  name = ""test-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.vc1.id
    priority           = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.vc2.id
    priority           = 3
  }
}","Has one resource ""aws_chime_voice_connector""
with require_encryption=""true""

Has another resource ""aws_chime_voice_connector""
with require_encryption=""true""

Has one resource ""aws_chime_voice_connector_group""
with a connector for each voice connector with the correct id
one connector should have priority=1 and the other priority=3","I'll help you create a Terraform configuration for two AWS Chime Voice Connectors with encryption and group them together in a voice connector group with different priorities.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Adjust region as needed
}

# First Voice Connector
resource ""aws_chime_voice_connector"" ""connector_1"" {
  name               = ""voice-connector-1""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Second Voice Connector
resource ""aws_chime_voice_connector"" ""connector_2"" {
  name               = ""voice-connector-2""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Voice Connector Group
resource ""aws_chime_voice_connector_group"" ""connector_group"" {
  name = ""voice-connector-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_2.id
    priority          = 3
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two voice connectors (connector_1 and connector_2) with encryption enabled
3. Creates a voice connector group that includes both connectors:
   - First connector with priority 1
   - Second connector with priority 3

The voice connectors are created with `require_encryption = true` and are assigned to the same AWS region. The voice connector group then references these connectors using their IDs and assigns the specified priorities.

Make sure you're in a region that supports AWS Chime Voice Connectors. The configuration above uses us-east-1, but you can modify this according to your needs.",,,,,,"I'll help you create two AWS Chime Voice Connectors with encryption and group them together in a voice connector group with different priorities. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# First Voice Connector
resource ""aws_chime_voice_connector"" ""connector_1"" {
  name               = ""voice-connector-1""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Second Voice Connector
resource ""aws_chime_voice_connector"" ""connector_2"" {
  name               = ""voice-connector-2""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Voice Connector Group
resource ""aws_chime_voice_connector_group"" ""connector_group"" {
  name = ""voice-connector-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_2.id
    priority          = 3
  }
}
```

This configuration:
1. Creates two voice connectors with encryption enabled
2. Creates a voice connector group that includes both connectors
3. Sets the priority of the first connector to 1 and the second connector to 3
4. Uses proper resource references to link the connectors to the group

The voice connectors are placed in the us-east-1 region, as this is required for AWS Chime Voice Connectors. Both connectors have encryption required through the `require_encryption = true` parameter.",,,,,,"I'll help you create a Terraform configuration for two AWS Chime Voice Connectors with encryption and group them together in a voice connector group with different priorities.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# First Voice Connector
resource ""aws_chime_voice_connector"" ""connector_1"" {
  name               = ""voice-connector-1""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Second Voice Connector
resource ""aws_chime_voice_connector"" ""connector_2"" {
  name               = ""voice-connector-2""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Voice Connector Group
resource ""aws_chime_voice_connector_group"" ""connector_group"" {
  name = ""voice-connector-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_2.id
    priority          = 3
  }
}
```

This configuration:
1. Creates two voice connectors with encryption enabled
2. Creates a voice connector group that includes both connectors
3. Sets priority 1 for the first connector and priority 3 for the second connector
4. Uses the required AWS provider
5. Properly references the voice connector IDs in the group configuration

The voice connectors are created in the us-east-1 region, but you can modify this according to your needs. Both connectors have encryption required as specified in the prompt.",,,,,
"aws_efs_file_system, aws_efs_file_system_policy, aws_iam_policy_document",create a AWS EFS with a system policy,"package terraform.validation

default is_valid_efs_setup = false

# Rule to check if a valid aws_efs_file_system and aws_efs_file_system_policy exists
is_valid_efs_setup {
    has_valid_efs_file_system
    has_valid_efs_file_system_policy
}

# Helper rule to check if a valid aws_efs_file_system exists
has_valid_efs_file_system {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_file_system""
}

# Helper rule to check if a valid aws_efs_file_system_policy exists with required arguments
has_valid_efs_file_system_policy {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_file_system_policy""
    resource.expressions.file_system_id
    resource.expressions.policy
}
",3,"resource ""aws_efs_file_system"" ""fs"" {
  creation_token = ""my-product""
}

data ""aws_iam_policy_document"" ""policy"" {
  statement {
    sid    = ""ExampleStatement01""
    effect = ""Allow""

    principals {
      type        = ""AWS""
      identifiers = [""*""]
    }

    actions = [
      ""elasticfilesystem:ClientMount"",
      ""elasticfilesystem:ClientWrite"",
    ]

    resources = [aws_efs_file_system.fs.arn]

    condition {
      test     = ""Bool""
      variable = ""aws:SecureTransport""
      values   = [""true""]
    }
  }
}

resource ""aws_efs_file_system_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.fs.id
  policy         = data.aws_iam_policy_document.policy.json
}","Have one ""aws_efs_file_system"" resource

Have one ""aws_efs_file_system_policy"" resource
with one ""file_system_id""
with one ""policy""","I'll help you create an AWS EFS (Elastic File System) with a system policy using Terraform. Here's a complete configuration that includes the required provider and creates an EFS with an associated system policy:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# EFS File System
resource ""aws_efs_file_system"" ""efs"" {
  creation_token = ""my-efs""
  encrypted      = true

  tags = {
    Name = ""MyEFS""
  }
}

# EFS System Policy
resource ""aws_efs_file_system_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.efs.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""AllowEFSAccess""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""elasticfilesystem:ClientMount"",
          ""elasticfilesystem:ClientWrite""
        ]
        Resource = aws_efs_file_system.efs.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"" = ""true""
          }
        }
      }
    ]
  })
}

# Optional: EFS Mount Target (you'll need this to actually use the EFS)
resource ""aws_efs_mount_target"" ""mount"" {
  file_system_id  = aws_efs_file_system.efs.id
  subnet_id       = ""subnet-xxxxxx"" # Replace with your subnet ID
  security_groups = [""sg-xxxxxx""]   # Replace with your security group ID
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates an EFS file system with encryption enabled
3. Attaches a system policy that:
   - Allows client mount and write operations
   - Requires secure transport (TLS)
   - Can be accessed by AWS principals (you might want to restrict this further)
4. Includes an optional mount target (you'll need to specify your own subnet and security group IDs)

Make sure to replace the subnet_id and security_groups with your actual values before applying this configuration.

The system policy provided is a basic example that enforces TLS usage. You can modify the policy statement to be more restrictive based on your security requirements, such as limiting access to specific IAM roles or AWS accounts.",,,,,,"I'll help you create an AWS EFS (Elastic File System) with a system policy using Terraform. Here's a configuration that sets up an EFS with the necessary system policy:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create EFS File System
resource ""aws_efs_file_system"" ""efs"" {
  creation_token = ""my-efs""
  encrypted      = true

  tags = {
    Name = ""MyEFS""
  }
}

# Create EFS Policy
resource ""aws_efs_file_system_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.efs.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""AllowRootAccess""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""elasticfilesystem:ClientMount"",
          ""elasticfilesystem:ClientWrite""
        ]
        Resource = aws_efs_file_system.efs.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"" = ""true""
          }
        }
      }
    ]
  })
}

# Create Mount Target (optional, but commonly needed)
resource ""aws_efs_mount_target"" ""mount"" {
  file_system_id  = aws_efs_file_system.efs.id
  subnet_id       = ""subnet-xxxxxxxx"" # Replace with your subnet ID
  security_groups = [""sg-xxxxxxxx""]   # Replace with your security group ID
}
```

This configuration:
1. Creates an EFS file system with encryption enabled
2. Attaches a policy that allows root access through secure transport (TLS)
3. Creates a mount target (you'll need to specify your subnet and security group IDs)

The policy allows:
- Client mount operations
- Client write operations
- Only through secure transport (TLS)

Remember to:
- Replace the subnet_id and security_groups with your actual values
- Adjust the policy according to your specific security requirements
- Consider adding backup policies if needed
- Add additional mount targets if you need access from multiple subnets

You can customize the policy further by adding more statements or restricting access to specific IAM roles/users instead of using ""*"" as the Principal.",,,,,,"I'll help you create an AWS EFS (Elastic File System) with a system policy using Terraform. Here's a complete example that includes the required provider, EFS creation, and the associated system policy:

```hcl
# Required provider block
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Configure the AWS Provider
provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create EFS File System
resource ""aws_efs_file_system"" ""example"" {
  creation_token = ""my-efs""
  encrypted      = true

  tags = {
    Name = ""MyEFS""
  }
}

# Create EFS Policy
resource ""aws_efs_file_system_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.example.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""ExampleStatement""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""elasticfilesystem:ClientMount"",
          ""elasticfilesystem:ClientWrite""
        ]
        Resource = aws_efs_file_system.example.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"": ""true""
          }
        }
      }
    ]
  })
}

# Create mount target (optional but recommended)
resource ""aws_efs_mount_target"" ""example"" {
  file_system_id  = aws_efs_file_system.example.id
  subnet_id       = ""subnet-xxxxxx""  # Replace with your subnet ID
  security_groups = [""sg-xxxxxx""]    # Replace with your security group ID
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an EFS file system with encryption enabled
3. Applies a system policy that:
   - Allows client mount and write operations
   - Requires secure transport (TLS)
   - Can be accessed by AWS principals (you may want to restrict this further)
4. Creates a mount target in a specific subnet with associated security groups

Remember to:
- Replace the subnet_id and security_groups with your actual values
- Adjust the region in the provider block
- Modify the policy according to your specific security requirements
- Add or modify tags as needed

The policy can be customized further based on your specific access control requirements.",,,,,
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration","create a bucket 'pike-680235478471'. Set up request payment configuration for an AWS S3 bucket named 'pike-680235478471', specifying the payer as 'Requester'.","package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_request_payment_configuration = false

default valid_configuration = false

has_s3_bucket {
    some i
    bucket := input.planned_values.root_module.resources[i]
    bucket.type == ""aws_s3_bucket""
    bucket.values.bucket == ""pike-680235478471""
}

has_s3_bucket_request_payment_configuration {
    some i
    payment_config := input.planned_values.root_module.resources[i]
    payment_config.type == ""aws_s3_bucket_request_payment_configuration""
    payment_config.values.bucket == ""pike-680235478471""
    payment_config.values.payer == ""Requester""
}

valid_configuration {
    has_s3_bucket
    has_s3_bucket_request_payment_configuration
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""a"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""pike"" {
  bucket = ""pike-680235478471""
  payer  = ""Requester""
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_request_payment_configuration resource
with bucket
with payer (BucketOwner or Requester)","I'll help you create a Terraform configuration for an S3 bucket with request payment configuration set to 'Requester'. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""pike_bucket"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""payment_config"" {
  bucket = aws_s3_bucket.pike_bucket.id
  payer  = ""Requester""
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates an S3 bucket named ""pike-680235478471""
3. Sets up the request payment configuration for the bucket, specifying the payer as ""Requester""

The `aws_s3_bucket_request_payment_configuration` resource configures who pays for the requests and data transfer costs associated with the bucket. When set to ""Requester"", the requester pays for requests and data transfer costs, while the bucket owner still pays for the storage costs.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with request payment configuration set to 'Requester'. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""pike_bucket"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""payment_config"" {
  bucket = aws_s3_bucket.pike_bucket.id
  payer  = ""Requester""
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates an S3 bucket named ""pike-680235478471""
3. Sets up the request payment configuration for the bucket, specifying the payer as ""Requester""

The `aws_s3_bucket_request_payment_configuration` resource configures who pays for the request and data transfer costs - in this case, the requester rather than the bucket owner.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with request payment configuration. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create S3 bucket with request payment configuration
resource ""aws_s3_bucket"" ""pike_bucket"" {
  bucket = ""pike-680235478471""
}

# Configure request payment settings
resource ""aws_s3_bucket_request_payment_configuration"" ""payment_config"" {
  bucket = aws_s3_bucket.pike_bucket.id
  payer  = ""Requester""
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates an S3 bucket named ""pike-680235478471""
3. Sets up the request payment configuration with the payer as ""Requester""

The `aws_s3_bucket_request_payment_configuration` resource configures who pays for the requests and data transfer costs: either the bucket owner or the request maker. In this case, it's set to ""Requester"", meaning the person making requests to the bucket will pay for the costs associated with those requests.",,,,,
"aws_vpc, aws_vpc, aws_vpc_peering_connection","establishes a VPC peering connection between two AWS Virtual Private Clouds (VPCs). The first VPC, named ""peer,"" is configured with a CIDR block of 10.0.0.0/24, and the second VPC, named ""base,"" has a CIDR block of 10.1.0.0/24. The peering connection, named ""pike,"" connects these two VPCs, allowing them to communicate with each other as if they were in the same network, and is tagged with pike = ""permissions"" to indicate its purpose or management policy.","package terraform.validation

# Set default validation states
default is_valid_vpcs = false
default is_valid_vpc_peering_connection = false

# Validate aws_vpc resources
is_valid_vpcs {
    # Validate the first VPC named ""peer""
    peer_vpc := input.configuration.root_module.resources[_]
    peer_vpc.type == ""aws_vpc""
    peer_vpc.name == ""peer""
    peer_vpc.expressions.cidr_block.constant_value == ""10.0.0.0/24""

    # Validate the second VPC named ""base""
    base_vpc := input.configuration.root_module.resources[_]
    base_vpc.type == ""aws_vpc""
    base_vpc.name == ""base""
    base_vpc.expressions.cidr_block.constant_value == ""10.1.0.0/24""

    # Ensure different VPCs
    peer_vpc != base_vpc
}

# Validate aws_vpc_peering_connection resource
is_valid_vpc_peering_connection {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_vpc_peering_connection""
    resource.name == ""pike""
    # Ensure connection between ""peer"" and ""base"" VPCs
    resource.expressions.peer_vpc_id.references[0] == ""aws_vpc.peer.id""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.base.id""
    # Check for the specific tag indicating its purpose
    resource.expressions.tags.constant_value[""pike""] == ""permissions""
}",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id

  tags = {
    pike = ""permissions""
  }
}

resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
}

resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
}","Has two ""aws_vpc"" resources:
The first VPC is named ""peer"" and is configured with a ""cidr_block"" of ""10.0.0.0/24"".
The second VPC is named ""base"" and is configured with a ""cidr_block"" of ""10.1.0.0/24"".

Has one ""aws_vpc_peering_connection"" resource:
Named ""pike"" to connect the two specified VPCs (""peer"" and ""base"").
Configured to allow the VPCs to communicate with each other as if they were in the same network.
Tagged with ""pike"" set to ""permissions"", indicating its purpose or management policy within the infrastructure.","I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs. Here's the solution:

```hcl
# Configure AWS Provider
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust region as needed
}

# Create the peer VPC
resource ""aws_vpc"" ""peer"" {
  cidr_block           = ""10.0.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""peer-vpc""
  }
}

# Create the base VPC
resource ""aws_vpc"" ""base"" {
  cidr_block           = ""10.1.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""base-vpc""
  }
}

# Create VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    pike = ""permissions""
    Name = ""pike-peering-connection""
  }
}

# Add route table entries for peer VPC
resource ""aws_route_table"" ""peer_route_table"" {
  vpc_id = aws_vpc.peer.id

  route {
    cidr_block                = aws_vpc.base.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""peer-route-table""
  }
}

# Add route table entries for base VPC
resource ""aws_route_table"" ""base_route_table"" {
  vpc_id = aws_vpc.base.id

  route {
    cidr_block                = aws_vpc.peer.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""base-route-table""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two VPCs (""peer"" and ""base"") with specified CIDR blocks
3. Establishes a VPC peering connection named ""pike"" between them
4. Creates route tables for both VPCs to enable bi-directional communication
5. Includes appropriate tags for identification and management
6. Enables DNS support and hostnames for both VPCs
7. Automatically accepts the peering connection

The VPCs will be able to communicate with each other through the peering connection once this configuration is applied.",,,,,,"I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create the peer VPC
resource ""aws_vpc"" ""peer"" {
  cidr_block           = ""10.0.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""peer-vpc""
  }
}

# Create the base VPC
resource ""aws_vpc"" ""base"" {
  cidr_block           = ""10.1.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""base-vpc""
  }
}

# Create VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    pike = ""permissions""
    Name = ""pike-peering-connection""
  }
}

# Add route to peer VPC route table
resource ""aws_route"" ""peer_to_base"" {
  route_table_id            = aws_vpc.peer.main_route_table_id
  destination_cidr_block    = aws_vpc.base.cidr_block
  vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
}

# Add route to base VPC route table
resource ""aws_route"" ""base_to_peer"" {
  route_table_id            = aws_vpc.base.main_route_table_id
  destination_cidr_block    = aws_vpc.peer.cidr_block
  vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates two VPCs (""peer"" and ""base"") with their specified CIDR blocks
3. Establishes a VPC peering connection named ""pike"" between the two VPCs
4. Adds the necessary routes to both VPCs' route tables to enable communication
5. Includes the requested tag `pike = ""permissions""` on the peering connection
6. Enables DNS support and hostnames for both VPCs
7. Uses auto-accept for the peering connection since both VPCs are in the same account

The configuration ensures that both VPCs can communicate with each other through the peering connection. Make sure to adjust the region in the provider block according to your needs.",,,,,,"I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Update with your desired region
}

# Create the peer VPC
resource ""aws_vpc"" ""peer_vpc"" {
  cidr_block           = ""10.0.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""peer""
  }
}

# Create the base VPC
resource ""aws_vpc"" ""base_vpc"" {
  cidr_block           = ""10.1.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""base""
  }
}

# Create VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer_vpc.id
  vpc_id      = aws_vpc.base_vpc.id
  auto_accept = true

  tags = {
    pike = ""permissions""
  }
}

# Add route table entries for peer VPC
resource ""aws_route_table"" ""peer_route_table"" {
  vpc_id = aws_vpc.peer_vpc.id

  route {
    cidr_block                = aws_vpc.base_vpc.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""peer-route-table""
  }
}

# Add route table entries for base VPC
resource ""aws_route_table"" ""base_route_table"" {
  vpc_id = aws_vpc.base_vpc.id

  route {
    cidr_block                = aws_vpc.peer_vpc.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""base-route-table""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two VPCs (""peer"" and ""base"") with specified CIDR blocks
3. Establishes a VPC peering connection named ""pike"" between the VPCs
4. Creates route tables for both VPCs to enable traffic flow
5. Includes appropriate tags for resource identification

The `auto_accept = true` parameter automatically accepts the peering connection request since both VPCs are in the same AWS account. If you're connecting VPCs across different accounts, you'll need to handle the acceptance separately.

Remember to:
- Update the region in the provider block to match your desired AWS region
- Ensure the CIDR blocks don't overlap
- Add route table associations if you have specific subnets that need to use these route tables",,,,,
"aws_iam_role, aws_iam_role_policy, aws_kendra_index",Generate a basic Kendra index resource with the default Document Metadata Configuration Updates,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
    some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kendra.amazonaws.com"") 
}

has_valid_kendra_index {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_index""
    resource.values.name
    resource.values.document_metadata_configuration_updates
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
}


has_valid_resources {
    has_valid_iam_role(input.resource_changes)
    has_valid_kendra_index
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""role"" {
  name = ""Kendra-Role""
  assume_role_policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": ""sts:AssumeRole"",
      ""Principal"": {
        ""Service"": ""kendra.amazonaws.com""
      },
      ""Effect"": ""Allow"",
      ""Sid"": """"
    }
  ]
}
EOF
}

resource ""aws_iam_role_policy"" ""policy"" {
  name = ""Kendra-Policy""
  role = aws_iam_role.role.id

  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:GetObject"",
        ""s3:ListBucket""
      ],
      ""Resource"": ""*""
    }
  ]
}
EOF
}

resource ""aws_kendra_index"" ""example"" {
  name     = ""example""
  role_arn = aws_iam_role.role.arn

  document_metadata_configuration_updates {
    name = ""_authors""
    type = ""STRING_LIST_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = false
    }
    relevance {
      importance = 1
    }
  }

  document_metadata_configuration_updates {
    name = ""_category""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_created_at""
    type = ""DATE_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      freshness  = false
      importance = 1
      duration   = ""25920000s""
      rank_order = ""ASCENDING""
    }
  }

  document_metadata_configuration_updates {
    name = ""_data_source_id""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_document_title""
    type = ""STRING_VALUE""
    search {
      displayable = true
      facetable   = false
      searchable  = true
      sortable    = true
    }
    relevance {
      importance            = 2
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_excerpt_page_number""
    type = ""LONG_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = false
    }
    relevance {
      importance = 2
      rank_order = ""ASCENDING""
    }
  }

  document_metadata_configuration_updates {
    name = ""_faq_id""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_file_type""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_language_code""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_last_updated_at""
    type = ""DATE_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      freshness  = false
      importance = 1
      duration   = ""25920000s""
      rank_order = ""ASCENDING""
    }
  }

  document_metadata_configuration_updates {
    name = ""_source_uri""
    type = ""STRING_VALUE""
    search {
      displayable = true
      facetable   = false
      searchable  = false
      sortable    = false
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_tenant_id""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_version""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_view_count""
    type = ""LONG_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance = 1
      rank_order = ""ASCENDING""
    }
  }
}","Has one ""aws_iam_role"" resource and one ""aws_kendra_index"" resource with all default ""document_metadata_configuration_updates"" set","I'll help you create a Terraform configuration for an AWS Kendra index resource with default document metadata configuration updates. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust the region as needed
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra Index with default document metadata configuration""
  role_arn    = aws_iam_role.kendra_role.arn

  document_metadata_configuration_updates {
    name          = ""_created_at""
    type          = ""DATE_VALUE""
    relevance {
      freshness = true
    }
  }

  edition      = ""ENTERPRISE_EDITION""
  tags = {
    Environment = ""production""
  }
}

# IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""kendra_policy"" {
  role       = aws_iam_role.kendra_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSKendraFullAccess""
}
```

This configuration creates:
1. A Kendra index with a default document metadata configuration for created_at dates
2. The necessary IAM role and policy attachments for Kendra to function
3. Uses the Enterprise Edition of Kendra
4. Includes basic tagging

Key features:
- Default document metadata configuration for date values
- Proper IAM role configuration with assume role policy
- Standard naming and tagging
- Uses the required AWS provider

Remember to:
- Adjust the region as needed
- Modify the index name and description to match your requirements
- Review the IAM permissions to ensure they match your security requirements
- Consider costs as Kendra Enterprise Edition has associated charges",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra index resource with default document metadata configuration updates. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra Index with default document metadata configuration""
  
  # Required role for Kendra to access other AWS services
  role_arn = aws_iam_role.kendra_role.arn

  # Default document metadata configuration
  document_metadata_configuration_updates {
    name = ""_created_at""
    type = ""DATE_VALUE""
    relevance {
      importance = 1
    }
    search {
      facetable   = true
      searchable  = true
      displayable = true
      sortable    = true
    }
  }

  # Edition and capacity units configuration
  edition = ""ENTERPRISE_EDITION""
  capacity_units {
    query_capacity_units   = 1
    storage_capacity_units = 1
  }

  tags = {
    Environment = ""production""
    Name        = ""example-kendra-index""
  }
}

# IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra_index_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""kendra_policy"" {
  role       = aws_iam_role.kendra_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSKendraServiceRole""
}
```

This configuration creates:
1. A Kendra index with default document metadata configuration
2. The required IAM role and policy attachments for Kendra to function
3. Basic capacity units configuration
4. Default metadata configuration for ""_created_at"" field

Key features of this configuration:
- Uses Enterprise Edition
- Sets up basic search capabilities (facetable, searchable, displayable, sortable)
- Includes necessary IAM permissions
- Configures basic capacity units
- Includes tags for resource management

Make sure to adjust the values (name, description, capacity units, etc.) according to your specific needs.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra index resource with default document metadata configuration updates. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra Index with default document metadata configuration""
  role_arn    = aws_iam_role.kendra_role.arn

  # Default document metadata configuration updates
  document_metadata_configuration_updates {
    name          = ""_created_at""
    type          = ""DATE_VALUE""
    relevance {
      importance = ""HIGH""
    }
    search {
      displayable = true
      facetable   = true
      searchable  = true
      sortable    = true
    }
  }

  tags = {
    Environment = ""production""
    Name        = ""example-kendra-index""
  }
}

# IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""kendra_policy"" {
  role       = aws_iam_role.kendra_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSKendraServiceRole""
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Kendra index with a default document metadata configuration for ""_created_at""
3. Creates the necessary IAM role and policy attachment for Kendra to function
4. Includes basic tags for resource management

The document metadata configuration is set up with:
- A DATE_VALUE type field
- HIGH importance for relevance
- Full search capabilities (displayable, facetable, searchable, and sortable)

Remember to adjust the names, descriptions, and tags according to your specific needs.",,,,,
"aws_kms_key, aws_s3_bucket, aws_s3_bucket_server_side_encryption_configuration",Create an AWS S3 bucket and apply KMS server side encryption that uses a defined KMS key resource.,"package aws_s3_bucket_sse
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some kms_key in input.configuration.root_module.resources
        kms_key.type == ""aws_kms_key""

        some sse in input.configuration.root_module.resources
        sse.type = ""aws_s3_bucket_server_side_encryption_configuration""
        bucket.address in sse.expressions.bucket.references
        some rule in sse.expressions.rule
        some rule_args in rule.apply_server_side_encryption_by_default
        kms_key.address in rule_args.kms_master_key_id.references
    rule_args.sse_algorithm.constant_value == ""aws:kms""
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some kms_key in input.configuration.root_module.resources
        kms_key.type == ""aws_kms_key""

        some sse in input.configuration.root_module.resources
        sse.type = ""aws_s3_bucket_server_side_encryption_configuration""
        bucket.expressions.bucket.constant_value == sse.expressions.bucket.constant_value
        some rule in sse.expressions.rule
        some rule_args in rule.apply_server_side_encryption_by_default
        kms_key.address in rule_args.kms_master_key_id.references
    rule_args.sse_algorithm.constant_value == ""aws:kms""
    
}",2,"resource ""aws_kms_key"" ""mykey"" {
  description             = ""This key is used to encrypt bucket objects""
  deletion_window_in_days = 10
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_server_side_encryption_configuration"" ""example"" {
  bucket = aws_s3_bucket.mybucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.mykey.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}","Has one resource ""aws_s3_bucket""

Has one resource ""aws_kms_key""

Has one resource ""aws_s3_bucket_server_side_encryption""
with bucket = bucket id OR name
with rule
    with apply_server_side_encryption_by_default
        with kms_master_key_id = kms key id
        and sse_algorithm = ""aws:kms""","I'll help you create a Terraform configuration for an S3 bucket with KMS server-side encryption using a defined KMS key resource. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# KMS key resource
resource ""aws_kms_key"" ""s3_key"" {
  description             = ""KMS key for S3 bucket encryption""
  deletion_window_in_days = 7
  enable_key_rotation    = true
}

# KMS key alias
resource ""aws_kms_alias"" ""s3_key_alias"" {
  name          = ""alias/s3-encryption-key""
  target_key_id = aws_kms_key.s3_key.key_id
}

# S3 bucket with KMS encryption
resource ""aws_s3_bucket"" ""encrypted_bucket"" {
  bucket = ""my-encrypted-bucket""
}

# S3 bucket server-side encryption configuration
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""bucket_encryption"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.s3_key.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}

# S3 bucket versioning
resource ""aws_s3_bucket_versioning"" ""bucket_versioning"" {
  bucket = aws_s3_bucket.encrypted_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}
```

This configuration:
1. Creates a KMS key with key rotation enabled
2. Creates an alias for the KMS key
3. Creates an S3 bucket
4. Configures server-side encryption using the created KMS key
5. Enables versioning on the bucket for additional security

The bucket will use KMS encryption by default for all objects stored in it. Make sure to replace ""my-encrypted-bucket"" with your desired bucket name.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with KMS server-side encryption using a defined KMS key resource. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# KMS key for S3 bucket encryption
resource ""aws_kms_key"" ""s3_key"" {
  description             = ""KMS key for S3 bucket encryption""
  deletion_window_in_days = 7
  enable_key_rotation    = true
  
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""Enable IAM User Permissions""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action   = ""kms:*""
        Resource = ""*""
      }
    ]
  })
}

# KMS key alias
resource ""aws_kms_alias"" ""s3_key_alias"" {
  name          = ""alias/s3-encryption-key""
  target_key_id = aws_kms_key.s3_key.key_id
}

# S3 bucket with KMS encryption
resource ""aws_s3_bucket"" ""encrypted_bucket"" {
  bucket = ""my-encrypted-bucket-name""
}

# S3 bucket server-side encryption configuration
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""bucket_encryption"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.s3_key.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}

# S3 bucket versioning
resource ""aws_s3_bucket_versioning"" ""versioning"" {
  bucket = aws_s3_bucket.encrypted_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}
```

This configuration:
1. Creates a KMS key with a basic policy and key rotation enabled
2. Creates an alias for the KMS key
3. Creates an S3 bucket
4. Configures server-side encryption on the bucket using the created KMS key
5. Enables versioning on the bucket for additional data protection

Make sure to:
- Replace ""my-encrypted-bucket-name"" with your desired bucket name
- Adjust the KMS key policy according to your security requirements
- Consider adding additional S3 bucket configurations like blocking public access or lifecycle rules as needed",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with KMS server-side encryption using a defined KMS key resource. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# KMS key resource
resource ""aws_kms_key"" ""s3_key"" {
  description             = ""KMS key for S3 bucket encryption""
  deletion_window_in_days = 7
  enable_key_rotation    = true
  
  tags = {
    Name = ""s3-bucket-key""
  }
}

# KMS key alias
resource ""aws_kms_alias"" ""s3_key_alias"" {
  name          = ""alias/s3-bucket-key""
  target_key_id = aws_kms_key.s3_key.key_id
}

# S3 bucket with KMS encryption
resource ""aws_s3_bucket"" ""encrypted_bucket"" {
  bucket = ""my-encrypted-bucket""
}

# Bucket server-side encryption configuration
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""bucket_encryption"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.s3_key.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}

# Block public access
resource ""aws_s3_bucket_public_access_block"" ""block_public_access"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
```

This configuration:
1. Creates a KMS key with key rotation enabled
2. Creates an alias for the KMS key
3. Creates an S3 bucket
4. Configures server-side encryption using the created KMS key
5. Blocks public access to the bucket as a security best practice

Make sure to replace ""my-encrypted-bucket"" with your desired bucket name. The bucket name must be globally unique across all AWS accounts.",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_route53_record, aws_route53_zone, aws_s3_bucket, aws_security_group, aws_subnet, aws_subnet, aws_vpc, aws_ami","The infrastructure for a video streaming site that has multiple servers to balance load. Accomplish this using the following resources: AWS Load Balancing, AWS Load Balancing target group, AWS Route53 zone, AWS Route53 record, and AWS VPC module, AWS VPC subnet, AWS VPC security group, AWS EC2 instance. Add any necessary resources.","package load_balancer_middle

import rego.v1

default valid := false

instance_valid(instance, ami) if {
	instance.expressions.instance_type
	ami.address in instance.expressions.ami.references
}

lb_valid(lb, security_group, subnets) if {
	every subnet in subnets {
		subnet.address in lb.expressions.subnets.references
	}
	security_group.address in lb.expressions.security_groups.references
}

lb_listener_valid(lb_listener, lb, target_group) if {
	some default_action in lb_listener.expressions.default_action
	target_group.address in default_action.target_group_arn.references
	default_action.type.constant_value == ""forward""
	lb.address in lb_listener.expressions.load_balancer_arn.references
}

lb_target_group_valid(target_group, vpc) if {
	vpc.address in target_group.expressions.vpc_id.references
	target_group.expressions.port
	target_group.expressions.protocol
}

lb_target_group_attachment_valid(target_group_attachment, target_group, instance) if {
	target_group.address in target_group_attachment.expressions.target_group_arn.references
	instance.address in target_group_attachment.expressions.target_id.references
}

lb_resources_valid(lb, lb_listener, target_group, target_group_attachment, instance, vpc, subnets, security_group) if {
	lb_valid(lb, security_group, subnets)
	lb_listener_valid(lb_listener, lb, target_group)
	lb_target_group_valid(target_group, vpc)
	lb_target_group_attachment_valid(target_group_attachment, target_group, instance)
}

route53_record_valid(record, zone, lb) if {
	zone.address in record.expressions.zone_id.references
	record.expressions.name
	record.expressions.type
	some alias in record.expressions.alias
	lb.address in alias.zone_id.references
	lb.address in alias.name.references
	alias.evaluate_target_health
}

route53_records_valid(records, zone, lb) if {
	some record_ipv4 in records
	route53_record_valid(record_ipv4, zone, lb)
}

route53_zone_valid(zone) if {
	zone.expressions.name
}

security_group_valid(security_group, vpc) if {
	vpc.address in security_group.expressions.vpc_id.references
}

subnet_valid(subnet, vpc) if {
	vpc.address in subnet.expressions.vpc_id.references
	subnet.expressions.cidr_block
}

subnets_valid(subnets, vpc) if {
	every subnet in subnets {
		subnet_valid(subnet, vpc)
	}
}

vpc_valid(vpc) if {
	vpc.expressions.cidr_block
}

valid if {
	resources := input.configuration.root_module.resources

	# ec2
	some instance in resources
	instance.type == ""aws_instance""
	some ami in resources
	ami.type == ""aws_ami""

	some vpc in resources
	vpc.type == ""aws_vpc""

	subnets := [subnet | subnet := resources[_]; subnet.type == ""aws_subnet""]
	count(subnets) > 1
	some security_group in resources
	security_group.type == ""aws_security_group""

	# lb resources
	some lb in resources
	lb.type == ""aws_lb""
	some lb_listener in resources
	lb_listener.type == ""aws_lb_listener""
	some target_group in resources
	target_group.type == ""aws_lb_target_group""
	some target_group_attachment in resources
	target_group_attachment.type == ""aws_lb_target_group_attachment""

	# route53
	records := [record | record := resources[_]; record.type == ""aws_route53_record""]
	some zone in resources
	zone.type == ""aws_route53_zone""

	# s3
	some s3_bucket in resources
	s3_bucket.type == ""aws_s3_bucket""

	instance_valid(instance, ami)
	lb_resources_valid(lb, lb_listener, target_group, target_group_attachment, instance, vpc, subnets, security_group)
	route53_records_valid(records, zone, lb)
	route53_zone_valid(zone)

	security_group_valid(security_group, vpc)

	subnets_valid(subnets, vpc)
	vpc_valid(vpc)
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_launch_configuration"" ""launch-config"" {
  name_prefix     = ""aws-asg-launch-config-""
  image_id        = data.aws_ami.ubuntu.id
  instance_type   = ""t2.micro""
  # user_data       = file(""user-data.sh"")  # load your script if needed
  security_groups = [aws_security_group.instance-sg.id]

  lifecycle {
    create_before_destroy = true
  }
}

resource ""aws_autoscaling_group"" ""asg"" {
  name                 = ""asg""
  min_size             = 1
  max_size             = 3
  desired_capacity     = 1
  launch_configuration = aws_launch_configuration.launch-config.name
  vpc_zone_identifier  = module.vpc.public_subnets

  lifecycle { 
    ignore_changes = [desired_capacity, target_group_arns]
  }

  health_check_type    = ""ELB""
}

resource ""aws_autoscaling_policy"" ""scale-down"" {
  name                   = ""scale-down""
  autoscaling_group_name = aws_autoscaling_group.asg.name
  adjustment_type        = ""ChangeInCapacity""
  scaling_adjustment     = -1
  cooldown               = 120
}

resource ""aws_cloudwatch_metric_alarm"" ""scale-down"" {
  alarm_description   = ""Monitors CPU utilization for Terramino ASG""
  alarm_actions       = [aws_autoscaling_policy.scale-down.arn]
  alarm_name          = ""scale-down""
  comparison_operator = ""LessThanOrEqualToThreshold""
  namespace           = ""AWS/EC2""
  metric_name         = ""CPUUtilization""
  threshold           = ""10""
  evaluation_periods  = ""2""
  period              = ""120""
  statistic           = ""Average""

  dimensions = {
    AutoScalingGroupName = aws_autoscaling_group.asg.name
  }
}

resource ""aws_autoscaling_policy"" ""scale-up"" {
  name                   = ""scale-up""
  autoscaling_group_name = aws_autoscaling_group.asg.name
  adjustment_type        = ""ChangeInCapacity""
  scaling_adjustment     = 1
  cooldown               = 120
}

resource ""aws_cloudwatch_metric_alarm"" ""scale-up"" {
  alarm_description   = ""Monitors CPU utilization for Terramino ASG""
  alarm_actions       = [aws_autoscaling_policy.scale-up.arn]
  alarm_name          = ""scale-up""
  comparison_operator = ""GreaterThanOrEqualToThreshold""
  namespace           = ""AWS/EC2""
  metric_name         = ""CPUUtilization""
  threshold           = ""80""
  evaluation_periods  = ""2""
  period              = ""120""
  statistic           = ""Average""

  dimensions = {
    AutoScalingGroupName = aws_autoscaling_group.asg.name
  }
}

resource ""aws_lb"" ""lb"" {
  name               = ""my-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
  subnets            = module.vpc.public_subnets
}

resource ""aws_lb_listener"" ""lb-listener"" {
  load_balancer_arn = aws_lb.lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target-group.arn
  }
}

resource ""aws_lb_target_group"" ""target-group"" {
  name     = ""my-lb-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_autoscaling_attachment"" ""as-attachment"" {
  autoscaling_group_name = aws_autoscaling_group.asg.id
  lb_target_group_arn   = aws_lb_target_group.target-group.arn
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_route53_zone"" ""test-video-stream"" {
  name = ""test-video-stream.com""
}

resource ""aws_route53_record"" ""lb_ipv4"" {
  type    = ""A""
  name    = ""lb""
  zone_id = aws_route53_zone.test-video-stream.zone_id

  alias {
    name                   = aws_lb.lb.dns_name
    zone_id                = aws_lb.lb.zone_id
    evaluate_target_health = true
  }
}

output ""lb_endpoint"" {
  value = ""http://${aws_lb.lb.dns_name}""
}

output ""application_endpoint"" {
  value = ""http://${aws_lb.lb.dns_name}/index.php""
}

output ""asg_name"" {
  value = aws_autoscaling_group.asg.name
}
","have one ""aws_s3_bucket"" resource
have one ""aws_lb"" resource
  with ""subnets"" argument linking to aws_subnet resources
  with ""security_groups"" argument linking aws_security_group resource
have one ""aws_lb_listener"" resource
  with ""load_balancer_arn"" argument linking to the aws_lb resource
  with ""default_action"" argument block
    with ""type"" argument with value ""forward""
    with ""target_group_arn"" argument linking to aws_lb_target_group resource
with one ""aws_lb_target_group"" resource
  with ""vpc_id"" argument linking to the aws_vpc resource
with one ""aws_lb_target_group_attachment"" resource
  with ""target_group_arn"" argument linking to the aws_lb_target_group resource
  with ""target_id"" argument linking to the aws_instance resource
have one ""aws_ami"" data resource
have one ""aws_instance"" resource
  with ""instance_type"" argument
  with ""ami"" argument linking to the aws_ami data resource
have one ""aws_vpc"" resource
  with ""cidr_block"" argument
have two ""aws_subnet"" resources
  with ""vpc_id"" argument linking to the aws_vpc resource
  with ""cidr_block"" argument
have one ""aws_security_group"" resource
  with ""vpc_id"" argument linking to the aws_vpc resource
have one ""aws_route53_zone"" resource
  with ""name"" argument
have one ""aws_route53_record"" resources
  with ""type"" argument
  with ""name"" argument
  with ""zone_id"" argument linking to the aws_route53_zone resource
  with ""alias"" argument block
    with ""name"" argument linking to the aws_lb dns_name
    with ""zone_id"" argument linking to aws_lb zone_id
    with ""evaluate_target_health"" argument","I'll help you create a Terraform configuration for a video streaming site with load balancing. Here's a comprehensive solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""streaming-vpc""
  }
}

# Public Subnets
resource ""aws_subnet"" ""public"" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.${count.index + 1}.0/24""
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = ""streaming-public-subnet-${count.index + 1}""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""streaming-igw""
  }
}

# Route Table
resource ""aws_route_table"" ""public"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""streaming-public-rt""
  }
}

# Security Group
resource ""aws_security_group"" ""web"" {
  name        = ""web-server-sg""
  description = ""Security group for web servers""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# EC2 Instances
resource ""aws_instance"" ""web"" {
  count         = 2
  ami           = ""ami-0c55b159cbfafe1f0""
  instance_type = ""t2.micro""
  subnet_id     = aws_subnet.public[count.index].id

  vpc_security_group_ids = [aws_security_group.web.id]

  tags = {
    Name = ""streaming-server-${count.index + 1}""
  }
}

# Application Load Balancer
resource ""aws_lb"" ""web"" {
  name               = ""streaming-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.web.id]
  subnets            = aws_subnet.public[*].id

  tags = {
    Name = ""streaming-alb""
  }
}

# ALB Target Group
resource ""aws_lb_target_group"" ""web"" {
  name     = ""streaming-tg""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = aws_vpc.main.id

  health_check {
    path                = ""/""
    healthy_threshold   = 2
    unhealthy_threshold = 10
  }
}

# ALB Target Group Attachment
resource ""aws_lb_target_group_attachment"" ""web"" {
  count            = 2
  target_group_arn = aws_lb_target_group.web.arn
  target_id        = aws_instance.web[",,,,,,"I'll help you create a Terraform configuration for a video streaming site with load balancing. Here's a comprehensive solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""streaming-vpc""
  }
}

# Public Subnets
resource ""aws_subnet"" ""public"" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.${count.index + 1}.0/24""
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = ""streaming-public-subnet-${count.index + 1}""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""streaming-igw""
  }
}

# Route Table
resource ""aws_route_table"" ""public"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""streaming-public-rt""
  }
}

# Security Group
resource ""aws_security_group"" ""web"" {
  name        = ""web-server-sg""
  description = ""Security group for web servers""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# EC2 Instances
resource ""aws_instance"" ""web"" {
  count         = 2
  ami           = ""ami-0c55b159cbfafe1f0""
  instance_type = ""t2.micro""
  subnet_id     = aws_subnet.public[count.index].id

  vpc_security_group_ids = [aws_security_group.web.id]

  tags = {
    Name = ""streaming-server-${count.index + 1}""
  }
}

# Application Load Balancer
resource ""aws_lb"" ""web"" {
  name               = ""streaming-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.web.id]
  subnets           = aws_subnet.public[*].id

  tags = {
    Name = ""streaming-alb""
  }
}

# ALB Target Group
resource ""aws_lb_target_group"" ""web"" {
  name     = ""streaming-tg""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = aws_vpc.main.id

  health_check {
    path                = ""/""
    healthy_threshold   = 2
    unhealthy_threshold = 10
  }
}

# ALB Target Group Attachment
resource ""aws_lb_target_group_attachment"" ""web"" {
  count            = 2
  target_group_arn = aws_lb_target_group.web.arn
  target_id        = aws_instance.web[",,,,,,"I'll help you create a Terraform configuration for a video streaming site with load balancing. Here's a comprehensive solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""
  version = ""~> 3.0""

  name = ""streaming-vpc""
  cidr = ""10.0.0.0/16""

  azs             = [""us-west-2a"", ""us-west-2b""]
  private_subnets = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets  = [""10.0.101.0/24"", ""10.0.102.0/24""]

  enable_nat_gateway = true
  enable_vpn_gateway = false
}

# Security Group
resource ""aws_security_group"" ""web_server"" {
  name        = ""web-server-sg""
  description = ""Security group for web servers""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# EC2 Instances
resource ""aws_instance"" ""web_servers"" {
  count         = 2
  ami           = ""ami-0c55b159cbfafe1f0""
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.private_subnets[count.index]

  vpc_security_group_ids = [aws_security_group.web_server.id]

  user_data = <<-EOF
              #!/bin/bash
              yum update -y
              yum install -y httpd
              systemctl start httpd
              systemctl enable httpd
              EOF

  tags = {
    Name = ""web-server-${count.index + 1}""
  }
}

# Application Load Balancer
resource ""aws_lb"" ""streaming_alb"" {
  name               = ""streaming-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.web_server.id]
  subnets           = module.vpc.public_subnets
}

# ALB Target Group
resource ""aws_lb_target_group"" ""streaming_tg"" {
  name     = ""streaming-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    timeout             = 5
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    matcher            = ""200""
  }
}

# Target Group Attachment
resource ""aws_lb_target_group_attachment"" ""streaming_tg_attachment"" {
  count            = 2
  target_group_arn = aws_lb_target_group.streaming_tg.arn
  target_id        = aws_instance.web_servers[count.index].id
  port             = 80
}

# ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.streaming_alb.arn
  port              = ""80""
  protocol          = """,,,,,
aws_ami,Create an AWS aws_ami resource for the latest Amazon Linux 2 AMI,"package terraform

default ami_latest_amazon_linux_2 = false

ami_latest_amazon_linux_2 {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_ami""
    resource.name == ""latest_amazon_linux_2""
    resource.expressions.filter[_].name.constant_value == ""name""
    resource.expressions.filter[_].values.constant_value[_] == ""amzn2-ami-hvm-*-x86_64-gp2""
    resource.expressions.most_recent.constant_value == true
    resource.expressions.owners.constant_value[_] == ""amazon""
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }
}",create aws_ami resource with name = any,"I'll help you create a Terraform configuration to get the latest Amazon Linux 2 AMI using the `aws_ami` data source. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }

  filter {
    name   = ""virtualization-type""
    values = [""hvm""]
  }

  filter {
    name   = ""root-device-type""
    values = [""ebs""]
  }
}

output ""ami_id"" {
  value = data.aws_ami.amazon_linux_2.id
}
```

This configuration:
1. Declares the AWS provider requirement
2. Uses the `aws_ami` data source to find the latest Amazon Linux 2 AMI
3. Filters for:
   - HVM AMIs
   - x86_64 architecture
   - EBS-backed instances
   - Owned by Amazon
4. Includes an output to display the AMI ID

The `most_recent = true` parameter ensures you get the latest version of the AMI. The AMI ID can be referenced in other resources using `data.aws_ami.amazon_linux_2.id`.",,,,,,"I'll help you create a Terraform configuration to get the latest Amazon Linux 2 AMI using the `aws_ami` data source. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }

  filter {
    name   = ""virtualization-type""
    values = [""hvm""]
  }

  filter {
    name   = ""root-device-type""
    values = [""ebs""]
  }

  filter {
    name   = ""state""
    values = [""available""]
  }
}

# Output the AMI ID (optional)
output ""amazon_linux_2_ami_id"" {
  value = data.aws_ami.amazon_linux_2.id
}
```

This configuration:
1. Specifies the AWS provider requirement
2. Uses the `aws_ami` data source to find the latest Amazon Linux 2 AMI
3. Filters for:
   - Most recent version
   - Amazon-owned AMIs
   - HVM virtualization
   - EBS-backed instances
   - Available state
4. Includes an optional output to display the AMI ID

You can reference this AMI in other resources using: `data.aws_ami.amazon_linux_2.id`",,,,,,"I'll help you create a Terraform configuration to get the latest Amazon Linux 2 AMI using the `aws_ami` data source. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }

  filter {
    name   = ""virtualization-type""
    values = [""hvm""]
  }

  filter {
    name   = ""root-device-type""
    values = [""ebs""]
  }

  filter {
    name   = ""state""
    values = [""available""]
  }
}

# Output the AMI ID (optional)
output ""amazon_linux_2_ami_id"" {
  value = data.aws_ami.amazon_linux_2.id
}
```

This configuration:
1. Declares the required AWS provider
2. Uses the `aws_ami` data source to find the latest Amazon Linux 2 AMI
3. Sets filters to ensure we get:
   - The most recent version
   - Only Amazon-owned AMIs
   - The HVM virtualization type
   - EBS-backed instances
   - Only available AMIs
4. Includes an optional output to display the AMI ID

You can reference this AMI in other resources using: `data.aws_ami.amazon_linux_2.id`",,,,,
aws_route53_zone,"Set up an aws_route_53 zone named ""example53.com""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false


# Validate aws_route53_zone resource
is_valid_r53_zone {
	some i
	resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
}
",1,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""","I'll help you create a Terraform configuration for an AWS Route 53 hosted zone named ""example53.com"". Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Specify your desired region
}

resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""
  
  tags = {
    Environment = ""production""
    Name        = ""example53-zone""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Route 53 hosted zone for the domain ""example53.com""
3. Includes some basic tags for better resource management

After applying this configuration, AWS will create a new hosted zone and provide you with a set of name servers that you can use to configure your domain registrar.

Note: You'll need to configure your domain registrar to use the name servers provided by Route 53 to make this hosted zone active for your domain.",,,,,,"I'll help you create a Terraform configuration for setting up an AWS Route 53 zone named ""example53.com"". Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
  
  tags = {
    Environment = ""production""
    Name        = ""example53.com""
  }
}
```

This configuration:
1. Declares the required AWS provider
2. Creates a Route 53 hosted zone named ""example53.com""
3. Includes some basic tags for better resource management

When applied, this will create a new public hosted zone in Route 53. The zone will be assigned a set of name servers that you can use to configure your domain registrar.

Note: Remember that after creating the zone, you'll need to update your domain's name servers at your domain registrar to point to the AWS name servers assigned to this zone.",,,,,,"I'll help you create a Terraform configuration for setting up an AWS Route 53 hosted zone named ""example53.com"". Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create Route 53 hosted zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
  
  tags = {
    Name        = ""example53.com""
    Environment = ""production""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Route 53 hosted zone with the domain name ""example53.com""
3. Includes basic tags for better resource management

Once applied, this will create a public hosted zone in Route 53 that can be used to manage DNS records for the example53.com domain. You'll need to configure your domain registrar to use the name servers provided by this hosted zone.",,,,,
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket where the bucket owner pays for fees.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""
        
        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""BucketOwner""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name
with payer = ""BucketOwner""","I'll help you create a Terraform configuration for an S3 bucket where the bucket owner pays for all fees (Requester Pays disabled). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""owner_pays_bucket"" {
  bucket = ""my-owner-pays-bucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""owner_pays"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id
  payer  = ""BucketOwner""  # BucketOwner means the bucket owner pays for all fees
}

resource ""aws_s3_bucket_versioning"" ""versioning"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

resource ""aws_s3_bucket_server_side_encryption_configuration"" ""encryption"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = ""AES256""
    }
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an S3 bucket
3. Configures the request payment settings to ensure the bucket owner pays for all fees
4. Enables versioning (as a best practice)
5. Enables default encryption (as a best practice)

The key part for your requirement is the `aws_s3_bucket_request_payment_configuration` resource where `payer = ""BucketOwner""` ensures that the bucket owner, not the requester, pays for all fees associated with the bucket.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket where the bucket owner pays for all fees using the `request_payer` attribute. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""owner_pays_bucket"" {
  bucket = ""my-owner-pays-bucket""

  # Enable bucket owner pays feature
  request_payer = ""BucketOwner""

  tags = {
    Name        = ""Owner Pays Bucket""
    Environment = ""Production""
  }
}

# Configure bucket versioning (recommended for data protection)
resource ""aws_s3_bucket_versioning"" ""versioning"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Configure bucket ownership controls
resource ""aws_s3_bucket_ownership_controls"" ""ownership"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id

  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}
```

This configuration:
1. Creates an S3 bucket where the bucket owner pays for all requests and data transfer costs
2. Sets the `request_payer` attribute to ""BucketOwner""
3. Enables versioning for better data protection
4. Configures bucket ownership controls to ensure the bucket owner has full control
5. Includes basic tags for resource management

Remember to replace ""my-owner-pays-bucket"" with your desired bucket name, as S3 bucket names must be globally unique.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket where the bucket owner pays for all fees (Requester Pays disabled). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""owner_pays_bucket"" {
  bucket = ""my-owner-pays-bucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id
  payer  = ""BucketOwner""  # BucketOwner means the bucket owner pays for all fees
}

resource ""aws_s3_bucket_versioning"" ""versioning"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

resource ""aws_s3_bucket_server_side_encryption_configuration"" ""encryption"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = ""AES256""
    }
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an S3 bucket
3. Configures the request payment settings to ensure the bucket owner pays for all fees
4. Enables versioning (best practice)
5. Enables server-side encryption (best practice)

The key part is the `aws_s3_bucket_request_payment_configuration` resource where `payer = ""BucketOwner""` ensures that the bucket owner, not the requester, pays for all requests and data transfer costs.

Remember to replace ""my-owner-pays-bucket"" with your desired bucket name, as S3 bucket names must be globally unique.",,,,,
"aws_eks_cluster, aws_eks_fargate_profile, aws_iam_role, aws_iam_role, aws_iam_role_policy_attachment, aws_subnet, aws_subnet, aws_vpc, null_resource, aws_eks_cluster_auth","Create an IAM role for EKS Fargate profiles with the necessary policies for pod execution. Attach the AmazonEKSFargatePodExecutionRolePolicy to the IAM role. Define an AWS EKS Fargate profile for the 'kube-system' namespace, associating it with the EKS cluster and specifying the pod execution role ARN. Ensure that the specified subnets have the required resource tag for cluster association. Use a null_resource to patch the Kubernetes deployment 'coredns' in the 'kube-system' namespace, using the EKS cluster's endpoint, certificate authority, and authentication token for communication.","package terraform.validation

default has_aws_iam_role_fargate = false
default has_aws_iam_role_policy_attachment_fargate = false
default has_aws_eks_fargate_profile = false
default has_aws_eks_cluster_auth = false
default has_null_resource_k8s_patcher = false

has_aws_iam_role_fargate {
    role := input.planned_values.root_module.resources[_]
    role.type == ""aws_iam_role""
    role.name == ""eks-fargate-profile""
    role.values.name == ""eks-fargate-profile""
    role.values.assume_role_policy != null
}

has_aws_iam_role_policy_attachment_fargate {
    attachment := input.planned_values.root_module.resources[_]
    attachment.type == ""aws_iam_role_policy_attachment""
    attachment.name == ""eks-fargate-profile""
    attachment.values.policy_arn == ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
    attachment.values.role == input.planned_values.root_module.resources[_].values.name  # Ensure role is correctly referenced
}

has_aws_eks_fargate_profile {
    fargate_profile := input.configuration.root_module.resources[_]
    fargate_profile.type == ""aws_eks_fargate_profile""
    fargate_profile.name == ""kube-system""
    fargate_profile.expressions.cluster_name != null
    fargate_profile.expressions.fargate_profile_name.constant_value == ""kube-system""
    fargate_profile.expressions.pod_execution_role_arn != null
    count(fargate_profile.expressions.subnet_ids.references) == 4  # Ensure there are two subnet IDs
    fargate_profile.expressions.selector[_].namespace.constant_value == ""kube-system""
}

has_aws_eks_cluster_auth {
    cluster_auth := input.configuration.root_module.resources[_]
    cluster_auth.type == ""aws_eks_cluster_auth""
    cluster_auth.name != null  # Check for proper referencing
}

has_null_resource_k8s_patcher {
    k8s_patcher := input.configuration.root_module.resources[_]
    k8s_patcher.type == ""null_resource""
    k8s_patcher.name == ""k8s_patcher""
    k8s_patcher.depends_on != null
    count(k8s_patcher.expressions.triggers.references) == 8  # Check for three triggers
    k8s_patcher.provisioners[0].type == ""local-exec""
    k8s_patcher.provisioners[0].expressions.command != null
}

valid_configuration {
    has_aws_iam_role_fargate
    has_aws_iam_role_policy_attachment_fargate
    has_aws_eks_fargate_profile
    has_aws_eks_cluster_auth
    has_null_resource_k8s_patcher
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

resource ""aws_subnet"" ""test_eks_1"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  availability_zone = data.aws_availability_zones.available.names[0]

  tags = {
    ""kubernetes.io/cluster/test"" = ""shared""
  }
}

resource ""aws_subnet"" ""test_eks_2"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""

  availability_zone = data.aws_availability_zones.available.names[1]

  tags = {
    ""kubernetes.io/cluster/test"" = ""shared""
  }
}

resource ""aws_iam_role"" ""eks-cluster"" {
  name = ""eks-cluster-1""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""eks.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.eks-cluster.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.eks-cluster.name
}


resource ""aws_eks_cluster"" ""cluster"" {
  name     = ""test""
  role_arn = aws_iam_role.eks-cluster.arn

  vpc_config {
    subnet_ids = [
      aws_subnet.test_eks_1.id,
      aws_subnet.test_eks_2.id
    ]
  }

  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy,
    aws_iam_role_policy_attachment.eks_service_policy,
  ]
}

resource ""aws_iam_role"" ""eks-fargate-profile"" {
  name = ""eks-fargate-profile""

  assume_role_policy = jsonencode({
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""eks-fargate-pods.amazonaws.com""
      }
    }]
    Version = ""2012-10-17""
  })
}

resource ""aws_iam_role_policy_attachment"" ""eks-fargate-profile"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
  role       = aws_iam_role.eks-fargate-profile.name
}

resource ""aws_eks_fargate_profile"" ""kube-system"" {
  cluster_name           = aws_eks_cluster.cluster.name
  fargate_profile_name   = ""kube-system""
  pod_execution_role_arn = aws_iam_role.eks-fargate-profile.arn

  subnet_ids = [
    aws_subnet.test_eks_1.id,
    aws_subnet.test_eks_2.id
  ]

  selector {
    namespace = ""kube-system""
  }

  depends_on = [ aws_iam_role_policy_attachment.eks-fargate-profile ]
}

data ""aws_eks_cluster_auth"" ""eks"" {
  name = aws_eks_cluster.cluster.id
}

resource ""null_resource"" ""k8s_patcher"" {
  depends_on = [aws_eks_fargate_profile.kube-system]

  triggers = {
    endpoint = aws_eks_cluster.cluster.endpoint
    ca_crt   = base64decode(aws_eks_cluster.cluster.certificate_authority[0].data)
    token    = data.aws_eks_cluster_auth.eks.token
  }

  provisioner ""local-exec"" {
    command = <<EOH
      cat >/tmp/ca.crt <<EOF
      ${base64decode(aws_eks_cluster.cluster.certificate_authority[0].data)}
      EOF
      kubectl \
        --server=""${aws_eks_cluster.cluster.endpoint}"" \
        --certificate_authority=/tmp/ca.crt \
        --token=""${data.aws_eks_cluster_auth.eks.token}"" \
        patch deployment coredns \
        -n kube-system --type json \
        -p='[{""op"": ""remove"", ""path"": ""/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type""}]'
      EOH
  }

  lifecycle {
    ignore_changes = [triggers]
  }
}","Resource ""aws_iam_role"" ""eks-fargate-profile""
has one ""name""
has one ""assume_role_policy""
Resource ""aws_iam_role_policy_attachment"" ""eks-fargate-profile""
has one ""policy_arn""
has one ""role""
Resource ""aws_eks_fargate_profile"" ""kube-system""
has one ""cluster_name""
has one ""fargate_profile_name""
has one ""pod_execution_role_arn""
has one ""subnet_ids""
has one ""namespace"" in ""selector""
Data ""aws_eks_cluster_auth"" ""eks""
has one ""name""
Resource ""null_resource"" ""k8s_patcher""
has multiple ""triggers"" (endpoint, ca_crt, token)
uses a ""local-exec"" provisioner to execute a command that patches the Kubernetes deployment
has a ""lifecycle"" policy to ignore changes in ""triggers""","I'll help you create a Terraform configuration for an EKS Fargate profile with the necessary IAM role and CoreDNS patch. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
    null = {
      source  = ""hashicorp/null""
      version = ""~> 3.0""
    }
  }
}

# IAM Role for Fargate Profile
resource ""aws_iam_role"" ""eks_fargate_pod_execution_role"" {
  name = ""eks-fargate-pod-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""eks-fargate-pods.amazonaws.com""
        }
      }
    ]
  })
}

# Attach the required policy for Fargate pod execution
resource ""aws_iam_role_policy_attachment"" ""eks_fargate_pod_execution_role_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
  role       = aws_iam_role.eks_fargate_pod_execution_role.name
}

# EKS Fargate Profile
resource ""aws_eks_fargate_profile"" ""kube_system"" {
  cluster_name           = var.cluster_name
  fargate_profile_name   = ""kube-system-profile""
  pod_execution_role_arn = aws_iam_role.eks_fargate_pod_execution_role.arn
  subnet_ids             = var.private_subnet_ids

  selector {
    namespace = ""kube-system""
  }

  depends_on = [aws_iam_role_policy_attachment.eks_fargate_pod_execution_role_policy]
}

# Data source to get EKS cluster auth token
data ""aws_eks_cluster"" ""cluster"" {
  name = var.cluster_name
}

data ""aws_eks_cluster_auth"" ""cluster"" {
  name = var.cluster_name
}

# Null resource to patch CoreDNS deployment
resource ""null_resource"" ""patch_coredns"" {
  depends_on = [aws_eks_fargate_profile.kube_system]

  provisioner ""local-exec"" {
    command = <<EOF
      aws eks get-token --cluster-name ${var.cluster_name} --region ${var.aws_region} | \
      kubectl patch deployment coredns \
        -n kube-system \
        --type json \
        -p='[{""op"": ""remove"", ""path"": ""/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type""},{""op"": ""replace"", ""path"": ""/spec/template/metadata/annotations"", ""value"": {""eks.amazonaws.com/compute-type"": ""fargate""}}]' \
        --kubeconfig <(echo '${data.aws_eks_cluster.cluster.certificate_authority[0].data}')
    EOF
  }
}

# Variables
variable ""cluster_name"" {
  description = ""Name of the EKS cluster""
  type        = string
}

variable ""private_subnet_ids"" {
  description = ""List of private subnet IDs for the Fargate profile""
  type        = list(string)
}

variable ""aws_region"" {
  description = ""AWS region""
  type        = string
}
```

This configuration:

1. Creates an IAM role for Fargate pod execution with the required trust policy
2. Attaches the AmazonEKSFargatePodExecutionRolePolicy to the IAM role
3. Creates a Fargate profile for the kube-system namespace
4. Uses a null_resource to patch the CoreDNS deployment to run on Fargate

Make sure to set the following variables when using this configuration:
- cluster_name: Your EKS cluster name
- private_subnet_ids:",,,,,,"I'll help you create a Terraform configuration for setting up an EKS Fargate profile with the necessary IAM roles and policies. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
    kubernetes = {
      source  = ""hashicorp/kubernetes""
      version = ""~> 2.0""
    }
  }
}

# IAM Role for EKS Fargate Profile
resource ""aws_iam_role"" ""eks_fargate_pod_execution_role"" {
  name = ""eks-fargate-pod-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""eks-fargate-pods.amazonaws.com""
        }
      }
    ]
  })
}

# Attach the required policy for Fargate pod execution
resource ""aws_iam_role_policy_attachment"" ""eks_fargate_pod_execution_role_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AmazonEKSFargatePodExecutionRolePolicy""
  role       = aws_iam_role.eks_fargate_pod_execution_role.name
}

# EKS Fargate Profile
resource ""aws_eks_fargate_profile"" ""kube_system"" {
  cluster_name           = var.cluster_name
  fargate_profile_name   = ""kube-system-profile""
  pod_execution_role_arn = aws_iam_role.eks_fargate_pod_execution_role.arn
  subnet_ids             = var.private_subnet_ids

  selector {
    namespace = ""kube-system""
  }

  depends_on = [aws_iam_role_policy_attachment.eks_fargate_pod_execution_role_policy]
}

# Data source for EKS cluster auth
data ""aws_eks_cluster"" ""cluster"" {
  name = var.cluster_name
}

data ""aws_eks_cluster_auth"" ""cluster"" {
  name = var.cluster_name
}

# Null resource to patch CoreDNS deployment
resource ""null_resource"" ""patch_coredns"" {
  depends_on = [aws_eks_fargate_profile.kube_system]

  provisioner ""local-exec"" {
    command = <<EOF
      aws eks get-token --cluster-name ${var.cluster_name} --region ${var.aws_region} | \
      kubectl patch deployment coredns \
        -n kube-system \
        --type json \
        -p='[{""op"": ""remove"", ""path"": ""/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type""}]' \
        --server=""${data.aws_eks_cluster.cluster.endpoint}"" \
        --certificate-authority=""${data.aws_eks_cluster.cluster.certificate_authority[0].data}"" \
        --token=""${data.aws_eks_cluster_auth.cluster.token}""
    EOF
  }
}

# Variables
variable ""cluster_name"" {
  type        = string
  description = ""Name of the EKS cluster""
}

variable ""private_subnet_ids"" {
  type        = list(string)
  description = ""List of private subnet IDs for the Fargate profile""
}

variable ""aws_region"" {
  type        = string
  description = ""AWS region""
}
```

This configuration:

1. Creates an IAM role for Fargate pod execution with the necessary trust relationship
2. Attaches the AmazonEKSFargatePodExecutionRolePolicy to the IAM role
3. Creates a Fargate profile for the kube-system namespace
4. Sets up data sources to get cluster authentication details
5. Creates a null_resource to patch the CoreDNS deployment to work with Fargate

Make sure to:
- Replace the variable values with your actual cluster name, subnet IDs, an",,,,,,"I'll help you create a Terraform configuration for an EKS Fargate profile with the necessary IAM role and CoreDNS patch. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
    null = {
      source  = ""hashicorp/null""
      version = ""~> 3.0""
    }
  }
}

# IAM Role for Fargate Profile
resource ""aws_iam_role"" ""eks_fargate_pod_execution_role"" {
  name = ""eks-fargate-pod-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""eks-fargate-pods.amazonaws.com""
        }
      }
    ]
  })
}

# Attach the required policy for Fargate pod execution
resource ""aws_iam_role_policy_attachment"" ""eks_fargate_pod_execution_role_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
  role       = aws_iam_role.eks_fargate_pod_execution_role.name
}

# EKS Fargate Profile
resource ""aws_eks_fargate_profile"" ""kube_system"" {
  cluster_name           = var.cluster_name
  fargate_profile_name   = ""kube-system-profile""
  pod_execution_role_arn = aws_iam_role.eks_fargate_pod_execution_role.arn
  subnet_ids             = var.private_subnet_ids

  selector {
    namespace = ""kube-system""
  }

  tags = {
    ""kubernetes.io/cluster/${var.cluster_name}"" = ""owned""
  }
}

# Data source to get EKS cluster auth token
data ""aws_eks_cluster"" ""cluster"" {
  name = var.cluster_name
}

data ""aws_eks_cluster_auth"" ""cluster"" {
  name = var.cluster_name
}

# Null resource to patch CoreDNS deployment
resource ""null_resource"" ""patch_coredns"" {
  depends_on = [aws_eks_fargate_profile.kube_system]

  provisioner ""local-exec"" {
    command = <<-EOT
      kubectl patch deployment coredns \
        -n kube-system \
        --type json \
        -p='[{""op"": ""remove"", ""path"": ""/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type""},{""op"": ""replace"", ""path"": ""/spec/template/spec/affinity"", ""value"": {""nodeAffinity"": {""requiredDuringSchedulingIgnoredDuringExecution"": {""nodeSelectorTerms"": [{""matchExpressions"": [{""key"": ""eks.amazonaws.com/compute-type"", ""operator"": ""In"", ""values"": [""fargate""]}]}}]}}}]' \
        --kubeconfig <(echo '${local.kubeconfig}')
    EOT
  }
}

# Local variable for kubeconfig
locals {
  kubeconfig = <<-EOT
    apiVersion: v1
    clusters:
    - cluster:
        server: ${data.aws_eks_cluster.cluster.endpoint}
        certificate-authority-data: ${data.aws_eks_cluster.cluster.certificate_authority[0].data}
      name: ${var.cluster_name}
    contexts:
    - context:
        cluster: ${var.cluster_name}
        user: ${var.cluster_name}
      name: ${var.cluster_name}
    current-context: ${var.cluster_name}
    kind: Config
    preferences: {}
    users:
    - name: ${var.cluster_name}
      user:
        token: ${data.aws_eks_cluster_auth.cluster.token}
    EOT
}

# Variables",,,,,
"aws_iam_role, aws_lambda_function, archive_file, aws_iam_policy_document","Create an AWS Lambda function. The function should be triggered by a nodejs18 runtime,use a ""lambda.js"" as the source code and a ""index.js"" as the handler. Please declare all necessary resources.","package main

import future.keywords.in

default allow = false

# Check for IAM Role for Lambda
aws_iam_role_for_lambda_valid(resources) {
    some resource in resources
    resource.type == ""aws_iam_role""
}

# Check for AWS Lambda Function with specific configurations
aws_lambda_function_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_function""
    resource.change.after.runtime == ""nodejs18.x""
    resource.change.after.handler == ""index.test""
    resource.change.after.filename == ""lambda_function_payload.zip""
}

# Check for Archive File for Lambda code
archive_file_for_lambda_valid(resources) {
    some resource in resources
    resource.type == ""archive_file""
    resource.values.source_file == ""lambda.js""
    resource.values.type == ""zip""
}

# Aggregate all checks
allow {
    aws_iam_role_for_lambda_valid(input.resource_changes)
    aws_lambda_function_valid(input.resource_changes)
    archive_file_for_lambda_valid(input.prior_state.values.root_module.resources)
}
",3,"data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""lambda.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""iam_for_lambda"" {
  name               = ""iam_for_lambda""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

data ""archive_file"" ""lambda"" {
  type        = ""zip""
  source_file = ""lambda.js""
  output_path = ""lambda_function_payload.zip""
}

resource ""aws_lambda_function"" ""test_lambda"" {
  filename      = ""lambda_function_payload.zip""
  function_name = ""lambda_function_name""
  role          = aws_iam_role.iam_for_lambda.arn
  handler       = ""index.test""

  runtime = ""nodejs18.x""
}","Needs to have a defined IAM role. Data for a .zip file, with source file = lambda.js, function name can be anything. For the aws_lambda_function use the defined zip, function name, set handler to anything, and runtime to ""nodejs.18x""","I'll help you create an AWS Lambda function with the specified requirements. Here's a complete Terraform configuration that includes all necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_execution_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy attachment for basic Lambda execution
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Archive file for Lambda function code
data ""archive_file"" ""lambda_zip"" {
  type        = ""zip""
  source_file = ""lambda.js""
  output_path = ""lambda_function.zip""
}

# Lambda function
resource ""aws_lambda_function"" ""example_lambda"" {
  filename         = ""lambda_function.zip""
  function_name    = ""example_lambda_function""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.js""
  source_code_hash = data.archive_file.lambda_zip.output_base64sha256
  runtime         = ""nodejs18.x""

  environment {
    variables = {
      NODE_ENV = ""production""
    }
  }
}
```

This configuration includes:

1. The required AWS provider configuration
2. An IAM role for the Lambda function with basic execution permissions
3. The necessary IAM policy attachment
4. An archive resource to zip the Lambda function code
5. The Lambda function resource itself with the specified:
   - NodeJS 18.x runtime
   - Source code from lambda.js
   - Handler set to index.js
   - Basic environment configuration

Make sure to:
1. Have your `lambda.js` file in the same directory as your Terraform configuration
2. Adjust the region in the provider block as needed
3. The Lambda function will be created with basic execution permissions. Add more IAM permissions if needed for your specific use case.

The configuration follows AWS best practices and includes all necessary components for a functioning Lambda function.",,,,,,"I'll help you create a Terraform configuration for an AWS Lambda function with the specified requirements. This will include the Lambda function itself, an IAM role, and necessary permissions.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_execution_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Basic Lambda execution policy attachment
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Lambda function
resource ""aws_lambda_function"" ""example_lambda"" {
  filename         = ""lambda.js""
  function_name    = ""example_lambda_function""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.js""
  runtime         = ""nodejs18.x""

  environment {
    variables = {
      ENVIRONMENT = ""production""
    }
  }
}
```

This configuration includes:

1. The required AWS provider declaration
2. An IAM role for the Lambda function with necessary assume role policy
3. The basic Lambda execution policy attachment
4. The Lambda function resource with the specified:
   - nodejs18.x runtime
   - lambda.js as the source code file
   - index.js as the handler
   - Basic environment configuration

Make sure to have the `lambda.js` file in your working directory when applying this configuration. The IAM role includes the basic execution permissions needed for CloudWatch Logs.",,,,,,"I'll help you create an AWS Lambda function with the specified requirements. Here's a complete Terraform configuration that includes all necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_execution_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy attachment for basic Lambda execution
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Archive file for Lambda function code
data ""archive_file"" ""lambda_zip"" {
  type        = ""zip""
  source_file = ""lambda.js""
  output_path = ""lambda_function.zip""
}

# Lambda function
resource ""aws_lambda_function"" ""example_lambda"" {
  filename         = ""lambda_function.zip""
  function_name    = ""example_lambda_function""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.js""
  source_code_hash = data.archive_file.lambda_zip.output_base64sha256
  runtime         = ""nodejs18.x""

  environment {
    variables = {
      NODE_ENV = ""production""
    }
  }
}
```

This configuration includes:

1. Required AWS provider declaration
2. An IAM role for Lambda execution
3. Basic Lambda execution policy attachment
4. Archive resource to zip the Lambda function code
5. The Lambda function itself with the specified:
   - NodeJS 18.x runtime
   - index.js as the handler
   - lambda.js as the source code (which will be zipped)

Make sure to:
1. Have your `lambda.js` file in the same directory as your Terraform configuration
2. Adjust the region in the provider block according to your needs
3. The handler should match your actual exported function name in the JavaScript file

The Lambda function will have basic execution permissions through the attached IAM role and policy. If you need additional permissions, you can attach more policies to the IAM role.",,,,,
"aws_iam_role, aws_kinesisanalyticsv2_application, aws_iam_policy_document",Generate a basic Amazon Kinesis Analytics Application with a CloudWatch log stream,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kinesisanalytics.amazonaws.com"") 
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
}

has_valid_cloudwatch_log_group {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_cloudwatch_log_group""
    resource.values.name
}

has_valid_cloudwatch_log_stream {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_cloudwatch_log_stream""
    resource.values.name
    role := input.configuration.root_module.resources[i]
    role.expressions.log_group_name

}

has_valid_kinesis_analytics_application {
        some i
        resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_analytics_application""
    resource.values.name
    resource.values.cloudwatch_logging_options
    
}

has_valid_resources {
	has_valid_bucket
    has_valid_iam_role(input.resource_changes)
    has_valid_cloudwatch_log_group
    has_valid_cloudwatch_log_stream
    has_valid_kinesis_analytics_application
}",3,"provider ""aws"" {
  region = ""us-west-2""
}

data ""aws_iam_policy_document"" ""KinesisAssume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}


resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_role""
  assume_role_policy = data.aws_iam_policy_document.KinesisAssume.json
}

resource ""aws_kinesisanalyticsv2_application"" ""flink_app"" {
  name                 = ""example-application1""
  runtime_environment  = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_role.arn
}","Has one ""aws_cloudwatch_log_group"" resource, one ""aws_cloudwatch_log_stream"" resource, one ""aws_iam_role"" resource, one ""aws_s3_bucket"" resource, and one ""aws_kinesis_analytics_application"" resource with a ""cloudwatch_logging_options"" block
","I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with CloudWatch log stream. Here's a complete solution that includes the required providers and necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""kinesis_analytics_logs"" {
  name              = ""/aws/kinesis-analytics/my-application""
  retention_in_days = 14
}

# Create CloudWatch Log Stream
resource ""aws_cloudwatch_log_stream"" ""kinesis_analytics_log_stream"" {
  name           = ""kinesis-analytics-log-stream""
  log_group_name = aws_cloudwatch_log_group.kinesis_analytics_logs.name
}

# Create IAM Role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-application-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Create IAM Role Policy for CloudWatch Logs
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-cloudwatch-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:PutLogEvents"",
          ""logs:DescribeLogStreams"",
          ""logs:DescribeLogGroups"",
          ""logs:CreateLogStream""
        ]
        Resource = [
          ""${aws_cloudwatch_log_group.kinesis_analytics_logs.arn}"",
          ""${aws_cloudwatch_log_group.kinesis_analytics_logs.arn}:*""
        ]
      }
    ]
  })
}

# Create Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""my-kinesis-analytics-app""

  cloudwatch_logging_options {
    log_stream_arn = aws_cloudwatch_log_stream.kinesis_analytics_log_stream.arn
    role_arn       = aws_iam_role.kinesis_analytics_role.arn
  }

  code = <<-EOF
    CREATE OR REPLACE STREAM ""DESTINATION_SQL_STREAM"" (
        ticker_symbol VARCHAR(4),
        price        DOUBLE,
        event_time   TIMESTAMP
    );
    CREATE OR REPLACE PUMP ""STREAM_PUMP"" AS
        INSERT INTO ""DESTINATION_SQL_STREAM""
        SELECT STREAM ticker_symbol, price, ROWTIME as event_time
        FROM ""SOURCE_SQL_STREAM_001"";
  EOF
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a CloudWatch Log Group and Log Stream
3. Creates an IAM role and policy for Kinesis Analytics to access CloudWatch Logs
4. Creates a basic Kinesis Analytics Application with:
   - CloudWatch logging configuration
   - Basic SQL code for stream processing
   - Proper IAM role association

The application will be able to write logs to CloudWatch through the specified log stream. The example includes a simple SQL code that creates a destination stream and pumps data from a source stream to it.

Remember to:
- Adjust the region in the provider block
- Modify the retention period for logs if needed
- Update the application name and log group names as per your requirements
- Customize the SQL code according to your actual stream processing needs",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with CloudWatch log stream integration. This will include the necessary resources and IAM roles for proper functionality.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# IAM role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-application-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""analytics_log_group"" {
  name              = ""/aws/kinesis-analytics/my-application""
  retention_in_days = 14
}

# CloudWatch Log Stream
resource ""aws_cloudwatch_log_stream"" ""analytics_log_stream"" {
  name           = ""kinesis-analytics-log-stream""
  log_group_name = aws_cloudwatch_log_group.analytics_log_group.name
}

# IAM policy for CloudWatch Logs
resource ""aws_iam_role_policy"" ""cloudwatch_logs_policy"" {
  name = ""kinesis-analytics-cloudwatch-logs-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:PutLogEvents"",
          ""logs:DescribeLogStreams"",
          ""logs:DescribeLogGroups"",
          ""logs:CreateLogStream""
        ]
        Resource = [
          ""${aws_cloudwatch_log_group.analytics_log_group.arn}"",
          ""${aws_cloudwatch_log_group.analytics_log_group.arn}:*""
        ]
      }
    ]
  })
}

# Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""my-kinesis-analytics-app""

  cloudwatch_logging_options {
    log_stream_arn = aws_cloudwatch_log_stream.analytics_log_stream.arn
    role_arn       = aws_iam_role.kinesis_analytics_role.arn
  }

  inputs {
    name_prefix = ""test_prefix""

    schema_version = ""1""

    kinesis_streams_input {
      resource_arn = ""arn:aws:kinesis:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:stream/example-stream""
      role_arn     = aws_iam_role.kinesis_analytics_role.arn
    }

    starting_position_configuration {
      starting_position = ""NOW""
    }
  }
}

# Get current region
data ""aws_region"" ""current"" {}

# Get current account ID
data ""aws_caller_identity"" ""current"" {}
```

This configuration creates:

1. An IAM role for the Kinesis Analytics application
2. A CloudWatch Log Group and Log Stream
3. An IAM policy allowing the Kinesis Analytics application to write to CloudWatch Logs
4. A Kinesis Analytics application with CloudWatch logging enabled

Key features:
- The CloudWatch Log Group has a 14-day retention period
- The IAM role has the necessary permissions for Kinesis Analytics to write to CloudWatch Logs
- The Kinesis Analytics application is configured with basic input settings and CloudWatch logging
- The configuration uses data sources to get the current region and account ID

Remember to:
1. Replace the region in the provider block with your desired region
2. Adjust the retention period for logs if",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with CloudWatch logging. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""kinesis_analytics_logs"" {
  name              = ""/aws/kinesis-analytics/myapp""
  retention_in_days = 14
}

# Create CloudWatch Log Stream
resource ""aws_cloudwatch_log_stream"" ""kinesis_analytics_log_stream"" {
  name           = ""kinesis-analytics-log-stream""
  log_group_name = aws_cloudwatch_log_group.kinesis_analytics_logs.name
}

# Create IAM Role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Create IAM Policy for CloudWatch Logs
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:PutLogEvents"",
          ""logs:DescribeLogStreams"",
          ""logs:DescribeLogGroups"",
          ""logs:CreateLogStream""
        ]
        Resource = [
          aws_cloudwatch_log_group.kinesis_analytics_logs.arn,
          ""${aws_cloudwatch_log_group.kinesis_analytics_logs.arn}:*""
        ]
      }
    ]
  })
}

# Create Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""my-kinesis-analytics-app""

  cloudwatch_logging_options {
    log_stream_arn = aws_cloudwatch_log_stream.kinesis_analytics_log_stream.arn
    role_arn       = aws_iam_role.kinesis_analytics_role.arn
  }

  inputs {
    name_prefix = ""TEST_PREFIX""

    schema_version = ""1""

    kinesis_streams_input {
      resource_arn = ""arn:aws:kinesis:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:stream/example-stream""
      role_arn     = aws_iam_role.kinesis_analytics_role.arn
    }

    starting_position_configuration {
      starting_position = ""NOW""
    }
  }
}

# Data sources for current region and account ID
data ""aws_region"" ""current"" {}
data ""aws_caller_identity"" ""current"" {}
```

This configuration creates:

1. A CloudWatch Log Group and Log Stream for the Kinesis Analytics application
2. An IAM role and policy for the Kinesis Analytics application to write logs
3. A Kinesis Analytics application with CloudWatch logging enabled
4. Necessary permissions and configurations for the application to function

Key features:
- Log retention is set to 14 days
- The application uses the current region and account ID
- Proper IAM permissions for CloudWatch logging
- Basic input configuration for the Kinesis Analytics application

Remember to:
1. Replace the region in the provider block if needed
2. Adjust the log retention period as per your requirements
3. Modify the input configuration according to your specific use case
4. Update the names and prefixes to match your naming conventions

This configuration provides a foundation",,,,,
"aws_dynamodb_table, aws_iam_role, aws_lambda_event_source_mapping, aws_lambda_function, archive_file, aws_iam_policy_document",Create a Lambda event source mapping from a DynamoDB,"package main

import future.keywords.in

default allow = false

aws_dynamodb_table_valid(resources) {
    some resource in resources
    resource.type == ""aws_dynamodb_table""
}

aws_lambda_function_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_function""
}

# Check for Lambda event source mapping from DynamoDB
aws_lambda_event_source_mapping_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_event_source_mapping""
}

# Aggregate all checks
allow {
    aws_dynamodb_table_valid(input.resource_changes)
    aws_lambda_function_valid(input.resource_changes)
    aws_lambda_event_source_mapping_valid(input.resource_changes)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""example_table"" {
  name           = ""example_table""
  hash_key       = ""id""
  read_capacity  = 10
  write_capacity = 10

  attribute {
    name = ""id""
    type = ""S""
  }

  # Enable DynamoDB Streams
  stream_enabled   = true
  stream_view_type = ""NEW_AND_OLD_IMAGES"" # Choose as per your requirement

  # Other configurations for your table
}

data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""lambda.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""iam_for_lambda"" {
  name               = ""iam_for_lambda""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

resource ""aws_iam_role_policy"" ""dynamodb_lambda_policy"" {
  name   = ""lambda-dynamodb-policy""
  role   = aws_iam_role.iam_for_lambda.id
  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
        ""Sid"": ""AllowLambdaFunctionToCreateLogs"",
        ""Action"": [ 
            ""logs:*"" 
        ],
        ""Effect"": ""Allow"",
        ""Resource"": [ 
            ""arn:aws:logs:*:*:*"" 
        ]
    },
    {
        ""Sid"": ""AllowLambdaFunctionInvocation"",
        ""Effect"": ""Allow"",
        ""Action"": [
            ""lambda:InvokeFunction""
        ],
        ""Resource"": [
            ""${aws_dynamodb_table.example_table.arn}/stream/*""
        ]
    },
    {
        ""Sid"": ""APIAccessForDynamoDBStreams"",
        ""Effect"": ""Allow"",
        ""Action"": [
            ""dynamodb:GetRecords"",
            ""dynamodb:GetShardIterator"",
            ""dynamodb:DescribeStream"",
            ""dynamodb:ListStreams""
        ],
        ""Resource"": ""${aws_dynamodb_table.example_table.arn}/stream/*""
    }
  ]
}
EOF
}

data ""archive_file"" ""lambda_zip_file"" {
  type        = ""zip""
  source_file = ""./supplement/app.js""
  output_path = ""./supplement/app.zip""
}

resource ""aws_lambda_function"" ""example_lambda"" {
  filename      = data.archive_file.lambda_zip_file.output_path
  function_name = ""lambda_app_function""
  source_code_hash = data.archive_file.lambda_zip_file.output_base64sha256
  role          = aws_iam_role.iam_for_lambda.arn
  handler       = ""app.handler""
  runtime       = ""nodejs18.x""
}

resource ""aws_lambda_event_source_mapping"" ""dynamodb_lambda_mapping"" {
  event_source_arn  = aws_dynamodb_table.example_table.stream_arn
  function_name     = aws_lambda_function.example_lambda.arn
  starting_position = ""LATEST"" # or ""TRIM_HORIZON"" as per your use case

}

output ""dynamodb_arn"" {
  value = aws_dynamodb_table.example_table.arn
  description = ""The ARN of the DynamoDB table""
}

output ""lambda_processing_arn"" {
  value = aws_lambda_function.example_lambda.arn
  description = ""The ARN of the Lambda function processing the DynamoDB stream""
}","Needs to have an aws_lambda_function defined (IAM role, zip). Create a DynamoDB table. Create lambda source event from an event source from DynamoDB table and function name.","I'll help you create a Terraform configuration for a Lambda event source mapping from a DynamoDB table. This configuration will include the necessary AWS provider and the event source mapping resource.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Lambda event source mapping
resource ""aws_lambda_event_source_mapping"" ""dynamodb_event_mapping"" {
  event_source_arn  = aws_dynamodb_table.example.stream_arn
  function_name     = aws_lambda_function.example.function_name
  starting_position = ""LATEST""
  
  # Optional configurations
  batch_size                     = 100
  maximum_batching_window_in_seconds = 5
  enabled                        = true

  # Optionally configure error handling
  maximum_retry_attempts         = 3
  bisect_batch_on_function_error = true
}

# Example DynamoDB table with streams enabled
resource ""aws_dynamodb_table"" ""example"" {
  name           = ""example-table""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  stream_enabled = true
  stream_view_type = ""NEW_AND_OLD_IMAGES""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# Example Lambda function
resource ""aws_lambda_function"" ""example"" {
  filename         = ""lambda_function.zip""
  function_name    = ""process_dynamodb_records""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      DYNAMODB_TABLE = aws_dynamodb_table.example.name
    }
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_dynamodb_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda to read from DynamoDB streams
resource ""aws_iam_role_policy"" ""lambda_dynamodb_policy"" {
  name = ""lambda_dynamodb_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:GetRecords"",
          ""dynamodb:GetShardIterator"",
          ""dynamodb:DescribeStream"",
          ""dynamodb:ListStreams""
        ]
        Resource = aws_dynamodb_table.example.stream_arn
      }
    ]
  })
}

# CloudWatch Logs policy for Lambda
resource ""aws_iam_role_policy_attachment"" ""lambda_logs"" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a DynamoDB table with streams enabled
3. Creates a Lambda function
4. Creates the event source mapping between DynamoDB and Lambda
5. Sets up the necessary IAM roles and policies

The main resource `aws_lambda_event_source_mapping` connects the DynamoDB stream to the Lambda function. The configuration includes:
- Event source ARN from the DynamoDB table stream
- Target Lambda function
- Starting position for reading the stream
- Batch size and batching window configurations
- Error handling settings

Make sure to:
1. Replace the region with your desired AWS region
2.",,,,,,"I'll help you create a Terraform configuration for a Lambda event source mapping from a DynamoDB table. This configuration will include the necessary resources to connect a Lambda function to a DynamoDB stream as an event source.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Lambda function event source mapping
resource ""aws_lambda_event_source_mapping"" ""dynamodb_mapping"" {
  event_source_arn  = aws_dynamodb_table.example.stream_arn
  function_name     = aws_lambda_function.example.function_name
  starting_position = ""LATEST""
  
  # Optional configurations
  batch_size                = 100
  maximum_retry_attempts    = 3
  maximum_record_age_in_seconds = 21600
  enabled                   = true
}

# Example DynamoDB table with streams enabled
resource ""aws_dynamodb_table"" ""example"" {
  name           = ""example-table""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  stream_enabled = true
  stream_view_type = ""NEW_AND_OLD_IMAGES""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# Example Lambda function
resource ""aws_lambda_function"" ""example"" {
  filename         = ""lambda_function.zip""
  function_name    = ""example_lambda""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_dynamodb_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda to read from DynamoDB streams
resource ""aws_iam_role_policy"" ""lambda_dynamodb_policy"" {
  name = ""lambda_dynamodb_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:GetRecords"",
          ""dynamodb:GetShardIterator"",
          ""dynamodb:DescribeStream"",
          ""dynamodb:ListStreams""
        ]
        Resource = aws_dynamodb_table.example.stream_arn
      }
    ]
  })
}

# Basic Lambda CloudWatch logs policy
resource ""aws_iam_role_policy_attachment"" ""lambda_logs"" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}
```

This configuration:
1. Creates a DynamoDB table with streaming enabled
2. Creates a Lambda function
3. Sets up the event source mapping between DynamoDB and Lambda
4. Creates necessary IAM roles and policies for Lambda to access DynamoDB streams

Key features:
- The DynamoDB table has streams enabled with `NEW_AND_OLD_IMAGES` view type
- The event source mapping is configured to start from the `LATEST` position
- Includes proper IAM permissions for Lambda to read from DynamoDB streams
- Includes basic CloudWatch logs permissions for Lambda
- Configures batch size and retry attempts for the event processing

Remember to:
1. Replace the region with your desired AWS region
2. Provide your actual Lambda function code in the `lambda_function.zip`
3. Adjust the function runtime and handler according to your Lambda function
4. Modify the DynamoDB table configuration",,,,,,"I'll help you create a Terraform configuration for a Lambda event source mapping from a DynamoDB table. This will allow your Lambda function to automatically process records from a DynamoDB stream.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust the region as needed
}

# Lambda function (assuming it already exists)
resource ""aws_lambda_function"" ""example"" {
  filename         = ""lambda_function.zip""
  function_name    = ""example_lambda""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""
}

# DynamoDB table with stream enabled
resource ""aws_dynamodb_table"" ""example"" {
  name           = ""example-table""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  
  attribute {
    name = ""id""
    type = ""S""
  }

  stream_enabled   = true
  stream_view_type = ""NEW_AND_OLD_IMAGES""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_dynamodb_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for DynamoDB stream access
resource ""aws_iam_role_policy"" ""lambda_dynamodb_policy"" {
  name = ""lambda_dynamodb_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:GetRecords"",
          ""dynamodb:GetShardIterator"",
          ""dynamodb:DescribeStream"",
          ""dynamodb:ListStreams""
        ]
        Resource = ""${aws_dynamodb_table.example.stream_arn}""
      }
    ]
  })
}

# Event source mapping
resource ""aws_lambda_event_source_mapping"" ""example_mapping"" {
  event_source_arn  = aws_dynamodb_table.example.stream_arn
  function_name     = aws_lambda_function.example.function_name
  starting_position = ""LATEST""
  
  batch_size        = 100
  enabled           = true
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a DynamoDB table with streaming enabled
3. Creates a Lambda function (assuming you have the function code)
4. Sets up the necessary IAM role and policy for Lambda to access DynamoDB streams
5. Creates the event source mapping between DynamoDB and Lambda

Key features:
- The DynamoDB table has streaming enabled with `stream_enabled = true`
- The event source mapping is configured to start reading from the latest records (`starting_position = ""LATEST""`)
- Batch size is set to 100 records (you can adjust this based on your needs)
- Proper IAM permissions are set up for Lambda to read from the DynamoDB stream

Make sure to:
1. Adjust the region as needed
2. Provide your Lambda function code in the specified zip file
3. Modify the table schema and attributes according to your needs
4. Adjust the batch size and other parameters based on your requirements",,,,,
"aws_iam_role, aws_iam_role_policy, aws_kendra_data_source, aws_kendra_index",Generate a Kendra data source resource With URL Exclusion and Inclusion Patterns,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
    some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kendra.amazonaws.com"") 
}

has_valid_kendra_index {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_index""
    resource.values.name
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
}

has_valid_kendra_data_source{
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_data_source""
    resource.values.name
    resource.values.type
    resource.values.configuration[_].web_crawler_configuration[_].url_exclusion_patterns
    resource.values.configuration[_].web_crawler_configuration[_].url_inclusion_patterns
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
    role.expressions.index_id
}


has_valid_resources {
    has_valid_iam_role(input.resource_changes)
    has_valid_kendra_index
    has_valid_kendra_data_source
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""role"" {
  name = ""Kendra-Role""
  assume_role_policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": ""sts:AssumeRole"",
      ""Principal"": {
        ""Service"": ""kendra.amazonaws.com""
      },
      ""Effect"": ""Allow"",
      ""Sid"": """"
    }
  ]
}
EOF
}

resource ""aws_iam_role_policy"" ""policy"" {
  name = ""Kendra-Policy""
  role = aws_iam_role.role.id

  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:GetObject"",
        ""s3:ListBucket""
      ],
      ""Resource"": ""*""
    }
  ]
}
EOF
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example""
  description = ""example""
  edition     = ""DEVELOPER_EDITION""
  role_arn    = aws_iam_role.role.arn
}

resource ""aws_kendra_data_source"" ""example"" {
  index_id = aws_kendra_index.example.id
  name     = ""example""
  type     = ""WEBCRAWLER""
  role_arn = aws_iam_role.role.arn

  configuration {
    web_crawler_configuration {
      url_exclusion_patterns = [""example""]
      url_inclusion_patterns = [""hello""]

      urls {
        seed_url_configuration {
          seed_urls = [
            ""https://docs.aws.amazon.com/codedeploy/latest/userguide/getting-started-create-service-role.html""
          ]
        }
      }
    }
  }
}","Has one ""aws_iam_role"" resource, one ""aws_kendra_index"" resource, and one ""aws_kendra_data_source"" resource with ""url_exclusion_patterns"" and ""url_inclusion_patterns""","I'll help you create a Terraform configuration for an AWS Kendra data source resource with URL exclusion and inclusion patterns. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_kendra_data_source"" ""example"" {
  index_id = ""your-kendra-index-id""
  name     = ""example-web-crawler""
  type     = ""WEBCRAWLER""

  configuration {
    web_crawler_configuration {
      urls {
        seed_url_configuration {
          seed_urls = [
            ""https://example.com""
          ]
          web_crawler_mode = ""HOST_ONLY""
        }

        url_exclusion_patterns = [
          ""*/archive/*"",
          ""*/temp/*"",
          ""*.pdf""
        ]

        url_inclusion_patterns = [
          ""*/products/*"",
          ""*/blog/*"",
          ""*/docs/*""
        ]
      }

      crawl_depth = 2
      max_links_per_page = 100
      max_content_size_per_page_in_mega_bytes = 5

      url_filtering_configuration {
        url_exclusion_patterns = [
          ""*/private/*"",
          ""*/internal/*""
        ]
      }
    }
  }

  role_arn = ""arn:aws:iam::123456789012:role/kendra-crawler-role""

  schedule = ""cron(0 12 * * ? *)""

  language_code = ""en""
  
  description = ""Web crawler data source with URL patterns""

  tags = {
    Environment = ""production""
    Purpose     = ""web-crawling""
  }
}
```

This configuration creates a Kendra data source with the following features:

1. Uses the WEBCRAWLER type data source
2. Includes URL inclusion patterns for specific paths (/products/, /blog/, /docs/)
3. Includes URL exclusion patterns to skip certain paths and file types
4. Sets up basic crawler configuration like crawl depth and max links per page
5. Includes a schedule using a cron expression
6. Specifies a role ARN for permissions
7. Includes tags for resource management

Make sure to:
1. Replace ""your-kendra-index-id"" with your actual Kendra index ID
2. Update the role_arn with your actual IAM role ARN
3. Adjust the URL patterns according to your specific needs
4. Modify the crawl configuration parameters based on your requirements

The configuration includes both URL exclusion and inclusion patterns at two levels:
- At the urls configuration level
- At the url_filtering_configuration level

This allows for fine-grained control over which URLs are crawled and which are skipped.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra data source resource with URL exclusion and inclusion patterns. This will include the required provider and proper configuration for the URL patterns.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust region as needed
}

resource ""aws_kendra_data_source"" ""example"" {
  index_id = ""your-kendra-index-id""  # Replace with your index ID
  name     = ""example-datasource""
  type     = ""WEBCRAWLER""

  configuration {
    web_crawler_configuration {
      urls {
        seed_url_configuration {
          seed_urls = [""https://example.com""]
          
          web_crawler_mode = ""HOST_ONLY""  # or ""SUBDOMAINS""
          
          url_inclusion_patterns = [
            ""https://example.com/include/*"",
            ""https://example.com/products/*""
          ]
          
          url_exclusion_patterns = [
            ""https://example.com/exclude/*"",
            ""https://example.com/private/*"",
            ""https://example.com/internal/*""
          ]
        }
      }

      crawl_depth = 2

      max_links_per_page = 100

      max_content_size_per_page_in_mega_bytes = 50

      max_urls_per_minute_crawl_rate = 300
    }
  }

  role_arn = aws_iam_role.kendra_datasource_role.arn

  schedule = ""cron(0 0 * * ? *)""  # Run daily at midnight

  tags = {
    Environment = ""production""
    Name        = ""example-kendra-datasource""
  }
}

# IAM role for Kendra data source
resource ""aws_iam_role"" ""kendra_datasource_role"" {
  name = ""kendra-datasource-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""kendra_datasource_policy"" {
  role       = aws_iam_role.kendra_datasource_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSKendraDataSourcePolicy""
}
```

This configuration creates:
1. A Kendra data source with web crawler configuration
2. URL inclusion and exclusion patterns
3. Required IAM role and policy attachments
4. Basic crawler settings like depth, rate limits, and schedule

Key features:
- URL inclusion patterns to specify which URLs should be crawled
- URL exclusion patterns to specify which URLs should be skipped
- Web crawler mode set to HOST_ONLY (can be changed to SUBDOMAINS)
- Daily schedule using cron expression
- Proper IAM role configuration for Kendra access

Remember to:
1. Replace ""your-kendra-index-id"" with your actual Kendra index ID
2. Adjust the region in the provider block
3. Modify the URL patterns according to your needs
4. Adjust the crawl settings (depth, rate limits) based on your requirements
5. Customize the IAM role name and policies as needed

The configuration includes all necessary components for a functional Kendra data source with URL filtering capabilities.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra data source resource with URL exclusion and inclusion patterns. This configuration will include the required provider block and the necessary resource definitions.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_kendra_data_source"" ""example"" {
  index_id = aws_kendra_index.example.id
  name     = ""example-data-source""
  type     = ""WEBCRAWLER""

  configuration {
    web_crawler_configuration {
      urls {
        seed_url_configuration {
          seed_urls = [""https://example.com""]
          
          web_crawler_mode = ""HOST_ONLY""
        }

        url_inclusion_patterns = [
          ""https://example.com/include/*"",
          ""https://example.com/documents/*""
        ]

        url_exclusion_patterns = [
          ""https://example.com/private/*"",
          ""https://example.com/internal/*"",
          ""*.pdf""
        ]
      }

      crawl_depth = 2
      max_links_per_page = 100
      max_url_characters = 300
    }
  }

  role_arn = aws_iam_role.kendra_data_source_role.arn

  schedule = ""cron(0 1 * * ? *)""

  description = ""Example Kendra web crawler data source with URL patterns""

  language_code = ""en""
  
  tags = {
    Environment = ""production""
    Name        = ""example-kendra-datasource""
  }
}

# Example IAM role for Kendra data source
resource ""aws_iam_role"" ""kendra_data_source_role"" {
  name = ""kendra-data-source-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}
```

This configuration creates a Kendra data source with the following features:
1. URL inclusion and exclusion patterns to control which URLs are crawled
2. Web crawler configuration with specific settings for crawl depth and limits
3. A scheduled crawl using a cron expression
4. Associated IAM role for the data source
5. Basic metadata like description, language code, and tags

Key points about the configuration:
- The `url_inclusion_patterns` specify which URLs should be crawled
- The `url_exclusion_patterns` specify which URLs should be skipped
- The `web_crawler_mode` is set to ""HOST_ONLY"" which restricts crawling to the specified domain
- The schedule is set using a cron expression that runs daily at 1 AM UTC
- The configuration includes necessary tags for resource management

Note: You'll need to have an existing Kendra index (referenced by `aws_kendra_index.example.id`) and appropriate IAM permissions set up for this configuration to work.",,,,,
"aws_iam_role, aws_kinesis_firehose_delivery_stream, aws_s3_bucket, aws_iam_policy_document",Generate a Kinesis Firehose Delivery Stream with an extended S3 Destination,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""firehose.amazonaws.com"") 
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
}

has_valid_firehose_delivery_stream {
        some i
        resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_firehose_delivery_stream""
    resource.values.name
    resource.values.destination == ""extended_s3""
    resource.values.extended_s3_configuration[_]
        role := input.configuration.root_module.resources[i]
    role.expressions.extended_s3_configuration[_].role_arn
}

has_valid_resources {
        has_valid_iam_role(input.resource_changes)
        has_valid_bucket
    has_valid_firehose_delivery_stream
}",3,"provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""januarysixteenth"" {
  bucket = ""januarysixteenth""
}

data ""aws_iam_policy_document"" ""firehose_assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""firehose.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""firehose_role"" {
  name               = ""firehose_test_role""
  assume_role_policy = data.aws_iam_policy_document.firehose_assume_role.json
}

resource ""aws_kinesis_firehose_delivery_stream"" ""extended_s3_stream"" {
  name        = ""terraform-kinesis-firehose-extended-s3-test-stream""
  destination = ""extended_s3""

  extended_s3_configuration {
    role_arn   = aws_iam_role.firehose_role.arn
    bucket_arn = aws_s3_bucket.januarysixteenth.arn
  }
}","Has one ""aws_iam_role"" resource, one ""aws_s3_bucket"" resource, and one ""aws_kinesis_firehose_delivery_stream"" resource with an ""extended_s3_configuration"" block 
","I'll help you create a Terraform configuration for an AWS Kinesis Firehose Delivery Stream with an extended S3 destination. Here's a complete configuration that includes the necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for Firehose destination
resource ""aws_s3_bucket"" ""destination"" {
  bucket = ""my-firehose-destination-bucket""
}

# IAM role for Firehose
resource ""aws_iam_role"" ""firehose_role"" {
  name = ""firehose-delivery-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""firehose.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Firehose
resource ""aws_iam_role_policy"" ""firehose_policy"" {
  name = ""firehose-delivery-policy""
  role = aws_iam_role.firehose_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:AbortMultipartUpload"",
          ""s3:GetBucketLocation"",
          ""s3:GetObject"",
          ""s3:ListBucket"",
          ""s3:ListBucketMultipartUploads"",
          ""s3:PutObject""
        ]
        Resource = [
          aws_s3_bucket.destination.arn,
          ""${aws_s3_bucket.destination.arn}/*""
        ]
      }
    ]
  })
}

# Kinesis Firehose Delivery Stream
resource ""aws_kinesis_firehose_delivery_stream"" ""extended_s3_stream"" {
  name        = ""my-firehose-stream""
  destination = ""extended_s3""

  extended_s3_configuration {
    role_arn   = aws_iam_role.firehose_role.arn
    bucket_arn = aws_s3_bucket.destination.arn

    buffering_size = 64
    buffering_interval = 60

    prefix = ""data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/""
    error_output_prefix = ""errors/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/!{firehose:error-output-type}/""

    compression_format = ""GZIP""

    processing_configuration {
      enabled = true

      processors {
        type = ""MetadataExtraction""
        parameters {
          parameter_name  = ""JsonParsingEngine""
          parameter_value = ""JQ-1.6""
        }
      }
    }

    cloudwatch_logging_options {
      enabled         = true
      log_group_name  = ""/aws/firehose/my-firehose-stream""
      log_stream_name = ""S3Delivery""
    }
  }
}
```

This configuration creates:

1. An S3 bucket as the destination
2. An IAM role and policy for Firehose to access the S3 bucket
3. A Kinesis Firehose Delivery Stream with extended S3 configuration including:
   - Buffer size and interval settings
   - Data prefix formatting with dynamic values
   - Error output prefix configuration
   - GZIP compression
   - Processing configuration with metadata extraction
   - CloudWatch logging options

The Firehose stream will buffer data for 60 seconds or until it reaches 64 MB before delivering to S3. The data will be organized in the S3 bucket using a year/month/day",,,,,,"I'll help you create a Terraform configuration for an AWS Kinesis Firehose Delivery Stream with an extended S3 destination. This will include the necessary provider configuration and the Kinesis Firehose resource with S3 as the destination.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# S3 bucket for Firehose destination
resource ""aws_s3_bucket"" ""firehose_bucket"" {
  bucket = ""my-firehose-destination-bucket""
}

# IAM role for Firehose
resource ""aws_iam_role"" ""firehose_role"" {
  name = ""firehose_delivery_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""firehose.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Firehose to write to S3
resource ""aws_iam_role_policy"" ""firehose_policy"" {
  name = ""firehose_delivery_policy""
  role = aws_iam_role.firehose_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:AbortMultipartUpload"",
          ""s3:GetBucketLocation"",
          ""s3:GetObject"",
          ""s3:ListBucket"",
          ""s3:ListBucketMultipartUploads"",
          ""s3:PutObject""
        ]
        Resource = [
          aws_s3_bucket.firehose_bucket.arn,
          ""${aws_s3_bucket.firehose_bucket.arn}/*""
        ]
      }
    ]
  })
}

# Kinesis Firehose Delivery Stream
resource ""aws_kinesis_firehose_delivery_stream"" ""extended_s3_stream"" {
  name        = ""my-firehose-stream""
  destination = ""extended_s3""

  extended_s3_configuration {
    role_arn   = aws_iam_role.firehose_role.arn
    bucket_arn = aws_s3_bucket.firehose_bucket.arn
    
    buffer_size     = 64
    buffer_interval = 60

    # Optional: Configure data processing
    processing_configuration {
      enabled = true

      processors {
        type = ""MetadataExtraction""
        parameters {
          parameter_name  = ""JsonParsingEngine""
          parameter_value = ""JQ-1.6""
        }
      }
    }

    # Optional: Configure S3 backup
    s3_backup_mode = ""Disabled""
    
    # Optional: Configure CloudWatch logging
    cloudwatch_logging_options {
      enabled = true
      log_group_name  = ""/aws/firehose/my-firehose-stream""
      log_stream_name = ""S3Delivery""
    }
  }
}
```

This configuration creates:
1. An S3 bucket as the destination
2. An IAM role and policy for Firehose to access S3
3. A Kinesis Firehose delivery stream with extended S3 configuration

The Firehose delivery stream includes:
- Buffer settings (64MB or 60 seconds)
- Data processing configuration (optional)
- S3 backup configuration (disabled in this example)
- CloudWatch logging configuration

You can customize the configuration by:
- Adjusting buffer sizes and intervals
- Modifying the processing configuration
- Enabling S3 backup
- Changing the CloudWatch logging settings
- Adding data transformation options
- Configuring error handling

Remember to adjust the bucket name and other parameters according to your needs.",,,,,,"I'll help you create a Terraform configuration for an AWS Kinesis Firehose Delivery Stream with an extended S3 destination. This will include the necessary provider configuration and the required resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Update with your desired region
}

# S3 bucket for Firehose destination
resource ""aws_s3_bucket"" ""firehose_bucket"" {
  bucket = ""my-firehose-destination-bucket""
}

# IAM role for Firehose
resource ""aws_iam_role"" ""firehose_role"" {
  name = ""firehose_delivery_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""firehose.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Firehose
resource ""aws_iam_role_policy"" ""firehose_policy"" {
  name = ""firehose_delivery_policy""
  role = aws_iam_role.firehose_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:AbortMultipartUpload"",
          ""s3:GetBucketLocation"",
          ""s3:GetObject"",
          ""s3:ListBucket"",
          ""s3:ListBucketMultipartUploads"",
          ""s3:PutObject""
        ]
        Resource = [
          aws_s3_bucket.firehose_bucket.arn,
          ""${aws_s3_bucket.firehose_bucket.arn}/*""
        ]
      }
    ]
  })
}

# Kinesis Firehose Delivery Stream
resource ""aws_kinesis_firehose_delivery_stream"" ""extended_s3_stream"" {
  name        = ""my-firehose-stream""
  destination = ""extended_s3""

  extended_s3_configuration {
    role_arn   = aws_iam_role.firehose_role.arn
    bucket_arn = aws_s3_bucket.firehose_bucket.arn
    
    buffering_size = 64
    buffering_interval = 60
    
    compression_format = ""GZIP""
    
    prefix = ""raw-data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/""
    error_output_prefix = ""errors/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/!{firehose:error-output-type}/""

    processing_configuration {
      enabled = true

      processors {
        type = ""MetadataExtraction""
        parameters {
          parameter_name  = ""JsonParsingEngine""
          parameter_value = ""JQ-1.6""
        }
      }
    }

    cloudwatch_logging_options {
      enabled         = true
      log_group_name  = ""/aws/firehose/my-firehose-stream""
      log_stream_name = ""S3Delivery""
    }
  }
}
```

This configuration creates:
1. An S3 bucket as the destination
2. An IAM role and policy for the Firehose service
3. A Kinesis Firehose Delivery Stream with extended S3 configuration including:
   - GZIP compression
   - Dynamic prefixing based on timestamp
   - Error output handling
   - Basic data processing configuration
   - CloudWatch logging

Key features of this configuration:
- The stream buffers data for 60 seconds or until it reaches 64MB
- Uses GZIP compression for stored data
- Implements a date-based prefix structure for organize",,,,,
"aws_network_acl, aws_subnet, aws_vpc",Create an AWS VPC resource with a subnet and associated network ACLs to control both ingress and egress traffic,"package vpc_net_acl
import future.keywords.in

default valid := false

valid {
        some net_acl_resource in input.configuration.root_module.resources
        net_acl_resource.type == ""aws_network_acl""
        net_acl_resource.expressions.egress
        net_acl_resource.expressions.ingress
   
        some vpc_resource in input.configuration.root_module.resources
        vpc_resource.type == ""aws_vpc""
        vpc_resource.address in net_acl_resource.expressions.vpc_id.references

        some subnet in input.configuration.root_module.resources
        subnet.type == ""aws_subnet""
        vpc_resource.address in subnet.expressions.vpc_id.references
}",3,"resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""snet"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_network_acl"" ""ntwk_acl"" {
  vpc_id      = aws_vpc.main.id

  egress {
    protocol   = ""tcp""
    rule_no    = 200
    action     = ""allow""
    cidr_block = ""10.3.0.0/18""
    from_port  = 443
    to_port    = 443
   }

   ingress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""10.3.0.0/18""
    from_port  = 80
    to_port    = 80
  }
}","Has one resource ""aws_vpc""

Has one resource ""aws_subnet""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id""

Has one resource ""aws_network_acl""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id""
with one ""egress"" with correct parameters
with one ""ingress"" with correct parameters","I'll help you create a Terraform configuration for an AWS VPC with a subnet and network ACLs to control both ingress and egress traffic. Here's a comprehensive solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create Subnet
resource ""aws_subnet"" ""main"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""main-subnet""
  }
}

# Create Network ACL
resource ""aws_network_acl"" ""main"" {
  vpc_id = aws_vpc.main.id

  # Ingress rules
  ingress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  ingress {
    protocol   = ""tcp""
    rule_no    = 200
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  # Allow SSH access
  ingress {
    protocol   = ""tcp""
    rule_no    = 300
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 22
    to_port    = 22
  }

  # Egress rules
  egress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  egress {
    protocol   = ""tcp""
    rule_no    = 200
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  # Allow all outbound responses
  egress {
    protocol   = ""tcp""
    rule_no    = 300
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 1024
    to_port    = 65535
  }

  tags = {
    Name = ""main-nacl""
  }
}

# Associate Network ACL with Subnet
resource ""aws_network_acl_association"" ""main"" {
  network_acl_id = aws_network_acl.main.id
  subnet_id      = aws_subnet.main.id
}
```

This configuration:
1. Creates a VPC with CIDR block 10.0.0.0/16
2. Creates a subnet with CIDR block 10.0.1.0/24
3. Creates a Network ACL with:
   - Ingress rules allowing HTTP (80), HTTPS (443), and SSH (22) traffic
   - Egress rules allowing HTTP (80), HTTPS (443), and ephemeral ports (1024-65535)
4. Associates the Network ACL with the subnet

The Network ACL rules are stateless, meaning you need to explicitly define both ingress and egress rules. The configuration includes basic security rules, but you might want to adjust them based on your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS VPC with a subnet and network ACLs to control both inbound and outbound traffic. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create Subnet
resource ""aws_subnet"" ""main"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""main-subnet""
  }
}

# Create Network ACL
resource ""aws_network_acl"" ""main"" {
  vpc_id = aws_vpc.main.id

  # Inbound rules
  ingress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  ingress {
    protocol   = ""tcp""
    rule_no    = 110
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  ingress {
    protocol   = ""tcp""
    rule_no    = 120
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 22
    to_port    = 22
  }

  # Outbound rules
  egress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  egress {
    protocol   = ""tcp""
    rule_no    = 110
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  egress {
    protocol   = ""-1""
    rule_no    = 120
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 0
    to_port    = 0
  }

  tags = {
    Name = ""main-nacl""
  }
}

# Associate Network ACL with Subnet
resource ""aws_network_acl_association"" ""main"" {
  network_acl_id = aws_network_acl.main.id
  subnet_id      = aws_subnet.main.id
}
```

This configuration:
1. Creates a VPC with CIDR block 10.0.0.0/16
2. Creates a subnet with CIDR block 10.0.1.0/24
3. Creates a Network ACL with:
   - Inbound rules allowing HTTP (80), HTTPS (443), and SSH (22)
   - Outbound rules allowing HTTP (80), HTTPS (443), and all outbound traffic
4. Associates the Network ACL with the subnet

The Network ACL rules are stateless and processed in order based on the rule number. Make sure to adjust the CIDR blocks, ports, and other parameters according to your specific security requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS VPC with a subnet and network ACLs. This configuration will include ingress and egress rules for network traffic control.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create Subnet
resource ""aws_subnet"" ""main"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""main-subnet""
  }
}

# Create Network ACL
resource ""aws_network_acl"" ""main"" {
  vpc_id     = aws_vpc.main.id
  subnet_ids = [aws_subnet.main.id]

  # Ingress rules
  ingress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  ingress {
    protocol   = ""tcp""
    rule_no    = 200
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  # Allow return traffic for ephemeral ports
  ingress {
    protocol   = ""tcp""
    rule_no    = 300
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 1024
    to_port    = 65535
  }

  # Egress rules
  egress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  egress {
    protocol   = ""tcp""
    rule_no    = 200
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  egress {
    protocol   = ""tcp""
    rule_no    = 300
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 1024
    to_port    = 65535
  }

  tags = {
    Name = ""main-nacl""
  }
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""main-igw""
  }
}

# Create Route Table
resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""main-route-table""
  }
}

# Associate Route Table with Subnet
resource ""aws_route_table_association"" ""main"" {
  subnet_id      = aws_subnet.main.id
  route_table_id = aws_route_table.main.id
}
```

This configuration creates:
1. A VPC with CIDR block 10.0.0.0/16
2. A subnet with CIDR block 10.0.1.0/24",,,,,
aws_db_instance,"create a basic AWS RDS instance, with io1 storage type","package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.storage_type.constant_value == ""io1""
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage.constant_value >= 100
    expressions.allocated_storage.constant_value <= 65536
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 100
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  storage_type        = ""io1""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""","I'll help you create a Terraform configuration for an AWS RDS instance with io1 (Provisioned IOPS) storage type. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_db_instance"" ""example"" {
  identifier           = ""my-rds-instance""
  engine              = ""mysql""                # You can change this to other supported engines
  engine_version      = ""8.0.28""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 100
  storage_type        = ""io1""
  iops                = 3000                   # Required for io1 storage type (min 1000)
  
  # Database credentials
  username            = ""admin""
  password            = ""your-secure-password"" # In production, use secrets management

  # Network settings
  publicly_accessible = false
  
  # Backup settings
  backup_retention_period = 7
  
  # Maintenance settings
  maintenance_window      = ""Mon:03:00-Mon:04:00""
  backup_window          = ""02:00-03:00""

  # Skip final snapshot for testing (set to true in production)
  skip_final_snapshot    = true
  
  # Enable encryption at rest
  storage_encrypted      = true

  tags = {
    Name        = ""example-rds-instance""
    Environment = ""development""
  }
}
```

This configuration creates an RDS instance with the following key features:
- Uses io1 storage type with 3000 IOPS
- MySQL 8.0.28 as the database engine
- 100GB of allocated storage
- Encrypted storage
- Basic backup and maintenance windows
- Tags for better resource management

Remember to:
1. Adjust the region in the provider block
2. Change the password to a secure value
3. Modify the instance class based on your needs
4. Adjust the IOPS value (must be between 1000 and 50000)
5. In production, set `skip_final_snapshot` to false
6. Consider using AWS Secrets Manager or SSM Parameter Store for the database password

You can customize other parameters like subnet groups, parameter groups, and security groups based on your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS RDS instance with io1 (Provisioned IOPS) storage type. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_db_instance"" ""example"" {
  identifier           = ""my-rds-instance""
  engine              = ""mysql""  # You can change this to other supported engines
  engine_version      = ""8.0""
  instance_class      = ""db.t3.medium""
  allocated_storage   = 100
  storage_type        = ""io1""
  iops                = 3000  # Must be between 1000-50000 for io1

  # Database configuration
  db_name             = ""mydb""
  username            = ""admin""
  password            = ""your-password-here""  # Consider using secrets management

  # Network and security
  publicly_accessible = false
  multi_az           = false

  # Backup and maintenance
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  # Security
  vpc_security_group_ids = [""sg-xxxxxxxx""]  # Replace with your security group
  db_subnet_group_name   = ""default""        # Replace with your subnet group

  # Additional configuration
  skip_final_snapshot = true
  
  tags = {
    Name        = ""example-rds""
    Environment = ""production""
  }
}
```

This configuration creates an RDS instance with the following key features:
- Uses io1 storage type with 3000 IOPS
- 100GB of allocated storage
- MySQL 8.0 database engine (you can modify this)
- Includes basic backup and maintenance windows
- Includes necessary network and security configurations

Remember to:
1. Replace the security group ID and subnet group name with your actual values
2. Change the password to a secure value (preferably using variables or secrets management)
3. Adjust the region to your preferred AWS region
4. Modify the instance class based on your performance needs
5. Adjust the IOPS value based on your performance requirements (must be between 1000-50000 for io1)

The configuration includes standard best practices for RDS deployment while specifically implementing the io1 storage type as requested.",,,,,,"I'll help you create a Terraform configuration for an AWS RDS instance with io1 (Provisioned IOPS) storage type. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_db_instance"" ""example"" {
  identifier           = ""my-rds-instance""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 100
  storage_type        = ""io1""
  iops                = 3000  # Must be between 1000-50000 for io1
  
  # Database configuration
  db_name             = ""mydb""
  username            = ""admin""
  password            = ""your-password-here""
  
  # Network settings
  publicly_accessible = false
  
  # Backup and maintenance
  backup_retention_period = 7
  maintenance_window     = ""Mon:03:00-Mon:04:00""
  backup_window         = ""02:00-03:00""
  
  # Skip final snapshot when destroying
  skip_final_snapshot  = true
  
  # Enable encryption at rest
  storage_encrypted    = true
}
```

This configuration creates an RDS instance with the following features:
- Uses io1 storage type with 3000 IOPS
- MySQL 8.0 as the database engine
- 100GB of allocated storage
- Basic security settings and backup configuration
- Encryption at rest enabled

Remember to:
1. Replace the password with a secure value
2. Adjust the region as needed
3. Modify the instance class based on your requirements
4. Adjust the IOPS value based on your performance needs (must be between 1000-50000 for io1)
5. Consider adding security groups and subnet groups if needed

Also note that using io1 storage type will incur additional costs compared to standard storage types.",,,,,
"aws_codebuild_project, aws_iam_policy, aws_iam_role, aws_iam_role_policy_attachment, aws_s3_bucket, aws_security_group, aws_subnet, aws_vpc, aws_iam_policy_document, aws_iam_policy_document",Infrastructure for an autograder for a CS class. The autograde needs to run students' code stored on GitHub in a specific environment and store the result. Make sure that the autograder prevents the students' code from reaching the internet. Accomplish this using AWS CodeBuild and AWS VPC.,"package autograder_high_jail

import rego.v1

codebuild_project_valid(codebuild_project, security_group, subnet, vpc) if {
	some artifact in codebuild_project.expressions.artifacts
	artifact.location
	artifact.type
	artifact.name

	some environment in codebuild_project.expressions.environment
	environment.compute_type
	environment.image
	environment.type

	some source in codebuild_project.expressions.source
	source.type.constant_value == ""GITHUB""
	source.location

	some vpc_config in codebuild_project.expressions.vpc_config
	security_group.address in vpc_config.security_group_ids.references
	subnet.address in vpc_config.subnets.references
	vpc.address in vpc_config.vpc_id.references
}

security_group_valid(security_group, vpc) if {
	vpc.address in security_group.expressions.vpc_id.references
}

subnet_valid(subnet, vpc) if {
	subnet.expressions.cidr_block
	vpc.address in subnet.expressions.vpc_id.references
}

vpc_valid(vpc) if {
	vpc.expressions.cidr_block
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources
	some codebuild_project in resources
	codebuild_project.type == ""aws_codebuild_project""
	some security_group in resources
	security_group.type == ""aws_security_group""
	some subnet in resources
	subnet.type == ""aws_subnet""
	some vpc in resources
	vpc.type == ""aws_vpc""
	codebuild_project_valid(codebuild_project, security_group, subnet, vpc)
	security_group_valid(security_group, vpc)
	subnet_valid(subnet, vpc)
	vpc_valid(vpc)
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""artifact-bucket-""
}

resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder_build""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip"" # include this
  }

  environment { # arguments required, exact value not specified
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1 # good to have, not required
    location        = ""github.com/source-location""
  }

  vpc_config {
    vpc_id             = aws_vpc.autograder_vpc.id
    subnets            = [aws_subnet.autograder_vpc_subnet.id]
    security_group_ids = [aws_security_group.autograder_vpc_securitygroup.id]
  }
}

resource ""aws_vpc"" ""autograder_vpc"" {
  cidr_block = ""10.0.0.0/16"" # extra value not specified
}

resource ""aws_subnet"" ""autograder_vpc_subnet"" {
  vpc_id     = aws_vpc.autograder_vpc.id
  cidr_block = ""10.0.0.0/24"" # include this
}

resource ""aws_security_group"" ""autograder_vpc_securitygroup"" {
  vpc_id = aws_vpc.autograder_vpc.id
}

resource ""aws_iam_role"" ""autograder_build_role"" {
  assume_role_policy = data.aws_iam_policy_document.autograder_build_policy_assume.json
}

data ""aws_iam_policy_document"" ""autograder_build_policy_assume"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }
  }
}

data ""aws_iam_policy_document"" ""autograder_build_policy"" {
  statement {
    effect = ""Allow""
    actions = [
      ""ec2:DescribeSubnets"",
      ""ec2:DescribeSecurityGroups"",
      ""ec2:DescribeVpcs""
    ]
    resources = [
      ""*""
    ]
  }

  statement {
    effect  = ""Allow""
    actions = [""s3:*""]
    resources = [
      aws_s3_bucket.artifact_bucket.arn,
      ""${aws_s3_bucket.artifact_bucket.arn}/*"",
    ]
  }
}

resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name        = ""lambda_policy""
  description = ""Grants permissions to Lambda to describe vpc, subnet, security group""

  policy = data.aws_iam_policy_document.autograder_build_policy.json
}

resource ""aws_iam_role_policy_attachment"" ""lambda_policy_attach"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}","have one ""aws_codebuild_project"" resource
  with ""artifacts"" argument block
    with ""type"" argument with value ""S3""
    with ""location"" argument
    with ""name"" argument
  with ""environment"" argument block
    with ""computer_type"" argument
    with ""image"" argument
    with ""type"" argument
  with ""source"" argument block
    with ""type"" argument with value ""GITHUB""
    with ""location"" argument
  with ""vpc_config"" argument block
    with ""vpc_id"" argument
    with ""subnets"" argument
    with ""security_group_ids"" argument
have one ""aws_vpc"" resource
  with ""cidr_block"" argument
have one ""aws_subnet"" argument
  with ""vpc_id"" argument
  with ""cidr_block"" argument
with one ""aws_security_group"" resource
  with ""vpc_id"" argument","I'll help create a Terraform configuration for an autograder system using AWS CodeBuild and VPC with proper network isolation. Here's a solution that implements these requirements:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""autograder_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""autograder-vpc""
  }
}

# Private Subnet
resource ""aws_subnet"" ""private_subnet"" {
  vpc_id            = aws_vpc.autograder_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""autograder-private-subnet""
  }
}

# VPC Endpoint for CodeBuild
resource ""aws_vpc_endpoint"" ""codebuild"" {
  vpc_id              = aws_vpc.autograder_vpc.id
  service_name        = ""com.amazonaws.us-west-2.codebuild""
  vpc_endpoint_type   = ""Interface""
  subnet_ids          = [aws_subnet.private_subnet.id]
  private_dns_enabled = true

  security_group_ids = [aws_security_group.codebuild_sg.id]
}

# Security Group for CodeBuild
resource ""aws_security_group"" ""codebuild_sg"" {
  name        = ""codebuild-sg""
  description = ""Security group for CodeBuild""
  vpc_id      = aws_vpc.autograder_vpc.id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# S3 Bucket for Build Artifacts
resource ""aws_s3_bucket"" ""build_artifacts"" {
  bucket = ""autograder-build-artifacts""
}

resource ""aws_s3_bucket_versioning"" ""build_artifacts_versioning"" {
  bucket = aws_s3_bucket.build_artifacts.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""autograder-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          aws_s3_bucket.build_artifacts.arn,
          ""${aws_s3_bucket.build_artifacts.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })",,,,,,"I'll help you create a Terraform configuration for an autograder system using AWS CodeBuild within a VPC that restricts internet access. Here's a solution that includes:
1. A VPC with private subnets
2. CodeBuild project with VPC configuration
3. S3 bucket for storing results
4. Necessary IAM roles and policies

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""autograder_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""autograder-vpc""
  }
}

resource ""aws_subnet"" ""private_subnet"" {
  vpc_id            = aws_vpc.autograder_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""autograder-private-subnet""
  }
}

# S3 Bucket for results
resource ""aws_s3_bucket"" ""results_bucket"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""results_versioning"" {
  bucket = aws_s3_bucket.results_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          aws_s3_bucket.results_bucket.arn,
          ""${aws_s3_bucket.results_bucket.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""ec2:CreateNetworkInterface"",
          ""ec2:DescribeNetworkInterfaces"",
          ""ec2:DeleteNetworkInterface""
        ]
      }
    ]
  })
}

# Security Group for CodeBuild
resource ""aws_security_group"" ""codebuild_sg"" {
  name        = ""autograder-codebuild-sg""
  description = ""Security group for autograder CodeBuild project""
  vpc_id      = aws_vpc.autograder_vpc.id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""10.0.0.0/16""]  # Only allow internal VPC communication
  }
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class""
  build_timeout = ""30""
  service_role  = aws_iam_role.",,,,,,"I'll help you create a Terraform configuration for an autograder system using AWS CodeBuild and VPC with proper network isolation. This solution will:
1. Create a VPC with private subnets (no internet access)
2. Set up CodeBuild project with VPC configuration
3. Configure necessary IAM roles and policies
4. Set up S3 bucket for storing results

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""autograder_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""autograder-vpc""
  }
}

resource ""aws_subnet"" ""private_subnet"" {
  vpc_id            = aws_vpc.autograder_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""autograder-private-subnet""
  }
}

# S3 Bucket for results
resource ""aws_s3_bucket"" ""results_bucket"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""results_versioning"" {
  bucket = aws_s3_bucket.results_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          aws_s3_bucket.results_bucket.arn,
          ""${aws_s3_bucket.results_bucket.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""ec2:CreateNetworkInterface"",
          ""ec2:DescribeNetworkInterfaces"",
          ""ec2:DeleteNetworkInterface""
        ]
      }
    ]
  })
}

# Security Group for CodeBuild
resource ""aws_security_group"" ""codebuild_sg"" {
  name        = ""autograder-codebuild-sg""
  description = ""Security group for autograder CodeBuild project""
  vpc_id      = aws_vpc.autograder_vpc.id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""10.0.0.0/16""]  # Only allow internal VPC communication
  }
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class""
  service_role  = aws_iam_role.codebuild_role.arn
  build_timeout = ""30""",,,,,
"aws_iam_group, aws_iam_group_membership, aws_iam_user, aws_iam_user",Create two basic AWS IAM users that are a part of a basic IAM group,"package iam_group_two_users
import future.keywords.in

default valid := false

valid {
    some user1 in input.configuration.root_module.resources
    user1.type == ""aws_iam_user""

    some user2 in input.configuration.root_module.resources
    user2.type == ""aws_iam_user""

    not user1 == user2

    some group in input.configuration.root_module.resources
    group.type == ""aws_iam_group""

    some group_membership in input.configuration.root_module.resources
    group_membership.type == ""aws_iam_group_membership""
    user1.address in group_membership.expressions.users.references
    user2.address in group_membership.expressions.users.references
    group.address in group_membership.expressions.group.references
}",3,"resource ""aws_iam_group_membership"" ""team"" {
  name = ""tf-testing-group-membership""

  users = [
    aws_iam_user.user_one.name,
    aws_iam_user.user_two.name,
  ]

  group = aws_iam_group.group.name
}

resource ""aws_iam_group"" ""group"" {
  name = ""test-group""
}

resource ""aws_iam_user"" ""user_one"" {
  name = ""test-user""
}

resource ""aws_iam_user"" ""user_two"" {
  name = ""test-user-two""
}","Has one ""aws_iam_group"" resource
with one ""name"" with any value

Has one ""aws_iam_user"" resource
with one ""name"" with any value

Has another ""aws_iam_user"" resource
with one ""name"" with any value

Has one ""aws_iam_group_membership"" resource
with one ""name"" with any value
with one ""users"" with the two user resource names
with one ""group"" with the group resource name","I'll help you create a Terraform configuration that creates two IAM users and adds them to an IAM group. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create an IAM group
resource ""aws_iam_group"" ""basic_group"" {
  name = ""basic_users_group""
}

# Create first IAM user
resource ""aws_iam_user"" ""user1"" {
  name = ""basic_user1""
  tags = {
    Description = ""Basic IAM User 1""
  }
}

# Create second IAM user
resource ""aws_iam_user"" ""user2"" {
  name = ""basic_user2""
  tags = {
    Description = ""Basic IAM User 2""
  }
}

# Add user1 to the group
resource ""aws_iam_user_group_membership"" ""user1_membership"" {
  user   = aws_iam_user.user1.name
  groups = [aws_iam_group.basic_group.name]
}

# Add user2 to the group
resource ""aws_iam_user_group_membership"" ""user2_membership"" {
  user   = aws_iam_user.user2.name
  groups = [aws_iam_group.basic_group.name]
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an IAM group called ""basic_users_group""
3. Creates two IAM users: ""basic_user1"" and ""basic_user2""
4. Adds both users to the created group using group membership resources

Each resource is properly defined and linked together using resource references. The configuration includes tags for better resource management and identification.",,,,,,"I'll help you create a Terraform configuration to create two IAM users and add them to an IAM group. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Define the IAM group
resource ""aws_iam_group"" ""basic_group"" {
  name = ""basic-users-group""
}

# Create first IAM user
resource ""aws_iam_user"" ""user1"" {
  name = ""basic-user-1""
  tags = {
    Description = ""Basic IAM User 1""
  }
}

# Create second IAM user
resource ""aws_iam_user"" ""user2"" {
  name = ""basic-user-2""
  tags = {
    Description = ""Basic IAM User 2""
  }
}

# Add first user to the group
resource ""aws_iam_user_group_membership"" ""user1_membership"" {
  user   = aws_iam_user.user1.name
  groups = [aws_iam_group.basic_group.name]
}

# Add second user to the group
resource ""aws_iam_user_group_membership"" ""user2_membership"" {
  user   = aws_iam_user.user2.name
  groups = [aws_iam_group.basic_group.name]
}
```

This configuration will:
1. Set up the AWS provider
2. Create an IAM group called ""basic-users-group""
3. Create two IAM users: ""basic-user-1"" and ""basic-user-2""
4. Add both users to the created IAM group using group membership resources

Each resource has proper naming and tags where appropriate. The configuration follows AWS IAM best practices for organizing users into groups.",,,,,,"I'll help you create a Terraform configuration to create two IAM users and add them to an IAM group. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM group
resource ""aws_iam_group"" ""basic_group"" {
  name = ""basic_users_group""
}

# Create first IAM user
resource ""aws_iam_user"" ""user1"" {
  name = ""basic_user1""
  tags = {
    Description = ""Basic IAM User 1""
  }
}

# Create second IAM user
resource ""aws_iam_user"" ""user2"" {
  name = ""basic_user2""
  tags = {
    Description = ""Basic IAM User 2""
  }
}

# Add users to the group
resource ""aws_iam_group_membership"" ""group_membership"" {
  name = ""basic_group_membership""

  users = [
    aws_iam_user.user1.name,
    aws_iam_user.user2.name
  ]

  group = aws_iam_group.basic_group.name
}
```

This configuration will:
1. Set up the AWS provider
2. Create a basic IAM group named ""basic_users_group""
3. Create two IAM users: ""basic_user1"" and ""basic_user2""
4. Add both users to the created group using group membership resource

You can apply this configuration using `terraform apply` and it will create all the specified resources in your AWS account.",,,,,
"aws_codebuild_project, aws_iam_role",create a Basic AWS codebuild project resource with example iam role and an example GITHUB source,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for aws_iam_role resource
has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""codebuild.amazonaws.com"") 
} 

has_valid_codebuild {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_codebuild_project""
        resource.values.artifacts
    resource.values.environment
    resource.values.name
    resource.values.source[_].type == ""GITHUB""
    role := input.configuration.root_module.resources[i]
    role.expressions.service_role
}

has_valid_resources {
        has_valid_iam_role(input.resource_changes)
    has_valid_codebuild
}
",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""test_role"" {
  name               = ""test_role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}


resource ""aws_codebuild_project"" ""example"" {
  name          = ""test-project""
  service_role  = aws_iam_role.test_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    location        = ""github.com/source-location""
    git_clone_depth = 1
  }
}","Has one ""aws_iam_role"" resource and one ""aws_codebuild_project"" resource with the ""type"" argument within the ""source"" block equal to ""GITHUB""","I'll help you create a Terraform configuration for an AWS CodeBuild project with an IAM role and GitHub source. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""example-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""example-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:*"",
          ""codecommit:*""
        ]
      }
    ]
  })
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""example"" {
  name          = ""example-project""
  description   = ""Example CodeBuild project""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  source_version = ""main""

  logs_config {
    cloudwatch_logs {
      group_name  = ""example-log-group""
      stream_name = ""example-log-stream""
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration creates:

1. An IAM role for CodeBuild with necessary permissions
2. An IAM role policy attached to the role
3. A CodeBuild project that:
   - Uses GitHub as source
   - Uses Linux container with standard AWS CodeBuild image
   - Has CloudWatch logs configuration
   - Has a 60-minute build timeout
   - Uses BUILD_GENERAL1_SMALL compute type

Remember to:
- Replace `your-org/your-repo.git` with your actual GitHub repository URL
- Adjust the IAM permissions based on your specific needs
- Modify the build environment and compute type according to your requirements
- Update the tags as needed

The configuration includes all required providers and necessary resources for a functional CodeBuild project with GitHub integration.",,,,,,"I'll help you create a Terraform configuration for an AWS CodeBuild project with an IAM role and GitHub source. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""example-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""example-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:*"",
          ""codecommit:*""
        ]
      }
    ]
  })
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""example"" {
  name          = ""example-project""
  description   = ""Example CodeBuild project with GitHub source""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  source_version = ""main""

  vpc_config {
    vpc_id = ""vpc-xxxxxx""  # Replace with your VPC ID if needed

    subnets = [
      ""subnet-xxxxxx"",     # Replace with your subnet IDs if needed
    ]

    security_group_ids = [
      ""sg-xxxxxx"",        # Replace with your security group IDs if needed
    ]
  }

  tags = {
    Environment = ""Test""
  }
}

# GitHub webhook for automation
resource ""aws_codebuild_webhook"" ""example"" {
  project_name = aws_codebuild_project.example.name
  
  filter_group {
    filter {
      type    = ""EVENT""
      pattern = ""PUSH""
    }

    filter {
      type    = ""HEAD_REF""
      pattern = ""main""
    }
  }
}
```

This configuration includes:

1. The required AWS provider configuration
2. An IAM role and policy for CodeBuild with basic permissions
3. A CodeBuild project with:
   - GitHub source configuration
   - Linux container environment
   - Basic VPC configuration (replace with your actual VPC details)
   - No artifacts (can be modified based on your needs)
4. A webhook configuration for automated builds on GitHub pushes to main branch

To use this configuration:

1. Replace the placeholder values:
   - VPC ID
   - Subnet IDs
   - Security Group IDs
   - GitHub repository URL
2. Adjust the IAM permissions based on your specific needs
3. Modify the build environment and other settings as required

Remember to:",,,,,,"I'll help you create a Terraform configuration for an AWS CodeBuild project with an IAM role and GitHub source. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""example-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""example-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:*"",
          ""codecommit:*""
        ]
      }
    ]
  })
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""example"" {
  name          = ""example-project""
  description   = ""Example CodeBuild project""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  source_version = ""main""

  logs_config {
    cloudwatch_logs {
      group_name  = ""example-log-group""
      stream_name = ""example-log-stream""
      status      = ""ENABLED""
    }
  }

  tags = {
    Environment = ""Test""
  }
}
```

This configuration includes:

1. The required AWS provider configuration
2. An IAM role for CodeBuild with basic permissions
3. An IAM role policy with necessary permissions for CloudWatch Logs, S3, and CodeCommit
4. A CodeBuild project resource that:
   - Uses GitHub as the source
   - Uses a Linux container environment
   - Has CloudWatch logs enabled
   - Has a 60-minute build timeout
   - Uses the created IAM role

To use this configuration:

1. Replace `your-org/your-repo.git` with your actual GitHub repository URL
2. Adjust the region in the provider block if needed
3. Modify the IAM permissions based on your specific needs
4. Adjust the environment compute type and image if needed
5. Modify the tags as appropriate for your environment

Remember to also configure GitHub authentication for CodeBuild separately, as this is typically done through the AWS Console or requires additional configuration.",,,,,
"aws_chime_voice_connector, aws_chime_voice_connector, aws_chime_voice_connector_group","Create two AWS Chime Voice Connectors, both with encryption. Then create an aws_chime_voice_connector_group resource with each connector as a connector argument","package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc1 in input.configuration.root_module.resources
    vc1.type == ""aws_chime_voice_connector""
    vc1.expressions.require_encryption.constant_value == true

    some vc2 in input.configuration.root_module.resources
    vc2.type == ""aws_chime_voice_connector""
    vc2.expressions.require_encryption.constant_value == true

    not vc1 == vc2
    
    some vcgroup in input.configuration.root_module.resources
    vcgroup.type == ""aws_chime_voice_connector_group""

    some connector1 in vcgroup.expressions.connector
    vc1.address in connector1.voice_connector_id.references
    some connector2 in vcgroup.expressions.connector
    vc2.address in connector2.voice_connector_id.references
    
    not connector1 == connector2
}
",3,"resource ""aws_chime_voice_connector"" ""vc1"" {
  name               = ""connector-test-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""vc2"" {
  name               = ""connector-test-2""
  require_encryption = true
  aws_region         = ""us-west-2""
}

resource ""aws_chime_voice_connector_group"" ""group"" {
  name = ""test-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.vc1.id
    priority           = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.vc2.id
    priority           = 3
  }
}","Has one resource ""aws_chime_voice_connector""
with require_encryption=""true""

Has another resource ""aws_chime_voice_connector""
with require_encryption=""true""

Has one resource ""aws_chime_voice_connector_group""
with a connector for each voice connector with the correct id","I'll help you create a Terraform configuration for two AWS Chime Voice Connectors with encryption and group them together in a voice connector group.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Adjust region as needed
}

# First Voice Connector
resource ""aws_chime_voice_connector"" ""connector_1"" {
  name               = ""voice-connector-1""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Second Voice Connector
resource ""aws_chime_voice_connector"" ""connector_2"" {
  name               = ""voice-connector-2""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Voice Connector Group
resource ""aws_chime_voice_connector_group"" ""connector_group"" {
  name = ""voice-connector-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_2.id
    priority          = 2
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two voice connectors with encryption enabled
3. Creates a voice connector group that includes both connectors with different priorities
4. Uses proper resource referencing to link the connectors to the group

Each voice connector has encryption required (`require_encryption = true`), and they are both added to the voice connector group with different priorities (1 and 2).",,,,,,"I'll help you create a Terraform configuration for two AWS Chime Voice Connectors with encryption and group them into a voice connector group.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# First Voice Connector
resource ""aws_chime_voice_connector"" ""connector_1"" {
  name               = ""voice-connector-1""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Second Voice Connector
resource ""aws_chime_voice_connector"" ""connector_2"" {
  name               = ""voice-connector-2""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Voice Connector Group
resource ""aws_chime_voice_connector_group"" ""connector_group"" {
  name = ""voice-connector-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_2.id
    priority          = 2
  }
}
```

This configuration:
1. Creates two voice connectors with encryption enabled
2. Creates a voice connector group that includes both connectors
3. Assigns different priorities to each connector in the group (1 and 2)
4. Uses proper resource referencing to link the connectors to the group

Both voice connectors are set up in the us-east-1 region and require encryption. The voice connector group references both connectors using their IDs and assigns them different priorities for failover purposes.",,,,,,"I'll help you create a Terraform configuration for two AWS Chime Voice Connectors with encryption and group them together in a voice connector group.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Adjust the region as needed
}

# First Voice Connector
resource ""aws_chime_voice_connector"" ""connector1"" {
  name               = ""voice-connector-1""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Second Voice Connector
resource ""aws_chime_voice_connector"" ""connector2"" {
  name               = ""voice-connector-2""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Voice Connector Group
resource ""aws_chime_voice_connector_group"" ""connector_group"" {
  name = ""voice-connector-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.connector1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.connector2.id
    priority          = 2
  }
}
```

This configuration:
1. Creates two voice connectors with encryption enabled
2. Creates a voice connector group that includes both connectors
3. Sets priorities for each connector in the group (1 and 2)
4. Uses proper resource dependencies through reference expressions

Make sure to adjust the region and names according to your needs. Both voice connectors have `require_encryption` set to true for security purposes.",,,,,
"aws_cloudfront_distribution, aws_cloudfront_origin_access_control, aws_route53_record, aws_route53_zone, aws_s3_bucket","The infrastructure for a video streaming site that distribute video content globally. Accomplish this using the following services: AWS CloudFront, AWS Route53, and AWS S3. Add any necessary services.","package netflix_middle

import data.set
import rego.v1

bucket_valid(bucket) := true

access_control_valid(access_control) if {
	access_control.expressions.name
	access_control.expressions.origin_access_control_origin_type.constant_value == ""s3""
	access_control.expressions.signing_behavior
	access_control.expressions.signing_protocol
}

distribution_valid(distribution, access_control, bucket) if {
	some origin in distribution.expressions.origin
	bucket.address in origin.domain_name.references
	access_control.address in origin.origin_access_control_id.references

	some cache_behavior in distribution.expressions.default_cache_behavior
	{method | method := cache_behavior.allowed_methods.constant_value[_]} == {""GET"", ""HEAD""}
	{method | method := cache_behavior.cached_methods.constant_value[_]} == {""GET"", ""HEAD""}
	cache_behavior.viewer_protocol_policy.constant_value == ""allow-all""

	origin.origin_id == cache_behavior.target_origin_id

	some restrictions in distribution.expressions.restrictions
	some restriction in restrictions.geo_restriction
	restriction.restriction_type
	restriction.locations
}

zone_valid(zone) if {
	zone.expressions.name
}

record_valid(record, zone, distribution) if {
	record.expressions.type
	record.expressions.name
	zone.address in record.expressions.zone_id.references

	some alias in record.expressions.alias
	distribution.address in alias.name.references
	distribution.address in alias.zone_id.references
	alias.evaluate_target_health
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	some access_control in resources
	access_control.type == ""aws_cloudfront_origin_access_control""

	some distribution in resources
	distribution.type == ""aws_cloudfront_distribution""

	some zone in resources
	zone.type == ""aws_route53_zone""

	some record in resources
	record.type == ""aws_route53_record""

	bucket_valid(bucket)
	access_control_valid(access_control)
	distribution_valid(distribution, access_control, bucket)
	zone_valid(zone)
	record_valid(record, zone, distribution)
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_s3_bucket"" ""video_content"" {
  bucket_prefix = ""video-content-""
}

resource ""aws_s3_object"" ""put_website"" {
  bucket = aws_s3_bucket.video_content.bucket
  key = ""index.html""
  source = ""./supplement/index.html""
  content_type = ""text/html""
}

resource ""aws_s3_object"" ""put_website_css"" {
  bucket = aws_s3_bucket.video_content.bucket
  key = ""css/style.css""
  source = ""./supplement/css/style.css""
  content_type = ""text/css""
}

resource ""aws_s3_bucket_policy"" ""bucket_policy"" {
  bucket = aws_s3_bucket.video_content.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""cloudfront.amazonaws.com""
        }
        Action = ""s3:GetObject""
        Resource = ""${aws_s3_bucket.video_content.arn}/*""
        Condition = {
          StringEquals = {
            ""AWS:SourceArn"" = aws_cloudfront_distribution.my_distribution.arn
          }
        }
      }
    ]
  })
}


locals {
  s3_origin_id = ""s3_video_content_origin""
}

resource ""aws_cloudfront_origin_access_control"" ""s3_oac"" {
  name                              = ""s3-oac""
  description                       = ""aws access control policy""
  origin_access_control_origin_type = ""s3""
  signing_behavior                  = ""always""
  signing_protocol                  = ""sigv4""
}

resource ""aws_cloudfront_distribution"" ""my_distribution"" {
  origin {
    domain_name              = aws_s3_bucket.video_content.bucket_regional_domain_name
    origin_access_control_id = aws_cloudfront_origin_access_control.s3_oac.id
    origin_id                = local.s3_origin_id
  }

  enabled             = true
  default_root_object = ""index.html""

  default_cache_behavior {
    allowed_methods  = [""GET"", ""HEAD""]
    cached_methods   = [""GET"", ""HEAD""]
    target_origin_id = local.s3_origin_id

    forwarded_values {
      query_string = false

      cookies {
        forward = ""none""
      }
    }

    viewer_protocol_policy = ""allow-all""
  }

  price_class = ""PriceClass_200""

  restrictions {
    geo_restriction {
      restriction_type = ""whitelist""
      locations        = [""US"", ""CA"", ""GB"", ""DE""]
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }
}

# Route53
resource ""aws_route53_zone"" ""my_zone"" {
  name = ""my-test-cloudfront.com""
}

resource ""aws_route53_record"" ""cdn_ipv4"" {
  type    = ""A""
  name    = ""cdn""
  zone_id = aws_route53_zone.my_zone.zone_id

  alias {
    name                   = aws_cloudfront_distribution.my_distribution.domain_name
    zone_id                = aws_cloudfront_distribution.my_distribution.hosted_zone_id
    evaluate_target_health = true
  }
}","have one ""aws_cloudfront_origin_access_control"" resource
    with ""name"" argument
    with ""origin_access_control_origin_type"" with value ""s3""
    with ""signing_behavior"" argument
    with ""signing_protocol"" argument
have one ""aws_cloudfront_distribution"" resource
    with ""origin.domain_name"" argument linking to ""aws_s3_bucket"" resource
    with ""origin.origin_access_control_id"" argument linking to ""aws_cloudfront_origin_access_control"" resource
    with ""origin.origin_id"" argument
    with ""default_cache_behavior.allowed_methods"" argument with value [""GET"", ""HEAD""]
    with ""default_cache_behavior.cached_methods"" argument with value [""GET"", ""HEAD""]
    with ""default_cache_behavior.target_origin_id"" argument having the same value as ""origin.origin_id"" argument
    with ""default_cache_behavior.viewer_protocol_policy"" argument with value ""allow-all""
    with ""restrictions.geo_restriction.restriction_type"" argument
    with ""restrictions.geo_restriction.locations"" argument
    with ""viewer_certificate.cloudfront_default_certificate"" argument
have one ""aws_s3_bucket"" resource
have one ""aws_route53_zone"" resource
    with ""name"" argument
have one ""aws_route53_record"" resource
    with ""type"" argument
    with ""name"" argument
    with ""zone_id"" argument linking to ""aws_route53_zone"" resource
    with ""alias.name"" argument linking to ""aws_cloudfront_distribution"" resource
    with ""alias.zone_id"" argument linking to ""aws_cloudfront_distribution"" resource
    with ""alias.evaluate_target_health"" argument","I'll help create a Terraform configuration for a global video streaming infrastructure using AWS CloudFront, Route53, and S3. This solution will include a CloudFront distribution connected to an S3 bucket origin, with Route53 for DNS management.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Primary region for CloudFront and Route53
}

# S3 bucket for video content
resource ""aws_s3_bucket"" ""video_bucket"" {
  bucket = ""video-streaming-content-bucket""
}

# Bucket policy to allow CloudFront access
resource ""aws_s3_bucket_policy"" ""allow_cloudfront_access"" {
  bucket = aws_s3_bucket.video_bucket.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowCloudFrontServicePrincipal""
        Effect    = ""Allow""
        Principal = {
          Service = ""cloudfront.amazonaws.com""
        }
        Action   = ""s3:GetObject""
        Resource = ""${aws_s3_bucket.video_bucket.arn}/*""
        Condition = {
          StringEquals = {
            ""AWS:SourceArn"" = aws_cloudfront_distribution.video_distribution.arn
          }
        }
      }
    ]
  })
}

# CloudFront Origin Access Control
resource ""aws_cloudfront_origin_access_control"" ""oac"" {
  name                              = ""video-streaming-oac""
  description                       = ""Video streaming OAC""
  origin_access_control_origin_type = ""s3""
  signing_behavior                  = ""always""
  signing_protocol                  = ""sigv4""
}

# CloudFront distribution
resource ""aws_cloudfront_distribution"" ""video_distribution"" {
  enabled             = true
  is_ipv6_enabled    = true
  price_class        = ""PriceClass_All""
  http_version       = ""http2""

  origin {
    domain_name              = aws_s3_bucket.video_bucket.bucket_regional_domain_name
    origin_access_control_id = aws_cloudfront_origin_access_control.oac.id
    origin_id               = ""S3Origin""
  }

  default_cache_behavior {
    allowed_methods        = [""GET"", ""HEAD"", ""OPTIONS""]
    cached_methods         = [""GET"", ""HEAD""]
    target_origin_id       = ""S3Origin""
    viewer_protocol_policy = ""redirect-to-https""
    
    forwarded_values {
      query_string = false
      cookies {
        forward = ""none""
      }
    }

    min_ttl                = 0
    default_ttl            = 3600
    max_ttl                = 86400
  }

  restrictions {
    geo_restriction {
      restriction_type = ""none""
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }
}

# Route53 hosted zone (assuming it exists)
data ""aws_route53_zone"" ""domain"" {
  name = ""example.com""  # Replace with your domain
}

# Route53 record for the CloudFront distribution
resource ""aws_route53_record"" ""video_domain"" {
  zone_id = data.aws_route53_zone.domain.zone_id
  name    = ""video.example.com""  # Replace with your subdomain
  type    = ""A""

  alias {
    name                   = aws_cloudfront_distribution.video_distribution.domain_name
    zone_id                = aws_cloudfront_distribution.video_distribution.hosted_zone_id
    evaluate_target_health = false
  }
}

# Output the CloudFront distribution domain name
output ""cloudfront_domain_name"" {
  value = aws_cloudfront_distribution.video_distribution.domain_name
}

# Output the website URL
output ""website",,,,,,"I'll help you create a Terraform configuration for a global video streaming infrastructure using AWS CloudFront, Route53, and S3. I'll include all necessary components and follow AWS best practices.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Primary region for CloudFront and Route53
}

# S3 bucket for video content
resource ""aws_s3_bucket"" ""video_content"" {
  bucket = ""my-video-streaming-content""
}

# Bucket policy to allow CloudFront access
resource ""aws_s3_bucket_policy"" ""allow_cloudfront"" {
  bucket = aws_s3_bucket.video_content.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowCloudFrontAccess""
        Effect    = ""Allow""
        Principal = {
          Service = ""cloudfront.amazonaws.com""
        }
        Action   = ""s3:GetObject""
        Resource = ""${aws_s3_bucket.video_content.arn}/*""
        Condition = {
          StringEquals = {
            ""AWS:SourceArn"" = aws_cloudfront_distribution.video_distribution.arn
          }
        }
      }
    ]
  })
}

# CloudFront Origin Access Control
resource ""aws_cloudfront_origin_access_control"" ""oac"" {
  name                              = ""video-streaming-oac""
  description                       = ""Origin Access Control for video streaming""
  origin_access_control_origin_type = ""s3""
  signing_behavior                  = ""always""
  signing_protocol                  = ""sigv4""
}

# CloudFront distribution
resource ""aws_cloudfront_distribution"" ""video_distribution"" {
  enabled             = true
  is_ipv6_enabled    = true
  default_root_object = ""index.html""
  price_class        = ""PriceClass_All""  # Global distribution

  origin {
    domain_name              = aws_s3_bucket.video_content.bucket_regional_domain_name
    origin_id                = ""S3Origin""
    origin_access_control_id = aws_cloudfront_origin_access_control.oac.id
  }

  default_cache_behavior {
    allowed_methods        = [""GET"", ""HEAD"", ""OPTIONS""]
    cached_methods         = [""GET"", ""HEAD""]
    target_origin_id       = ""S3Origin""
    viewer_protocol_policy = ""redirect-to-https""
    compress              = true

    forwarded_values {
      query_string = false
      cookies {
        forward = ""none""
      }
    }

    min_ttl     = 0
    default_ttl = 3600
    max_ttl     = 86400
  }

  restrictions {
    geo_restriction {
      restriction_type = ""none""
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }
}

# Route 53 hosted zone (assuming domain is already registered)
resource ""aws_route53_zone"" ""main"" {
  name = ""videostreaming.example.com""
}

# Route 53 record for CloudFront distribution
resource ""aws_route53_record"" ""video_distribution"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""video.videostreaming.example.com""
  type    = ""A""

  alias {
    name                   = aws_cloudfront_distribution.video_distribution.domain_name
    zone_id                = aws_cloudfront_distribution.video_distribution.hosted_zone_id
    evaluate_target_health = false
  }
}

# S3 bucket versioning
resource ""aws_s3_bucket_versioning"" ""video_content_versioning"" {
  bucket = aws_s3_bucket.video_content.id
  versioning_configuration {
    status = ""Enabled""
  ",,,,,,"I'll help you create a Terraform configuration for a global video streaming infrastructure using AWS CloudFront, Route53, and S3. I'll include the necessary components for a secure and scalable solution.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Primary region for CloudFront and Route53
}

# S3 bucket for video content
resource ""aws_s3_bucket"" ""video_content"" {
  bucket = ""video-streaming-content-bucket""
}

# Bucket policy to allow CloudFront access
resource ""aws_s3_bucket_policy"" ""allow_cloudfront_access"" {
  bucket = aws_s3_bucket.video_content.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowCloudFrontServicePrincipal""
        Effect    = ""Allow""
        Principal = {
          Service = ""cloudfront.amazonaws.com""
        }
        Action   = ""s3:GetObject""
        Resource = ""${aws_s3_bucket.video_content.arn}/*""
        Condition = {
          StringEquals = {
            ""AWS:SourceArn"" = aws_cloudfront_distribution.video_distribution.arn
          }
        }
      }
    ]
  })
}

# CloudFront Origin Access Control
resource ""aws_cloudfront_origin_access_control"" ""oac"" {
  name                              = ""video-streaming-oac""
  description                       = ""Origin Access Control for video streaming""
  origin_access_control_origin_type = ""s3""
  signing_behavior                  = ""always""
  signing_protocol                  = ""sigv4""
}

# CloudFront distribution
resource ""aws_cloudfront_distribution"" ""video_distribution"" {
  enabled             = true
  is_ipv6_enabled    = true
  comment            = ""Video streaming distribution""
  price_class        = ""PriceClass_All""
  
  origin {
    domain_name              = aws_s3_bucket.video_content.bucket_regional_domain_name
    origin_access_control_id = aws_cloudfront_origin_access_control.oac.id
    origin_id                = ""S3Origin""
  }

  default_cache_behavior {
    allowed_methods        = [""GET"", ""HEAD"", ""OPTIONS""]
    cached_methods         = [""GET"", ""HEAD""]
    target_origin_id       = ""S3Origin""
    viewer_protocol_policy = ""redirect-to-https""
    
    forwarded_values {
      query_string = false
      cookies {
        forward = ""none""
      }
    }

    min_ttl                = 0
    default_ttl            = 3600
    max_ttl                = 86400
  }

  restrictions {
    geo_restriction {
      restriction_type = ""none""
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }
}

# Route 53 hosted zone (assuming it exists)
data ""aws_route53_zone"" ""selected"" {
  name = ""example.com.""  # Replace with your domain
}

# Route 53 record for the CloudFront distribution
resource ""aws_route53_record"" ""video_streaming"" {
  zone_id = data.aws_route53_zone.selected.zone_id
  name    = ""video.example.com""  # Replace with your subdomain
  type    = ""A""

  alias {
    name                   = aws_cloudfront_distribution.video_distribution.domain_name
    zone_id                = aws_cloudfront_distribution.video_distribution.hosted_zone_id
    evaluate_target_health = false
  }
}

# Optional: S3 bucket versioning
resource ""aws_s3_bucket_versioning"" ""video_content_versioning"" {
  bucket = aws_s3_bucket.video_content.id
  versioning_configuration {
    status",,,,,
aws_db_instance,create a basic AWS SQL database,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        requirement1(resource.expressions)
        requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
        expressions.allocated_storage
        expressions.engine.constant_value == ""mysql""
    expressions.username
        is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
        expressions.instance_class
        is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
        expressions.password
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

requirement3(expressions) {
        expressions.snapshot_identifier
}

requirement3(expressions) {
        expressions.replicate_source_db
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 10
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""","I'll help you create a basic AWS RDS (Relational Database Service) SQL database using Terraform. This example will create a simple MySQL database instance with basic configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create a security group for the RDS instance
resource ""aws_security_group"" ""rds_sg"" {
  name        = ""rds_security_group""
  description = ""Security group for RDS instance""

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]  # Note: In production, restrict this to specific IP ranges
  }
}

# Create the RDS instance
resource ""aws_db_instance"" ""default"" {
  identifier           = ""my-sql-db""
  allocated_storage    = 20
  storage_type        = ""gp2""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  username            = ""admin""
  password            = ""your_password_here""  # Change this to a secure password
  skip_final_snapshot = true

  vpc_security_group_ids = [aws_security_group.rds_sg.id]

  tags = {
    Name = ""MyRDSInstance""
  }
}

# Output the endpoint
output ""rds_endpoint"" {
  value = aws_db_instance.default.endpoint
}
```

This configuration will:
1. Set up the AWS provider
2. Create a security group allowing inbound MySQL traffic (port 3306)
3. Create an RDS instance with MySQL 8.0
4. Output the database endpoint

Important notes:
- Remember to replace `your_password_here` with a secure password
- In production, you should use more secure practices like:
  - Storing sensitive data in AWS Secrets Manager
  - Restricting security group access to specific IP ranges
  - Using a private subnet in a VPC
  - Enabling backup and encryption
- The `skip_final_snapshot` is set to true for simplicity, but in production, you might want to set it to false
- The instance class `db.t3.micro` is suitable for development but might need to be larger for production use

You can customize the configuration by modifying values like storage size, instance class, and other parameters according to your needs.",,,,,,"I'll help you create a basic AWS RDS (Relational Database Service) instance using Terraform. This example will create a MySQL database with basic configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create a security group for the RDS instance
resource ""aws_security_group"" ""rds_sg"" {
  name        = ""rds_security_group""
  description = ""Security group for RDS instance""

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]  # Warning: In production, restrict this to specific IP ranges
  }
}

# Create an RDS instance
resource ""aws_db_instance"" ""default"" {
  identifier           = ""my-sql-db""
  allocated_storage    = 20
  storage_type        = ""gp2""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  db_name             = ""mydb""
  username            = ""admin""
  password            = ""your-password-here""  # In production, use secrets management
  skip_final_snapshot = true

  vpc_security_group_ids = [aws_security_group.rds_sg.id]

  tags = {
    Name = ""MyRDSInstance""
  }
}

output ""db_endpoint"" {
  value = aws_db_instance.default.endpoint
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a security group allowing inbound MySQL traffic (port 3306)
3. Creates an RDS instance with MySQL 8.0
4. Uses a t3.micro instance type (suitable for development/testing)
5. Outputs the database endpoint

Important security notes:
- In production, you should use secrets management for the database password
- The security group should be restricted to specific IP ranges
- Consider using a VPC and private subnets for better security
- Enable backup and encryption for production workloads
- The `skip_final_snapshot` is set to true for simplicity, but you might want to change this in production

You can customize the configuration by modifying:
- The region
- Instance size
- Storage allocation
- Engine version
- Security group rules
- Other RDS parameters as needed",,,,,,"I'll help you create a basic AWS RDS (Relational Database Service) instance using Terraform. Here's a configuration that sets up a basic SQL database:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_db_instance"" ""default"" {
  identifier           = ""my-sql-database""
  allocated_storage    = 20
  storage_type        = ""gp2""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  db_name             = ""mydb""
  username            = ""admin""
  password            = ""your-password-here""  # Consider using variables or secrets management
  
  # Network settings
  publicly_accessible = false
  
  # Backup settings
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  
  # Maintenance settings
  maintenance_window     = ""Mon:04:00-Mon:05:00""
  
  # Security settings
  skip_final_snapshot    = true  # For development only; set to false in production
  
  # Performance insights
  performance_insights_enabled = false
  
  # Tags
  tags = {
    Name        = ""MyDatabase""
    Environment = ""Development""
  }
}

# Security Group for the RDS instance
resource ""aws_security_group"" ""rds_sg"" {
  name        = ""rds-security-group""
  description = ""Security group for RDS database""

  ingress {
    from_port   = 3306  # MySQL default port
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]  # Adjust this to your VPC CIDR
  }

  tags = {
    Name = ""rds-security-group""
  }
}
```

This configuration creates:
1. A MySQL RDS instance with basic configuration
2. A security group for the database instance
3. Basic backup and maintenance windows
4. Basic security settings

Important notes:
- Replace the password with a secure value
- Adjust the CIDR blocks in the security group to match your network
- Consider using variables for sensitive information
- In production, you should set `skip_final_snapshot = false`
- The instance class `db.t3.micro` is suitable for development; choose an appropriate size for production
- The configuration uses MySQL, but you can change the engine to other supported databases like PostgreSQL

Remember to properly secure your database in a production environment by:
- Using encryption at rest
- Placing it in a private subnet
- Using more restrictive security group rules
- Implementing proper backup strategies
- Using secrets management for sensitive information",,,,,
"aws_internet_gateway, aws_route_table, aws_vpc","sets up a dedicated-tenancy AWS VPC with DNS support and hostnames enabled, an internet gateway, and a route table for outbound internet access, all tagged with a variable name.","package terraform.validation

default is_valid_vpc = false
default is_valid_internet_gateway = false
default is_valid_route_table = false

# Validate aws_vpc resource
is_valid_vpc {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_vpc""
    resource.expressions.cidr_block != null
    resource.expressions.enable_dns_support.constant_value == true
    resource.expressions.enable_dns_hostnames.constant_value == true
}

# Validate aws_internet_gateway resource
is_valid_internet_gateway {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_internet_gateway""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.dgraph.id""
    resource.expressions.tags.constant_value[""Name""] != null
}

# Validate aws_route_table resource
is_valid_route_table {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route_table""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.dgraph.id""
    resource.expressions.route.references[0] == ""aws_internet_gateway.dgraph_gw.id""
    resource.expressions.tags.constant_value[""Name""] != null
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""dgraph"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_support   = true
  instance_tenancy     = ""dedicated""

  # For enabling assignment of private dns addresses within AWS.
  enable_dns_hostnames = true

  tags = {
    Name = ""var.name""
  }
}

resource ""aws_internet_gateway"" ""dgraph_gw"" {
  vpc_id = aws_vpc.dgraph.id

  tags = {
    Name = ""var.name""
  }
}

resource ""aws_route_table"" ""dgraph_igw"" {
  vpc_id = aws_vpc.dgraph.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.dgraph_gw.id
  }

  tags = {
    Name = ""var.name""
  }
}","Has one ""aws_vpc"" resource
with a specified ""cidr_block""
with ""enable_dns_support"" and ""enable_dns_hostnames"" set to true

Has one ""aws_internet_gateway"" resource
with ""vpc_id"" referencing the ""aws_vpc"" resource
with a ""tags"" map including a ""Name"" key referencing a variable

Has one ""aws_route_table"" resource
with ""vpc_id"" referencing the ""aws_vpc"" resource
containing a default route for internet access through the ""aws_internet_gateway""
with a ""tags"" map including a ""Name"" key referencing a variable","I'll help you create a Terraform configuration for a dedicated-tenancy VPC with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # You can change this to your desired region
}

variable ""vpc_name"" {
  description = ""Name tag for the VPC and related resources""
  type        = string
  default     = ""dedicated-vpc""
}

# Create VPC with dedicated tenancy
resource ""aws_vpc"" ""main"" {
  cidr_block                       = ""10.0.0.0/16""
  instance_tenancy                 = ""dedicated""
  enable_dns_support               = true
  enable_dns_hostnames            = true

  tags = {
    Name = var.vpc_name
  }
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""${var.vpc_name}-igw""
  }
}

# Create Route Table
resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""${var.vpc_name}-rt""
  }
}

# Associate Route Table with VPC (Main Route Table)
resource ""aws_main_route_table_association"" ""main"" {
  vpc_id         = aws_vpc.main.id
  route_table_id = aws_route_table.main.id
}
```

This configuration:
1. Sets up the AWS provider and required provider block
2. Creates a variable for the VPC name that can be used for tagging
3. Creates a VPC with dedicated tenancy, DNS support, and DNS hostnames enabled
4. Creates an Internet Gateway and attaches it to the VPC
5. Creates a route table with a route for outbound internet access (0.0.0.0/0)
6. Associates the route table as the main route table for the VPC
7. Tags all resources appropriately using the provided variable

All resources are properly tagged with the variable name, and the configuration follows AWS best practices for VPC setup. You can customize the CIDR block and region according to your needs.",,,,,,"I'll help you create a Terraform configuration for a dedicated-tenancy VPC with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

variable ""vpc_name"" {
  description = ""Name tag for the VPC and related resources""
  type        = string
  default     = ""dedicated-vpc""
}

# Create VPC with dedicated tenancy
resource ""aws_vpc"" ""main"" {
  cidr_block                       = ""10.0.0.0/16""
  instance_tenancy                 = ""dedicated""
  enable_dns_support              = true
  enable_dns_hostnames            = true

  tags = {
    Name = var.vpc_name
  }
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""${var.vpc_name}-igw""
  }
}

# Create Route Table
resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""${var.vpc_name}-rt""
  }
}

# Associate Route Table with VPC (Main Route Table)
resource ""aws_main_route_table_association"" ""main"" {
  vpc_id         = aws_vpc.main.id
  route_table_id = aws_route_table.main.id
}
```

This configuration:
1. Sets up the AWS provider and required provider block
2. Creates a variable for the VPC name that can be used for tagging
3. Creates a VPC with dedicated tenancy, DNS support, and DNS hostnames enabled
4. Creates an Internet Gateway and attaches it to the VPC
5. Creates a route table with a route for outbound internet access (0.0.0.0/0)
6. Associates the route table as the main route table for the VPC
7. Tags all resources appropriately using the variable name

You can customize the CIDR block and region according to your needs. The VPC name can be specified when applying the configuration by setting the `vpc_name` variable.",,,,,,"I'll help you create a Terraform configuration for a dedicated-tenancy VPC with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

variable ""vpc_name"" {
  description = ""Name tag for the VPC and related resources""
  type        = string
  default     = ""dedicated-vpc""
}

# Create VPC with dedicated tenancy
resource ""aws_vpc"" ""main"" {
  cidr_block                       = ""10.0.0.0/16""
  instance_tenancy                 = ""dedicated""
  enable_dns_support               = true
  enable_dns_hostnames            = true

  tags = {
    Name = var.vpc_name
  }
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""${var.vpc_name}-igw""
  }
}

# Create Route Table
resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""${var.vpc_name}-rt""
  }
}

# Associate Route Table with VPC (Main Route Table)
resource ""aws_main_route_table_association"" ""main"" {
  vpc_id         = aws_vpc.main.id
  route_table_id = aws_route_table.main.id
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a variable for the VPC name
3. Creates a VPC with dedicated tenancy and enabled DNS support/hostnames
4. Creates an Internet Gateway attached to the VPC
5. Creates a route table with a route to the internet (0.0.0.0/0)
6. Associates the route table as the main route table for the VPC
7. Tags all resources with the provided name variable

You can customize the CIDR block and region according to your needs. To use this configuration, simply provide a value for the vpc_name variable or use the default ""dedicated-vpc"" value.",,,,,
"aws_elastic_beanstalk_application, aws_elastic_beanstalk_application_version, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment, aws_route53_record, aws_route53_zone, aws_s3_bucket, aws_s3_object","Configure Route 53 resources to manage a domain and directs traffic to an Elastic Beanstalk environment. 
Name the zone ""main"", the environment ""eb-env"", the iam role ""eb_ec2_role"", the instance profile ""eb_ec2_profile"", the bucket (with unique name), the object ""examplebucket_object"", the application ""myapp"" and the application version ""version""","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_s3_bucket = false

default is_valid_s3_object = false

default is_valid_eb_app = false

default is_valid_eb_env = false

default is_valid_r53_zone = false

default is_valid_r53_record = false



# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.name
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records.references[0] == ""aws_elastic_beanstalk_environment.eb-env.cname""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.main.zone_id""

}

is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0] == ""aws_iam_role.eb_ec2_role.name""
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0] == ""aws_iam_role.eb_ec2_role.name""
}

is_valid_s3_bucket {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    resource.name
    # resource.expressions.bucket.constant_value == ""sampleapril26426""
}

is_valid_s3_object {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_s3_object""
    # resource.expressions.bucket.references[0] == ""aws_s3_bucket.sampleapril26426.id""
    resource.expressions.key
    resource.expressions.source
    
}

# Validate aws_eb_app
is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}

is_valid_eb_app_version {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application_version""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.myapp.name""
    resource.expressions.bucket.references[0] == ""aws_s3_object.examplebucket_object.bucket""
    resource.expressions.key.references[0] == ""aws_s3_object.examplebucket_object.key""
}
# Validate aws_eb_env
is_valid_eb_env {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.myapp.name""
    resource.expressions.solution_stack_name
    resource.expressions.version_label.references[0] == ""aws_elastic_beanstalk_application_version.version.name""
        some a
    resource.expressions.setting[a].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
}


# Combine all checks into a final rule
is_configuration_valid {
        is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
        is_valid_r53_zone
        is_valid_r53_record
    is_valid_s3_bucket
    is_valid_s3_object 
    is_valid_eb_app_version
    is_valid_eb_app
    is_valid_eb_env
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}


# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Assuming you have a hosted zone for your domain in Route 53.
# Route 53 Hosted Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example56.com""
}

# DNS Record to point to the Elastic Beanstalk environment
resource ""aws_route53_record"" ""www"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""app.example56.com""
  type    = ""CNAME""
  ttl     = 60
  records = [aws_elastic_beanstalk_environment.eb-env.cname]
}

resource ""aws_s3_bucket"" ""sample_bucket"" {
  bucket_prefix = ""sample-""
}

resource ""aws_s3_object"" ""examplebucket_object"" {
  key    = ""randofile""
  bucket = aws_s3_bucket.sample_bucket.id
  source = ""./supplement/app.zip""
}

# Define the Elastic Beanstalk application
resource ""aws_elastic_beanstalk_application"" ""myapp"" {
  name        = ""something-application""
}

# Define the Elastic Beanstalk application version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""app-version""
  application = aws_elastic_beanstalk_application.myapp.name
  bucket = aws_s3_object.examplebucket_object.bucket
  key    = aws_s3_object.examplebucket_object.key
}


# Elastic Beanstalk environment
resource ""aws_elastic_beanstalk_environment"" ""eb-env"" {
  name                = ""awesomeenv""
  application         = aws_elastic_beanstalk_application.myapp.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  version_label = aws_elastic_beanstalk_application_version.version.name

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""

Has one ""aws_s3_bucket"" resource
    with ""bucket""

Has one ""aws_s3_object"" resource
    with ""key""
    with ""source""
    with ""bucket"" referencing the ""aws_s3_bucket"" resource

Has one ""aws_elastic_beanstalk_application_version"" resource
    with ""name
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""bucket"" referencing the ""bucket"" from the ""aws_s3_object"" resource
    with ""key"" referencing the ""key"" from the ""aws_s3_object"" resource

Has one ""aws_elastic_beanstalk_environment"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with ""version_label"" referencing the ""aws_elastic_beanstalk_application_version"" resource
    with one ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource

Has one ""aws_route53_zone"" resource
    with ""name""

Has one ""aws_route53_record"" resources
    with ""name""
    with ""type""
    with ""ttl""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
    with ""records"" referencing one of the ""aws_elastic_beanstalk_environment"" resources
","I'll help you create a Terraform configuration for Route 53 resources directing traffic to an Elastic Beanstalk environment. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""  # Replace with your domain name
}

# IAM Role for EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# EC2 Instance Profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# S3 Bucket for application versions
resource ""aws_s3_bucket"" ""eb_bucket"" {
  bucket = ""my-unique-elastic-beanstalk-bucket-name-2023""  # Make sure this is unique
}

# S3 Bucket Object
resource ""aws_s3_object"" ""examplebucket_object"" {
  bucket = aws_s3_bucket.eb_bucket.id
  key    = ""beanstalk/application-version.zip""
  source = ""path/to/your/application.zip""  # Replace with your application source
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""myapp"" {
  name        = ""myapp""
  description = ""My Application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""version""
  application = aws_elastic_beanstalk_application.myapp.name
  description = ""application version created by terraform""
  bucket      = aws_s3_bucket.eb_bucket.id
  key         = aws_s3_object.examplebucket_object.key
}

# Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""eb-env"" {
  name                = ""eb-env""
  application         = aws_elastic_beanstalk_application.myapp.name
  solution_stack_name = ""64bit Amazon Linux 2 v3.5.4 running Python 3.8""  # Change as needed

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Route 53 Record
resource ""aws_route53_record"" ""www"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""  # Replace with your domain
  type    = ""A""

  alias {
    name                   = aws_elastic_beanstalk_environment.eb-env.cname
    zone_id               = aws_elastic_beanstalk_environment.eb-env.id
    evaluate_target_health = true
  }
}
```

This configuration:
1. Creates a Route 53 zone
2. Sets up the necessary IAM role and instance profile
3. Creates an S3 bucket and object for the application version
4. Configures the Elastic Beanstalk application and environment
5. Creates a Route 53 record to direct traffic to the Elastic Beanstalk environment

Remember to:
- Replace ""example.com"" with your actual domain name
- Update the region if needed",,,,,,"I'll help you create a Terraform configuration for Route 53 with Elastic Beanstalk integration. Here's a comprehensive solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""  # Replace with your domain name
}

# IAM Role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# Instance Profile for the IAM role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# S3 Bucket for application versions
resource ""aws_s3_bucket"" ""eb_bucket"" {
  bucket = ""my-unique-eb-bucket-name-12345""  # Replace with your unique bucket name
}

# S3 Bucket Object for application code
resource ""aws_s3_object"" ""examplebucket_object"" {
  bucket = aws_s3_bucket.eb_bucket.id
  key    = ""beanstalk/app.zip""
  source = ""path/to/your/app.zip""  # Replace with your application source
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""myapp"" {
  name        = ""myapp""
  description = ""My Application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""version""
  application = aws_elastic_beanstalk_application.myapp.name
  description = ""application version created by terraform""
  bucket      = aws_s3_bucket.eb_bucket.id
  key         = aws_s3_object.examplebucket_object.key
}

# Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""eb-env"" {
  name                = ""eb-env""
  application         = aws_elastic_beanstalk_application.myapp.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.4 running Node.js 18""  # Replace with your desired solution stack
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Route 53 Record for Elastic Beanstalk Environment
resource ""aws_route53_record"" ""eb_env_record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""app.example.com""  # Replace with your desired subdomain
  type    = ""CNAME""
  ttl     = ""300""
  records = [aws_elastic_beanstalk_environment.eb-env.cname]
}
```

This configuration:
1. Creates a Route 53 zone
2. Sets up an IAM role and instance profile for Elastic Beanstalk
3. Creates an S3 bucket and object for the application code
4. Configures an Elastic Beanstalk application, version, and environment
5. Creates a Route 53 record pointing to the Elastic Beanstalk environment

Remember to:
- Replace ""example.com"" with your actual domain name
- Replace the S",,,,,,"I'll help you create a Terraform configuration for Route 53 resources directing traffic to an Elastic Beanstalk environment. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""  # Replace with your domain name
}

# IAM Role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Instance Profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# S3 Bucket for application versions
resource ""aws_s3_bucket"" ""eb_bucket"" {
  bucket = ""my-unique-eb-bucket-name-123""  # Replace with your unique bucket name
}

# S3 Bucket Object
resource ""aws_s3_object"" ""examplebucket_object"" {
  bucket = aws_s3_bucket.eb_bucket.id
  key    = ""beanstalk/application.zip""
  source = ""path/to/your/application.zip""  # Replace with your application source
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""myapp"" {
  name        = ""myapp""
  description = ""My Application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""version""
  application = aws_elastic_beanstalk_application.myapp.name
  description = ""application version created by terraform""
  bucket      = aws_s3_bucket.eb_bucket.id
  key         = aws_s3_object.examplebucket_object.key
}

# Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""eb-env"" {
  name                = ""eb-env""
  application         = aws_elastic_beanstalk_application.myapp.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.4 running Node.js 18""  # Update with your desired solution stack
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Route 53 Record
resource ""aws_route53_record"" ""www"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""  # Replace with your domain name
  type    = ""A""

  alias {
    name                   = aws_elastic_beanstalk_environment.eb-env.cname
    zone_id               = aws_elastic_beanstalk_environment.eb-env.zone_id
    evaluate_target_health = true
  }
}
```

This configuration:
1. Creates a Route 53 zone
2. Sets up the necessary IAM role and instance profile for Elastic Beanstalk
3. Creates an S3 bucket and object for the application code
4. Creates an Elastic Beanstalk application and version
5. Sets up an Elastic Beanstalk environment
6. Creates a Route 53 record that points to the Elastic Beanstalk",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer with IP Target Group,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.values.subnet_mapping != null
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.ipv4_ipam_pool_id.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.ipv6_cider_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value != ""forward""
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    resources.values.target_type == ""ip""
    
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group""
        resource.expressions.port != null
        resource.expressions.protocol != null
        resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
        is_valid_lb
        is_valid_lb_listener
        is_valid_lb_target_group_attachment
        is_valid_lb_target_group
        is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.public_subnets[0]
  security_groups = [ aws_security_group.instance-sg.id ]
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
  target_type = ""ip""
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.private_ip
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id
    with target_type = ""ip""

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you create a Terraform configuration for an AWS Load Balancer with an IP Target Group. This configuration will include the required AWS provider and necessary resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Replace with your desired region
}

# Create VPC for the Load Balancer
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  tags = {
    Name = ""main-vpc""
  }
}

# Create public subnet for the Load Balancer
resource ""aws_subnet"" ""public"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""public-subnet""
  }
}

# Create Application Load Balancer
resource ""aws_lb"" ""example"" {
  name               = ""example-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb_sg.id]
  subnets            = [aws_subnet.public.id]

  tags = {
    Environment = ""production""
  }
}

# Create Target Group with IP targets
resource ""aws_lb_target_group"" ""ip_target"" {
  name        = ""ip-target-group""
  port        = 80
  protocol    = ""HTTP""
  target_type = ""ip""
  vpc_id      = aws_vpc.main.id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Create a listener for the Load Balancer
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.ip_target.arn
  }
}

# Security Group for the Load Balancer
resource ""aws_security_group"" ""lb_sg"" {
  name        = ""lb-security-group""
  description = ""Security group for load balancer""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""lb-security-group""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. An IP-based Target Group
3. A listener on port 80
4. The necessary VPC, subnet, and security group resources

Key features of this configuration:
- The target group is specifically set for IP-based targets (`target_type = ""ip""`)
- The Load Balancer is public-facing (`internal = false`)
- Basic health checks are configured
- Security group allows HTTP traffic on port 80

To use this configuration:
1. Replace the region with your desired AWS region
2. Adjust the CIDR blocks and other network settings as needed
3. Modify the health check settings according to your application requirements
4. Add your IP targets using the `aws_lb_target_group_attachment` resource (not included in this example)

Remember to properly configure your AWS credentials before applying this configuration.",,,,,,"I'll help you create a Terraform configuration for an AWS Load Balancer with an IP-based Target Group. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create VPC (Load Balancer needs a VPC)
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  tags = {
    Name = ""main-vpc""
  }
}

# Create public subnet
resource ""aws_subnet"" ""public"" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.${count.index + 1}.0/24""
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = ""public-subnet-${count.index + 1}""
  }
}

# Data source for AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Create Application Load Balancer
resource ""aws_lb"" ""app_lb"" {
  name               = ""ip-target-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb_sg.id]
  subnets           = aws_subnet.public[*].id

  tags = {
    Environment = ""production""
  }
}

# Create Target Group
resource ""aws_lb_target_group"" ""ip_target"" {
  name        = ""ip-target-group""
  port        = 80
  protocol    = ""HTTP""
  target_type = ""ip""
  vpc_id      = aws_vpc.main.id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.app_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.ip_target.arn
  }
}

# Security Group for Load Balancer
resource ""aws_security_group"" ""lb_sg"" {
  name        = ""allow_http""
  description = ""Allow HTTP inbound traffic""
  vpc_id      = aws_vpc.main.id

  ingress {
    description = ""HTTP from anywhere""
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""allow_http""
  }
}

# Example of adding IP targets (optional)
resource ""aws_lb_target_group_attachment"" ""ip_target"" {
  target_group_arn = aws_lb_target_group.ip_target.arn
  target_id        = ""10.0.1.10""  # Replace with your IP address
  port             = 80
}
```

This configuration creates:
1. A VPC and public subnets (required for the ALB)
2. An Application Load Balancer
3. An IP-based Target Group
4. A listener on port 80
5. Necessary security group
6. An example target group attachment for an IP address

Key points:
-",,,,,,"I'll help you create a Terraform configuration for an AWS Load Balancer with an IP Target Group. This configuration will include the required AWS provider and the necessary resources for creating an Application Load Balancer (ALB) with an IP-based target group.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create VPC (assuming you need one for the ALB)
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  tags = {
    Name = ""main-vpc""
  }
}

# Create public subnets
resource ""aws_subnet"" ""public_1"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""Public Subnet 1""
  }
}

resource ""aws_subnet"" ""public_2"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""Public Subnet 2""
  }
}

# Create Application Load Balancer
resource ""aws_lb"" ""alb"" {
  name               = ""ip-target-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [aws_subnet.public_1.id, aws_subnet.public_2.id]

  tags = {
    Environment = ""production""
  }
}

# Create Target Group
resource ""aws_lb_target_group"" ""ip_target"" {
  name        = ""ip-target-group""
  port        = 80
  protocol    = ""HTTP""
  target_type = ""ip""
  vpc_id      = aws_vpc.main.id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.alb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.ip_target.arn
  }
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB with IP targets""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""alb-security-group""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. An IP-based target group
3. A listener on port 80
4. The necessary networking components (VPC, subnets)
5. Security group for the ALB

Key points about this configuration:
- The target group is specifically set to `target_type = ""ip""` for IP-based targeting
- The ALB is set up in a public",,,,,
"aws_db_instance, aws_db_instance, aws_elastic_beanstalk_application, aws_elastic_beanstalk_environment, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment","Configure production and staging Elastic Beanstalk environments with a respective db_instance from RDS. Name the instance profile ""eb_ec2_profile"", the production database ""prod_db"", and the staging database ""staging_db""
","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_eb_app = false

default is_valid_eb_env = false

default is_valid_db_instance = false


is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0]
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0]
}

# Validate aws_eb_app
is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}

# Validate aws_eb_env
is_valid_eb_env {
        some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.expressions.name
    resource.expressions.application.references[0]
    resource.expressions.solution_stack_name
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_elastic_beanstalk_environment""
    resource2.expressions.name
    resource2.expressions.application.references[0]
    resource2.expressions.solution_stack_name
    some a, b, c, d, e, f, g, h
    resource.expressions.setting[a].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource.expressions.setting[b].value.references[0] == ""aws_db_instance.prod_db.username""
    resource.expressions.setting[c].value.references[0] == ""aws_db_instance.prod_db.password""
    resource.expressions.setting[d].value.references[0] == ""aws_db_instance.prod_db.address""
    resource2.expressions.setting[e].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource2.expressions.setting[f].value.references[0] == ""aws_db_instance.staging_db.username""
    resource2.expressions.setting[g].value.references[0] == ""aws_db_instance.staging_db.password""
    resource2.expressions.setting[h].value.references[0] == ""aws_db_instance.staging_db.address""

}

is_valid_db_instance {
                some i, j
        resource := input.configuration.root_module.resources[i]
    resource2 := input.configuration.root_module.resources[j]
    resource.type == ""aws_db_instance""
    resource2.type == ""aws_db_instance""
    has_required_main_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_main_db_arguments {
                some i, j
    resource := input.configuration.root_module.resources[i]
    resource2 := input.configuration.root_module.resources[j]
    resource.type == ""aws_db_instance""
    resource2.type == ""aws_db_instance""
    # Check for allocated_storage, engine, instance_class, username, password, and skip_final_snapshot
    requirement1(resource.expressions)
    requirement1(resource2.expressions)
    # Check for instance_class validity
    requirement2(resource.expressions)
    requirement2(resource2.expressions)
}



# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
    expressions.allocated_storage
    expressions.engine
    expressions.username
    expressions.password
    is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
    expressions.snapshot_identifier
}

# Check for instance_class validity
requirement2(expressions) {
    expressions.instance_class
    is_valid_instance_class(expressions.instance_class.constant_value)
}


# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}


# Combine all checks into a final rule
is_configuration_valid {
        is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
    is_valid_eb_app
    is_valid_eb_env
    is_valid_db_instance
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}


# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}


# RDS Database for Production
resource ""aws_db_instance"" ""prod_db"" {
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  username             = ""dbuser""
  password             = ""prodsecret""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
  identifier = ""production""
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""my_app"" {
  name        = ""my-app""
  description = ""My awesome application""
}

# Elastic Beanstalk Environment for Production
resource ""aws_elastic_beanstalk_environment"" ""prod_env"" {
  name                = ""my-app-prod""
  application         = aws_elastic_beanstalk_application.my_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.prod_db.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.prod_db.username
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.prod_db.password
  }

  setting {
      namespace = ""aws:autoscaling:launchconfiguration""
      name      = ""IamInstanceProfile""
      value     = aws_iam_instance_profile.eb_ec2_profile.name
    }
}

# RDS Database for Staging
resource ""aws_db_instance"" ""staging_db"" {
  allocated_storage    = 10
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  username             = ""dbuser""
  password             = ""stagingsecret""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
  identifier = ""staging""
}

# Elastic Beanstalk Environment for Staging
resource ""aws_elastic_beanstalk_environment"" ""staging_env"" {
  name                = ""my-app-staging""
  application         = aws_elastic_beanstalk_application.my_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.staging_db.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.staging_db.username
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.staging_db.password
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""

Has a ""aws_elastic_beanstalk_environment"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with one ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource
    with a ""setting"" block that has a value that references the ""address"" from the production ""db_instance"" resource
    with a ""setting"" block that has a value that references the ""username"" from the production ""db_instance"" resource 
    with a ""setting"" block that has a value that references the ""password"" from the production ""db_instance"" resource

Has a ""aws_elastic_beanstalk_environment"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with one ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource
    with a ""setting"" block that has a ""value""  that references the ""address"" from the staging ""db_instance"" resource
    with a ""setting"" block that has a ""value""  that references the ""username"" from the staging ""db_instance"" resource 
    with a ""setting"" block that has a ""value""  that references the ""password"" from the staging ""db_instance"" resource  

Has two ""aws_db_instance"" resources
    with the ""identifier"" for one set to ""production"" and ""identifier"" for the other set to ""staging""
    with ""instance_class""
    with ""allocated_storage""
    with ""engine""
    with ""username""
    with ""password""
    with ""backup_retention_period"" set to a non-zero number
    with ""skip_final_snapshot""

","I'll help you create a Terraform configuration for Elastic Beanstalk environments with RDS databases for both production and staging environments. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM role and instance profile for Elastic Beanstalk
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# RDS Production Database
resource ""aws_db_instance"" ""prod_db"" {
  identifier           = ""prod-db""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  username            = ""admin""
  password            = ""password123""  # Change this in production
  skip_final_snapshot = true

  tags = {
    Environment = ""production""
  }
}

# RDS Staging Database
resource ""aws_db_instance"" ""staging_db"" {
  identifier           = ""staging-db""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  username            = ""admin""
  password            = ""password123""  # Change this in production
  skip_final_snapshot = true

  tags = {
    Environment = ""staging""
  }
}

# Elastic Beanstalk Production Environment
resource ""aws_elastic_beanstalk_application"" ""app"" {
  name        = ""my-app""
  description = ""My Application""
}

resource ""aws_elastic_beanstalk_environment"" ""prod_environment"" {
  name                = ""production-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.4 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""RDS_HOSTNAME""
    value     = aws_db_instance.prod_db.endpoint
  }
}

# Elastic Beanstalk Staging Environment
resource ""aws_elastic_beanstalk_environment"" ""staging_environment"" {
  name                = ""staging-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.4 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""RDS_HOSTNAME""
    value     = aws_db_instance.staging_db.endpoint
  }
}
```

This configuration:
1. Creates an IAM role and instance profile named """,,,,,,"I'll help you create a Terraform configuration for Elastic Beanstalk environments with RDS databases for both production and staging environments.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM role and instance profile for Elastic Beanstalk
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Production RDS Instance
resource ""aws_db_instance"" ""prod_db"" {
  identifier           = ""prod-db""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  username            = ""admin""
  password            = ""password123""  # Use AWS Secrets Manager in production
  skip_final_snapshot = true
  
  tags = {
    Environment = ""Production""
  }
}

# Staging RDS Instance
resource ""aws_db_instance"" ""staging_db"" {
  identifier           = ""staging-db""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  username            = ""admin""
  password            = ""password123""  # Use AWS Secrets Manager in production
  skip_final_snapshot = true
  
  tags = {
    Environment = ""Staging""
  }
}

# Production Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""app"" {
  name        = ""my-app""
  description = ""My Application""
}

# Production Environment
resource ""aws_elastic_beanstalk_environment"" ""prod_env"" {
  name                = ""production-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.4 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.prod_db.endpoint
  }
}

# Staging Environment
resource ""aws_elastic_beanstalk_environment"" ""staging_env"" {
  name                = ""staging-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.4 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.staging_db.endpoint
  }
}
```

This configuration:
1. Creates an IAM role and instance profile named ""eb_ec2_profile""
2. Sets up two RDS instances: ""prod_db"" and ""staging_",,,,,,"I'll help you create a Terraform configuration for Elastic Beanstalk environments with RDS databases for both production and staging environments. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM role and instance profile for Elastic Beanstalk
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# RDS Databases
resource ""aws_db_instance"" ""prod_db"" {
  identifier           = ""prod-db""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  username           = ""admin""
  password           = ""password123""  # Replace with secure password
  skip_final_snapshot = true
  
  tags = {
    Environment = ""production""
  }
}

resource ""aws_db_instance"" ""staging_db"" {
  identifier           = ""staging-db""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  username           = ""admin""
  password           = ""password123""  # Replace with secure password
  skip_final_snapshot = true
  
  tags = {
    Environment = ""staging""
  }
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""app"" {
  name        = ""my-app""
  description = ""My Application""
}

# Production Environment
resource ""aws_elastic_beanstalk_environment"" ""production"" {
  name                = ""production-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.4 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.prod_db.endpoint
  }
}

# Staging Environment
resource ""aws_elastic_beanstalk_environment"" ""staging"" {
  name                = ""staging-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.4 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.staging_db.endpoint
  }
}
```

This configuration:
1. Creates an IAM role and instance profile named ""eb_ec2_profile""
2. Sets up two RDS instances",,,,,
"aws_vpc, aws_vpc_dhcp_options, aws_vpc_dhcp_options_association","sets up a basic AWS network infrastructure consisting of a Virtual Private Cloud (VPC) with a custom DHCP options set. The VPC is configured with a CIDR block of 192.168.0.0/16. The DHCP options include a domain name (windomain.local), a combination of custom and Google's DNS servers (192.168.56.102 and 8.8.8.8), and a NetBIOS name server (192.168.56.102). These DHCP options are then associated with the created VPC, customizing its DHCP settings.






","package terraform.validation

# Set default validation states
default is_valid_vpc = false

default is_valid_dhcp_options = false

default is_valid_dhcp_options_association = false

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value == ""192.168.0.0/16""
}

# Validate aws_vpc_dhcp_options resource
is_valid_dhcp_options {
        some i
        resource := input.planned_values.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options""
        resource.values.domain_name == ""windomain.local""
    resource.values.domain_name_servers == [""192.168.56.102"", ""8.8.8.8""]
        resource.values.netbios_name_servers == [""192.168.56.102""]
}

# Validate aws_vpc_dhcp_options_association resource
is_valid_dhcp_options_association {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options_association""
        resource.expressions.dhcp_options_id.references[0] == ""aws_vpc_dhcp_options.default.id""
        resource.expressions.vpc_id.references[0] == ""aws_vpc.default.id""
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""default"" {
  cidr_block = ""192.168.0.0/16""
}

resource ""aws_vpc_dhcp_options"" ""default"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = concat([""192.168.56.102""], [""8.8.8.8""])
  netbios_name_servers = [""192.168.56.102""]
}

resource ""aws_vpc_dhcp_options_association"" ""default"" {
  vpc_id          = aws_vpc.default.id
  dhcp_options_id = aws_vpc_dhcp_options.default.id
}","Has one ""aws_vpc"" resource:
Configured with a ""cidr_block"" of ""192.168.0.0/16""
Includes associated custom DHCP options

Has one ""aws_vpc_dhcp_options"" resource:
Specifies a ""domain_name"" of ""windomain.local""
Includes DNS servers with a combination of a custom DNS server (""192.168.56.102"") and Google's DNS server (""8.8.8.8"")
Configures a ""NetBIOS"" name server at ""192.168.56.102""

Has one ""aws_vpc_dhcp_options_association"" resource:
Associates the ""aws_vpc_dhcp_options"" with the created ""aws_vpc""
Customizes DHCP settings specifically for the created VPC environment","I'll help you create a Terraform configuration that sets up a VPC with custom DHCP options. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""dhcp_association"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with CIDR block 192.168.0.0/16 and enables DNS support
3. Creates a custom DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The configuration includes proper tagging for resources and follows AWS best practices for VPC setup with custom DHCP options.",,,,,,"I'll help you create a Terraform configuration for setting up a VPC with custom DHCP options based on your requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_support   = true
  enable_dns_hostnames = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options Set with VPC
resource ""aws_vpc_dhcp_options_association"" ""dhcp_association"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a custom DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The configuration enables DNS support and DNS hostnames in the VPC for proper name resolution. All resources are properly tagged for better resource management.",,,,,,"I'll help you create a Terraform configuration for setting up a VPC with custom DHCP options. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""dhcp_association"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a custom DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The VPC is configured with DNS hostnames and DNS support enabled for better network functionality. All resources are properly tagged for better resource management.",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer with Lambda Target Group,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.values.subnet_mapping != null
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.ipv4_ipam_pool_id.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.ipv6_cider_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value != ""forward""
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    resources.values.target_type == ""lambda""
    
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group""
        resource.expressions.port != null
        resource.expressions.protocol != null
        resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
        is_valid_lb
        is_valid_lb_listener
        is_valid_lb_target_group_attachment
        is_valid_lb_target_group
        is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_iam_role"" ""lambda_role"" {
  name = ""example-lambda-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy_attachment"" ""lambda_policy_attachment"" {
  name       = ""example-lambda-policy-attachment""
  roles      = [aws_iam_role.lambda_role.name]
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}

data ""archive_file"" ""lambda-func"" {
  type        = ""zip""
  source_file = ""./supplement/index.js""
  output_path = ""./supplement/lambda_func.zip""
}

resource ""aws_lambda_function"" ""example"" {
  function_name = ""example-lambda""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs18.x""

  filename         = data.archive_file.lambda-func.output_path
  source_code_hash = data.archive_file.lambda-func.output_base64sha256
}

resource ""aws_lambda_permission"" ""elb_invocation"" {
  statement_id  = ""AllowExecutionFromALB""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.example.function_name
  principal     = ""elasticloadbalancing.amazonaws.com""
  source_arn    = aws_lb_target_group.target_group.arn
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  target_type = ""lambda""
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_lambda_function.example.arn
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id
    with target_type = ""lambda""

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Lambda target group. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create ALB
resource ""aws_lb"" ""example"" {
  name               = ""example-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb.id]
  subnets            = [aws_subnet.example1.id, aws_subnet.example2.id]

  tags = {
    Environment = ""production""
  }
}

# Create ALB target group
resource ""aws_lb_target_group"" ""lambda"" {
  name        = ""example-lambda-tg""
  target_type = ""lambda""
}

# Create ALB listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.lambda.arn
  }
}

# Create Lambda permission for ALB
resource ""aws_lambda_permission"" ""with_lb"" {
  statement_id  = ""AllowExecutionFromlb""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.example.function_name
  principal     = ""elasticloadbalancing.amazonaws.com""
  source_arn    = aws_lb_target_group.lambda.arn
}

# Target group attachment
resource ""aws_lb_target_group_attachment"" ""lambda"" {
  target_group_arn = aws_lb_target_group.lambda.arn
  target_id        = aws_lambda_function.example.arn
  depends_on       = [aws_lambda_permission.with_lb]
}

# Example security group for ALB
resource ""aws_security_group"" ""alb"" {
  name        = ""allow_http""
  description = ""Allow HTTP inbound traffic""
  vpc_id      = aws_vpc.example.id

  ingress {
    description = ""HTTP from anywhere""
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""allow_http""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. A Lambda target group
3. An ALB listener on port 80
4. Required Lambda permissions for the ALB
5. Target group attachment to connect the Lambda function
6. A security group for the ALB

Note: You'll need to have the following resources already defined or create them:
- VPC and subnets (`aws_vpc.example`, `aws_subnet.example1`, `aws_subnet.example2`)
- Lambda function (`aws_lambda_function.example`)

The ALB will forward HTTP requests to the Lambda function. The security group allows inbound HTTP traffic and all outbound traffic. Adjust the security group rules according to your security requirements.",,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Lambda target group. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create the Lambda function
resource ""aws_lambda_function"" ""example"" {
  filename      = ""lambda_function.zip""  # Your Lambda function code
  function_name = ""example-function""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""  # Change to your desired runtime
}

# Create ALB
resource ""aws_lb"" ""example"" {
  name               = ""example-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb_sg.id]
  subnets            = [""subnet-xxxxxxxx"", ""subnet-yyyyyyyy""]  # Replace with your subnet IDs

  tags = {
    Environment = ""production""
  }
}

# Create target group
resource ""aws_lb_target_group"" ""lambda"" {
  name        = ""example-lambda-target-group""
  target_type = ""lambda""
}

# Create listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.lambda.arn
  }
}

# Attach Lambda to target group
resource ""aws_lb_target_group_attachment"" ""lambda"" {
  target_group_arn = aws_lb_target_group.lambda.arn
  target_id        = aws_lambda_function.example.arn
}

# Lambda permission to be invoked by ALB
resource ""aws_lambda_permission"" ""with_lb"" {
  statement_id  = ""AllowExecutionFromALB""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.example.function_name
  principal     = ""elasticloadbalancing.amazonaws.com""
  source_arn    = aws_lb_target_group.lambda.arn
}

# Security group for the ALB
resource ""aws_security_group"" ""lb_sg"" {
  name        = ""allow_http""
  description = ""Allow HTTP inbound traffic""
  vpc_id      = ""vpc-xxxxxxxx""  # Replace with your VPC ID

  ingress {
    description = ""HTTP from anywhere""
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""allow_http""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_execution_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Basic execution policy for Lambda
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name",,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Lambda target group. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create the ALB
resource ""aws_lb"" ""example"" {
  name               = ""example-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb_sg.id]
  subnets            = [aws_subnet.example1.id, aws_subnet.example2.id]

  tags = {
    Environment = ""production""
  }
}

# Create target group for Lambda
resource ""aws_lb_target_group"" ""lambda"" {
  name        = ""example-lambda-tg""
  target_type = ""lambda""

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval           = 30
    timeout            = 5
    matcher            = ""200""
    path               = ""/health""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    unhealthy_threshold = 3
  }
}

# Create listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.lambda.arn
  }
}

# Lambda permission for ALB
resource ""aws_lambda_permission"" ""with_lb"" {
  statement_id  = ""AllowExecutionFromlb""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.example.function_name
  principal     = ""elasticloadbalancing.amazonaws.com""
  source_arn    = aws_lb_target_group.lambda.arn
}

# Attach Lambda to target group
resource ""aws_lb_target_group_attachment"" ""lambda"" {
  target_group_arn = aws_lb_target_group.lambda.arn
  target_id        = aws_lambda_function.example.arn
  depends_on       = [aws_lambda_permission.with_lb]
}

# Security group for the ALB
resource ""aws_security_group"" ""lb_sg"" {
  name        = ""allow_http""
  description = ""Allow HTTP inbound traffic""
  vpc_id      = aws_vpc.example.id

  ingress {
    description = ""HTTP from anywhere""
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""allow_http""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. A Lambda target group
3. A listener on port 80
4. The necessary Lambda permissions for the ALB
5. The target group attachment
6. A security group for the ALB

Note: This configuration assumes you have already created:
- A VPC with subnets (referenced as aws_subnet.example1 and aws_subnet.example2)
- A Lambda function (referenced as aws_lambda_function.example)
- A VPC (referenced as aws_vpc.example)

You'll need to adjust the subnet IDs, VPC ID, and other specific values according to your environment.",,,,,
"aws_cloudwatch_log_group, aws_cloudwatch_log_resource_policy, aws_route53_query_log, aws_route53_zone, aws_iam_policy_document","Configure a query log that can create a log stream and put log events using Route 53 resources. Name the zone ""primary"", the cloudwatch log group ""aws_route53_example_com"", and the cloudwatch log resource policy ""route53-query-logging-policy""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_cloudwatch_log_group = false

default is_valid_cloudwatch_log_resource_policy = false

default is_valid_route53_query_log = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name
}

is_valid_cloudwatch_log_group {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_cloudwatch_log_group""
}

is_valid_cloudwatch_log_resource_policy {
    some i
    resource := input.resource_changes[i]
    resource.type == ""aws_cloudwatch_log_resource_policy""
    contains(resource.change.after.policy_document, ""logs:PutLogEvents"")
    contains(resource.change.after.policy_document, ""logs:CreateLogStream"")
    resource.change.after.policy_name
}

is_valid_route53_query_log {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_query_log""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.primary.zone_id""
    resource.expressions.cloudwatch_log_group_arn.references[0] == ""aws_cloudwatch_log_group.aws_route53_example_com.arn""
    resource.depends_on[0] == ""aws_cloudwatch_log_resource_policy.route53-query-logging-policy""
}

# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_cloudwatch_log_group
    is_valid_cloudwatch_log_resource_policy
    is_valid_route53_query_log
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}


resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""
}

resource ""aws_cloudwatch_log_group"" ""aws_route53_example_com"" {
  name              = ""/aws/route53/${aws_route53_zone.primary.name}""
  retention_in_days = 30
}

# Example CloudWatch log resource policy to allow Route53 to write logs
# to any log group under /aws/route53/*

data ""aws_iam_policy_document"" ""route53-query-logging-policy"" {
  statement {
    actions = [
      ""logs:CreateLogStream"",
      ""logs:PutLogEvents"",
    ]

    resources = [""arn:aws:logs:*:*:log-group:/aws/route53/*""]

    principals {
      identifiers = [""route53.amazonaws.com""]
      type        = ""Service""
    }
  }
}

resource ""aws_cloudwatch_log_resource_policy"" ""route53-query-logging-policy"" {
  policy_document = data.aws_iam_policy_document.route53-query-logging-policy.json
  policy_name     = ""route53-query-logging-policy""
}

resource ""aws_route53_query_log"" ""example_com"" {
  depends_on = [aws_cloudwatch_log_resource_policy.route53-query-logging-policy]

  cloudwatch_log_group_arn = aws_cloudwatch_log_group.aws_route53_example_com.arn
  zone_id                  = aws_route53_zone.primary.zone_id
}","Has one ""aws_route53_zone"" resource
    with ""name""

Has one ""aws_cloudwatch_log_group"" resource

Has one ""aws_cloudwatch_log_resource_policy"" resource
    with ""policy_document"" enabling ""logs:CreateLogStream"" and ""logs:PutLogEvents""
    with ""policy_name""

Has one ""aws_route53_query_log"" resource
    with ""cloudwatch_log_group_arn"" referencing the ""aws_cloudwatch_log_group"" resource
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
    and depends on the ""aws_cloudwatch_log_resource_policy"" resource","I'll help you create a Terraform configuration for Route 53 query logging with CloudWatch Log Group integration. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust region as needed
}

# Create Route 53 zone
resource ""aws_route53_zone"" ""primary"" {
  name = ""example.com""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""aws_route53_example_com"" {
  name              = ""/aws/route53/example.com""
  retention_in_days = 30
}

# Create CloudWatch Log Resource Policy
resource ""aws_cloudwatch_log_resource_policy"" ""route53-query-logging-policy"" {
  policy_name = ""route53-query-logging-policy""

  policy_document = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""route53.amazonaws.com""
        }
        Action = [
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = ""${aws_cloudwatch_log_group.aws_route53_example_com.arn}:*""
      }
    ]
  })
}

# Create Route 53 Query Log
resource ""aws_route53_query_log"" ""query_log"" {
  depends_on = [aws_cloudwatch_log_resource_policy.route53-query-logging-policy]

  cloudwatch_log_group_arn = aws_cloudwatch_log_group.aws_route53_example_com.arn
  zone_id                  = aws_route53_zone.primary.zone_id
}
```

This configuration:
1. Creates a Route 53 hosted zone named ""primary""
2. Creates a CloudWatch Log Group named ""aws_route53_example_com""
3. Sets up a CloudWatch Log Resource Policy that allows Route 53 to create log streams and put log events
4. Creates a Route 53 Query Log that links the hosted zone with the CloudWatch Log Group

The configuration includes all necessary permissions and dependencies to ensure proper creation and functioning of the query logging system. The `depends_on` attribute ensures that the resource policy is created before attempting to create the query log.",,,,,,"I'll help you create a Terraform configuration for Route 53 query logging with CloudWatch Log groups. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create Route 53 zone
resource ""aws_route53_zone"" ""primary"" {
  name = ""example.com""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""aws_route53_example_com"" {
  name              = ""/aws/route53/example.com""
  retention_in_days = 30
}

# Create CloudWatch Log Resource Policy
resource ""aws_cloudwatch_log_resource_policy"" ""route53-query-logging-policy"" {
  policy_name = ""route53-query-logging-policy""

  policy_document = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""route53.amazonaws.com""
        }
        Action = [
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = ""${aws_cloudwatch_log_group.aws_route53_example_com.arn}:*""
      }
    ]
  })
}

# Enable Route 53 Query Logging
resource ""aws_route53_query_log"" ""query_log"" {
  depends_on = [aws_cloudwatch_log_resource_policy.route53-query-logging-policy]

  cloudwatch_log_group_arn = aws_cloudwatch_log_group.aws_route53_example_com.arn
  zone_id                  = aws_route53_zone.primary.zone_id
}
```

This configuration:
1. Creates a Route 53 hosted zone named ""primary""
2. Creates a CloudWatch Log Group named ""aws_route53_example_com""
3. Creates a CloudWatch Log Resource Policy named ""route53-query-logging-policy""
4. Enables query logging for the Route 53 zone

The policy allows Route 53 to create log streams and put log events in the specified CloudWatch Log Group. The `depends_on` ensures that the policy is created before enabling query logging.",,,,,,"I'll help you create a Terraform configuration for Route 53 query logging with CloudWatch Log groups. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust region as needed
}

# Create Route 53 zone
resource ""aws_route53_zone"" ""primary"" {
  name = ""example.com""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""aws_route53_example_com"" {
  name              = ""/aws/route53/example.com""
  retention_in_days = 30
}

# Create CloudWatch Log Resource Policy
resource ""aws_cloudwatch_log_resource_policy"" ""route53-query-logging-policy"" {
  policy_name = ""route53-query-logging-policy""

  policy_document = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""route53.amazonaws.com""
        }
        Action = [
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = ""${aws_cloudwatch_log_group.aws_route53_example_com.arn}:*""
      }
    ]
  })
}

# Create Route 53 Query Log
resource ""aws_route53_query_log"" ""query_log"" {
  depends_on = [aws_cloudwatch_log_resource_policy.route53-query-logging-policy]

  cloudwatch_log_group_arn = aws_cloudwatch_log_group.aws_route53_example_com.arn
  zone_id                  = aws_route53_zone.primary.zone_id
}
```

This configuration:
1. Creates a Route 53 zone named ""primary""
2. Creates a CloudWatch Log Group named ""aws_route53_example_com""
3. Creates a CloudWatch Log Resource Policy named ""route53-query-logging-policy""
4. Sets up Route 53 query logging with the necessary permissions
5. Links everything together using the appropriate resource references

The configuration includes all necessary permissions for Route 53 to create log streams and put log events in the CloudWatch Log Group. The `depends_on` ensures that the policy is created before attempting to create the query log.",,,,,
"aws_db_instance, aws_db_subnet_group, aws_elastic_beanstalk_application, aws_elastic_beanstalk_environment, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment, aws_internet_gateway, aws_route_table, aws_route_table_association, aws_route_table_association, aws_security_group, aws_subnet, aws_subnet, aws_vpc","Configure an AWS VPC with internet gateway, two public subnets, a security group, and a route table associated with the public subnets. Then connect these resources to two Elastic Beanstalk environments. These environments are also connected to a shared RDS database that is linked to the subnets. Name the database ""shared_rds"", the security group ""eb_env_sg"", the vpc ""eb_vpc"", and the subnets ""eb_subnet_public_1"" and ""eb_subnet_public_2""","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_eb_app = false

default is_valid_eb_env = false

default is_valid_db_instance = false

default is_valid_vpc = false

default is_valid_internet_gateway = false

default is_valid_subnet = false

default is_valid_subnet_group = false

default is_valid_security_group = false

default is_valid_route_table = false

default is_valid_route_table_association = false


is_valid_vpc {
                some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_vpc""
    resource.expressions.cidr_block
}

is_valid_internet_gateway {
                        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_internet_gateway""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.eb_vpc.id""
}

is_valid_subnet {
                some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_subnet""
    resource.expressions.cidr_block
    resource.expressions.availability_zone
    
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_subnet""
    resource2.expressions.cidr_block
    resource2.expressions.availability_zone
    resource2.expressions.vpc_id.references[0] == resource.expressions.vpc_id.references[0]

}

is_valid_subnet_group {
                some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_db_subnet_group""
    resource.expressions.subnet_ids.references[0]

}

is_valid_security_group {
                some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_security_group""
    resource.expressions.vpc_id.references[0]
    resource.expressions.egress
    resource.expressions.ingress
}

is_valid_route_table {
                some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route_table""
    resource.expressions.route.references[0]
    resource.expressions.vpc_id.references
}

is_valid_route_table_association {
                some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route_table_association""
    resource.expressions.subnet_id.references[0]
    resource.expressions.route_table_id.references[0]
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_route_table_association""
    resource2.expressions.subnet_id.references[0]
    resource2.expressions.route_table_id.references[0]
    resource2.expressions.route_table_id.references[0] == resource.expressions.route_table_id.references[0]
    resource2.expressions.subnet_id.references[0] != resource.expressions.subnet_id.references[0]

}

is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0]
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0]
}

# Validate aws_eb_app
is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}

# Validate aws_eb_env
is_valid_eb_env {
        some a, b
    resource := input.configuration.root_module.resources[a]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.expressions.name
    resource.expressions.application.references[0]
    resource.expressions.solution_stack_name
    
    resource2 := input.configuration.root_module.resources[b]
    resource2.type == ""aws_elastic_beanstalk_environment""
    resource2.expressions.name
    resource2.expressions.application.references[0]
    resource2.expressions.solution_stack_name
    
    some c, d, e, f, g, h, i, j, k, l, m, n, o, p
    resource.expressions.setting[c].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource.expressions.setting[d].value.references[0] == ""aws_db_instance.shared_rds.username""
    resource.expressions.setting[e].value.references[0] == ""aws_db_instance.shared_rds.password""
    resource.expressions.setting[f].value.references[0] == ""aws_db_instance.shared_rds.address""
    resource.expressions.setting[g].value.references[0] == ""aws_security_group.eb_env_sg.id""
    resource.expressions.setting[h].value.references[0] == ""aws_vpc.eb_vpc.id""
    resource.expressions.setting[i].value.references[0] == ""aws_subnet.eb_subnet_public_1.id""
    resource.expressions.setting[i].value.references[2] == ""aws_subnet.eb_subnet_public_2.id""
    
    resource2.expressions.setting[j].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource2.expressions.setting[k].value.references[0] == ""aws_db_instance.shared_rds.username""
    resource2.expressions.setting[l].value.references[0] == ""aws_db_instance.shared_rds.password""
    resource2.expressions.setting[m].value.references[0] == ""aws_db_instance.shared_rds.address""
    resource2.expressions.setting[n].value.references[0] == ""aws_security_group.eb_env_sg.id""
    resource2.expressions.setting[o].value.references[0] == ""aws_vpc.eb_vpc.id""
    resource2.expressions.setting[p].value.references[0] == ""aws_subnet.eb_subnet_public_1.id""
    resource2.expressions.setting[p].value.references[2] == ""aws_subnet.eb_subnet_public_2.id""
}

is_valid_db_instance {
                some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_db_instance""
    has_required_main_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_main_db_arguments {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_db_instance""
    resource.expressions.db_subnet_group_name.references[0]
    resource.expressions.vpc_security_group_ids.references[0]
    # Check for allocated_storage, engine, instance_class, username, password, and skip_final_snapshot
    requirement1(resource.expressions)
    # Check for instance_class validity
    requirement2(resource.expressions)
}



# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
    expressions.allocated_storage
    expressions.engine
    expressions.username
    expressions.password
    is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
    expressions.snapshot_identifier
}

# Check for instance_class validity
requirement2(expressions) {
    expressions.instance_class
    is_valid_instance_class(expressions.instance_class.constant_value)
}


# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}


# Combine all checks into a final rule
is_configuration_valid {
        is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
    is_valid_eb_app
    is_valid_eb_env
    is_valid_db_instance
    is_valid_vpc
    is_valid_internet_gateway
    is_valid_subnet
    is_valid_subnet_group
    is_valid_security_group
    is_valid_route_table
    is_valid_route_table_association
    
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# VPC
resource ""aws_vpc"" ""eb_vpc"" {
  cidr_block = ""10.0.0.0/16""
  enable_dns_support   = true
  enable_dns_hostnames = true
}

# Internet Gateway
resource ""aws_internet_gateway"" ""eb_igw"" {
  vpc_id = aws_vpc.eb_vpc.id
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Subnets
resource ""aws_subnet"" ""eb_subnet_public_1"" {
  vpc_id     = aws_vpc.eb_vpc.id
  cidr_block = ""10.0.1.0/24""

  map_public_ip_on_launch = true
  availability_zone       = data.aws_availability_zones.available.names[0]
}

resource ""aws_subnet"" ""eb_subnet_public_2"" {
  vpc_id     = aws_vpc.eb_vpc.id
  cidr_block = ""10.0.2.0/24""

  map_public_ip_on_launch = true
  availability_zone       = data.aws_availability_zones.available.names[1]
}

# Security groups for Elastic Beanstalk environments
resource ""aws_security_group"" ""eb_env_sg"" {
  name        = ""eb-env-sg""
  description = ""Security group for Elastic Beanstalk environments""
  vpc_id      = aws_vpc.eb_vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.eb_env_sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  from_port         = 80
  ip_protocol       = ""tcp""
  to_port           = 80
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress2"" {
  security_group_id = aws_security_group.eb_env_sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  from_port         = 443
  ip_protocol       = ""tcp""
  to_port           = 443
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.eb_env_sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  ip_protocol       = ""-1""
}

# DB subnet group for RDS instance
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name       = ""rds-subnet-group""
  subnet_ids = [aws_subnet.eb_subnet_public_1.id, aws_subnet.eb_subnet_public_2.id]
}

resource ""aws_route_table"" ""eb_route_table"" {
  vpc_id = aws_vpc.eb_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.eb_igw.id
  }
}

resource ""aws_route_table_association"" ""eb_route_table_association_1"" {
  subnet_id      = aws_subnet.eb_subnet_public_1.id
  route_table_id = aws_route_table.eb_route_table.id
}

resource ""aws_route_table_association"" ""eb_route_table_association_2"" {
  subnet_id      = aws_subnet.eb_subnet_public_2.id
  route_table_id = aws_route_table.eb_route_table.id
}

# RDS instance
resource ""aws_db_instance"" ""shared_rds"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""dbadmin""
  password             = ""securepassword"" 
  backup_retention_period = 0
  skip_final_snapshot  = true
  db_subnet_group_name  = aws_db_subnet_group.rds_subnet_group.name

  vpc_security_group_ids = [aws_security_group.eb_env_sg.id]
}

# Elastic Beanstalk application
resource ""aws_elastic_beanstalk_application"" ""microservice_app"" {
  name        = ""MicroserviceApplication""
  description = ""An application for microservices""
}

resource ""aws_elastic_beanstalk_environment"" ""microservice_env1"" {
  name                = ""microservice-env1""
  application         = aws_elastic_beanstalk_application.microservice_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  # Elastic Beanstalk environment variables for RDS connection
  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOSTNAME""
    value     = aws_db_instance.shared_rds.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.shared_rds.username
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.shared_rds.password
  }

  setting {
    namespace = ""aws:ec2:vpc""
    name      = ""VPCId""
    value     = aws_vpc.eb_vpc.id
  }

  setting {
    namespace = ""aws:ec2:vpc""
    name      = ""Subnets""
    value     = ""${aws_subnet.eb_subnet_public_1.id},${aws_subnet.eb_subnet_public_2.id}""
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""SecurityGroups""
    value     = aws_security_group.eb_env_sg.id
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

resource ""aws_elastic_beanstalk_environment"" ""microservice_env2"" {
  name                = ""microservice-env2""
  application         = aws_elastic_beanstalk_application.microservice_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  # Elastic Beanstalk environment variables for RDS connection
  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOSTNAME""
    value     = aws_db_instance.shared_rds.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.shared_rds.username
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.shared_rds.password
  }

  setting {
    namespace = ""aws:ec2:vpc""
    name      = ""VPCId""
    value     = aws_vpc.eb_vpc.id
  }

  setting {
    namespace = ""aws:ec2:vpc""
    name      = ""Subnets""
    value     = ""${aws_subnet.eb_subnet_public_1.id},${aws_subnet.eb_subnet_public_2.id}""
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""SecurityGroups""
    value     = aws_security_group.eb_env_sg.id
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has one ""aws_vpc"" resource
    with ""cidr_block"" set to a valid value

Has one ""aws_internet_gateway"" resource
    with ""vpc_id"" referencing the ""aws_vpc"" resource

Has two ""aws_subnet"" resources
    with ""vpc_id"" referencing the ""aws_vpc"" resource
    with ""cidr_block"" set to a valid value
    with ""availability_zone"" set to a zone within the region

Has one ""aws_security_group"" resource
     with ""vpc_id"" referencing the ""aws_vpc"" resource
     with ""ingress"" blocks
     with ""egress"" block

Has one ""aws_db_subnet_group"" resource
     with ""subnet_ids"" referencing the ""aws_subnet"" resources

Has one ""aws_route_table"" resources
     with ""vpc_id"" referencing the ""aws_vpc"" resource
     with the ""route"" block containing ""gateway_id"" referencing the ""aws_internet_gateway"" resource

Has two ""aws_route_table_association"" resources
     with ""subnet_id"" referencing one of the ""aws_subnet"" resources
     with ""route_table_id"" referencing the ""aws_route_table"" resource

Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""


Has two ""aws_elastic_beanstalk_environment"" resources
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with a ""setting"" block that has a ""value"" that references the ""aws_iam_instance_profile"" resource
    with a ""setting"" block that has a ""value"" that references the ""id"" from all the ""aws_subnet"" resources
    with a ""setting"" block that has a ""value"" that references the ""id"" from the ""aws_vpc"" resource
    with a ""setting"" block that has a ""value"" that references the ""id"" from the ""aws_security_group"" resource
    with a ""setting"" block that has a ""value"" that references the ""address"" from the ""db_instance"" resource
    with a ""setting"" block that has a ""value"" that references the ""username"" from the ""db_instance"" resource 
    with a ""setting"" block that has a ""value"" that references the ""password"" from the ""db_instance"" resource 

Has a ""aws_db_instance"" resource
    with ""instance_class""
    with ""allocated_storage""
    with ""engine""
    with ""username""
    with ""password""
    with ""backup_retention_period"" set to a non-zero number
    with ""skip_final_snapshot""
    with ""db_subnet_group_name"" referencing the ""aws_db_subnet_group"" resource
    with ""vpc_security_group_ids"" referencing the ""aws_security_group"" resource","I'll help you create a Terraform configuration for your AWS infrastructure. Here's a solution that includes a VPC, internet gateway, public subnets, security groups, route table, Elastic Beanstalk environments, and a shared RDS database:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC
resource ""aws_vpc"" ""eb_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""eb_vpc""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""eb_igw"" {
  vpc_id = aws_vpc.eb_vpc.id

  tags = {
    Name = ""eb_igw""
  }
}

# Public Subnets
resource ""aws_subnet"" ""eb_subnet_public_1"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.1.0/24""
  availability_zone       = ""us-east-1a""
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_1""
  }
}

resource ""aws_subnet"" ""eb_subnet_public_2"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.2.0/24""
  availability_zone       = ""us-east-1b""
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_2""
  }
}

# Route Table
resource ""aws_route_table"" ""eb_public_rt"" {
  vpc_id = aws_vpc.eb_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.eb_igw.id
  }

  tags = {
    Name = ""eb_public_rt""
  }
}

# Route Table Associations
resource ""aws_route_table_association"" ""public_1"" {
  subnet_id      = aws_subnet.eb_subnet_public_1.id
  route_table_id = aws_route_table.eb_public_rt.id
}

resource ""aws_route_table_association"" ""public_2"" {
  subnet_id      = aws_subnet.eb_subnet_public_2.id
  route_table_id = aws_route_table.eb_public_rt.id
}

# Security Group
resource ""aws_security_group"" ""eb_env_sg"" {
  name        = ""eb_env_sg""
  description = ""Security group for Elastic Beanstalk environments""
  vpc_id      = aws_vpc.eb_vpc.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# RDS Subnet Group
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name       = ""rds-subnet-group""
  subnet_ids = [aws_subnet.eb_subnet_public_1.id, aws_subnet.eb_subnet_public_2.id]

  tags = {
    Name = ""RDS subnet group""
  }
}

# RDS Instance
resource ""aws_",,,,,,"I'll help you create a Terraform configuration for your AWS VPC setup with Elastic Beanstalk environments and RDS. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC
resource ""aws_vpc"" ""eb_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""eb_vpc""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""igw"" {
  vpc_id = aws_vpc.eb_vpc.id

  tags = {
    Name = ""eb_igw""
  }
}

# Public Subnets
resource ""aws_subnet"" ""eb_subnet_public_1"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.1.0/24""
  availability_zone       = ""us-east-1a""
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_1""
  }
}

resource ""aws_subnet"" ""eb_subnet_public_2"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.2.0/24""
  availability_zone       = ""us-east-1b""
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_2""
  }
}

# Route Table
resource ""aws_route_table"" ""public_rt"" {
  vpc_id = aws_vpc.eb_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.igw.id
  }

  tags = {
    Name = ""eb_public_rt""
  }
}

# Route Table Associations
resource ""aws_route_table_association"" ""public_1"" {
  subnet_id      = aws_subnet.eb_subnet_public_1.id
  route_table_id = aws_route_table.public_rt.id
}

resource ""aws_route_table_association"" ""public_2"" {
  subnet_id      = aws_subnet.eb_subnet_public_2.id
  route_table_id = aws_route_table.public_rt.id
}

# Security Group
resource ""aws_security_group"" ""eb_env_sg"" {
  name        = ""eb_env_sg""
  description = ""Security group for Elastic Beanstalk environments""
  vpc_id      = aws_vpc.eb_vpc.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# RDS Subnet Group
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name       = ""eb-rds-subnet-group""
  subnet_ids = [aws_subnet.eb_subnet_public_1.id, aws_subnet.eb_subnet_public_2.id]

  tags = {
    Name = ""EB RDS subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""shared_rds"" {
  identifier           = ""shared-rds""
  allocated_storage",,,,,,"I'll help you create a Terraform configuration for AWS VPC with Elastic Beanstalk environments and RDS. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""eb_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""eb_vpc""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.eb_vpc.id

  tags = {
    Name = ""eb_igw""
  }
}

# Public Subnets
resource ""aws_subnet"" ""eb_subnet_public_1"" {
  vpc_id            = aws_vpc.eb_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""eb_subnet_public_1""
  }
}

resource ""aws_subnet"" ""eb_subnet_public_2"" {
  vpc_id            = aws_vpc.eb_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""eb_subnet_public_2""
  }
}

# Route Table
resource ""aws_route_table"" ""public"" {
  vpc_id = aws_vpc.eb_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""eb_public_rt""
  }
}

# Route Table Associations
resource ""aws_route_table_association"" ""public1"" {
  subnet_id      = aws_subnet.eb_subnet_public_1.id
  route_table_id = aws_route_table.public.id
}

resource ""aws_route_table_association"" ""public2"" {
  subnet_id      = aws_subnet.eb_subnet_public_2.id
  route_table_id = aws_route_table.public.id
}

# Security Group
resource ""aws_security_group"" ""eb_env_sg"" {
  name        = ""eb_env_sg""
  description = ""Security group for Elastic Beanstalk environments""
  vpc_id      = aws_vpc.eb_vpc.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# RDS Subnet Group
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name       = ""rds-subnet-group""
  subnet_ids = [aws_subnet.eb_subnet_public_1.id, aws_subnet.eb_subnet_public_2.id]
}

# RDS Instance
resource ""aws_db_instance"" ""shared_rds"" {
  identifier           = ""shared-rds""
  allocated_storage    = 20
  storage_type        = ""gp2""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  username            = ""admin""
  password            = ""your-password-here""
  skip_final_snapshot = true

  db_subnet_group_name   = aws_db_subnet_group.rds_subnet_group.name
  vpc_security_group_ids",,,,,
"aws_codebuild_project, aws_iam_policy, aws_iam_role, aws_iam_role_policy_attachment, aws_s3_bucket, aws_iam_policy_document, aws_iam_policy_document",Infrastructure for an autograder for a CS class. The autograder needs to run students' code stored on GitHub in a specific environment and store the result. Accomplish this using AWS CodeBuild.,"package autograder_high

import rego.v1

codebuild_project_valid(codebuild_project) if {
        some artifact in codebuild_project.expressions.artifacts
        artifact.location
        artifact.name
        artifact.type
        some environment in codebuild_project.expressions.environment
        environment.compute_type
        environment.image
        environment.type

        some source in codebuild_project.expressions.source
        source.type.constant_value == ""GITHUB""
        source.location
}

default valid := false

valid if {
        resources := input.configuration.root_module.resources
        some codebuild_project in resources
        codebuild_project.type == ""aws_codebuild_project""
        some security_group in resources
        codebuild_project_valid(codebuild_project)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""artifact-bucket-""
}

resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder_build""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip"" # include this
  }

  environment { # arguments required, exact value not specified
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1 # good to have, not required
    location        = ""github.com/source-location""
  }
}

resource ""aws_iam_role"" ""autograder_build_role"" {
  assume_role_policy = data.aws_iam_policy_document.autograder_build_policy_assume.json
}

data ""aws_iam_policy_document"" ""autograder_build_policy_assume"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }
  }

}

data ""aws_iam_policy_document"" ""autograder_build_policy"" {
  statement {
    effect = ""Allow""
    actions = [
      ""ec2:DescribeSubnets"",
      ""ec2:DescribeSecurityGroups"",
      ""ec2:DescribeVpcs""
    ]
    resources = [
      ""*""
    ]
  }
}

resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name        = ""lambda_policy""
  description = ""Grants permissions to Lambda to describe vpc, subnet, security group""

  policy = data.aws_iam_policy_document.autograder_build_policy.json
}

resource ""aws_iam_role_policy_attachment"" ""lambda_policy_attach"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}","have one ""aws_codebuild_project"" resource
  with ""artifacts"" argument block
    with ""type"" argument
    with ""location"" argument
    with ""name"" argument
  with ""environment"" argument block
    with ""computer_type"" argument
    with ""image"" argument
    with ""type"" argument
  with ""source"" argument block
    with ""type"" argument with value ""GITHUB""
    with ""location"" argument","I'll help you create a Terraform configuration for an AWS CodeBuild-based autograder. This solution will include a CodeBuild project that can pull code from GitHub, run the tests, and store the results.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# S3 bucket to store build results
resource ""aws_s3_bucket"" ""autograder_results"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""autograder_results_versioning"" {
  bucket = aws_s3_bucket.autograder_results.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""autograder-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.autograder_results.arn}"",
          ""${aws_s3_bucket.autograder_results.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Automated grading system for CS assignments""
  build_timeout = ""30""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.autograder_results.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""RESULTS_BUCKET""
      value = aws_s3_bucket.autograder_results.bucket
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""autograder-log-group""
      stream_name = ""autograder-log-stream""
    }
  }
}

# GitHub webhook for automated builds (Optional)
resource ""aws_codebuild_webhook"" ""autograder_webhook"" {
  project_name = aws_codebuild_project.autograder.",,,,,,"I'll help you create a Terraform configuration for an AWS CodeBuild-based autograder system. This solution will include a CodeBuild project that can pull code from GitHub, run tests, and store results.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# S3 bucket to store build results
resource ""aws_s3_bucket"" ""autograder_results"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""autograder_results_versioning"" {
  bucket = aws_s3_bucket.autograder_results.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild role
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""autograder-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.autograder_results.arn}"",
          ""${aws_s3_bucket.autograder_results.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""cs-class-autograder""
  description   = ""Autograder for CS class assignments""
  build_timeout = ""15""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.autograder_results.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""TEST_ENVIRONMENT""
      value = ""production""
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""autograder-log-group""
      stream_name = ""autograder-log-stream""
      status      = ""ENABLED""
    }

    s3_logs {
      status   = ""ENABLED""
      location = ""${aws_s3_bucket.autograder_results.id}/build-logs""
    }
  }
}

# CloudWatch Log",,,,,,"I'll help you create a Terraform configuration for an AWS CodeBuild-based autograder system. This solution will include a CodeBuild project that can pull code from GitHub, run tests, and store results.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust region as needed
}

# S3 bucket to store build results
resource ""aws_s3_bucket"" ""autograder_results"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""autograder_results_versioning"" {
  bucket = aws_s3_bucket.autograder_results.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.autograder_results.arn}"",
          ""${aws_s3_bucket.autograder_results.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class assignments""
  build_timeout = ""30""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.autograder_results.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""RESULTS_BUCKET""
      value = aws_s3_bucket.autograder_results.bucket
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""autograder-log-group""
      stream_name = ""autograder-log-stream""
      status      = ""ENABLED""
    }
  }

  cache {
    type  = ""LOCAL""
    modes = [""LOCAL_DOCKER_LAYER_CACHE""]
  }
}

# CloudWatch Log Group for CodeBuild logs
resource ""aws_cloudwatch_log_group""",,,,,
"aws_lightsail_disk, aws_lightsail_disk, aws_lightsail_disk_attachment, aws_lightsail_disk_attachment, aws_lightsail_instance, aws_availability_zones",Create a Lightsail instance with multiple attached disks,"package terraform.validation

default has_valid_resources = false

# Rule for multiple aws_lightsail_disk resources
has_valid_lightsail_disks {
    count([disk | disk := input.planned_values.root_module.resources[_]; disk.type == ""aws_lightsail_disk""; disk.values.name; disk.values.size_in_gb; disk.values.availability_zone]) > 0
}

# Rule for multiple aws_lightsail_disk_attachment resources
has_valid_lightsail_disk_attachments {
    count([attachment | attachment := input.planned_values.root_module.resources[_]; attachment.type == ""aws_lightsail_disk_attachment""; attachment.values.disk_name; attachment.values.instance_name; attachment.values.disk_path]) > 0
}

# Rule for aws_lightsail_instance resource with specific arguments
has_valid_lightsail_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id
    resource.values.bundle_id
}

# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lightsail_disks
    has_valid_lightsail_disk_attachments
    has_valid_lightsail_instance
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""

  filter {
    name   = ""opt-in-status""
    values = [""opt-in-not-required""]
  }
}

resource ""aws_lightsail_disk"" ""test1"" {
  name              = ""test-disk-1""
  size_in_gb        = 8
  availability_zone = data.aws_availability_zones.available.names[0]
}

resource ""aws_lightsail_disk"" ""test2"" {
  name              = ""test-disk-2""
  size_in_gb        = 8
  availability_zone = data.aws_availability_zones.available.names[0]
}

resource ""aws_lightsail_instance"" ""test"" {
  name              = ""test-instance""
  availability_zone = data.aws_availability_zones.available.names[0]
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_1_0""
}

resource ""aws_lightsail_disk_attachment"" ""test1"" {
  disk_name     = aws_lightsail_disk.test1.name
  instance_name = aws_lightsail_instance.test.name
  disk_path     = ""/dev/xvdf""
}

resource ""aws_lightsail_disk_attachment"" ""test2"" {
  disk_name     = aws_lightsail_disk.test2.name
  instance_name = aws_lightsail_instance.test.name
  disk_path     = ""/dev/xvdg""
}","have multiple aws_lightsail_disk resource
    with name
    with size_in_gb
    with availability_zone

have multiple aws_lightsail_disk_attachment
    with disk_name
    with instance_name
    with disk_path

have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument","I'll help you create a Terraform configuration for an AWS Lightsail instance with multiple attached disks. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

# Create Lightsail Instance
resource ""aws_lightsail_instance"" ""example_instance"" {
  name              = ""example-lightsail-instance""
  availability_zone = ""us-east-1a""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""  # Smallest instance size

  tags = {
    Environment = ""Development""
  }
}

# Create first attached disk
resource ""aws_lightsail_disk"" ""disk1"" {
  name              = ""example-disk-1""
  size_in_gb       = 32
  availability_zone = aws_lightsail_instance.example_instance.availability_zone
}

# Attach first disk
resource ""aws_lightsail_disk_attachment"" ""disk1_attachment"" {
  disk_name     = aws_lightsail_disk.disk1.name
  instance_name = aws_lightsail_instance.example_instance.name
}

# Create second attached disk
resource ""aws_lightsail_disk"" ""disk2"" {
  name              = ""example-disk-2""
  size_in_gb       = 64
  availability_zone = aws_lightsail_instance.example_instance.availability_zone
}

# Attach second disk
resource ""aws_lightsail_disk_attachment"" ""disk2_attachment"" {
  disk_name     = aws_lightsail_disk.disk2.name
  instance_name = aws_lightsail_instance.example_instance.name
}

# Create third attached disk
resource ""aws_lightsail_disk"" ""disk3"" {
  name              = ""example-disk-3""
  size_in_gb       = 128
  availability_zone = aws_lightsail_instance.example_instance.availability_zone
}

# Attach third disk
resource ""aws_lightsail_disk_attachment"" ""disk3_attachment"" {
  disk_name     = aws_lightsail_disk.disk3.name
  instance_name = aws_lightsail_instance.example_instance.name
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail instance using Amazon Linux 2
3. Creates three different disks of varying sizes (32GB, 64GB, and 128GB)
4. Attaches all three disks to the Lightsail instance

Each disk is created in the same availability zone as the instance and is attached using a separate disk attachment resource. You can modify the disk sizes, instance blueprint, and bundle ID according to your needs.

Remember to:
- Adjust the region and availability zone as needed
- Modify the instance blueprint_id and bundle_id based on your requirements
- Adjust disk sizes according to your storage needs
- Add any additional tags or configurations as required",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail instance with multiple attached disks. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Lightsail Instance
resource ""aws_lightsail_instance"" ""example_instance"" {
  name              = ""example-instance""
  availability_zone = ""us-west-2a""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""  # Smallest instance size

  tags = {
    Environment = ""Development""
  }
}

# Create first attached disk
resource ""aws_lightsail_disk"" ""disk1"" {
  name              = ""example-disk-1""
  size_in_gb       = 32
  availability_zone = ""us-west-2a""
}

# Create second attached disk
resource ""aws_lightsail_disk"" ""disk2"" {
  name              = ""example-disk-2""
  size_in_gb       = 64
  availability_zone = ""us-west-2a""
}

# Attach first disk to instance
resource ""aws_lightsail_disk_attachment"" ""disk1_attachment"" {
  disk_name     = aws_lightsail_disk.disk1.name
  instance_name = aws_lightsail_instance.example_instance.name
  disk_path     = ""/dev/xvdf""
}

# Attach second disk to instance
resource ""aws_lightsail_disk_attachment"" ""disk2_attachment"" {
  disk_name     = aws_lightsail_disk.disk2.name
  instance_name = aws_lightsail_instance.example_instance.name
  disk_path     = ""/dev/xvdg""
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail instance using Amazon Linux 2
3. Creates two separate Lightsail disks (32GB and 64GB)
4. Attaches both disks to the Lightsail instance with different device paths

Key features:
- The instance uses the `nano_2_0` bundle (can be changed based on needs)
- Disks are created in the same availability zone as the instance
- Each disk has a unique device path for mounting
- All resources are properly tagged and named

Remember to:
- Adjust the region and availability zone as needed
- Modify disk sizes according to your requirements
- Change the instance blueprint_id and bundle_id if you need a different OS or instance size",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail instance with multiple attached disks. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Lightsail Instance
resource ""aws_lightsail_instance"" ""example_instance"" {
  name              = ""example-lightsail-instance""
  availability_zone = ""us-west-2a""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""  # Smallest instance size

  tags = {
    Environment = ""Development""
  }
}

# Create first attached disk
resource ""aws_lightsail_disk"" ""disk1"" {
  name              = ""example-disk-1""
  size_in_gb       = 32
  availability_zone = ""us-west-2a""
}

# Create second attached disk
resource ""aws_lightsail_disk"" ""disk2"" {
  name              = ""example-disk-2""
  size_in_gb       = 64
  availability_zone = ""us-west-2a""
}

# Attach first disk to instance
resource ""aws_lightsail_disk_attachment"" ""disk1_attachment"" {
  disk_name     = aws_lightsail_disk.disk1.name
  instance_name = aws_lightsail_instance.example_instance.name
  disk_path     = ""/dev/xvdf""
}

# Attach second disk to instance
resource ""aws_lightsail_disk_attachment"" ""disk2_attachment"" {
  disk_name     = aws_lightsail_disk.disk2.name
  instance_name = aws_lightsail_instance.example_instance.name
  disk_path     = ""/dev/xvdg""
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail instance using Amazon Linux 2
3. Creates two separate disks (32GB and 64GB)
4. Attaches both disks to the Lightsail instance with different device paths

Make sure to:
- Adjust the region and availability zone according to your needs
- Modify the instance blueprint_id and bundle_id as required
- Adjust the disk sizes based on your requirements
- Change the disk_path values if needed for your specific use case

The disks will be available to mount and use within your Lightsail instance after they're attached.",,,,,
aws_kinesis_stream,Generate a basic Amazon Kinesis stream,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_kinesis_stream {
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_stream""
    resource.values.name
}

has_valid_resources {
	has_valid_kinesis_stream
}",2,"provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_stream"" ""test_stream"" {
  name             = ""drow1""
  shard_count      = 1
  retention_period = 48

  shard_level_metrics = [
    ""IncomingBytes"",
    ""OutgoingBytes"",
  ]

  stream_mode_details {
    stream_mode = ""PROVISIONED""
  }

  tags = {
    Environment = ""test""
  }
}","Has one ""aws_kinesis_stream"" resource","I'll help you create a basic Amazon Kinesis stream using Terraform. Here's a configuration that sets up a Kinesis stream with basic settings:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Replace with your desired region
}

resource ""aws_kinesis_stream"" ""example_stream"" {
  name             = ""example-kinesis-stream""
  shard_count      = 1
  retention_period = 24

  stream_mode_details {
    stream_mode = ""PROVISIONED""
  }

  tags = {
    Environment = ""production""
    Application = ""example""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Kinesis stream with:
   - A single shard (can be increased based on your throughput needs)
   - 24-hour retention period (default)
   - Provisioned capacity mode
   - Basic tags for resource management

You can modify the configuration by adjusting:
- The `shard_count` based on your throughput requirements
- The `retention_period` (between 24 to 168 hours)
- The tags according to your organization's tagging strategy
- The stream name to match your naming convention",,,,,,"I'll help you create a basic Amazon Kinesis stream using Terraform. Here's a configuration that sets up a Kinesis stream with basic settings:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

resource ""aws_kinesis_stream"" ""example_stream"" {
  name             = ""example-kinesis-stream""
  shard_count      = 2  # Number of shards for the stream
  retention_period = 24 # Data retention period in hours (default is 24)

  stream_mode_details {
    stream_mode = ""PROVISIONED""  # PROVISIONED or ON_DEMAND
  }

  tags = {
    Environment = ""production""
    Project     = ""example""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Kinesis stream with 2 shards
3. Sets a retention period of 24 hours
4. Uses PROVISIONED mode for the stream
5. Includes basic tags for resource management

You can adjust the `shard_count`, `retention_period`, and tags according to your specific needs. The stream_mode can also be changed to ""ON_DEMAND"" if you prefer dynamic scaling.",,,,,,"I'll help you create a basic Amazon Kinesis stream using Terraform. Here's a clean and properly formatted configuration that includes the required AWS provider and a Kinesis stream resource:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_kinesis_stream"" ""example_stream"" {
  name             = ""example-kinesis-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""production""
    Project     = ""example""
  }

  stream_mode_details {
    stream_mode = ""PROVISIONED""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Kinesis stream with:
   - A single shard
   - 24-hour retention period
   - PROVISIONED stream mode
   - Basic tagging
   - A descriptive name

You can modify the `shard_count`, `retention_period`, and other parameters based on your specific needs. The stream will be created in the specified AWS region with the basic configuration needed for most use cases.",,,,,
"aws_instance, aws_placement_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc",Create 3 EC2 instances within an EC2 placement group usign clustering.,"package main

import future.keywords.in

default allow = false

# Check for VPC with public and private subnets
vpc_exists {
    some i
    input.resource_changes[i].type == ""aws_vpc""
}

public_subnet_exists(resources) {
    some resource in resources
    resource.type == ""aws_subnet""
    resource.change.after.map_public_ip_on_launch == true
}

private_subnet_exists(resources) {
    some resource in resources
    resource.type == ""aws_subnet""
    resource.change.after.map_public_ip_on_launch == false
}

aws_instances[resource] {
    resource := input.resource_changes[_]
    resource.type == ""aws_instance""
}

# Aggregate all checks
allow {
    vpc_exists
    public_subnet_exists(input.resource_changes)
    private_subnet_exists(input.resource_changes)
    count(aws_instances) == 3
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

# Create a Virtual Private Cloud (VPC)
resource ""aws_vpc"" ""my_vpc"" {
  cidr_block = ""10.0.0.0/16""
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

# Create Public Subnet
resource ""aws_subnet"" ""public_subnet"" {
  vpc_id                  = aws_vpc.my_vpc.id
  cidr_block              = ""10.0.1.0/24""
  availability_zone       = data.aws_availability_zones.azs.names[0]
  map_public_ip_on_launch = true
}

# Create Private Subnet
resource ""aws_subnet"" ""private_subnet"" {
  vpc_id            = aws_vpc.my_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = data.aws_availability_zones.azs.names[1]
}

# Create EC2 Placement Group
resource ""aws_placement_group"" ""my_placement_group"" {
  name     = ""my-placement-group""
  strategy = ""cluster""
}

# Create Security Group for EC2 instances
resource ""aws_security_group"" ""ec2_sg"" {
  vpc_id = aws_vpc.my_vpc.id
  name   = ""ec2_sg""
}

data ""aws_ami"" ""amzn2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

# Create EC2 instances in the Placement Group in the Private Subnet
resource ""aws_instance"" ""ec2_instance"" {
  count           = 3
  ami             = data.aws_ami.amzn2.id
  instance_type   = ""m5.large""
  subnet_id       = aws_subnet.private_subnet.id
  security_groups = [aws_security_group.ec2_sg.id]
  placement_group = aws_placement_group.my_placement_group.name
}",,"I'll help you create a Terraform configuration for 3 EC2 instances within a placement group using the clustering strategy. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create the placement group
resource ""aws_placement_group"" ""cluster"" {
  name     = ""cluster-placement-group""
  strategy = ""cluster""
}

# Create EC2 instances within the placement group
resource ""aws_instance"" ""cluster_instances"" {
  count = 3

  ami           = ""ami-0c55b159cbfafe1f0""  # Replace with your desired AMI ID
  instance_type = ""c5.large""  # Use instance type that supports placement groups

  placement_group = aws_placement_group.cluster.id

  tags = {
    Name = ""cluster-instance-${count.index + 1}""
  }

  # Optional: Add subnet_id if you want to specify a particular subnet
  # subnet_id = ""subnet-xxxxxx""

  root_block_device {
    volume_size = 20
    volume_type = ""gp2""
  }
}

# Output the instance IDs
output ""instance_ids"" {
  value = aws_instance.cluster_instances[*].id
}

# Output the placement group name
output ""placement_group_name"" {
  value = aws_placement_group.cluster.name
}
```

This configuration:
1. Creates a placement group with clustering strategy
2. Launches 3 EC2 instances within that placement group using count
3. Tags each instance with a unique name
4. Outputs the instance IDs and placement group name

Note: 
- Remember to replace the AMI ID with a valid one for your region
- Cluster placement groups have specific instance type requirements and networking considerations
- All instances in a cluster placement group should be launched at the same time for best performance
- Cluster placement groups are recommended for applications that benefit from low network latency and high network throughput",,,,,,"I'll help you create a Terraform configuration to launch 3 EC2 instances within a clustered placement group. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create the placement group
resource ""aws_placement_group"" ""cluster"" {
  name     = ""cluster-placement-group""
  strategy = ""cluster""
}

# Create 3 EC2 instances in the placement group
resource ""aws_instance"" ""cluster_instances"" {
  count = 3

  ami           = ""ami-0735c191cf914754d""  # Replace with your desired AMI ID
  instance_type = ""c5.large""  # Use instance types that support placement groups

  placement_group = aws_placement_group.cluster.id

  tags = {
    Name = ""cluster-instance-${count.index + 1}""
  }

  # Add other necessary configuration like VPC, subnet, security groups as needed
  vpc_security_group_ids = []  # Add your security group IDs
  subnet_id              = """"  # Add your subnet ID

  root_block_device {
    volume_size = 20
    volume_type = ""gp3""
  }
}

# Output the instance IDs
output ""instance_ids"" {
  value = aws_instance.cluster_instances[*].id
}

# Output the placement group name
output ""placement_group_name"" {
  value = aws_placement_group.cluster.name
}
```

This configuration:
1. Creates a placement group with the ""cluster"" strategy
2. Launches 3 EC2 instances within that placement group using a count
3. Tags each instance with a unique name
4. Includes outputs for instance IDs and placement group name

Note: 
- Remember to replace the AMI ID with one appropriate for your region
- Add your specific VPC, subnet, and security group configurations
- Cluster placement groups have specific instance type requirements, so make sure to use compatible instance types
- The instances should be launched in the same AZ for cluster placement groups",,,,,,"I'll help you create a Terraform configuration for 3 EC2 instances within a placement group using clustering strategy. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create the placement group
resource ""aws_placement_group"" ""cluster"" {
  name     = ""cluster-placement-group""
  strategy = ""cluster""
}

# Create 3 EC2 instances in the placement group
resource ""aws_instance"" ""cluster_instances"" {
  count = 3

  ami           = ""ami-0c55b159cbfafe1f0""  # Replace with your desired AMI ID
  instance_type = ""c5.large""  # Use instance type that supports placement groups

  # Place the instances in the placement group
  placement_group = aws_placement_group.cluster.id

  tags = {
    Name = ""cluster-instance-${count.index + 1}""
  }

  # Make sure the instances are in the same AZ for clustering
  availability_zone = ""us-west-2a""  # Change to match your region

  root_block_device {
    volume_size = 20
    volume_type = ""gp3""
  }
}
```

This configuration:
1. Creates a placement group with clustering strategy
2. Launches 3 EC2 instances within that placement group
3. Uses count to create multiple instances
4. Places all instances in the same AZ (required for cluster placement groups)
5. Tags each instance with a unique name

Note: Make sure to:
- Replace the AMI ID with a valid one for your region
- Choose an appropriate instance type that supports placement groups
- Update the region and availability zone according to your needs
- Placement groups with clustering strategy require instances to be in the same AZ for optimal performance",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer that Specifying private IP addresses for an internal-facing load balancer,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource

is_valid_lb {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.expressions.load_balancer_type.constant_value == ""network""
    resource.expressions.subnet_mapping[0].private_ipv4_address != null
    resource.expressions.subnet_mapping[0].subnet_id != null
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    valid_type := {""instance"", ""ip"", ""alb""}
    type := resources.values.target_type
    valid_type[type]

    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_target_group""
    resource.expressions.port != null
    resource.expressions.protocol != null
    resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
    is_valid_lb
    is_valid_lb_listener
    is_valid_lb_target_group_attachment
    is_valid_lb_target_group
    is_valid_instance
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.azs.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""     # Replace with your desired instance type
  subnet_id     = module.vpc.public_subnets[0]
}

resource ""aws_lb"" ""test"" {
  load_balancer_type = ""network""
  
  subnet_mapping {
    subnet_id            = module.vpc.public_subnet_arns[0]
    private_ipv4_address = ""10.0.4.30""
  }

  subnet_mapping {
    subnet_id            = module.vpc.public_subnet_arns[1]
    private_ipv4_address = ""10.0.5.15""
  }
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.id
}","has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument

has one ""aws_lb"" resource
    with ""subnet_mapping""
        with private_ipv4_address
    with load_balancer_type = ""network""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument
    with one protocol argument
        with one of following values: TCP, UDP, TLS, TCP_UDP

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument

has at least one ""aws_eip""","I'll help you create a Terraform configuration for an internal-facing Application Load Balancer (ALB) with specified private IP addresses. Here's a solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC (assuming you have an existing VPC)
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  tags = {
    Name = ""main-vpc""
  }
}

# Private Subnets
resource ""aws_subnet"" ""private_subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""private-subnet-1""
  }
}

resource ""aws_subnet"" ""private_subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""private-subnet-2""
  }
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""internal-alb-sg""
  description = ""Security group for internal ALB""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Internal Application Load Balancer
resource ""aws_lb"" ""internal"" {
  name               = ""internal-alb""
  internal           = true
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [aws_subnet.private_subnet_1.id, aws_subnet.private_subnet_2.id]

  subnet_mapping {
    subnet_id            = aws_subnet.private_subnet_1.id
    private_ipv4_address = ""10.0.1.100""
  }

  subnet_mapping {
    subnet_id            = aws_subnet.private_subnet_2.id
    private_ipv4_address = ""10.0.2.100""
  }

  tags = {
    Environment = ""production""
  }
}

# ALB Target Group
resource ""aws_lb_target_group"" ""internal"" {
  name     = ""internal-tg""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = aws_vpc.main.id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.internal.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.internal.arn
  }
}
```

This configuration creates:
1. An internal Application Load Balancer with specified private IP addresses
2. The necessary networking components (VPC and private subnets)
3. A security group for the ALB
4. A target group and listener
5. Subnet mappings with specific private IP addresses",,,,,,"I'll help you create a Terraform configuration for an internal Application Load Balancer (ALB) with specified private IP addresses. This is commonly used when you need predictable IP addresses for your internal load balancer.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC (assuming you have an existing VPC)
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  enable_dns_hostnames = true
  enable_dns_support   = true
  
  tags = {
    Name = ""main-vpc""
  }
}

# Private subnets
resource ""aws_subnet"" ""private_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""private-subnet-1""
  }
}

resource ""aws_subnet"" ""private_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""private-subnet-2""
  }
}

# Security Group for ALB
resource ""aws_security_group"" ""alb"" {
  name        = ""internal-alb-sg""
  description = ""Security group for internal ALB""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Internal Application Load Balancer
resource ""aws_lb"" ""internal"" {
  name               = ""internal-alb""
  internal           = true
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb.id]
  subnets            = [aws_subnet.private_1.id, aws_subnet.private_2.id]

  subnet_mapping {
    subnet_id            = aws_subnet.private_1.id
    private_ipv4_address = ""10.0.1.10""
  }

  subnet_mapping {
    subnet_id            = aws_subnet.private_2.id
    private_ipv4_address = ""10.0.2.10""
  }

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
  }
}

# ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.internal.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type = ""fixed-response""

    fixed_response {
      content_type = ""text/plain""
      message_body = ""Fixed response content""
      status_code  = ""200""
    }
  }
}
```

This configuration:
1. Creates an internal Application Load Balancer
2. Specifies private IP addresses (10.0.1.10 and 10.0.2.10) for the load balancer in each subnet
3. Sets up the necessary networking components (VPC, subnets)
4. Creates a security group allowing internal traffic
5. Configures a basic HTTP listener

Key points:
- The ALB is marked as `internal = true`
- Private IP addresses are specified using `subnet_mapping` blocks
- The ALB is placed in private subnets
- The security group allows internal traffic only

Remember to:
- Adjust the CIDR blocks and IP addresses according to your network design
-",,,,,,"I'll help you create a Terraform configuration for an internal Application Load Balancer (ALB) with specified private IP addresses. Here's a solution that includes the required provider and necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC and Subnet resources (assuming they exist)
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  tags = {
    Name = ""main-vpc""
  }
}

resource ""aws_subnet"" ""private_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""private-subnet-1""
  }
}

resource ""aws_subnet"" ""private_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""private-subnet-2""
  }
}

# Security Group for the internal ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""internal-alb-sg""
  description = ""Security group for internal ALB""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Internal Application Load Balancer
resource ""aws_lb"" ""internal"" {
  name               = ""internal-alb""
  internal           = true
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [aws_subnet.private_1.id, aws_subnet.private_2.id]

  subnet_mapping {
    subnet_id            = aws_subnet.private_1.id
    private_ipv4_address = ""10.0.1.15""
  }

  subnet_mapping {
    subnet_id            = aws_subnet.private_2.id
    private_ipv4_address = ""10.0.2.15""
  }

  tags = {
    Environment = ""production""
    Name        = ""internal-alb""
  }
}

# ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.internal.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type = ""fixed-response""

    fixed_response {
      content_type = ""text/plain""
      message_body = ""Fixed response content""
      status_code  = ""200""
    }
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC and two private subnets (you might want to reference existing ones instead)
3. Creates a security group for the ALB
4. Creates an internal Application Load Balancer with specific private IP addresses (10.0.1.15 and 10.0.2.15)
5. Sets up a basic listener on port 80

Key features of the internal ALB:
- `internal = true` makes it an internal load balancer
- `subnet_mapping` blocks specify the private IP addresses for each subnet
- The ALB is created across two availability zones for high availability
- Security group allows internal traffic on port 80

Remember to:
- Adjust the CIDR blocks and IP addresses according to your network design
- Modify the security group rules base",,,,,
"aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_s3_bucket",An AWS service that holds a web server which allows you to upload cat pictures and provides random cat pictures on demand. Accomplishes this with API Gateway and S3.,"package caas_high

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
	api.address in cat.expressions.parent_id.references
	api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
	method.expressions.http_method.constant_value == method_string
	resource.address in method.expressions.resource_id.references
	api.address in method.expressions.rest_api_id.references
	method.expressions.authorization
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some api in resources
	api.type == ""aws_api_gateway_rest_api""

	some cat in resources
	cat.type == ""aws_api_gateway_resource""

	some method_get in resources
	method_get.type == ""aws_api_gateway_method""

	some method_put in resources
	method_put.type == ""aws_api_gateway_method""

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	api_valid(api)
	cat_valid(cat, api)
	method_valid(method_get, ""GET"", cat, api)
	method_valid(method_put, ""PUT"", cat, api)
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource
    with ""http_method"" argument with values ""GET"" and ""PUT""
    with ""authorization"" argument
have one ""aws_s3_bucket"" resource","I'll help you create a Terraform configuration for an API Gateway and S3 setup that can handle cat picture uploads and random picture retrieval. Here's a solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-storage""
}

# S3 bucket policy to allow API Gateway access
resource ""aws_s3_bucket_policy"" ""allow_access"" {
  bucket = aws_s3_bucket.cat_pictures.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowAPIGatewayAccess""
        Effect    = ""Allow""
        Principal = {
          Service = ""apigateway.amazonaws.com""
        }
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Resource = ""${aws_s3_bucket.cat_pictures.arn}/*""
      }
    ]
  })
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat-pictures-api""
}

# API Gateway resource for uploads
resource ""aws_api_gateway_resource"" ""upload"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""upload""
}

# API Gateway resource for random pictures
resource ""aws_api_gateway_resource"" ""random"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""random""
}

# POST method for uploads
resource ""aws_api_gateway_method"" ""upload_post"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.upload.id
  http_method   = ""POST""
  authorization = ""NONE""
}

# GET method for random pictures
resource ""aws_api_gateway_method"" ""random_get"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.random.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# Integration with S3 for uploads
resource ""aws_api_gateway_integration"" ""s3_upload"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  resource_id = aws_api_gateway_resource.upload.id
  http_method = aws_api_gateway_method.upload_post.http_method
  type        = ""AWS""
  integration_http_method = ""POST""
  uri         = ""arn:aws:apigateway:${var.region}:s3:path/${aws_s3_bucket.cat_pictures.bucket}""
}

# Integration with S3 for random pictures
resource ""aws_api_gateway_integration"" ""s3_random"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  resource_id = aws_api_gateway_resource.random.id
  http_method = aws_api_gateway_method.random_get.http_method
  type        = ""AWS""
  integration_http_method = ""GET""
  uri         = ""arn:aws:apigateway:${var.region}:s3:path/${aws_s3_bucket.cat_pictures.bucket}""
}

# Deployment of the API
resource ""aws_api_gateway_deployment"" ""deployment"" {
  rest_api_",,,,,,"I'll help you create a Terraform configuration for a service that handles cat pictures using API Gateway and S3. This solution will include:
1. An S3 bucket to store cat pictures
2. An API Gateway to handle uploads and random picture retrieval
3. Required IAM roles and policies

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-bucket""
}

# S3 bucket policy to allow API Gateway access
resource ""aws_s3_bucket_policy"" ""allow_access"" {
  bucket = aws_s3_bucket.cat_pictures.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowAPIGatewayAccess""
        Effect    = ""Allow""
        Principal = {
          Service = ""apigateway.amazonaws.com""
        }
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          ""${aws_s3_bucket.cat_pictures.arn}"",
          ""${aws_s3_bucket.cat_pictures.arn}/*""
        ]
      }
    ]
  })
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat-pictures-api""
}

# API Gateway resources
resource ""aws_api_gateway_resource"" ""upload"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""upload""
}

resource ""aws_api_gateway_resource"" ""random"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""random""
}

# API Gateway methods
resource ""aws_api_gateway_method"" ""upload_post"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.upload.id
  http_method   = ""POST""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""random_get"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.random.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# API Gateway integrations with S3
resource ""aws_api_gateway_integration"" ""upload_integration"" {
  rest_api_id             = aws_api_gateway_rest_api.cat_api.id
  resource_id             = aws_api_gateway_resource.upload.id
  http_method             = aws_api_gateway_method.upload_post.http_method
  integration_http_method = ""PUT""
  type                   = ""AWS""
  uri                    = ""arn:aws:apigateway:${data.aws_region.current.name}:s3:path/${aws_s3_bucket.cat_pictures.bucket}/{filename}""

  request_parameters = {
    ""integration.request.path.filename"" = ""method.request.querystring.filename""
  }
}

resource ""aws_api_gateway_integration"" ""random_integration"" {
  rest_api_id             = aws_api_gateway_rest_api.cat_api.id
  resource_id             = aws_api_gateway_resource.random.id
  http_method             = aws_api_gateway_method.random_get.http_metho",,,,,,"I'll help you create a Terraform configuration for a web service that handles cat pictures using API Gateway and S3. Here's a solution that includes the necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-bucket""
}

# Enable versioning for the bucket
resource ""aws_s3_bucket_versioning"" ""cat_pictures_versioning"" {
  bucket = aws_s3_bucket.cat_pictures.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# S3 bucket policy to allow API Gateway access
resource ""aws_s3_bucket_policy"" ""allow_access"" {
  bucket = aws_s3_bucket.cat_pictures.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowAPIGatewayAccess""
        Effect    = ""Allow""
        Principal = {
          Service = ""apigateway.amazonaws.com""
        }
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          ""${aws_s3_bucket.cat_pictures.arn}"",
          ""${aws_s3_bucket.cat_pictures.arn}/*""
        ]
      }
    ]
  })
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name        = ""cat-pictures-api""
  description = ""API for managing cat pictures""
}

# API Gateway resource for /pictures
resource ""aws_api_gateway_resource"" ""pictures"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""pictures""
}

# GET method for random cat picture
resource ""aws_api_gateway_method"" ""get_picture"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.pictures.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# POST method for uploading cat pictures
resource ""aws_api_gateway_method"" ""post_picture"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.pictures.id
  http_method   = ""POST""
  authorization = ""NONE""
}

# Integration with S3 for GET method
resource ""aws_api_gateway_integration"" ""s3_get_integration"" {
  rest_api_id             = aws_api_gateway_rest_api.cat_api.id
  resource_id             = aws_api_gateway_resource.pictures.id
  http_method             = aws_api_gateway_method.get_picture.http_method
  integration_http_method = ""GET""
  type                    = ""AWS""
  uri                     = ""arn:aws:apigateway:${data.aws_region.current.name}:s3:path/${aws_s3_bucket.cat_pictures.bucket}/random""
}

# Integration with S3 for POST method
resource ""aws_api_gateway_integration"" ""s3_post_integration"" {
  rest_api_id             = aws_api_gateway_rest_api.cat_api.id
  resource_id             = aws_api_gateway_resource.pictures.id
  http_method             = aws_api_gateway_method.post_picture.http_method
  integration_http_method = ""POST""
  type                    = ""AWS""
  uri                     = ""arn:aws:apigateway",,,,,
"aws_msk_serverless_cluster, aws_security_group, aws_subnet, aws_subnet, aws_subnet, aws_vpc, aws_availability_zones",Create a serverless MSK with 3 broker nodes.,"package main

import future.keywords.in

default allow = false

# Check if MSK cluster is serverless
is_serverless(resource) {
resource.type == ""aws_msk_serverless_cluster""
}


# Check if cluster spans three AZs
has_three_azs(resource) {
resource.type == ""aws_msk_serverless_cluster""
count(resource.expressions.vpc_config[_].subnet_ids.references) == 6
}

# Main rule combining all checks
allow {
some resource in input.configuration.root_module.resources
is_serverless(resource)
has_three_azs(resource)
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""vpc"" {
    cidr_block = ""192.168.0.0/22""
    enable_dns_support = true
    enable_dns_hostnames = true
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

resource ""aws_subnet"" ""subnet_az1"" {
    availability_zone = data.aws_availability_zones.azs.names[0]
    cidr_block = ""192.168.0.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az2"" {
    availability_zone = data.aws_availability_zones.azs.names[1]
    cidr_block = ""192.168.1.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az3"" {
    availability_zone = data.aws_availability_zones.azs.names[2]
    cidr_block = ""192.168.2.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.vpc.id
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }
}

resource ""aws_route_table_association"" ""first"" {
  subnet_id      = aws_subnet.subnet_az1.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""second"" {
  subnet_id      = aws_subnet.subnet_az2.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""third"" {
  subnet_id      = aws_subnet.subnet_az3.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_security_group"" ""sg"" {
    vpc_id = aws_vpc.vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4 = ""0.0.0.0/0""
  ip_protocol = ""-1""
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  ip_protocol       = ""-1""
}

resource ""aws_msk_serverless_cluster"" ""example"" {
    cluster_name = ""Example""

    vpc_config {
        subnet_ids = [aws_subnet.subnet_az1.id, aws_subnet.subnet_az2.id, aws_subnet.subnet_az3.id]
        security_group_ids = [aws_security_group.sg.id]
    }

    client_authentication {
        sasl {
            iam {
                enabled = true
            }
        }
    }
}","Has one ""aws_msk_serverless_cluster"" resource
with one ""vpc_config""
with one ""subnet_ids"" with 6 references (3 subnets)","I'll help you create a Terraform configuration for a serverless Amazon MSK (Managed Streaming for Apache Kafka) cluster with 3 broker nodes. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_msk_serverless_cluster"" ""msk_serverless"" {
  cluster_name = ""msk-serverless-example""

  vpc_config {
    subnet_ids = [
      ""subnet-12345678"",  # Replace with your subnet IDs
      ""subnet-87654321"",
      ""subnet-11223344""
    ]
    security_group_ids = [
      ""sg-12345678""  # Replace with your security group ID
    ]
  }

  client_authentication {
    sasl {
      iam {
        enabled = true
      }
    }
  }

  tags = {
    Environment = ""production""
    Name        = ""msk-serverless-cluster""
  }
}

resource ""aws_msk_configuration"" ""msk_config"" {
  kafka_versions = [""2.8.1""]
  name           = ""msk-config-example""

  server_properties = <<PROPERTIES
auto.create.topics.enable=true
delete.topic.enable=true
PROPERTIES
}

resource ""aws_cloudwatch_log_group"" ""msk_log_group"" {
  name              = ""/aws/msk/serverless-cluster""
  retention_in_days = 7
}
```

This configuration creates:

1. A serverless MSK cluster with:
   - 3 broker nodes (automatically managed by AWS)
   - IAM authentication enabled
   - VPC configuration (you'll need to replace the subnet IDs and security group ID with your own)

2. An MSK configuration with some basic Kafka properties

3. A CloudWatch log group for cluster logging

Important notes:
- Replace the subnet IDs and security group ID with your actual VPC resources
- The serverless MSK cluster automatically manages the scaling of broker nodes
- Make sure the subnets are in different Availability Zones
- The security group should allow the necessary Kafka ports (9092 for plaintext, 9094 for TLS)
- IAM authentication is enabled by default for enhanced security

Remember to also ensure that your AWS credentials have the necessary permissions to create these resources.",,,,,,"I'll help you create a Terraform configuration for a serverless Amazon MSK (Managed Streaming for Apache Kafka) cluster with 3 broker nodes. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_msk_serverless_cluster"" ""msk_serverless"" {
  cluster_name = ""msk-serverless-example""

  vpc_config {
    subnet_ids = [
      ""subnet-xxxxxxxx"",  # Replace with your subnet IDs
      ""subnet-yyyyyyyy"",
      ""subnet-zzzzzzzz""
    ]
    security_group_ids = [
      ""sg-xxxxxxxx""  # Replace with your security group ID
    ]
  }

  client_authentication {
    sasl {
      iam {
        enabled = true
      }
    }
  }

  tags = {
    Environment = ""production""
    Name        = ""msk-serverless-cluster""
  }
}

resource ""aws_msk_configuration"" ""msk_config"" {
  kafka_versions = [""2.8.1""]
  name           = ""msk-config""
  description    = ""MSK Configuration for serverless cluster""

  server_properties = <<PROPERTIES
auto.create.topics.enable=true
default.replication.factor=3
num.partitions=3
PROPERTIES
}

resource ""aws_cloudwatch_log_group"" ""msk_log_group"" {
  name              = ""/aws/msk/serverless-cluster""
  retention_in_days = 7
}
```

This configuration creates:
1. A serverless MSK cluster with 3 broker nodes (managed automatically by AWS)
2. IAM authentication enabled for security
3. Basic configuration for Kafka settings
4. CloudWatch log group for monitoring
5. VPC configuration (you'll need to replace the subnet IDs and security group ID with your own)

Note that with serverless MSK:
- The broker nodes are managed automatically by AWS
- Capacity is adjusted automatically based on your workload
- You only pay for the actual resources used
- You need to specify at least two subnet IDs in different Availability Zones

Remember to:
1. Replace the subnet IDs and security group ID with your actual values
2. Adjust the region as needed
3. Modify the tags and other parameters according to your requirements
4. Ensure your VPC has proper routing and internet access configured

The configuration includes basic Kafka properties, but you can adjust these based on your specific needs.",,,,,,"I'll help you create a Terraform configuration for a serverless Amazon MSK (Managed Streaming for Apache Kafka) cluster with 3 broker nodes. Here's the code:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_msk_serverless_cluster"" ""msk_serverless"" {
  cluster_name = ""msk-serverless-example""

  vpc_config {
    subnet_ids = [
      ""subnet-xxxxxx1"",  # Replace with your subnet IDs
      ""subnet-xxxxxx2"",
      ""subnet-xxxxxx3""
    ]
    security_group_ids = [
      ""sg-xxxxxx""  # Replace with your security group ID
    ]
  }

  client_authentication {
    sasl {
      iam {
        enabled = true
      }
    }
  }

  number_of_broker_nodes = 3

  tags = {
    Environment = ""production""
    Name        = ""msk-serverless-cluster""
  }
}

# IAM policy for MSK access
resource ""aws_iam_policy"" ""msk_policy"" {
  name        = ""msk-serverless-access-policy""
  description = ""Policy for MSK Serverless access""

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kafka-cluster:Connect"",
          ""kafka-cluster:DescribeCluster"",
          ""kafka-cluster:*Topic*"",
          ""kafka-cluster:WriteData"",
          ""kafka-cluster:ReadData""
        ]
        Resource = aws_msk_serverless_cluster.msk_serverless.arn
      }
    ]
  })
}
```

This configuration creates:
1. A serverless MSK cluster with 3 broker nodes
2. IAM authentication enabled
3. A basic IAM policy for cluster access
4. VPC configuration (you'll need to replace the subnet IDs and security group ID with your own)

Note: Make sure to:
- Replace the subnet IDs and security group ID with your actual values
- Adjust the region as needed
- Review and modify the IAM policy according to your security requirements
- Ensure your VPC has the necessary routing and internet access configured

The serverless MSK cluster will automatically scale based on your workload, while maintaining the specified number of broker nodes.",,,,,
"aws_api_gateway_integration, aws_api_gateway_integration, aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_dynamodb_table, aws_iam_role, aws_iam_role, aws_lambda_function, aws_lambda_function, aws_lambda_permission, aws_lambda_permission, aws_s3_bucket, archive_file, archive_file","An API Gateway Rest API named ""caas"" that has a resource endpoint named ""caas_cat"" with a path_part ""cat"" linking to two methods ""caas_get"" and ""caas_put"" with http_method ""GET"" and ""PUT."" Each of the methods should have a corresponding Lambda function that the API Gateway will have permission to access. A S3 bucket that stores cat pictures which will be accessed by a Lambda function, and a DynamoDB table with one keyed attribute, strings ""name"" of the cat.","package caas

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
        api.address in cat.expressions.parent_id.references
        api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
        method.expressions.http_method.constant_value == method_string
        resource.address in method.expressions.resource_id.references
        api.address in method.expressions.rest_api_id.references
}

table_valid(table) if {
        some attribute in table.expressions.attribute
        attribute.name.constant_value == ""name""
        attribute.type.constant_value == ""S""

        table.expressions.hash_key.constant_value == ""name""
}

lambda_valid(lambda, bucket) if {
        some env in lambda.expressions.environment
        bucket.address in env.variables.references
}

permission_valid(permission, lambda, api) if {
        permission.expressions.action.constant_value == ""lambda:InvokeFunction""
        lambda.address in permission.expressions.function_name.references
        permission.expressions.principal.constant_value == ""apigateway.amazonaws.com""
        api.address in permission.expressions.source_arn.references
}

integration_valid(integration, lambda, method, resource, api, integration_method) if {
        method.address in integration.expressions.http_method.references
        resource.address in integration.expressions.resource_id.references
        api.address in integration.expressions.rest_api_id.references
        integration.expressions.integration_http_method.constant_value == integration_method
        integration.expressions.type.constant_value == ""AWS_PROXY""
        lambda.address in integration.expressions.uri.references
}

default valid := false

valid if {
        resources := input.configuration.root_module.resources

        some api in resources
        api.type == ""aws_api_gateway_rest_api""

        some cat in resources
        cat.type == ""aws_api_gateway_resource""

        some method_get in resources
        method_get.type == ""aws_api_gateway_method""

        some method_put in resources
        method_put.type == ""aws_api_gateway_method""

        some table in resources
        table.type == ""aws_dynamodb_table""

        some lambda_get in resources
        lambda_get.type == ""aws_lambda_function""

        some lambda_put in resources
        lambda_put.type == ""aws_lambda_function""

        some bucket in resources
        bucket.type == ""aws_s3_bucket""

        some permission_get in resources
        permission_get.type == ""aws_lambda_permission""

        some permission_put in resources
        permission_put.type == ""aws_lambda_permission""

        some integration_get in resources
        integration_get.type == ""aws_api_gateway_integration""

        some integration_put in resources
        integration_put.type == ""aws_api_gateway_integration""

        api_valid(api)
        cat_valid(cat, api)
        method_valid(method_get, ""GET"", cat, api)
        method_valid(method_put, ""PUT"", cat, api)
        lambda_valid(lambda_get, bucket)
        lambda_valid(lambda_put, bucket)
        permission_valid(permission_get, lambda_get, api)
        permission_valid(permission_put, lambda_put, api)
        integration_valid(integration_get, lambda_get, method_get, cat, api, ""GET"")
        integration_valid(integration_put, lambda_put, method_put, cat, api, ""PUT"")
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument name
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument with value ""cat""
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_api resource id
    with ""resource_id"" linking to aws_api_gateway_resource resource
    with ""authorization"" argument with value ""NONE""
    with ""http_method"" argument with values ""GET"" and ""PUT""
have one ""aws_s3_bucket"" resource
have one ""aws_dynamodb_table"" resource
    with ""name"" argument
    with ""hash_key"" argument with value ""name""
    with ""attribute"" argument block
        with ""name"" argument with value ""name""
        with ""type"" argument with value ""S""
with two ""aws_lambda_function"" resources
    with ""function_name"" argument
    with ""role"" argument linking to aws_iam_role resources
    with ""environment"" argument block
        with ""variables"" argument block linking to the aws_s3_bucket
with two ""aws_lambda_permission"" resources
    with ""action"" argument with value ""lambda:InvokeFunction""
    with ""principal"" argument with value ""apigateway.amazonaws.com""
    with ""function_name"" argument linking to aws_lambda_function resource
    with ""source_arn"" argument linking to aws_api_gateway_rest_api (two different end points)
with two ""aws_api_gateway_integration"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource resource
    with ""http_method"" argument linking to aws_api_gateway_method resource
    with ""type"" argument with value ""AWS_PROXY""
    with ""integration_http_method"" argument with values ""GET"" and ""PUT""","I'll help you create a Terraform configuration for your API Gateway setup with Lambda functions, S3 bucket, and DynamoDB table. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

# API Gateway Resource
resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

# GET Method
resource ""aws_api_gateway_method"" ""caas_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# PUT Method
resource ""aws_api_gateway_method"" ""caas_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

# Lambda Functions
resource ""aws_lambda_function"" ""get_function"" {
  filename      = ""get_function.zip""
  function_name = ""caas_get_function""
  role         = aws_iam_role.lambda_role.arn
  handler      = ""index.handler""
  runtime      = ""nodejs14.x""
}

resource ""aws_lambda_function"" ""put_function"" {
  filename      = ""put_function.zip""
  function_name = ""caas_put_function""
  role         = aws_iam_role.lambda_role.arn
  handler      = ""index.handler""
  runtime      = ""nodejs14.x""
}

# Lambda permissions for API Gateway
resource ""aws_lambda_permission"" ""get_lambda_permission"" {
  statement_id  = ""AllowAPIGatewayInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.get_function.function_name
  principal     = ""apigateway.amazonaws.com""
  source_arn    = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*/*""
}

resource ""aws_lambda_permission"" ""put_lambda_permission"" {
  statement_id  = ""AllowAPIGatewayInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.put_function.function_name
  principal     = ""apigateway.amazonaws.com""
  source_arn    = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*/*""
}

# S3 Bucket for cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""caas-cat-pictures""
}

resource ""aws_s3_bucket_versioning"" ""cat_pictures"" {
  bucket = aws_s3_bucket.cat_pictures.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# DynamoDB Table
resource ""aws_dynamodb_table"" ""cats"" {
  name           = ""cats""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""name""

  attribute {
    name = ""name""
    type = ""S""
  }
}

# IAM Role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""caas_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10",,,,,,"I'll help you create a Terraform configuration for the requested AWS resources. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

# API Gateway Resource
resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

# GET Method
resource ""aws_api_gateway_method"" ""caas_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# PUT Method
resource ""aws_api_gateway_method"" ""caas_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

# Lambda Functions
resource ""aws_lambda_function"" ""get_function"" {
  filename      = ""get_function.zip""
  function_name = ""caas_get_function""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""
}

resource ""aws_lambda_function"" ""put_function"" {
  filename      = ""put_function.zip""
  function_name = ""caas_put_function""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""
}

# Lambda permissions for API Gateway
resource ""aws_lambda_permission"" ""get_lambda_permission"" {
  statement_id  = ""AllowAPIGatewayInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.get_function.function_name
  principal     = ""apigateway.amazonaws.com""
  source_arn    = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/${aws_api_gateway_method.caas_get.http_method}${aws_api_gateway_resource.caas_cat.path}""
}

resource ""aws_lambda_permission"" ""put_lambda_permission"" {
  statement_id  = ""AllowAPIGatewayInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.put_function.function_name
  principal     = ""apigateway.amazonaws.com""
  source_arn    = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/${aws_api_gateway_method.caas_put.http_method}${aws_api_gateway_resource.caas_cat.path}""
}

# S3 Bucket for cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-bucket""
}

resource ""aws_s3_bucket_versioning"" ""cat_pictures_versioning"" {
  bucket = aws_s3_bucket.cat_pictures.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# DynamoDB Table
resource ""aws_dynamodb_table"" ""cats"" {
  name           = ""cats""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""name""

  attribute {
    name = ""name""
    type = ""S""
  }
}

# IAM",,,,,,"I'll help you create a Terraform configuration for your API Gateway, Lambda functions, S3 bucket, and DynamoDB table setup. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""caas"" {
  name        = ""caas""
  description = ""Cats as a Service API""
}

# API Gateway Resource
resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

# GET Method
resource ""aws_api_gateway_method"" ""caas_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# PUT Method
resource ""aws_api_gateway_method"" ""caas_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

# Lambda Functions
resource ""aws_lambda_function"" ""get_function"" {
  filename      = ""get_function.zip""
  function_name = ""caas_get_function""
  role         = aws_iam_role.lambda_role.arn
  handler      = ""index.handler""
  runtime      = ""nodejs14.x""
}

resource ""aws_lambda_function"" ""put_function"" {
  filename      = ""put_function.zip""
  function_name = ""caas_put_function""
  role         = aws_iam_role.lambda_role.arn
  handler      = ""index.handler""
  runtime      = ""nodejs14.x""
}

# Lambda Permissions for API Gateway
resource ""aws_lambda_permission"" ""get_lambda_permission"" {
  statement_id  = ""AllowAPIGatewayInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.get_function.function_name
  principal     = ""apigateway.amazonaws.com""
  source_arn    = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/${aws_api_gateway_method.caas_get.http_method}${aws_api_gateway_resource.caas_cat.path}""
}

resource ""aws_lambda_permission"" ""put_lambda_permission"" {
  statement_id  = ""AllowAPIGatewayInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.put_function.function_name
  principal     = ""apigateway.amazonaws.com""
  source_arn    = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/${aws_api_gateway_method.caas_put.http_method}${aws_api_gateway_resource.caas_cat.path}""
}

# S3 Bucket for Cat Pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-bucket""
}

resource ""aws_s3_bucket_versioning"" ""cat_pictures_versioning"" {
  bucket = aws_s3_bucket.cat_pictures.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# DynamoDB Table
resource ""aws_dynamodb_table"" ""cats"" {
  name           = ""cats""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""name",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate an Application Load Balancer,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# has valid subnet
is_valid_subnet {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_subnet""
    resource.expressions.vpc_id != null
    have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_listener""
    resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    valid_type := {""instance"", ""ip"", ""alb""}
    type := resources.values.target_type
    valid_type[type]

    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_target_group""
    resource.expressions.port != null
    resource.expressions.protocol != null
    resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
    is_valid_lb
    is_valid_lb_listener
    is_valid_lb_target_group_attachment
    is_valid_lb_target_group
    is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.public_subnets[0]
  security_groups = [ aws_security_group.instance-sg.id ]
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.id
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you create a Terraform configuration for an AWS Application Load Balancer (ALB). This will include the ALB itself, a target group, and the necessary provider configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create ALB
resource ""aws_lb"" ""application_lb"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [""subnet-12345678"", ""subnet-87654321""]  # Replace with your subnet IDs

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
    Name        = ""my-application-lb""
  }
}

# Create ALB Target Group
resource ""aws_lb_target_group"" ""alb_tg"" {
  name        = ""my-alb-target-group""
  port        = 80
  protocol    = ""HTTP""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID
  target_type = ""instance""

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.application_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.alb_tg.arn
  }
}

# Create Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""alb-security-group""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. A target group for the ALB
3. A listener on port 80
4. A security group for the ALB

Note: You'll need to replace the following placeholder values with your actual values:
- subnet IDs in the ALB resource
- VPC ID in the target group and security group
- Adjust the region in the provider block
- Modify the security group rules according to your needs
- Adjust any tags or names to match your naming conventions

The ALB is configured as an internet-facing load balancer (internal = false) and has deletion protection disabled. The target group is configured for HTTP traffic on port 80 with basic health checks.",,,,,,"I'll help you create a Terraform configuration for an AWS Application Load Balancer (ALB). This will include the ALB itself, a target group, and the necessary provider configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create the Application Load Balancer
resource ""aws_lb"" ""app_lb"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [""subnet-12345678"", ""subnet-87654321""]  # Replace with your subnet IDs

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
    Name        = ""my-application-lb""
  }
}

# Create a target group
resource ""aws_lb_target_group"" ""app_tg"" {
  name        = ""my-target-group""
  port        = 80
  protocol    = ""HTTP""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID
  target_type = ""instance""

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Create a listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.app_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.app_tg.arn
  }
}

# Create security group for the ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""alb-security-group""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. A target group for the ALB
3. A listener on port 80
4. A security group for the ALB

Note: You'll need to replace the following placeholder values with your actual values:
- subnet IDs in the `subnets` list
- VPC ID
- Region (if different from us-west-2)

The ALB is configured as an internet-facing load balancer (internal = false) and has deletion protection disabled. The target group is configured for HTTP traffic on port 80 with standard health check settings. The security group allows inbound HTTP traffic (port 80) from anywhere and allows all outbound traffic.",,,,,,"I'll help you create a Terraform configuration for an AWS Application Load Balancer (ALB). This will include the ALB itself, a target group, and the necessary security group.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create ALB Security Group
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""alb-security-group""
  }
}

# Create Application Load Balancer
resource ""aws_lb"" ""application_lb"" {
  name               = ""application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [aws_subnet.public_1.id, aws_subnet.public_2.id]

  enable_deletion_protection = false

  tags = {
    Name = ""application-lb""
  }
}

# Create Target Group
resource ""aws_lb_target_group"" ""target_group"" {
  name     = ""alb-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = aws_vpc.main.id

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }

  tags = {
    Name = ""alb-target-group""
  }
}

# Create Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.application_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}
```

This configuration creates:
1. A security group for the ALB allowing HTTP and HTTPS traffic
2. An Application Load Balancer
3. A target group for the ALB
4. A listener on port 80

Note: This configuration assumes you have already defined:
- A VPC (aws_vpc.main)
- At least two public subnets (aws_subnet.public_1 and aws_subnet.public_2)

You may need to adjust the following according to your needs:
- VPC and subnet references
- Security group rules
- Port configurations
- Health check settings
- Tags

Also, consider adding HTTPS listener and SSL certificate if you need secure communications.",,,,,
"aws_db_instance, aws_db_instance, aws_elastic_beanstalk_application, aws_elastic_beanstalk_application, aws_elastic_beanstalk_environment, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment, aws_route53_record, aws_route53_record, aws_route53_zone","Configure multi-region application deployment with Route 53 resources to manage a domain and direct traffic to Elastic Beanstalk environments that have a connected RDS database. Provision the respective resources in either ""us-east-1"" or ""eu-west-1"" regions and name the aliases ""us-east-1"" and ""eu-west-1"" respectively. Name the environments ""myenv_us_east"" and ""myenv_eu_west"", the applications ""myapp_us_east"" and ""myapp_eu_west"", the instance profile ""eb_ec2_profile3"", and the databases ""main_db_us_east"" and ""main_db_eu_west""","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_eb_app = false

default is_valid_eb_env = false

default is_valid_db_instance = false

default is_valid_r53_zone = false

default is_valid_r53_record = false



# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.name
}

# Validate aws_route53_record
is_valid_r53_record {
        some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.provider_config_key == ""aws.us_east_1""
    resource.expressions.name
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.set_identifier
    resource.expressions.latency_routing_policy[0].region.constant_value == ""us-east-1""
    resource.expressions.records.references[0] == ""aws_elastic_beanstalk_environment.myenv_us_east.cname""
    resource.expressions.zone_id.references[0]
    
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_route53_record""    
    resource2.provider_config_key == ""aws.eu_west_1""
    resource2.expressions.name
    resource2.expressions.type
    resource2.expressions.ttl
    resource2.expressions.set_identifier
    resource2.expressions.latency_routing_policy[0].region.constant_value == ""eu-west-1""
    resource2.expressions.records.references[0] == ""aws_elastic_beanstalk_environment.myenv_eu_west.cname""
    resource2.expressions.zone_id.references[0]

}


is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0]
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0]
}

# Validate aws_eb_app
is_valid_eb_app {
        some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
    resource.provider_config_key == ""aws.us_east_1""
        
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_elastic_beanstalk_application""
    resource2.expressions.name
    resource2.provider_config_key == ""aws.eu_west_1""
}

# Validate aws_eb_env
is_valid_eb_env {
        some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.provider_config_key == ""aws.us_east_1""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.myapp_us_east.name""
    resource.expressions.solution_stack_name
    
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_elastic_beanstalk_environment""
    resource2.provider_config_key == ""aws.eu_west_1""
    resource2.expressions.name
    resource2.expressions.application.references[0] == ""aws_elastic_beanstalk_application.myapp_eu_west.name""
    resource2.expressions.solution_stack_name
    
            some a, b, c, d, e, f, g, h
    resource.expressions.setting[a].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile3.name""
    resource.expressions.setting[b].value.references[0] == ""aws_db_instance.main_db_us_east.username""
    resource.expressions.setting[c].value.references[0] == ""aws_db_instance.main_db_us_east.password""
    resource.expressions.setting[d].value.references[0] == ""aws_db_instance.main_db_us_east.address""
    
    resource2.expressions.setting[e].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile3.name""
    resource2.expressions.setting[f].value.references[0] == ""aws_db_instance.main_db_eu_west.username""
    resource2.expressions.setting[g].value.references[0] == ""aws_db_instance.main_db_eu_west.password""
    resource2.expressions.setting[h].value.references[0] == ""aws_db_instance.main_db_eu_west.address""

}

is_valid_db_instance {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_db_instance""
    has_required_main_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_main_db_arguments {
                some i, j
    resource := input.configuration.root_module.resources[i]
    resource2 := input.configuration.root_module.resources[j]
    resource.type == ""aws_db_instance""
    resource.provider_config_key == ""aws.us_east_1""
    resource2.type == ""aws_db_instance""
    resource2.provider_config_key == ""aws.eu_west_1""
    # Check for allocated_storage, engine, instance_class, username, password, and skip_final_snapshot
    requirement1(resource.expressions)
    requirement1(resource2.expressions)
    # Check for instance_class validity
    requirement2(resource.expressions)
    requirement2(resource2.expressions)
}



# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
    expressions.allocated_storage
    expressions.engine
    expressions.username
    expressions.password
    is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
    expressions.snapshot_identifier
}

# Check for instance_class validity
requirement2(expressions) {
    expressions.instance_class
    is_valid_instance_class(expressions.instance_class.constant_value)
}


# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
    is_valid_eb_app
    is_valid_eb_env
    is_valid_db_instance
    is_valid_r53_zone
    is_valid_r53_record
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""

      configuration_aliases = [ aws.us_east_1, aws.eu_west_1 ]
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  alias  = ""us_east_1""
  region = ""us-east-1""

  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

provider ""aws"" {
  alias  = ""eu_west_1""
  region = ""eu-west-1""

  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}


resource ""aws_iam_role"" ""eb_ec2_role"" {
  provider = aws.us_east_1
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  provider = aws.us_east_1
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}


# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  provider = aws.us_east_1
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# RDS in US East (N. Virginia)
resource ""aws_db_instance"" ""main_db_us_east"" {
  provider             = aws.us_east_1
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  username             = ""dbadmin""
  password             = ""mysecurepassword""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
  multi_az             = true
}

# Elastic Beanstalk Application in US East
resource ""aws_elastic_beanstalk_application"" ""myapp_us_east"" {
  provider = aws.us_east_1
  name     = ""my-application-us-east""
}

# Elastic Beanstalk Environment in US East
resource ""aws_elastic_beanstalk_environment"" ""myenv_us_east"" {
  provider            = aws.us_east_1
  name                = ""my-application-env-us-east""
  application         = aws_elastic_beanstalk_application.myapp_us_east.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.main_db_us_east.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.main_db_us_east.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.main_db_us_east.address
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# RDS in EU West (Ireland)
resource ""aws_db_instance"" ""main_db_eu_west"" {
  provider             = aws.eu_west_1
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  multi_az             = true
  username             = ""dbadmin""
  password             = ""mysecurepassword""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
}

# Elastic Beanstalk Application in EU West
resource ""aws_elastic_beanstalk_application"" ""myapp_eu_west"" {
  provider = aws.eu_west_1
  name     = ""my-application-eu-west""
}

# Elastic Beanstalk Environment in EU West
resource ""aws_elastic_beanstalk_environment"" ""myenv_eu_west"" {
  provider            = aws.eu_west_1
  name                = ""my-application-env-eu-west""
  application         = aws_elastic_beanstalk_application.myapp_eu_west.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.main_db_eu_west.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.main_db_eu_west.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.main_db_eu_west.address
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Main Route 53 Hosted Zone
resource ""aws_route53_zone"" ""main"" {
  provider = aws.us_east_1
  name = ""myapp.com""
}

# Latency Based Routing in Route 53 for US East Elastic Beanstalk Environment
resource ""aws_route53_record"" ""eb_env_us_east"" {
  provider = aws.us_east_1
  zone_id = aws_route53_zone.main.zone_id
  name    = ""us-east.myapp.com""
  type    = ""CNAME""
  ttl     = ""60""
  records = [aws_elastic_beanstalk_environment.myenv_us_east.cname]
  set_identifier = ""EastEnv""
  latency_routing_policy {
    region = ""us-east-1""
  }
}

resource ""aws_route53_record"" ""eb_env_eu_west"" {
  provider = aws.eu_west_1
  zone_id = aws_route53_zone.main.zone_id
  name    = ""eu-west.myapp.com""
  type    = ""CNAME""
  ttl     = ""60""
  records = [aws_elastic_beanstalk_environment.myenv_eu_west.cname]
  set_identifier = ""WestEnv""
  latency_routing_policy {
    region = ""eu-west-1""
  }
}
","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has two ""aws_elastic_beanstalk_application"" resource provisioned in different regions
    with ""name""

Has two ""aws_elastic_beanstalk_environment"" resources provisioned in different regions
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with a ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource
    with a ""setting"" block that has a ""value""  that references the ""address"" from one of the ""db_instance"" resource
    with a ""setting"" block that has a ""value""  that references the ""username"" from one of the ""db_instance"" resource 
    with a ""setting"" block that has a ""value""  that references the ""password"" from one of the ""db_instance"" resource 

Has two ""aws_db_instance"" resources provisioned in different regions
    with ""instance_class""
    with ""allocated_storage""
    with ""engine""
    with ""username""
    with ""password""
    with ""backup_retention_period"" set to a non-zero number
    with ""skip_final_snapshot""


Has one ""aws_route53_zone"" resource
    with ""name""

Has two ""aws_route53_record"" resources provisioned in different regions
    with ""name""
    with ""type""
    with ""ttl""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
    with ""set_identifier""
    with ""records"" referencing one of the ""aws_elastic_beanstalk_environment"" resource
    with ""latency_routing_policy"" containing ""region"" 


","I'll help you create a Terraform configuration for a multi-region application deployment with Route 53, Elastic Beanstalk, and RDS. I'll break this down into multiple provider configurations and resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# US East Provider
provider ""aws"" {
  alias  = ""us_east""
  region = ""us-east-1""
}

# EU West Provider
provider ""aws"" {
  alias  = ""eu_west""
  region = ""eu-west-1""
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  provider = aws.us_east
  name     = ""example.com""
}

# IAM Role for Elastic Beanstalk
resource ""aws_iam_role"" ""eb_ec2_role"" {
  provider = aws.us_east
  name     = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  provider = aws.us_east
  name     = ""eb_ec2_profile3""
  role     = aws_iam_role.eb_ec2_role.name
}

# US East Resources
resource ""aws_elastic_beanstalk_application"" ""us_east"" {
  provider = aws.us_east
  name     = ""myapp_us_east""
}

resource ""aws_db_instance"" ""us_east"" {
  provider          = aws.us_east
  identifier        = ""main-db-us-east""
  engine            = ""mysql""
  instance_class    = ""db.t3.micro""
  allocated_storage = 20
  db_name          = ""main_db_us_east""
  username         = ""admin""
  password         = ""password123""  # Use AWS Secrets Manager in production
}

resource ""aws_elastic_beanstalk_environment"" ""us_east"" {
  provider    = aws.us_east
  name        = ""myenv_us_east""
  application = aws_elastic_beanstalk_application.us_east.name
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# EU West Resources
resource ""aws_elastic_beanstalk_application"" ""eu_west"" {
  provider = aws.eu_west
  name     = ""myapp_eu_west""
}

resource ""aws_db_instance"" ""eu_west"" {
  provider          = aws.eu_west
  identifier        = ""main-db-eu-west""
  engine            = ""mysql""
  instance_class    = ""db.t3.micro""
  allocated_storage = 20
  db_name          = ""main_db_eu_west""
  username         = ""admin""
  password         = ""password123""  # Use AWS Secrets Manager in production
}

resource ""aws_elastic_beanstalk_environment"" ""eu_west"" {
  provider    = aws.eu_west
  name        = ""myenv_eu_west""
  application = aws_elastic_beanstalk_application.eu_west.name
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Route 53 Records
resource ""aws_route53_record"" ""us_east"" {
  provider = aws.us_east
  zone_id  = aws_",,,,,,"I'll help you create a Terraform configuration for a multi-region application deployment with Route 53, Elastic Beanstalk, and RDS. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# US East Provider
provider ""aws"" {
  alias  = ""us_east""
  region = ""us-east-1""
}

# EU West Provider
provider ""aws"" {
  alias  = ""eu_west""
  region = ""eu-west-1""
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  provider = aws.us_east
  name     = ""example.com""
}

# IAM Role for Elastic Beanstalk
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile3""
  role = aws_iam_role.eb_ec2_role.name
}

# US East Resources
resource ""aws_elastic_beanstalk_application"" ""us_east"" {
  provider = aws.us_east
  name     = ""myapp_us_east""
}

resource ""aws_db_instance"" ""us_east"" {
  provider          = aws.us_east
  identifier        = ""main-db-us-east""
  engine           = ""mysql""
  instance_class   = ""db.t3.micro""
  allocated_storage = 20
  username         = ""admin""
  password         = ""password123""  # Change in production
}

resource ""aws_elastic_beanstalk_environment"" ""us_east"" {
  provider    = aws.us_east
  name        = ""myenv_us_east""
  application = aws_elastic_beanstalk_application.us_east.name
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# EU West Resources
resource ""aws_elastic_beanstalk_application"" ""eu_west"" {
  provider = aws.eu_west
  name     = ""myapp_eu_west""
}

resource ""aws_db_instance"" ""eu_west"" {
  provider          = aws.eu_west
  identifier        = ""main-db-eu-west""
  engine           = ""mysql""
  instance_class   = ""db.t3.micro""
  allocated_storage = 20
  username         = ""admin""
  password         = ""password123""  # Change in production
}

resource ""aws_elastic_beanstalk_environment"" ""eu_west"" {
  provider    = aws.eu_west
  name        = ""myenv_eu_west""
  application = aws_elastic_beanstalk_application.eu_west.name
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Route 53 Records
resource ""aws_route53_record"" ""us_east"" {
  provider = aws.us_east
  zone_id  = aws_route53_zone.main.zone_id
  name     = ""us-east-1.example.com""
  type     = ""A""

  alias {
    name                   = aws_elastic_beanstalk_environment.us_east.cname
    zone_id                =",,,,,,"I'll help you create a Terraform configuration for a multi-region application deployment with Route 53, Elastic Beanstalk, and RDS. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# US East Provider
provider ""aws"" {
  region = ""us-east-1""
  alias  = ""us_east_1""
}

# EU West Provider
provider ""aws"" {
  region = ""eu-west-1""
  alias  = ""eu_west_1""
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  provider = aws.us_east_1
  name     = ""example.com""
}

# IAM Role for EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile3""
  role = aws_iam_role.eb_ec2_role.name
}

# US East Resources
resource ""aws_elastic_beanstalk_application"" ""us_east"" {
  provider = aws.us_east_1
  name     = ""myapp_us_east""
}

resource ""aws_db_instance"" ""us_east"" {
  provider          = aws.us_east_1
  identifier        = ""main-db-us-east""
  engine           = ""mysql""
  instance_class   = ""db.t3.micro""
  allocated_storage = 20
  username         = ""admin""
  password         = ""password123""
}

resource ""aws_elastic_beanstalk_environment"" ""us_east"" {
  provider    = aws.us_east_1
  name        = ""myenv_us_east""
  application = aws_elastic_beanstalk_application.us_east.name

  solution_stack_name = ""64bit Amazon Linux 2 v5.6.1 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# EU West Resources
resource ""aws_elastic_beanstalk_application"" ""eu_west"" {
  provider = aws.eu_west_1
  name     = ""myapp_eu_west""
}

resource ""aws_db_instance"" ""eu_west"" {
  provider          = aws.eu_west_1
  identifier        = ""main-db-eu-west""
  engine           = ""mysql""
  instance_class   = ""db.t3.micro""
  allocated_storage = 20
  username         = ""admin""
  password         = ""password123""
}

resource ""aws_elastic_beanstalk_environment"" ""eu_west"" {
  provider    = aws.eu_west_1
  name        = ""myenv_eu_west""
  application = aws_elastic_beanstalk_application.eu_west.name

  solution_stack_name = ""64bit Amazon Linux 2 v5.6.1 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Route 53 Records
resource ""aws_route53_record"" ""us_east"" {
  provider = aws.us_east_1
  zone_id  = aws_route53_zone.",,,,,
aws_lightsail_database,"create AWS Lightsail that creates a mysql database. It should allow daily backups to take place between 16:00 and 16:30 each day and  requires any maintiance tasks (anything that would cause an outage, including changing some attributes) to take place on Tuesdays between 17:00 and 17:30","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.bundle_id
    resource.values.preferred_backup_window == ""16:00-16:30""
    resource.values.preferred_maintenance_window == ""Tue:17:00-Tue:17:30""
    
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name     = ""test""
  availability_zone            = ""us-east-1a""
  master_database_name         = ""testdatabasename""
  master_password              = ""testdatabasepassword""
  master_username              = ""test""
  blueprint_id                 = ""mysql_8_0""
  bundle_id                    = ""micro_1_0""
  preferred_backup_window      = ""16:00-16:30""
  preferred_maintenance_window = ""Tue:17:00-Tue:17:30""
}","have one ""aws_lightsail_database"" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id
    with bundle_id
    with  preferred_backup_window    = ""16:00-16:30""
  preferred_maintenance_window = ""Tue:17:00-Tue:17:30""

    ","I'll help you create a Terraform configuration for an AWS Lightsail MySQL database with the specified backup and maintenance windows. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

resource ""aws_lightsail_database"" ""mysql_db"" {
  relational_database_name = ""my-mysql-database""
  availability_zone       = ""us-east-1a""  # Change to match your region
  master_database_name   = ""main_db""
  master_username        = ""admin""
  master_password        = ""YourSecurePassword123!""  # Change this to a secure password
  blueprint_id           = ""mysql_8_0""
  bundle_id             = ""micro_1_0""  # Smallest instance size, adjust as needed
  
  # Backup window: 16:00-16:30 UTC daily
  preferred_backup_window = ""16:00-16:30""
  
  # Maintenance window: Tuesdays 17:00-17:30 UTC
  preferred_maintenance_window = ""tue:17:00-tue:17:30""
  
  # Enable automated backups
  backup_retention_enabled = true
  
  # Additional optional configurations
  apply_immediately       = false
  publicly_accessible    = false
  
  tags = {
    Name        = ""mysql-database""
    Environment = ""production""
  }
}

# Optional: Create a static IP for the database
resource ""aws_lightsail_database_static_ip_attachment"" ""mysql_static_ip"" {
  static_ip_name          = aws_lightsail_static_ip.mysql_static_ip.id
  database_name           = aws_lightsail_database.mysql_db.id
}

resource ""aws_lightsail_static_ip"" ""mysql_static_ip"" {
  name = ""database-static-ip""
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a MySQL database in Lightsail with:
   - Daily backup window from 16:00 to 16:30
   - Maintenance window on Tuesdays from 17:00 to 17:30
   - Uses MySQL 8.0
   - Includes basic security settings
3. Optionally creates and attaches a static IP to the database

Key points to note:
- Remember to replace the master password with a secure value
- Adjust the region and availability zone as needed
- The bundle_id can be changed based on your performance needs
- The backup and maintenance windows are in UTC time
- Tags can be customized based on your needs

Make sure to handle the master password securely, preferably using AWS Secrets Manager or similar service in a production environment.",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail MySQL database with your specified backup and maintenance windows. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

resource ""aws_lightsail_database"" ""mysql_db"" {
  relational_database_name = ""my-mysql-database""
  availability_zone       = ""us-east-1a""  # Change to match your region
  master_database_name   = ""main_db""
  master_username       = ""admin""
  master_password       = ""YourStrongPassword123!""  # Change this to a secure password
  blueprint_id          = ""mysql_8_0""
  bundle_id            = ""micro_1_0""  # Smallest instance size, adjust as needed
  
  # Backup window: 16:00-16:30 UTC daily
  preferred_backup_window = ""16:00-16:30""
  
  # Maintenance window: Tuesdays 17:00-17:30 UTC
  preferred_maintenance_window = ""tue:17:00-tue:17:30""
  
  # Enable automated backups
  backup_retention_enabled = true
  
  # Configure other optional settings
  apply_immediately = false
  publicly_accessible = false
  
  tags = {
    Name = ""mysql-lightsail-db""
    Environment = ""production""
  }
}

# Optional: Create a static IP for the database
resource ""aws_lightsail_database_public_ip"" ""mysql_db_ip"" {
  relational_database_name = aws_lightsail_database.mysql_db.id
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a MySQL 8.0 database in Lightsail
3. Configures daily backups between 16:00-16:30 UTC
4. Sets maintenance window for Tuesdays 17:00-17:30 UTC
5. Enables automated backups
6. Includes optional static IP allocation

Important notes:
- Remember to replace the master password with a secure value
- Adjust the availability zone to match your desired region
- The bundle_id can be changed based on your performance needs
- You may want to store sensitive information like passwords in variables or secrets management
- The maintenance and backup windows are in UTC time, adjust accordingly for your timezone

Make sure to test this in a non-production environment first and adjust the configuration according to your specific needs.",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail MySQL database with the specified backup and maintenance windows. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change this to your desired region
}

resource ""aws_lightsail_database"" ""mysql_db"" {
  relational_database_name = ""my-mysql-database""
  availability_zone       = ""us-east-1a""  # Change this to match your region
  master_database_name   = ""main_db""
  master_username       = ""admin""
  master_password       = ""YourSecurePassword123!""  # Change this to a secure password
  blueprint_id          = ""mysql_8_0""
  bundle_id            = ""micro_1_0""  # Smallest instance size, adjust as needed
  
  # Backup window: 16:00-16:30 daily (UTC)
  preferred_backup_window = ""16:00-16:30""
  
  # Maintenance window: Tuesday 17:00-17:30 (UTC)
  preferred_maintenance_window = ""tue:17:00-tue:17:30""
  
  # Enable automated backups
  backup_retention_enabled = true
  
  # Additional optional configurations
  apply_immediately = false  # Set to true if you want changes to apply immediately
  skip_final_snapshot = true  # Set to false if you want a final snapshot when deleting
  
  tags = {
    Name = ""mysql-lightsail-db""
    Environment = ""production""
  }
}

# Optional: Create a static IP for the database
resource ""aws_lightsail_database_public_ip"" ""mysql_db_ip"" {
  relational_database_name = aws_lightsail_database.mysql_db.name
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a MySQL 8.0 database in AWS Lightsail
3. Configures daily backups between 16:00-16:30 UTC
4. Sets maintenance window for Tuesdays between 17:00-17:30 UTC
5. Enables automated backups
6. Includes optional public IP allocation

Key features:
- Uses MySQL 8.0 as the database engine
- Uses the micro bundle (smallest size, adjust bundle_id as needed)
- Sets backup and maintenance windows as specified
- Includes basic security and configuration options

Remember to:
1. Change the region and availability zone to match your needs
2. Set a secure master password
3. Adjust the bundle_id based on your performance needs
4. Consider whether you want to make the database publicly accessible

Note: Times are in UTC, so you may need to adjust the backup and maintenance windows based on your local timezone.",,,,,
"aws_cloudwatch_log_group, aws_msk_cluster, aws_security_group, aws_subnet, aws_subnet, aws_subnet, aws_vpc, aws_availability_zones",Create a managed MSK cluster in us-east1 logging to a CloudWatch group.,"package main

import future.keywords.in

default allow = false

# Check if any MSK cluster is being created
msk_cluster_created(resources) {
some resource in resources
resource.type == ""aws_msk_cluster""
resource.change.actions[_] == ""create""
}

# Check if CloudWatch logging is enabled for broker logs
cloudwatch_logging_enabled(resource) {
resource.type == ""aws_msk_cluster""
resource.change.after.logging_info[_].broker_logs[_].cloudwatch_logs[_].enabled == true
}

# Aggregate all checks
allow {
msk_cluster_created(input.resource_changes)
some resource in input.resource_changes
cloudwatch_logging_enabled(resource)
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""vpc"" {
    cidr_block = ""192.168.0.0/22""
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

resource ""aws_subnet"" ""subnet_az1"" {
    availability_zone = data.aws_availability_zones.azs.names[0]
    cidr_block = ""192.168.0.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az2"" {
    availability_zone = data.aws_availability_zones.azs.names[1]
    cidr_block = ""192.168.1.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az3"" {
    availability_zone = data.aws_availability_zones.azs.names[2]
    cidr_block = ""192.168.2.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.vpc.id
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }
}

resource ""aws_route_table_association"" ""first"" {
  subnet_id      = aws_subnet.subnet_az1.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""second"" {
  subnet_id      = aws_subnet.subnet_az2.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""third"" {
  subnet_id      = aws_subnet.subnet_az3.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_security_group"" ""sg"" {
    vpc_id = aws_vpc.vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4 = ""0.0.0.0/0""
  ip_protocol = ""-1""
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  ip_protocol       = ""-1""
}

resource ""aws_cloudwatch_log_group"" ""test"" {
    name = ""msk_broker_logs""
}

resource ""aws_msk_cluster"" ""example"" {
    cluster_name = ""example""
    kafka_version = ""3.2.0""
    number_of_broker_nodes = 3

    broker_node_group_info {
        instance_type = ""kafka.t3.small""
        client_subnets = [
            aws_subnet.subnet_az1.id,
            aws_subnet.subnet_az2.id,
            aws_subnet.subnet_az3.id,
        ]

        storage_info {
            ebs_storage_info {
                volume_size = 1000
            }
        }

        security_groups = [aws_security_group.sg.id]
    }

    open_monitoring {
        prometheus {
            jmx_exporter {
                enabled_in_broker = true
            }
            node_exporter {
                enabled_in_broker = true
            }
        }
    }

    logging_info {
        broker_logs {
            cloudwatch_logs {
                enabled = true
                log_group = aws_cloudwatch_log_group.test.name
            }
            firehose {
                enabled = false
            }
            s3 {
                enabled = false
            }
        }
    }
}

output ""zookeeper_connect_string"" {
    value = aws_msk_cluster.example.zookeeper_connect_string
}

output ""bootstrap_brokers_tls"" {
    description = ""TLS connection host:port pairs""
    value = aws_msk_cluster.example.bootstrap_brokers_tls
}","Has one ""aws_msk_cluster"" resource
with one ""logging_info""
with one ""broker_logs""
with one ""cloudwatch_logs""
with one ""enabled"" = true","I'll help you create a Terraform configuration for an AWS MSK (Managed Streaming for Apache Kafka) cluster with CloudWatch logging. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""msk_log_group"" {
  name              = ""/aws/msk/cluster/msk-cluster-logs""
  retention_in_days = 30
}

# Create MSK Configuration
resource ""aws_msk_configuration"" ""msk_config"" {
  kafka_versions = [""2.8.1""]
  name          = ""msk-configuration""

  server_properties = <<PROPERTIES
auto.create.topics.enable=true
delete.topic.enable=true
PROPERTIES
}

# Create Security Group for MSK
resource ""aws_security_group"" ""msk_sg"" {
  name        = ""msk-security-group""
  description = ""Security group for MSK cluster""
  vpc_id      = ""vpc-xxxxx"" # Replace with your VPC ID

  ingress {
    from_port   = 9092
    to_port     = 9092
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""] # Replace with your CIDR block
  }
}

# Create MSK Cluster
resource ""aws_msk_cluster"" ""msk_cluster"" {
  cluster_name           = ""msk-cluster""
  kafka_version         = ""2.8.1""
  number_of_broker_nodes = 3

  broker_node_group_info {
    instance_type   = ""kafka.t3.small""
    client_subnets  = [""subnet-xxxxx"", ""subnet-yyyyy"", ""subnet-zzzzz""] # Replace with your subnet IDs
    security_groups = [aws_security_group.msk_sg.id]
    storage_info {
      ebs_storage_info {
        volume_size = 100
      }
    }
  }

  encryption_info {
    encryption_at_rest_kms_key_arn = ""arn:aws:kms:us-east-1:123456789012:key/xxxxx"" # Replace with your KMS key ARN
  }

  logging_info {
    broker_logs {
      cloudwatch_logs {
        enabled   = true
        log_group = aws_cloudwatch_log_group.msk_log_group.name
      }
    }
  }

  tags = {
    Environment = ""production""
  }
}

# Output the MSK cluster ARN
output ""msk_cluster_arn"" {
  value = aws_msk_cluster.msk_cluster.arn
}
```

This configuration:
1. Creates a CloudWatch Log Group for MSK logs
2. Sets up an MSK configuration with basic Kafka properties
3. Creates a security group for the MSK cluster
4. Creates an MSK cluster with:
   - 3 broker nodes
   - CloudWatch logging enabled
   - Encryption at rest
   - Proper networking configuration

Note: You'll need to replace the following placeholder values with your actual values:
- vpc-xxxxx: Your VPC ID
- subnet-xxxxx, subnet-yyyyy, subnet-zzzzz: Your subnet IDs
- KMS key ARN
- CIDR blocks in the security group

The cluster will be created in the us-east-1 region as specified in the provider block. Make sure you have sufficient permissions and the necessary networking infrastructure in place before applying this configuration.",,,,,,"I'll help you create a Terraform configuration for an AWS MSK (Managed Streaming for Apache Kafka) cluster with CloudWatch logging enabled. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""msk_log_group"" {
  name              = ""/aws/msk/cluster/msk-cluster-logs""
  retention_in_days = 30
}

# Create MSK Configuration
resource ""aws_msk_configuration"" ""msk_config"" {
  kafka_versions = [""2.8.1""]
  name          = ""msk-config""

  server_properties = <<PROPERTIES
auto.create.topics.enable=true
delete.topic.enable=true
PROPERTIES
}

# Create Security Group for MSK
resource ""aws_security_group"" ""msk_sg"" {
  name        = ""msk-security-group""
  description = ""Security group for MSK cluster""
  vpc_id      = ""vpc-12345678"" # Replace with your VPC ID

  ingress {
    from_port   = 9092
    to_port     = 9092
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""] # Replace with your CIDR block
  }
}

# Create MSK Cluster
resource ""aws_msk_cluster"" ""msk_cluster"" {
  cluster_name           = ""msk-cluster""
  kafka_version         = ""2.8.1""
  number_of_broker_nodes = 3

  broker_node_group_info {
    instance_type   = ""kafka.t3.small""
    client_subnets  = [""subnet-12345678"", ""subnet-87654321"", ""subnet-11223344""] # Replace with your subnet IDs
    security_groups = [aws_security_group.msk_sg.id]
    storage_info {
      ebs_storage_info {
        volume_size = 100
      }
    }
  }

  encryption_info {
    encryption_in_transit {
      client_broker = ""TLS""
      in_cluster    = true
    }
  }

  logging_info {
    broker_logs {
      cloudwatch_logs {
        enabled   = true
        log_group = aws_cloudwatch_log_group.msk_log_group.name
      }
    }
  }

  tags = {
    Environment = ""production""
  }
}

# Output the Bootstrap Brokers
output ""bootstrap_brokers_tls"" {
  description = ""TLS connection host:port pairs""
  value       = aws_msk_cluster.msk_cluster.bootstrap_brokers_tls
}
```

This configuration:
1. Creates a CloudWatch Log Group for MSK logs
2. Sets up an MSK configuration with basic properties
3. Creates a security group for the MSK cluster
4. Creates an MSK cluster with:
   - 3 broker nodes
   - TLS encryption
   - CloudWatch logging enabled
   - T3.small instances
   - 100GB EBS storage per broker

Note: You'll need to replace the following placeholder values with your actual values:
- vpc_id
- client_subnets
- CIDR blocks in the security group
- Adjust the kafka_version, instance_type, and other parameters as needed for your use case

The configuration includes all necessary security settings and follows AWS best practices for MSK cluster deployment.",,,,,,"I'll help you create a Terraform configuration for an AWS MSK (Managed Streaming for Apache Kafka) cluster with CloudWatch logging enabled. Here's a comprehensive solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""msk_log_group"" {
  name              = ""/aws/msk/cluster/msk-cluster-logs""
  retention_in_days = 30
}

# Create MSK Configuration
resource ""aws_msk_configuration"" ""msk_config"" {
  kafka_versions = [""2.8.1""]
  name          = ""msk-cluster-config""

  server_properties = <<PROPERTIES
auto.create.topics.enable=true
delete.topic.enable=true
PROPERTIES
}

# Create MSK Cluster
resource ""aws_msk_cluster"" ""msk_cluster"" {
  cluster_name           = ""msk-cluster""
  kafka_version         = ""2.8.1""
  number_of_broker_nodes = 3

  broker_node_group_info {
    instance_type   = ""kafka.t3.small""
    client_subnets  = [""subnet-12345678"", ""subnet-87654321"", ""subnet-11223344""] # Replace with your subnet IDs
    security_groups = [""sg-12345678""] # Replace with your security group ID
    
    storage_info {
      ebs_storage_info {
        volume_size = 100
      }
    }
  }

  encryption_info {
    encryption_in_transit {
      client_broker = ""TLS""
      in_cluster    = true
    }
  }

  logging_info {
    broker_logs {
      cloudwatch_logs {
        enabled   = true
        log_group = aws_cloudwatch_log_group.msk_log_group.name
      }
    }
  }

  tags = {
    Environment = ""production""
  }
}

# Output the Bootstrap Brokers
output ""bootstrap_brokers_tls"" {
  description = ""TLS connection host:port pairs""
  value       = aws_msk_cluster.msk_cluster.bootstrap_brokers_tls
}
```

This configuration:

1. Sets up the AWS provider for us-east-1
2. Creates a CloudWatch Log Group for MSK logs
3. Creates an MSK configuration with some basic Kafka properties
4. Creates an MSK cluster with:
   - 3 broker nodes
   - TLS encryption
   - CloudWatch logging enabled
   - EBS storage of 100GB per broker
   - T3.small instances for the brokers

Note: You'll need to replace the following placeholder values with your actual values:
- subnet IDs in the client_subnets array
- security group ID in the security_groups array

The configuration also outputs the bootstrap brokers' connection string for TLS connections.

Remember to ensure that:
1. The subnets specified are in different Availability Zones
2. The security group allows the necessary Kafka ports (9092 for plaintext, 9094 for TLS)
3. Your AWS credentials have the necessary permissions to create these resources",,,,,
"aws_backup_plan, aws_backup_selection, aws_backup_vault, aws_iam_role, aws_iam_role_policy_attachment, aws_instance, aws_iam_policy_document","An AWS service that provisions an EC2 instance and backs it up every day at midnight. Accomplish this using the following services: AWS EC2, AWS Backup. Add any necessary services.","package cloud_desktop_high

import rego.v1

default valid := false

instance_valid(instance, ami) if {
        instance.expressions.instance_type
        ami.address in instance.expressions.ami.references
}

backup_plan_valid(backup_plan, backup_vault) if {
        # advanced_backup_setting
        some backup_setting in backup_plan.expressions.advanced_backup_setting
        backup_setting.backup_options

        backup_setting.resource_type.constant_value == ""EC2""

        # rule
        some rule in backup_plan.expressions.rule
        backup_vault.address in rule.target_vault_name.references
        rule.schedule.constant_value == ""cron(0 0 * * ? *)""
}

backup_selection_valid(backup_selection, backup_plan, instance) if {
        backup_plan.address in backup_selection.expressions.plan_id.references
        instance.address in backup_selection.expressions.resources.references
}

valid if {
        resources := input.configuration.root_module.resources
        some backup_plan in resources
        backup_plan.type == ""aws_backup_plan""
        some backup_vault in resources
        backup_vault.type == ""aws_backup_vault""
        some backup_selection in resources
        backup_selection.type == ""aws_backup_selection""
        some instance in resources
        instance.type == ""aws_instance""
        some ami in resources
        ami.type == ""aws_ami""

        instance_valid(instance, ami)
        backup_plan_valid(backup_plan, backup_vault)
        backup_selection_valid(backup_selection, backup_plan, instance)
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_ami"" ""amzn2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}


resource ""aws_instance"" ""cloud_desktop"" {
  instance_type = ""t2.micro""
  ami = data.aws_ami.amzn2.id
}

resource ""aws_backup_plan"" ""cloud_desktop_backup"" {
  name = ""cloud_desktop_backup_plan""
  rule {
    rule_name         = ""cloud_desktop_backup_rule"" # required, not specified
    target_vault_name = aws_backup_vault.valut.name # required, not specified
    schedule          = ""cron(0 0 * * ? *)""
  }

  advanced_backup_setting {
    backup_options = {
      WindowsVSS = ""enabled""
    }
    resource_type = ""EC2""
  }
}

resource ""aws_backup_vault"" ""valut"" {
  name = ""cloud_desktop_backup_valut""
}

resource ""aws_backup_selection"" ""backup_ec2"" {
  iam_role_arn = aws_iam_role.backup.arn

  name    = ""backup_ec2""
  plan_id = aws_backup_plan.cloud_desktop_backup.id

  resources = [
    aws_instance.cloud_desktop.arn,
  ]
}

data ""aws_iam_policy_document"" ""backup_assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""backup.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""backup"" {
  name               = ""backup""
  assume_role_policy = data.aws_iam_policy_document.backup_assume_role.json
}

resource ""aws_iam_role_policy_attachment"" ""backup"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup""
  role       = aws_iam_role.backup.name
}","have one ""aws_ami"" data resource
have one ""aws_instance"" resource
  with ""ami"" argument linking to the aws_ami data resource
  with ""instance_type"" argument
have one ""aws_backup_plan"" resource
  with ""name"" argument
  with ""rule"" argument block
    with ""rule_name"" argument
    with ""target_vault_name"" argument linking to backup_vault
    with ""schedule"" argument with value ""cron(0 0 * * ? *)""
  with ""advanced_backup_setting"" argument block
    with ""backup_options"" argument block
      with ""WindowsVSS"" argument
    with ""resource_type"" argument with value ""EC2""
have one ""aws_backup_vault"" resource
  with ""name"" argument
have one ""aws_backup_selection"" resource
  with ""iam_role_arn"" argument
  with ""name"" argument
  with ""plan_id"" argument linking to aws_backup_plan resource
  with ""resources"" argument including the aws_instance resource","I'll help you create a Terraform configuration that provisions an EC2 instance and sets up AWS Backup to create daily backups at midnight. Here's the solution:

```hcl
# Configure AWS Provider
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create VPC and networking components
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

resource ""aws_subnet"" ""main"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""main-subnet""
  }
}

# Create EC2 instance
resource ""aws_instance"" ""example"" {
  ami           = ""ami-0c55b159cbfafe1f0"" # Replace with desired AMI ID
  instance_type = ""t2.micro""
  subnet_id     = aws_subnet.main.id

  tags = {
    Name = ""example-instance""
  }
}

# Create AWS Backup vault
resource ""aws_backup_vault"" ""example"" {
  name = ""example-backup-vault""
}

# Create AWS Backup plan
resource ""aws_backup_plan"" ""example"" {
  name = ""example-backup-plan""

  rule {
    rule_name         = ""daily_backup""
    target_vault_name = aws_backup_vault.example.name
    schedule          = ""cron(0 0 * * ? *)"" # Runs at midnight daily

    lifecycle {
      delete_after = 14 # Keep backups for 14 days
    }
  }
}

# Create IAM role for AWS Backup
resource ""aws_iam_role"" ""backup_role"" {
  name = ""backup-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""backup.amazonaws.com""
        }
      }
    ]
  })
}

# Attach AWS Backup service role policy
resource ""aws_iam_role_policy_attachment"" ""backup_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup""
  role       = aws_iam_role.backup_role.name
}

# Create AWS Backup selection
resource ""aws_backup_selection"" ""example"" {
  name         = ""example-backup-selection""
  plan_id      = aws_backup_plan.example.id
  iam_role_arn = aws_iam_role.backup_role.arn

  resources = [
    aws_instance.example.arn
  ]
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a basic VPC and subnet for the EC2 instance
3. Provisions an EC2 instance
4. Creates an AWS Backup vault to store the backups
5. Sets up a backup plan that runs daily at midnight
6. Creates necessary IAM roles and permissions for AWS Backup
7. Creates a backup selection that targets the EC2 instance

Make sure to:
- Replace the AMI ID with one that's available in your region
- Adjust the backup retention period (currently set to 14 days) as needed
- Modify the instance type and networking configuration according to your requirements

The backup will run automatically every day at midnight (UTC) and store the backups in the created vault.",,,,,,"I'll help you create a Terraform configuration that provisions an EC2 instance and sets up AWS Backup to create daily backups at midnight. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create a VPC for the EC2 instance
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main""
  }
}

# Create a subnet in the VPC
resource ""aws_subnet"" ""main"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""main""
  }
}

# Create EC2 instance
resource ""aws_instance"" ""example"" {
  ami           = ""ami-0c55b159cbfafe1f0"" # Replace with your desired AMI
  instance_type = ""t2.micro""
  subnet_id     = aws_subnet.main.id

  tags = {
    Name = ""example-instance""
  }
}

# Create AWS Backup vault
resource ""aws_backup_vault"" ""example"" {
  name = ""example-backup-vault""
}

# Create AWS Backup plan
resource ""aws_backup_plan"" ""example"" {
  name = ""example-backup-plan""

  rule {
    rule_name         = ""daily_backup""
    target_vault_name = aws_backup_vault.example.name
    schedule          = ""cron(0 0 * * ? *)"" # Midnight every day

    lifecycle {
      delete_after = 14 # Keep backups for 14 days
    }
  }
}

# Create IAM role for AWS Backup
resource ""aws_iam_role"" ""backup_role"" {
  name = ""backup-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""backup.amazonaws.com""
        }
      }
    ]
  })
}

# Attach AWS Backup service role policy
resource ""aws_iam_role_policy_attachment"" ""backup_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup""
  role       = aws_iam_role.backup_role.name
}

# Create AWS Backup selection
resource ""aws_backup_selection"" ""example"" {
  name         = ""example-backup-selection""
  iam_role_arn = aws_iam_role.backup_role.arn
  plan_id      = aws_backup_plan.example.id

  resources = [
    aws_instance.example.arn
  ]
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a basic VPC and subnet for the EC2 instance
3. Provisions an EC2 instance
4. Creates an AWS Backup vault to store the backups
5. Creates a backup plan that runs daily at midnight
6. Sets up the necessary IAM role and permissions for AWS Backup
7. Creates a backup selection that targets the EC2 instance

The backup will occur every day at midnight (UTC) and retain backups for 14 days. You can modify the schedule using cron expressions and adjust the retention period as needed.

Remember to:
- Replace the AMI ID with a valid one for your region
- Adjust the instance type if needed
- Modify the backup retention period as per your requirements
- Add any additional security groups or networking configurations as needed",,,,,,"I'll help you create a Terraform configuration that provisions an EC2 instance and sets up AWS Backup to create daily backups at midnight. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create VPC and networking components
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

resource ""aws_subnet"" ""main"" {
  vpc_id                  = aws_vpc.main.id
  cidr_block              = ""10.0.1.0/24""
  availability_zone       = ""us-west-2a""
  map_public_ip_on_launch = true

  tags = {
    Name = ""main-subnet""
  }
}

# EC2 Instance
resource ""aws_instance"" ""example"" {
  ami           = ""ami-0c55b159cbfafe1f0"" # Replace with desired AMI
  instance_type = ""t2.micro""
  subnet_id     = aws_subnet.main.id

  tags = {
    Name = ""example-instance""
  }
}

# AWS Backup vault
resource ""aws_backup_vault"" ""example"" {
  name = ""example-backup-vault""
}

# AWS Backup plan
resource ""aws_backup_plan"" ""example"" {
  name = ""example-backup-plan""

  rule {
    rule_name         = ""daily_backup""
    target_vault_name = aws_backup_vault.example.name
    schedule          = ""cron(0 0 * * ? *)"" # Midnight every day

    lifecycle {
      delete_after = 14 # Keep backups for 14 days
    }
  }
}

# IAM role for AWS Backup
resource ""aws_iam_role"" ""backup_role"" {
  name = ""backup-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""backup.amazonaws.com""
        }
      }
    ]
  })
}

# Attach AWS Backup service role policy
resource ""aws_iam_role_policy_attachment"" ""backup_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup""
  role       = aws_iam_role.backup_role.name
}

# AWS Backup selection
resource ""aws_backup_selection"" ""example"" {
  name         = ""example-backup-selection""
  iam_role_arn = aws_iam_role.backup_role.arn
  plan_id      = aws_backup_plan.example.id

  resources = [
    aws_instance.example.arn
  ]
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a basic VPC and subnet for the EC2 instance
3. Provisions an EC2 instance
4. Creates an AWS Backup vault to store the backups
5. Defines a backup plan that runs daily at midnight
6. Creates the necessary IAM role and attaches the required policy for AWS Backup
7. Creates a backup selection that targets the EC2 instance

The backup will run automatically every day at midnight (UTC) and keep backups for 14 days. You can modify the schedule and retention period by adjusting the `schedule` and `delete_after` values in the backup plan.

Remember to:
- Replace the AMI ID with one that's appropriate for your region and use case
- Adjust the VPC and subnet CIDR blocks if needed
- Modify the instance type based on your requirements
- Adjust the backup retention period as needed",,,,,
"aws_lex_bot, aws_lex_intent",create an aws_lex_bot,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for multiple aws_lex_intent resources
has_valid_lex_intents {
    count([intent | intent := input.planned_values.root_module.resources[_]; intent.type == ""aws_lex_intent""; intent.values.name; intent.values.fulfillment_activity])
}

# Rule for aws_lex_bot resource with specific arguments
has_valid_lex_bot_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lex_bot""
    is_boolean(resource.values.child_directed)
    resource.values.clarification_prompt
    resource.values.abort_statement
    resource.values.name
    # is_boolean(resource.values.create_version)
    # resource.values.description
    # is_boolean(resource.values.detect_sentiment)
    # resource.values.idle_session_ttl_in_seconds
    # resource.values.process_behavior
    # resource.values.voice_id
    # resource.values.locale
    # is_boolean(resource.values.enable_model_improvements)
    # resource.values.nlu_intent_confidence_threshold
}

# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lex_intents
    has_valid_lex_bot_instance
}
",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_lex_intent"" ""order_flowers"" {
  name = ""OrderFlowers""
  description = ""Intent to order a bouquet of flowers for pick up""

  sample_utterances = [
    ""I would like to pick up flowers"",
    ""I would like to order some flowers"",
  ]

  slot {
    name                     = ""FlowerType""
    description              = ""Type of flower to order""
    slot_constraint          = ""Required"" 
    slot_type                = ""AMAZON.AlphaNumeric""
    priority                 = 1

    sample_utterances = [""I would like to get {FlowerType} flower.""]

    value_elicitation_prompt {
      message {
        content             = ""What type of flower would you like?""
        content_type        = ""PlainText""
      }
      max_attempts         = 2
    }
  }

  slot {
    name                     = ""PickupDate""
    description              = ""Date of flower pickup""
    slot_constraint          = ""Required"" 
    slot_type                = ""AMAZON.DATE""
    priority                 = 2

    value_elicitation_prompt {
      message {
        content             = ""When would you like to pickup your flower?""
        content_type        = ""PlainText""
      }
      max_attempts         = 2
    }
  }

  confirmation_prompt {
    max_attempts = 2

    message {
      content      = ""Okay, your {FlowerType} will be ready for pickup on {PickupDate}.  Does this sound okay?""
      content_type = ""PlainText""
    }
  }

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  rejection_statement {
    message {
      content      = ""Okay, I will not place your order.""
      content_type = ""PlainText""
    }
  }
}

resource ""aws_lex_bot"" ""order_flowers"" {
  name     = ""OrderFlowers""
  description = ""Bot to order flowers on the behalf of a user""
  locale   = ""en-US""
  process_behavior = ""BUILD""
  voice_id = ""Salli""
  child_directed = false
  create_version = false
  detect_sentiment            = false
  idle_session_ttl_in_seconds = 600

  abort_statement {
    message {
      content_type = ""PlainText""
      content      = ""Sorry, I am not able to assist at this time""
    }
  }

  clarification_prompt {
    max_attempts = 2

    message {
      content_type = ""PlainText""
      content      = ""I didn't understand you, what would you like to do?""
    }
  }
  
  intent {
    intent_name    = aws_lex_intent.order_flowers.name
    intent_version = aws_lex_intent.order_flowers.version
  }
}","Has one resource ""aws_lex_bot"" and one resource ""aws_lex_intent""","I'll help you create a Terraform configuration for an AWS Lex bot. Here's a code solution that includes the required provider and creates a basic Lex bot:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust the region as needed
}

resource ""aws_lex_bot"" ""example_bot"" {
  name = ""ExampleBot""
  description = ""An example Lex bot created with Terraform""
  
  abort_statement {
    message {
      content      = ""Sorry, I'm having trouble understanding. Could you try again?""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, what would you like to do?""
      content_type = ""PlainText""
    }
  }

  child_directed = false
  
  idle_session_ttl_in_seconds = 300
  
  process_behavior = ""BUILD""
  voice_id        = ""Salli""

  locale = ""en-US""

  create_version = false

  enable_model_improvements = true
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Lex bot with basic configuration including:
   - Name and description
   - Abort statement for when the bot can't proceed
   - Clarification prompt for when the bot needs more information
   - Session timeout settings
   - Voice settings
   - Locale configuration
   - Model improvement settings

You can customize the values according to your specific needs. Remember to add intents and slots as needed for your specific use case.",,,,,,"I'll help you create a Terraform configuration for an AWS Lex bot. I'll include the required AWS provider and create a basic Lex bot configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust the region as needed
}

resource ""aws_lex_bot"" ""example_bot"" {
  name = ""ExampleBot""
  description = ""An example Lex bot created with Terraform""
  
  abort_statement {
    message {
      content      = ""Sorry, I am not able to assist at this time""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, could you please rephrase that?""
      content_type = ""PlainText""
    }
  }

  child_directed = false

  idle_session_ttl_in_seconds = 300

  process_behavior = ""BUILD""
  voice_id        = ""Salli""

  intent {
    intent_name    = ""HelloIntent""
    intent_version = ""1""
  }

  detect_sentiment = false

  enable_model_improvements = true
  nlu_intent_confidence_threshold = 0.40

  locale = ""en-US""
}
```

This configuration creates a basic AWS Lex bot with the following features:
- A name and description
- An abort statement for when the conversation needs to end
- A clarification prompt for when the bot doesn't understand the input
- Basic session management settings
- A simple intent configuration
- English (US) locale
- Voice settings using ""Salli"" as the voice ID
- Sentiment detection disabled
- Model improvements enabled with a confidence threshold

You can customize this further by:
1. Adding more intents
2. Modifying the messages and prompts
3. Adjusting the confidence threshold
4. Changing the voice ID
5. Modifying the session timeout
6. Adding additional language support

Remember to replace the region with your desired AWS region and adjust the bot settings according to your specific needs.",,,,,,"I'll help you create a Terraform configuration for an AWS Lex bot. Here's a configuration that includes the required provider and creates a basic Lex bot:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust the region as needed
}

resource ""aws_lex_bot"" ""example_bot"" {
  name = ""example_chatbot""
  description = ""An example Lex chatbot""
  
  abort_statement {
    message {
      content      = ""Sorry, I'm having trouble understanding. Could you try again?""
      content_type = ""PlainText""
    }
  }

  child_directed = false

  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, could you please rephrase that?""
      content_type = ""PlainText""
    }
  }

  idle_session_ttl_in_seconds = 300

  process_behavior = ""BUILD""
  voice_id        = ""Salli""

  locale = ""en-US""

  create_version = false
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lex bot with basic configuration including:
   - Name and description
   - Abort statement for when the bot can't proceed
   - Child directed flag (COPPA compliance)
   - Clarification prompt for when the bot needs clarification
   - Session timeout
   - Processing behavior and voice settings
   - Locale settings

You can customize the values according to your specific needs. Remember to add intents and slots as needed for your specific use case.",,,,,
"aws_iam_role, aws_sagemaker_code_repository, aws_sagemaker_notebook_instance, aws_iam_policy_document, aws_sagemaker_prebuilt_ecr_image","Create an aws_sagemaker_notebook instance from ""https://github.com/hashicorp/terraform-provider-aws.git""","package main

import future.keywords.in

default allow = false

aws_iam_role_valid(resources) {
    some resource in resources
    resource.type == ""aws_iam_role""
}

aws_sagemaker_code_repository_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_code_repository""
    resource.change.after.git_config[0].repository_url ==  ""https://github.com/hashicorp/terraform-provider-aws.git""
}

aws_sagemaker_notebook_instance_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_notebook_instance""
    resource.change.after.default_code_repository != null
}

# Aggregate all checks
allow {
    aws_iam_role_valid(input.resource_changes)
    aws_sagemaker_code_repository_valid(input.resource_changes)
    aws_sagemaker_notebook_instance_valid(input.resource_changes)
}",3,"resource ""aws_sagemaker_code_repository"" ""example"" {
  code_repository_name = ""my-notebook-instance-code-repo""

  git_config {
    repository_url = ""https://github.com/hashicorp/terraform-provider-aws.git""
  }
}

resource ""aws_sagemaker_notebook_instance"" ""ni"" {
  name                    = ""my-notebook-instance""
  role_arn                = aws_iam_role.example.arn
  instance_type           = ""ml.t2.medium""
  default_code_repository = aws_sagemaker_code_repository.example.code_repository_name
}

resource ""aws_iam_role"" ""example"" {
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""sagemaker.amazonaws.com""]
    }
  }
}

data ""aws_sagemaker_prebuilt_ecr_image"" ""test"" {
  repository_name = ""kmeans""
}",,"I'll help you create a Terraform configuration for an AWS SageMaker notebook instance. I'll include the required provider block and all necessary configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-notebook-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""sagemaker_policy"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

resource ""aws_sagemaker_notebook_instance"" ""notebook"" {
  name                    = ""terraform-notebook-instance""
  role_arn               = aws_iam_role.sagemaker_role.arn
  instance_type          = ""ml.t2.medium""
  platform_identifier    = ""notebook-al2-v1""
  
  # Optional configurations
  volume_size            = 50
  subnet_id              = null  # Specify if you want to launch in a VPC
  security_groups        = []    # Specify if you want to launch in a VPC
  
  tags = {
    Name = ""terraform-sagemaker-notebook""
    Environment = ""development""
  }

  lifecycle_config_name  = null  # Optional: Specify a lifecycle configuration name
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an IAM role for SageMaker with necessary permissions
3. Creates a SageMaker notebook instance with:
   - A specified instance type (ml.t2.medium)
   - 50GB storage volume
   - Platform identifier for Amazon Linux 2
   - Basic tagging
   
You can customize the configuration by:
- Changing the instance type
- Adjusting the volume size
- Adding VPC configurations (subnet_id and security_groups)
- Adding lifecycle configurations
- Modifying tags
- Adding additional IAM policies as needed

Remember to replace the region with your desired AWS region and adjust the instance name and other parameters according to your needs.",,,,,,"I'll help you create a Terraform configuration for an AWS SageMaker Notebook instance. Here's a complete solution with the required provider and necessary configurations:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-notebook-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""sagemaker_policy"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

resource ""aws_sagemaker_notebook_instance"" ""example"" {
  name                    = ""terraform-notebook-instance""
  role_arn               = aws_iam_role.sagemaker_role.arn
  instance_type          = ""ml.t2.medium""
  platform_identifier    = ""notebook-al2-v1""
  
  tags = {
    Name = ""terraform-sagemaker-notebook""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role for the SageMaker notebook instance
3. Attaches the necessary SageMaker policy to the IAM role
4. Creates a SageMaker notebook instance with basic configurations

The notebook instance will be created with:
- An ml.t2.medium instance type (you can change this based on your needs)
- Amazon Linux 2 platform
- The necessary IAM role for SageMaker access
- Basic tagging for resource management

You can customize the configuration by modifying instance type, adding additional tags, or including other optional parameters like volume size, security groups, or subnet configurations.",,,,,,"I'll help you create a Terraform configuration for an AWS SageMaker notebook instance. Here's a complete example with the required configurations:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-notebook-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""sagemaker_policy"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

resource ""aws_sagemaker_notebook_instance"" ""example"" {
  name                    = ""my-notebook-instance""
  role_arn               = aws_iam_role.sagemaker_role.arn
  instance_type          = ""ml.t2.medium""
  platform_identifier    = ""notebook-al2-v1""
  
  # Optional configurations
  volume_size            = 50
  subnet_id              = null  # Specify if you want to launch in a VPC
  security_groups        = []    # Specify if you want to launch in a VPC
  
  tags = {
    Name = ""sagemaker-notebook""
    Environment = ""development""
  }

  lifecycle {
    prevent_destroy = false
  }
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates an IAM role for the SageMaker notebook instance
3. Attaches the necessary SageMaker policy to the IAM role
4. Creates the SageMaker notebook instance with:
   - A specified name
   - The created IAM role
   - Instance type ml.t2.medium (you can change this based on your needs)
   - A 50GB volume size
   - Optional VPC configurations (subnet and security groups)
   - Tags for better resource management
   - A lifecycle policy to allow destruction

You can modify the instance type, volume size, and other parameters according to your specific requirements. If you want to launch the notebook instance in a VPC, you'll need to specify the subnet_id and security_groups.",,,,,
aws_nat_gateway,"creates a NAT Gateway associated with a specified subnet and Elastic IP allocation ID. The NAT Gateway is configured for public connectivity and tagged with a key-value pair (pike = ""permissions""), indicating its purpose or ownership.","package terraform.validation

# Set default validation state
default is_valid_nat_gateway = false

# Validate aws_nat_gateway resource
is_valid_nat_gateway {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_nat_gateway""

        # Ensure it is associated with a specified subnet
        resource.expressions.subnet_id != null

        # Ensure it uses a specific Elastic IP allocation ID
        resource.expressions.allocation_id != null

        # Check for the specific tag indicating its purpose or ownership
        resource.expressions.tags.constant_value.pike == ""permissions""
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_nat_gateway"" ""pike"" {
  subnet_id         = ""subnet-0562ef1d304b968f4""
  allocation_id     = ""eipalloc-0047fa56c40637c3b""
  connectivity_type = ""public""
  tags = {
    pike = ""permissions""
  }
}","Has one ""aws_nat_gateway"" resource:
Associated with a specified ""aws_subnet"" resource for hosting the NAT Gateway.
Utilizes an ""Elastic IP allocation ID"" to provide the NAT Gateway with a public IP address.
Configured for public connectivity, allowing resources within the private subnet to access the internet securely.
Tagged with a key-value pair: ""pike"" set to ""permissions"", which categorizes its role, purpose, or ownership within the infrastructure.","I'll help you create a Terraform configuration for an AWS NAT Gateway with the specified requirements. Here's the HCL code:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Resource for Elastic IP
resource ""aws_eip"" ""nat_eip"" {
  domain = ""vpc""

  tags = {
    Name = ""nat-gateway-eip""
  }
}

# Resource for NAT Gateway
resource ""aws_nat_gateway"" ""main"" {
  allocation_id = aws_eip.nat_eip.id
  subnet_id     = var.subnet_id  # Reference to the subnet where NAT Gateway will be placed
  
  connectivity_type = ""public""

  tags = {
    pike = ""permissions""
  }

  # Wait for the EIP to be ready
  depends_on = [aws_eip.nat_eip]
}

# Variable definition for subnet_id
variable ""subnet_id"" {
  description = ""The ID of the subnet where the NAT Gateway will be created""
  type        = string
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an Elastic IP (EIP) for the NAT Gateway
3. Creates a NAT Gateway with:
   - Association to the provided subnet
   - Public connectivity type
   - The required ""pike = permissions"" tag
   - Association with the created EIP
4. Includes a variable for the subnet ID input

You would need to provide the subnet_id when applying this configuration. The NAT Gateway will be created in the specified subnet and will use the automatically created Elastic IP for outbound internet connectivity.",,,,,,"I'll help you create a Terraform configuration for an AWS NAT Gateway with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create Elastic IP for NAT Gateway
resource ""aws_eip"" ""nat_eip"" {
  domain = ""vpc""

  tags = {
    Name = ""nat-gateway-eip""
  }
}

# Create NAT Gateway
resource ""aws_nat_gateway"" ""main"" {
  allocation_id = aws_eip.nat_eip.id
  subnet_id     = var.public_subnet_id  # Reference to the subnet ID where NAT Gateway will be placed
  
  connectivity_type = ""public""

  tags = {
    pike = ""permissions""
  }

  # Wait for EIP to be available
  depends_on = [aws_eip.nat_eip]
}

# Variable for subnet ID
variable ""public_subnet_id"" {
  description = ""The ID of the public subnet where the NAT Gateway will be created""
  type        = string
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an Elastic IP (EIP) for the NAT Gateway
3. Creates a NAT Gateway with:
   - Association to the provided subnet
   - Public connectivity type
   - The specified tag (pike = ""permissions"")
   - Association with the created EIP
4. Includes a variable for the subnet ID that needs to be provided

To use this configuration, you would need to provide the public subnet ID when applying the Terraform configuration. The NAT Gateway will be created in the specified subnet and will be accessible for private subnets to route their internet traffic through.",,,,,,"I'll help you create a Terraform configuration for a NAT Gateway with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create Elastic IP for NAT Gateway
resource ""aws_eip"" ""nat_eip"" {
  domain = ""vpc""
  tags = {
    Name = ""nat-gateway-eip""
  }
}

# Create NAT Gateway
resource ""aws_nat_gateway"" ""main"" {
  allocation_id = aws_eip.nat_eip.id
  subnet_id     = var.subnet_id  # Reference to the subnet where NAT Gateway will be placed
  
  connectivity_type = ""public""
  
  tags = {
    pike = ""permissions""
  }

  # Wait for the EIP to be ready
  depends_on = [aws_eip.nat_eip]
}

# Variable for subnet ID
variable ""subnet_id"" {
  description = ""The ID of the subnet where the NAT Gateway will be created""
  type        = string
}
```

This configuration:
1. Sets up the AWS provider requirement
2. Creates an Elastic IP for the NAT Gateway
3. Creates a NAT Gateway with:
   - Association to the specified subnet
   - Public connectivity type
   - The required tag (pike = ""permissions"")
4. Includes a variable for the subnet ID input
5. Uses depends_on to ensure proper resource creation order

To use this configuration, you would need to provide the subnet_id when applying the Terraform configuration.",,,,,
"aws_s3_bucket, aws_s3_bucket_acl, aws_s3_bucket_policy","Create a Terraform configuration that defines an AWS S3 bucket, an ACL for the bucket, and a bucket policy allowing s3:GetObject actions with a condition based on the AWS Referer header. The bucket should have a unique name like ""my_unique_bucket_name,"" and the policy should grant access to all principals. ","package terraform.validation

default has_aws_s3_bucket = false
default has_aws_s3_bucket_acl = false
default has_aws_s3_bucket_policy = false
has_aws_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""my_bucket""
    bucket.values.bucket == ""my_unique_bucket_name""
}

has_aws_s3_bucket_acl {
    acl := input.planned_values.root_module.resources[_]
    acl.type == ""aws_s3_bucket_acl""
    acl.name == ""my_bucket_acl""
    acl.values.bucket == input.planned_values.root_module.resources[_].values.id  # Ensure correct bucket reference
    acl.values.acl == ""private""
}

has_aws_s3_bucket_policy {
    policy := input.planned_values.root_module.resources[_]
    policy.type == ""aws_s3_bucket_policy""
    policy.name == ""my_bucket_policy""
    policy.values.bucket == input.planned_values.root_module.resources[_].values.id  # Ensure correct bucket reference
}

valid_configuration {
    has_aws_s3_bucket
    has_aws_s3_bucket_acl
    has_aws_s3_bucket_policy
}
",3,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""my_bucket"" {
  bucket = ""my_unique_bucket_name""
}

resource ""aws_s3_bucket_acl"" ""my_bucket_acl"" {
  bucket = aws_s3_bucket.my_bucket.id
  acl    = ""private""
}

resource ""aws_s3_bucket_policy"" ""my_bucket_policy"" {
  bucket = aws_s3_bucket.my_bucket.id
  policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Sid       = ""ExampleStatement1"",
        Effect    = ""Allow"",
        Principal = ""*"",
        Action    = [
          ""s3:GetObject""
        ],
        Resource = [
          ""${aws_s3_bucket.my_bucket.arn}/*""
        ],
        Condition = {
          StringEquals = {
            ""aws:Referer"" = ""http://www.example.com""
          }
        }
      }
    ]
  })
}","Resource ""aws_s3_bucket""
has one ""bucket""

Resource ""aws_s3_bucket_acl""
has one ""bucket""
has one ""acl""

Resource ""aws_s3_bucket_policy""
has one ""bucket""
has one ""policy""","I'll help you create a Terraform configuration for an S3 bucket with ACL and a bucket policy that includes referer-based conditions. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# S3 Bucket
resource ""aws_s3_bucket"" ""website_bucket"" {
  bucket = ""my-unique-bucket-name-2023""  # Change this to your desired unique name
}

# S3 Bucket ACL
resource ""aws_s3_bucket_acl"" ""bucket_acl"" {
  bucket = aws_s3_bucket.website_bucket.id
  acl    = ""private""
}

# S3 Bucket Policy
resource ""aws_s3_bucket_policy"" ""allow_access_with_referer"" {
  bucket = aws_s3_bucket.website_bucket.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowGetObjectWithReferer""
        Effect    = ""Allow""
        Principal = ""*""
        Action    = ""s3:GetObject""
        Resource  = ""${aws_s3_bucket.website_bucket.arn}/*""
        Condition = {
          StringLike = {
            ""aws:Referer"" = [
              ""https://example.com/*"",
              ""https://www.example.com/*""
            ]
          }
        }
      }
    ]
  })
}

# S3 Bucket Public Access Block
resource ""aws_s3_bucket_public_access_block"" ""bucket_public_access_block"" {
  bucket = aws_s3_bucket.website_bucket.id

  block_public_acls       = true
  block_public_policy     = false
  ignore_public_acls      = true
  restrict_public_buckets = false
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates an S3 bucket with a unique name
3. Applies a private ACL to the bucket
4. Creates a bucket policy that:
   - Allows s3:GetObject actions
   - Applies to all principals (*)
   - Includes a condition based on the AWS Referer header
   - Only allows access from specific domains (example.com in this case)
5. Configures public access block settings to maintain security while allowing the bucket policy to work

Make sure to:
- Replace the bucket name with your desired unique name
- Update the region if needed
- Modify the referer domains in the condition to match your requirements
- Review and adjust the public access block settings based on your security needs

The configuration follows AWS best practices while implementing the requested functionality with referer-based access control.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with the specified ACL and bucket policy. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""my-unique-bucket-name-2023""  # Ensure this is globally unique
}

# Create bucket ACL
resource ""aws_s3_bucket_acl"" ""bucket_acl"" {
  bucket = aws_s3_bucket.example_bucket.id
  acl    = ""private""
}

# Create bucket policy
resource ""aws_s3_bucket_policy"" ""bucket_policy"" {
  bucket = aws_s3_bucket.example_bucket.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""PublicReadGetObject""
        Effect    = ""Allow""
        Principal = ""*""
        Action    = ""s3:GetObject""
        Resource  = ""${aws_s3_bucket.example_bucket.arn}/*""
        Condition = {
          StringLike = {
            ""aws:Referer"" = [""http://www.example.com/*""]
          }
        }
      }
    ]
  })
}

# Enable bucket versioning (optional but recommended)
resource ""aws_s3_bucket_versioning"" ""versioning"" {
  bucket = aws_s3_bucket.example_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Block public access (optional security measure)
resource ""aws_s3_bucket_public_access_block"" ""public_access_block"" {
  bucket = aws_s3_bucket.example_bucket.id

  block_public_acls       = true
  block_public_policy     = false
  ignore_public_acls      = true
  restrict_public_buckets = false
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an S3 bucket with a unique name
3. Configures the bucket ACL as private
4. Creates a bucket policy that:
   - Allows s3:GetObject actions
   - Applies to all principals (*)
   - Includes a condition based on the AWS Referer header
5. Enables versioning (optional but recommended)
6. Configures public access blocks (optional security measure)

Make sure to:
- Replace ""my-unique-bucket-name-2023"" with your desired unique bucket name
- Adjust the region in the provider block
- Modify the Referer condition URL (http://www.example.com/*) to match your needs
- Review and adjust the public access block settings based on your security requirements",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with ACL and a bucket policy that includes a referer-based condition. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""website_bucket"" {
  bucket = ""my-unique-bucket-name-2023""
}

# Create bucket ACL
resource ""aws_s3_bucket_acl"" ""bucket_acl"" {
  bucket = aws_s3_bucket.website_bucket.id
  acl    = ""private""
}

# Create bucket policy
resource ""aws_s3_bucket_policy"" ""allow_access_with_referer"" {
  bucket = aws_s3_bucket.website_bucket.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowGetObjectWithReferer""
        Effect    = ""Allow""
        Principal = ""*""
        Action    = ""s3:GetObject""
        Resource  = ""${aws_s3_bucket.website_bucket.arn}/*""
        Condition = {
          StringLike = {
            ""aws:Referer"" = [
              ""http://example.com/*"",
              ""https://example.com/*""
            ]
          }
        }
      }
    ]
  })
}

# Enable bucket versioning (optional but recommended)
resource ""aws_s3_bucket_versioning"" ""versioning"" {
  bucket = aws_s3_bucket.website_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Block public access (optional security measure)
resource ""aws_s3_bucket_public_access_block"" ""public_access"" {
  bucket = aws_s3_bucket.website_bucket.id

  block_public_acls       = true
  block_public_policy     = false
  ignore_public_acls      = true
  restrict_public_buckets = false
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an S3 bucket with a unique name
3. Sets up a private ACL for the bucket
4. Creates a bucket policy that:
   - Allows s3:GetObject actions
   - Applies to all principals (*)
   - Includes a condition based on the Referer header
5. Enables versioning for added protection
6. Configures public access blocks for security

The bucket policy allows access only when the request comes from specific referer URLs (example.com in this case). You can modify the referer URLs in the policy to match your requirements.

Remember to replace ""my-unique-bucket-name-2023"" with your desired bucket name, keeping in mind that S3 bucket names must be globally unique across all AWS accounts.",,,,,
aws_lightsail_instance,create AWS Lightsail with WordPress blueprint,"package terraform.validation

default has_valid_lightsail_instance = false

# Main rule to check for a valid aws_lightsail_instance
has_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id == ""wordpress""
    resource.values.bundle_id
}",1,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""wordpress""
  bundle_id         = ""nano_2_0""
 }","have one ""aws_lightsail_instance"" resource
with ""name"" argument
with ""availability_zone"" argument
with ""blueprint_id"" = wordpress
with ""bundle_id"" argument","I'll help you create a Terraform configuration for an AWS Lightsail instance with WordPress blueprint. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change this to your desired region
}

resource ""aws_lightsail_instance"" ""wordpress_instance"" {
  name              = ""wordpress-instance""
  availability_zone = ""us-east-1a""  # Change this to match your region
  blueprint_id      = ""wordpress""
  bundle_id         = ""micro_2_0""   # Smallest instance size, adjust as needed

  tags = {
    Name = ""WordPress-Site""
    Environment = ""Production""
  }
}

# Create a static IP for the Lightsail instance
resource ""aws_lightsail_static_ip"" ""wordpress_static_ip"" {
  name = ""wordpress-static-ip""
}

# Attach the static IP to the instance
resource ""aws_lightsail_static_ip_attachment"" ""wordpress_static_ip_attachment"" {
  static_ip_name = aws_lightsail_static_ip.wordpress_static_ip.name
  instance_name  = aws_lightsail_instance.wordpress_instance.name
}

# Open required ports for WordPress
resource ""aws_lightsail_instance_public_ports"" ""wordpress_ports"" {
  instance_name = aws_lightsail_instance.wordpress_instance.name

  port_info {
    protocol  = ""tcp""
    from_port = 80
    to_port   = 80
  }

  port_info {
    protocol  = ""tcp""
    from_port = 443
    to_port   = 443
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail instance using the WordPress blueprint
3. Assigns a static IP to the instance
4. Configures the necessary ports (80 and 443) for web traffic

Key features:
- Uses the WordPress blueprint for automatic WordPress installation
- Creates and attaches a static IP for consistent access
- Opens HTTP and HTTPS ports
- Uses the micro_2_0 bundle (can be adjusted based on your needs)

You can modify the region, availability zone, and instance size (bundle_id) according to your requirements.",,,,,,"I'll help you create an AWS Lightsail instance with WordPress blueprint using Terraform. Here's a configuration that sets up a Lightsail instance with WordPress:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_lightsail_instance"" ""wordpress_instance"" {
  name              = ""wordpress-instance""
  availability_zone = ""us-west-2a""  # Change this to match your region
  blueprint_id      = ""wordpress""
  bundle_id         = ""nano_2_0""    # Smallest instance size, adjust as needed

  tags = {
    Environment = ""Production""
    Application = ""WordPress""
  }
}

# Create a static IP for the Lightsail instance
resource ""aws_lightsail_static_ip"" ""wordpress_static_ip"" {
  name = ""wordpress-static-ip""
}

# Attach the static IP to the instance
resource ""aws_lightsail_static_ip_attachment"" ""wordpress_static_ip_attachment"" {
  static_ip_name = aws_lightsail_static_ip.wordpress_static_ip.name
  instance_name  = aws_lightsail_instance.wordpress_instance.name
}

# Optional: Create an opening in the firewall for HTTP and HTTPS
resource ""aws_lightsail_instance_public_ports"" ""wordpress_ports"" {
  instance_name = aws_lightsail_instance.wordpress_instance.name

  port_info {
    protocol  = ""tcp""
    from_port = 80
    to_port   = 80
  }

  port_info {
    protocol  = ""tcp""
    from_port = 443
    to_port   = 443
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail instance using the WordPress blueprint
3. Assigns a static IP to the instance
4. Opens ports 80 and 443 for HTTP/HTTPS traffic

A few notes:
- The `bundle_id` specifies the instance size. ""nano_2_0"" is the smallest size, but you can change it to other options like ""micro_2_0"", ""small_2_0"", etc.
- You may want to adjust the region and availability zone to match your requirements
- The WordPress blueprint comes pre-installed with the basic WordPress setup
- The static IP ensures your instance maintains the same IP address even after restarts

You can apply this configuration using `terraform init` followed by `terraform apply`.",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail instance with WordPress blueprint. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_lightsail_instance"" ""wordpress_instance"" {
  name              = ""wordpress-instance""
  availability_zone = ""us-west-2a""  # Change this to match your region
  blueprint_id      = ""wordpress""
  bundle_id         = ""nano_2_0""    # Smallest instance size, adjust as needed

  tags = {
    Name = ""WordPress-Site""
    Environment = ""Production""
  }
}

# Optional: Create a static IP for the instance
resource ""aws_lightsail_static_ip"" ""wordpress_static_ip"" {
  name = ""wordpress-static-ip""
}

# Attach static IP to the instance
resource ""aws_lightsail_static_ip_attachment"" ""wordpress_static_ip_attachment"" {
  static_ip_name = aws_lightsail_static_ip.wordpress_static_ip.name
  instance_name  = aws_lightsail_instance.wordpress_instance.name
}

# Open required ports
resource ""aws_lightsail_instance_public_ports"" ""wordpress_ports"" {
  instance_name = aws_lightsail_instance.wordpress_instance.name

  port_info {
    protocol  = ""tcp""
    from_port = 80
    to_port   = 80
  }

  port_info {
    protocol  = ""tcp""
    from_port = 443
    to_port   = 443
  }
}
```

This configuration:
1. Creates a Lightsail instance using the WordPress blueprint
2. Assigns a static IP to the instance
3. Opens ports 80 and 443 for HTTP and HTTPS traffic
4. Uses the nano bundle (smallest size) but you can adjust the `bundle_id` as needed

Common bundle IDs include:
- nano_2_0 (1 vCPU, 512MB RAM)
- micro_2_0 (1 vCPU, 1GB RAM)
- small_2_0 (2 vCPU, 2GB RAM)
- medium_2_0 (2 vCPU, 4GB RAM)

Remember to:
1. Change the region to your preferred AWS region
2. Adjust the availability zone accordingly
3. Modify the bundle_id based on your needs
4. Update the tags as needed for your environment

You can access your WordPress site using the static IP address once the deployment is complete.",,,,,
"aws_db_instance, aws_db_parameter_group, aws_db_subnet_group, aws_kms_key, aws_security_group, aws_subnet, aws_subnet","Provision a secure PostgreSQL database (version is 15 and storage size is 50GB) instance within a specified AWS VPC, leveraging AWS services like RDS, subnets, and KMS for encryption. It sets up a custom parameter group for PostgreSQL settings, and a security group to manage access. The database instance is configured with specifications like storage size, engine version, multi-AZ deployment for high availability, and encryption using a KMS key.","package terraform.validation

default is_valid_db_instance = false

default is_valid_security_group = false

default is_valid_db_subnet_group = false

default is_valid_db_parameter_group = false

default is_valid_kms_key = false

# Validate aws_db_instance resource
is_valid_db_instance {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_instance""
        resource.expressions.allocated_storage.constant_value == 20
        resource.expressions.engine.constant_value == ""postgres""
    resource.expressions.engine_version.constant_value == ""15""
        resource.expressions.instance_class != null
        resource.expressions.username != null
        resource.expressions.password != null
        resource.expressions.multi_az.constant_value == true
        resource.expressions.storage_encrypted.constant_value == true
        resource.expressions.kms_key_id != null
}

# Validate aws_security_group resource
is_valid_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""
        count(resource.expressions.ingress) > 0
        count(resource.expressions.egress) > 0
}

# Validate aws_db_subnet_group resource
is_valid_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}

# Validate aws_db_parameter_group resource
is_valid_db_parameter_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_parameter_group""
        # Additional checks for specific parameter values can be added here if needed
}

# Validate aws_kms_key resource
is_valid_kms_key {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_kms_key""
        # Additional checks for KMS key attributes can be added here if needed
}
",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

resource ""aws_db_subnet_group"" ""db-subnet-group"" {
  name       = ""db-subnet-group""
  subnet_ids = module.vpc.public_subnets
}

resource ""aws_security_group"" ""db-sg"" {
  name = ""db-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-sg-ingress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.db-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-sg-egress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.db-sg.id
}

resource ""aws_db_parameter_group"" ""postgre-param-group"" {
  name   = ""pgparamgrp15""
  family = ""postgres15""

  parameter {
    name  = ""password_encryption""
    value = ""scram-sha-256""
  }

  parameter {
    name  = ""rds.force_ssl""
    value = ""0""
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource ""aws_kms_key"" ""rds-key"" {
  description             = ""kmsrds""
  deletion_window_in_days = 14
}

resource ""aws_db_instance"" ""postgres"" {
  identifier                      = ""pg""
  skip_final_snapshot             = true
  allocated_storage               = 50
  backup_retention_period         = 7
  db_subnet_group_name            = aws_db_subnet_group.db-subnet-group.id
  parameter_group_name            = aws_db_parameter_group.postgre-param-group.name
  enabled_cloudwatch_logs_exports = [""postgresql"", ""upgrade""]
  engine                          = ""postgres""
  engine_version                  = ""15""
  instance_class                  = ""db.t3.micro""
  db_name                         = ""postgres"" # Initial database name
  username                        = ""myusername""
  password                        = ""mypassword""
  vpc_security_group_ids          = [aws_security_group.db-sg.id]
  # Other security settings
  publicly_accessible             = true
  multi_az                        = true
  storage_encrypted               = true
  kms_key_id                      = aws_kms_key.rds-key.arn
  # Default daily backup window
  # https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage""
with one ""engine"" and a valid engine value, such as ""postgres""
with one ""instance_class"" and a valid instance class type, such as ""db.t3.micro""
with one ""password""
with one ""username""
with ""multi_az"" deployment set to true for high availability
with ""storage_encrypted"" set to true
with one ""kms_key_id"" referencing a valid AWS KMS key

Has one ""aws_security_group"" resource
with rules to manage access to the PostgreSQL database instance

Has one ""aws_db_subnet_group"" resource
with ""subnet_ids"" including at least two subnets within the VPC

Has one ""aws_db_parameter_group"" resource
with customized parameters for PostgreSQL settings, such as ""password_encryption"" and ""rds.force_ssl""

Has one ""aws_kms_key"" resource
for encrypting the PostgreSQL database storage","I'll help you create a Terraform configuration for a secure PostgreSQL RDS instance with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# KMS key for RDS encryption
resource ""aws_kms_key"" ""rds_encryption_key"" {
  description             = ""KMS key for RDS encryption""
  deletion_window_in_days = 7
  enable_key_rotation    = true
}

# Parameter group for PostgreSQL
resource ""aws_db_parameter_group"" ""postgres_params"" {
  family = ""postgres15""
  name   = ""custom-postgres15-params""

  parameter {
    name  = ""max_connections""
    value = ""100""
  }

  parameter {
    name  = ""shared_buffers""
    value = ""4096""
  }
}

# Security group for RDS
resource ""aws_security_group"" ""rds_sg"" {
  name        = ""postgres-rds-sg""
  description = ""Security group for PostgreSQL RDS instance""
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [var.vpc_cidr]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""postgres-rds-sg""
  }
}

# DB subnet group
resource ""aws_db_subnet_group"" ""postgres_subnet_group"" {
  name       = ""postgres-subnet-group""
  subnet_ids = var.subnet_ids

  tags = {
    Name = ""PostgreSQL DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier        = ""postgres-instance""
  engine            = ""postgres""
  engine_version    = ""15""
  instance_class    = ""db.t3.medium""
  allocated_storage = 50
  
  db_name  = ""mypostgresdb""
  username = var.db_username
  password = var.db_password

  db_subnet_group_name   = aws_db_subnet_group.postgres_subnet_group.name
  vpc_security_group_ids = [aws_security_group.rds_sg.id]
  parameter_group_name   = aws_db_parameter_group.postgres_params.name

  multi_az               = true
  storage_encrypted      = true
  kms_key_id            = aws_kms_key.rds_encryption_key.arn

  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  skip_final_snapshot    = false
  final_snapshot_identifier = ""postgres-final-snapshot""

  tags = {
    Name = ""PostgreSQL RDS Instance""
  }
}

# Variables
variable ""vpc_id"" {
  description = ""ID of the VPC where RDS will be deployed""
  type        = string
}

variable ""vpc_cidr"" {
  description = ""CIDR block of the VPC""
  type        = string
}

variable ""subnet_ids"" {
  description = ""List of subnet IDs for the RDS instance""
  type        = list(string)
}

variable ""db_username"" {
  description = ""Database administrator username""
  type        = string
  sensitive   = true
}

variable ""db_password"" {
  description = ""Database administrator password""
  type        = string
  sensitive   = true
}
```

This configuration:

1. Sets up a KMS key for encryption
2. Creates a custom parameter group for PostgreSQL 15
3. Configures",,,,,,"I'll help you create a Terraform configuration for a secure PostgreSQL RDS instance with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# KMS key for RDS encryption
resource ""aws_kms_key"" ""rds_encryption_key"" {
  description             = ""KMS key for RDS database encryption""
  deletion_window_in_days = 7
  enable_key_rotation     = true
}

# Parameter group for PostgreSQL
resource ""aws_db_parameter_group"" ""postgres_params"" {
  family = ""postgres15""
  name   = ""custom-postgres15-params""

  parameter {
    name  = ""log_connections""
    value = ""1""
  }

  parameter {
    name  = ""log_disconnections""
    value = ""1""
  }
}

# Security group for RDS
resource ""aws_security_group"" ""rds_sg"" {
  name        = ""postgresql-security-group""
  description = ""Security group for PostgreSQL RDS instance""
  vpc_id      = var.vpc_id

  ingress {
    from_port       = 5432
    to_port         = 5432
    protocol        = ""tcp""
    security_groups = [var.app_security_group_id]
  }
}

# Subnet group for RDS
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name        = ""postgresql-subnet-group""
  description = ""Subnet group for PostgreSQL RDS instance""
  subnet_ids  = var.subnet_ids
}

# RDS PostgreSQL instance
resource ""aws_db_instance"" ""postgresql"" {
  identifier        = ""postgresql-instance""
  engine            = ""postgres""
  engine_version    = ""15""
  instance_class    = ""db.t3.medium""
  allocated_storage = 50

  db_name  = ""mydb""
  username = var.db_username
  password = var.db_password

  db_subnet_group_name   = aws_db_subnet_group.rds_subnet_group.name
  vpc_security_group_ids = [aws_security_group.rds_sg.id]
  parameter_group_name   = aws_db_parameter_group.postgres_params.name

  storage_encrypted = true
  kms_key_id       = aws_kms_key.rds_encryption_key.arn

  multi_az               = true
  publicly_accessible    = false
  skip_final_snapshot    = false
  final_snapshot_identifier = ""postgresql-final-snapshot""

  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  tags = {
    Name        = ""PostgreSQL RDS Instance""
    Environment = ""Production""
  }
}

# Variables
variable ""vpc_id"" {
  description = ""ID of the VPC where RDS will be deployed""
  type        = string
}

variable ""subnet_ids"" {
  description = ""List of subnet IDs for RDS subnet group""
  type        = list(string)
}

variable ""app_security_group_id"" {
  description = ""Security group ID of the application layer""
  type        = string
}

variable ""db_username"" {
  description = ""Database administrator username""
  type        = string
  sensitive   = true
}

variable ""db_password"" {
  description = ""Database administrator password""
  type        = string
  sensitive   = true
}

# Outputs
output ""db_endpoint"" {
  description = ""The connection endpoint for the RDS instance""
  value       = aws_db_instance.postgresql.endpoint
}

output ""db_arn"" {
  description = ""The ARN of the RDS instance""
  value       = aws_db_instance.postgresql.arn
}
```

This configuration:
1.",,,,,,"I'll help you create a Terraform configuration for a secure PostgreSQL RDS instance with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# KMS key for RDS encryption
resource ""aws_kms_key"" ""rds_encryption_key"" {
  description             = ""KMS key for RDS database encryption""
  deletion_window_in_days = 7
  enable_key_rotation    = true
}

# Parameter group for PostgreSQL
resource ""aws_db_parameter_group"" ""postgres_params"" {
  family = ""postgres15""
  name   = ""custom-postgres15-params""

  parameter {
    name  = ""log_connections""
    value = ""1""
  }

  parameter {
    name  = ""log_disconnections""
    value = ""1""
  }
}

# Security group for RDS
resource ""aws_security_group"" ""rds_sg"" {
  name        = ""postgres-rds-sg""
  description = ""Security group for PostgreSQL RDS instance""
  vpc_id      = var.vpc_id

  ingress {
    from_port       = 5432
    to_port         = 5432
    protocol        = ""tcp""
    cidr_blocks     = [var.vpc_cidr]
    description     = ""Allow PostgreSQL traffic""
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""postgres-rds-security-group""
  }
}

# Subnet group for RDS
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name       = ""postgres-subnet-group""
  subnet_ids = var.subnet_ids

  tags = {
    Name = ""PostgreSQL DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgres-db""
  engine              = ""postgres""
  engine_version      = ""15""
  instance_class      = ""db.t3.medium""
  allocated_storage   = 50
  storage_type        = ""gp2""
  
  db_name             = ""mypostgresdb""
  username           = var.db_username
  password           = var.db_password

  multi_az            = true
  storage_encrypted   = true
  kms_key_id         = aws_kms_key.rds_encryption_key.arn

  vpc_security_group_ids = [aws_security_group.rds_sg.id]
  db_subnet_group_name  = aws_db_subnet_group.rds_subnet_group.name
  parameter_group_name  = aws_db_parameter_group.postgres_params.name

  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  skip_final_snapshot    = false
  final_snapshot_identifier = ""postgres-final-snapshot""

  tags = {
    Name = ""PostgreSQL RDS Instance""
  }
}

# Variables
variable ""vpc_id"" {
  description = ""ID of the VPC""
  type        = string
}

variable ""vpc_cidr"" {
  description = ""CIDR block of the VPC""
  type        = string
}

variable ""subnet_ids"" {
  description = ""List of subnet IDs for the RDS instance""
  type        = list(string)
}

variable ""db_username"" {
  description = ""Database administrator username""
  type        = string
  sensitive   = true
}

variable ""db_password"" {
  description = ""Database administrator password""
  type        = string
  sensitive   = true
}
```

This configuration includes:",,,,,
"aws_codebuild_project, aws_iam_policy, aws_iam_role, aws_iam_role_policy_attachment, aws_s3_bucket, aws_iam_policy_document, aws_iam_policy_document","Infrastructure for an autograder for a CS class. The autograder needs to run students' code stored on GitHub in a specific environment and store the result. Accomplish this using the following resources: AWS S3 Bucket, AWS CodeBuild Project. Add any necessary resources.","package autograder_middle

import rego.v1

codebuild_project_valid(codebuild_project, s3_bucket) if {
        some artifact in codebuild_project.expressions.artifacts
        s3_bucket.address in artifact.location.references
        artifact.name
        artifact.type.constant_value == ""S3""

        some environment in codebuild_project.expressions.environment
        environment.compute_type
        environment.image
        environment.type

        some source in codebuild_project.expressions.source
        source.type.constant_value == ""GITHUB""
        source.location
}

default valid := false

valid if {
        resources := input.configuration.root_module.resources
        some codebuild_project in resources
        codebuild_project.type == ""aws_codebuild_project""
        some s3_bucket in resources
        s3_bucket.type == ""aws_s3_bucket""
        some security_group in resources
        codebuild_project_valid(codebuild_project, s3_bucket)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""artifact-bucket-""
}

resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder_build""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip"" # include this
  }

  environment { # arguments required, exact value not specified
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1 # good to have, not required
    location        = ""github.com/source-location""
  }
}

resource ""aws_iam_role"" ""autograder_build_role"" {
  assume_role_policy = data.aws_iam_policy_document.autograder_build_policy_assume.json
}

data ""aws_iam_policy_document"" ""autograder_build_policy_assume"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }
  }

}

data ""aws_iam_policy_document"" ""autograder_build_policy"" {
  statement {
    effect = ""Allow""
    actions = [
      ""ec2:DescribeSubnets"",
      ""ec2:DescribeSecurityGroups"",
      ""ec2:DescribeVpcs""
    ]
    resources = [
      ""*""
    ]
  }
}

resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name        = ""lambda_policy""
  description = ""Grants permissions to Lambda to describe vpc, subnet, security group""

  policy = data.aws_iam_policy_document.autograder_build_policy.json
}

resource ""aws_iam_role_policy_attachment"" ""lambda_policy_attach"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}","have one ""aws_s3_bucket"" resource
have one ""aws_codebuild_project"" resource
  with ""artifacts"" argument block
    with ""type"" argument with value ""S3""
    with ""location"" argument
    with ""name"" argument
  with ""environment"" argument block
    with ""computer_type"" argument
    with ""image"" argument
    with ""type"" argument
  with ""source"" argument block
    with ""type"" argument with value ""GITHUB""
    with ""location"" argument","I'll help you create a Terraform configuration for an autograder system using AWS CodeBuild and S3. This solution will include:
1. S3 bucket for storing results
2. IAM roles and policies for CodeBuild
3. CodeBuild project configuration
4. Required policies and permissions

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# S3 bucket for storing autograder results
resource ""aws_s3_bucket"" ""autograder_results"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""autograder_versioning"" {
  bucket = aws_s3_bucket.autograder_results.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.autograder_results.arn}"",
          ""${aws_s3_bucket.autograder_results.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class""
  build_timeout = ""30""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.autograder_results.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""RESULTS_BUCKET""
      value = aws_s3_bucket.autograder_results.bucket
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1
    
    git_submodules_config {
      fetch_submodules = true
    }
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""/aws/codebuild/autograder""
      stream_name = ""build-logs""
      status      = ""ENABLED""
    }
  }
}

# CloudWatch Log Group for CodeBuild logs
resource ""aws_cloudwatch_log_group"" ""autogra",,,,,,"I'll help you create a Terraform configuration for an autograder system using AWS CodeBuild and S3. This solution will include:
- S3 bucket for storing results
- CodeBuild project to run the tests
- IAM roles and policies for CodeBuild
- Necessary permissions and configurations

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# S3 bucket for storing autograder results
resource ""aws_s3_bucket"" ""autograder_results"" {
  bucket = ""autograder-results-bucket""
}

# S3 bucket versioning
resource ""aws_s3_bucket_versioning"" ""autograder_results_versioning"" {
  bucket = aws_s3_bucket.autograder_results.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          aws_s3_bucket.autograder_results.arn,
          ""${aws_s3_bucket.autograder_results.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class assignments""
  build_timeout = ""30""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.autograder_results.id
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""RESULTS_BUCKET""
      value = aws_s3_bucket.autograder_results.id
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1
    
    git_submodules_config {
      fetch_submodules = true
    }
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""autograder-log-group""
      stream_name = ""autograder-log-stream""
    }
  }
}

# CloudWatch Log Group for CodeBuild
resource ""aws_cloudwatch_log_group"" ""aut",,,,,,"I'll help you create a Terraform configuration for an autograder system using AWS CodeBuild and S3. This solution will include:
1. S3 bucket for storing results
2. IAM roles and policies for CodeBuild
3. CodeBuild project configuration
4. Required CloudWatch log group

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# S3 bucket for storing autograder results
resource ""aws_s3_bucket"" ""autograder_results"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""autograder_results_versioning"" {
  bucket = aws_s3_bucket.autograder_results.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          aws_s3_bucket.autograder_results.arn,
          ""${aws_s3_bucket.autograder_results.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_cloudwatch_log_group.autograder_logs.arn}*""
        ]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CloudWatch log group for CodeBuild
resource ""aws_cloudwatch_log_group"" ""autograder_logs"" {
  name              = ""/aws/codebuild/autograder""
  retention_in_days = 14
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class assignments""
  build_timeout = ""30""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.autograder_results.id
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""RESULTS_BUCKET""
      value = aws_s3_bucket.autograder_results.id
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {",,,,,
"aws_route53_record, aws_route53_zone","Set up a Pointer record for reverse DNS using Route 53 resources. The domain name should be ""host.example53.com"" and name the zone ""reverse_zone""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""2.0.192.in-addr.arpa""
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type.constant_value == ""PTR""
    resource.expressions.ttl
    resource.expressions.records.constant_value[0] == ""host.example.com""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.reverse_zone.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

# Create Reverse DNS Hosted Zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""2.0.192.in-addr.arpa""
}

# Create a PTR Record for a specific IP address within that zone
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""53.2.0.192.in-addr.arpa""
  type    = ""PTR""
  ttl     = ""3600""
  records = [""host.example.com""]
}","Has one ""aws_route53_zone"" resource
    with ""name"" ending in "".in-addr.arpa""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type"" set to ""PTR""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
","I'll help you create a Terraform configuration for setting up a PTR (Pointer) record for reverse DNS using AWS Route 53. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create the reverse DNS zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""1.168.192.in-addr.arpa""  # Example reverse DNS zone for 192.168.1.0/24
}

# Create the PTR record
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""10.1.168.192.in-addr.arpa""  # Example for IP 192.168.1.10
  type    = ""PTR""
  ttl     = ""300""
  records = [""host.example53.com.""]  # Note the trailing dot
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a reverse DNS zone named ""reverse_zone""
3. Creates a PTR record pointing to ""host.example53.com""

Note that:
- The reverse DNS zone name follows the in-addr.arpa format
- The PTR record name is the reverse of the IP address with .in-addr.arpa appended
- The trailing dot in the PTR record value is required
- You'll need to adjust the IP addresses in both the zone and PTR record to match your specific network configuration

Make sure you have the proper AWS credentials configured to apply this configuration.",,,,,,"I'll help you create a Terraform configuration for setting up a PTR (Pointer) record for reverse DNS using AWS Route 53. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Specify your desired region
}

# Create the reverse DNS zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""1.0.10.in-addr.arpa""  # Example reverse DNS zone for 10.0.1.x network
}

# Create the PTR record
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""1.1""  # Last octets in reverse order for the IP address (example: for 10.0.1.1)
  type    = ""PTR""
  ttl     = ""300""
  records = [""host.example53.com""]
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a reverse DNS zone (using an example IP range of 10.0.1.x)
3. Creates a PTR record pointing to ""host.example53.com""

Note: 
- The reverse DNS zone name follows the format of reversed IP address with "".in-addr.arpa"" appended
- The PTR record name should be the last octets of the IP address in reverse order
- Adjust the zone name and PTR record name according to your actual IP address range
- The TTL (Time To Live) is set to 300 seconds but can be adjusted as needed

Make sure to replace the IP-related values with your actual network configuration.",,,,,,"I'll help you create a Terraform configuration for setting up a PTR (Pointer) record for reverse DNS using AWS Route 53. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Adjust the region as needed
}

# Create the reverse DNS zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""1.0.10.in-addr.arpa""  # Example reverse DNS zone for 10.0.1.x network
}

# Create the PTR record
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""1""  # Last octet of the IP address
  type    = ""PTR""
  ttl     = ""300""
  records = [""host.example53.com.""]  # Note the trailing dot
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a reverse DNS zone for a sample network (10.0.1.x in this example)
3. Creates a PTR record that points to ""host.example53.com""

Note that:
- The reverse zone name follows the in-addr.arpa format
- The PTR record includes a trailing dot in the target domain name
- You may need to adjust the reverse zone name based on your actual IP address range
- TTL is set to 300 seconds but can be adjusted as needed

Make sure to replace the network details with your actual network configuration if different from the example.",,,,,
"aws_lex_bot, aws_lex_intent",create an aws_lex_bot for booking a trip with all arguments filled,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for multiple aws_lex_intent resources
has_valid_lex_intents {
    count([intent | intent := input.planned_values.root_module.resources[_]; intent.type == ""aws_lex_intent""; intent.values.name; intent.values.fulfillment_activity])
}

# Rule for aws_lex_bot resource with specific arguments
has_valid_lex_bot_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lex_bot""
    is_boolean(resource.values.child_directed)
    resource.values.clarification_prompt
    resource.values.abort_statement
    resource.values.name
    is_boolean(resource.values.create_version)
    resource.values.description
    is_boolean(resource.values.detect_sentiment)
    resource.values.idle_session_ttl_in_seconds
    resource.values.process_behavior
    resource.values.voice_id
    resource.values.locale
    is_boolean(resource.values.enable_model_improvements)
    resource.values.nlu_intent_confidence_threshold
}

# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lex_intents
    has_valid_lex_bot_instance
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_lex_intent"" ""BookTrip"" {
  description    = ""Book a trip""
  name           = ""BookTrip""

  sample_utterances = [
    ""I want to book a trip"",
    ""I would like to book a trip"",
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

resource ""aws_lex_bot"" ""BookTripBot"" {
  description                   = ""Bot for booking trips""
  idle_session_ttl_in_seconds   = 600
  locale                        = ""en-US""
  name                          = ""BookTripBot""
  process_behavior              = ""BUILD""
  voice_id                      = ""Salli""
  detect_sentiment = false
  enable_model_improvements = true
  nlu_intent_confidence_threshold = 0
  child_directed                = false

  abort_statement {
    message {
      content      = ""Sorry, I cannot assist you to book the trip right now.""
      content_type = ""PlainText""
    }
  }
  
  clarification_prompt {
    max_attempts = 3

    message {
      content      = ""I'm sorry, I didn't understand. Could you please repeat that?""
      content_type = ""PlainText""
    }
  }

  intent {
    intent_name    = aws_lex_intent.BookTrip.name
    intent_version = aws_lex_intent.BookTrip.version
  }
}","Has one resource ""aws_lex_bot"" with all arguments initialized and at least one ""aws_lex_intent"" resource ","I'll help you create an AWS Lex bot for booking a trip using Terraform. This configuration will include all necessary arguments for a complete Lex bot setup.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_lex_bot"" ""trip_booking_bot"" {
  name = ""TripBookingBot""
  description = ""Bot to help users book travel arrangements""
  
  abort_statement {
    message {
      content      = ""Sorry, I could not complete your booking at this time. Please try again later.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 3
    message {
      content      = ""I didn't understand your request. Could you please rephrase that?""
      content_type = ""PlainText""
    }
  }

  idle_session_ttl_in_seconds = 600
  
  intent {
    intent_name    = ""BookTrip""
    intent_version = ""1""
  }

  locale = ""en-US""

  child_directed = false

  process_behavior = ""BUILD""
  
  voice_id = ""Salli""

  create_version = true

  detect_sentiment = true

  enable_model_improvements = true

  nlu_intent_confidence_threshold = 0.80

}

resource ""aws_lex_intent"" ""book_trip"" {
  name = ""BookTrip""
  description = ""Intent to book a trip""
  
  sample_utterances = [
    ""I want to book a trip"",
    ""Book a vacation"",
    ""Plan my travel"",
    ""Make a travel reservation""
  ]

  conclusion_statement {
    message {
      content = ""Your trip has been booked successfully. Have a great journey!""
      content_type = ""PlainText""
    }
  }

  confirmation_prompt {
    max_attempts = 3
    message {
      content = ""Should I go ahead and book this trip for you?""
      content_type = ""PlainText""
    }
  }

  rejection_statement {
    message {
      content = ""Okay, I won't book the trip. Let me know if you want to try again.""
      content_type = ""PlainText""
    }
  }

  slot {
    name        = ""Location""
    description = ""Destination city""
    priority    = 1

    slot_constraint = ""Required""
    slot_type      = ""AMAZON.City""

    value_elicitation_prompt {
      max_attempts = 3
      message {
        content      = ""Which city would you like to visit?""
        content_type = ""PlainText""
      }
    }
  }

  slot {
    name        = ""CheckInDate""
    description = ""Check-in date for the trip""
    priority    = 2

    slot_constraint = ""Required""
    slot_type      = ""AMAZON.Date""

    value_elicitation_prompt {
      max_attempts = 3
      message {
        content      = ""When would you like to check in?""
        content_type = ""PlainText""
      }
    }
  }

  slot {
    name        = ""Nights""
    description = ""Number of nights for the stay""
    priority    = 3

    slot_constraint = ""Required""
    slot_type      = ""AMAZON.Number""

    value_elicitation_prompt {
      max_attempts = 3
      message {
        content      = ""How many nights would you like to stay?""
        content_type = ""PlainText""
      }
    }
  }
}
```

This configuration creates:

1. A Lex bot named ""TripBookingBot"" with:
   - Abort statement for error handling
   - Clarification prompt for unclear requests
   - Session timeout settings
   - Language settings
   - Voice settings
   - Sentiment detection
   - Model improvement settings

2. An intent named ""BookTrip"" with:
   - Sample utterances
   - Confirmation prompts
   - Rejection statements",,,,,,"I'll help you create an AWS Lex bot for booking a trip with all the necessary arguments. Here's a comprehensive Terraform configuration for an AWS Lex bot:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_lex_bot"" ""trip_booking_bot"" {
  name = ""TripBookingBot""
  description = ""Bot for booking travel arrangements""
  
  abort_statement {
    message {
      content      = ""Sorry, I'm having trouble understanding. Please try again later.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 3
    message {
      content      = ""I didn't understand you. Could you please rephrase that?""
      content_type = ""PlainText""
    }
  }

  child_directed = false

  create_version = true

  detect_sentiment = true

  enable_model_improvements = true

  idle_session_ttl_in_seconds = 300

  intent {
    intent_name    = ""BookTrip""
    intent_version = ""1""
  }

  locale = ""en-US""

  process_behavior = ""BUILD""

  voice_id = ""Salli""

  nlu_intent_confidence_threshold = 0.40

  tags = {
    Environment = ""production""
    Purpose     = ""TravelBooking""
  }
}

# Associated intents, slots, and other components would typically be defined separately
resource ""aws_lex_intent"" ""book_trip"" {
  name = ""BookTrip""
  description = ""Intent for booking travel arrangements""
  
  sample_utterances = [
    ""I want to book a trip"",
    ""Book a vacation"",
    ""Plan my travel""
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  conclusion_statement {
    message {
      content      = ""I can help you book your trip. What destination are you interested in?""
      content_type = ""PlainText""
    }
  }

  slot {
    name        = ""Destination""
    description = ""The destination city""
    priority    = 1

    slot_constraint = ""Required""
    slot_type      = ""AMAZON.City""

    value_elicitation_prompt {
      max_attempts = 3
      message {
        content      = ""Which city would you like to visit?""
        content_type = ""PlainText""
      }
    }
  }

  slot {
    name        = ""CheckInDate""
    description = ""The check-in date""
    priority    = 2

    slot_constraint = ""Required""
    slot_type      = ""AMAZON.Date""

    value_elicitation_prompt {
      max_attempts = 3
      message {
        content      = ""When would you like to check in?""
        content_type = ""PlainText""
      }
    }
  }
}
```

This configuration creates an AWS Lex bot designed for trip booking with the following features:

1. Basic bot configuration with name and description
2. Abort and clarification statements
3. Session management settings
4. Sentiment detection enabled
5. Voice configuration
6. Associated BookTrip intent with:
   - Sample utterances
   - Slots for destination and check-in date
   - Fulfillment activity
   - Conclusion statement

The bot includes common settings for a production environment, including:
- Sentiment detection
- Model improvements enabled
- Reasonable session timeout
- NLU confidence threshold
- Appropriate tags for environment tracking

Note that in a real-world scenario, you might want to:
1. Add more intents
2. Include additional slots (like duration, number of travelers, etc.)
3. Configure more complex dialog management
4. Set up Lambda functions for fulfillment
5. Add validation rules

Remember to adjust the region and other parameters according to your specific requirements.",,,,,,"I'll help you create an AWS Lex bot for booking a trip using Terraform. This will include all necessary arguments for a complete configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_lex_bot"" ""trip_booking_bot"" {
  name = ""TripBookingBot""
  description = ""Bot to help users book travel arrangements""
  
  abort_statement {
    message {
      content      = ""Sorry, I'm having trouble understanding. Could you try again?""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 3
    message {
      content      = ""I didn't understand you. Could you please rephrase that?""
      content_type = ""PlainText""
    }
  }

  child_directed = false

  idle_session_ttl_in_seconds = 300

  intent {
    intent_name    = ""BookTrip""
    intent_version = ""1""
  }

  process_behavior = ""BUILD""
  
  voice_id = ""Salli""

  locale = ""en-US""

  create_version = false

  detect_sentiment = true

  enable_model_improvements = true

  nlu_intent_confidence_threshold = 0.40

  tags = {
    Environment = ""production""
    Purpose     = ""TravelBooking""
  }
}

# Associated IAM role for the Lex bot
resource ""aws_iam_role"" ""lex_bot_role"" {
  name = ""lex_bot_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lex.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for the Lex bot
resource ""aws_iam_role_policy"" ""lex_bot_policy"" {
  name = ""lex_bot_policy""
  role = aws_iam_role.lex_bot_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""polly:SynthesizeSpeech"",
          ""comprehend:DetectSentiment"",
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = ""*""
      }
    ]
  })
}
```

This configuration creates:
1. An AWS Lex bot configured for trip booking
2. Required IAM role and policy for the bot to function
3. All necessary bot settings including:
   - Abort statement
   - Clarification prompt
   - Session timeout
   - Intent configuration
   - Voice settings
   - Locale settings
   - Sentiment detection
   - Model improvement settings
   - Confidence threshold
   - Appropriate tags

The bot is configured with:
- A 5-minute session timeout
- English (US) locale
- Salli voice
- Sentiment detection enabled
- Model improvements enabled
- 40% confidence threshold for intent matching
- Appropriate error handling and clarification prompts

Note that you'll need to have the corresponding intent (""BookTrip"") already created and published with version ""1"" for this configuration to work properly.",,,,,
"aws_vpc_dhcp_options, aws_vpc_dhcp_options_association","creates DHCP options named ""pike"" for an AWS VPC, specifying Google's DNS servers, and then associates these DHCP options with a specified VPC, tagging both the DHCP options and their association with ""permissions"".","package terraform.validation

# Set default validation states
default is_valid_dhcp_options = false

default is_valid_dhcp_options_association = false

# Validate aws_vpc_dhcp_options resource
is_valid_dhcp_options {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options""
        resource.expressions.tags.constant_value != null
}

# Validate aws_vpc_dhcp_options_association resource
is_valid_dhcp_options_association {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options_association""
        resource.expressions.dhcp_options_id.references[0] == ""aws_vpc_dhcp_options.pike.id""
        resource.expressions.vpc_id != null
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}


resource ""aws_vpc_dhcp_options_association"" ""pike"" {
  dhcp_options_id = aws_vpc_dhcp_options.pike.id
  vpc_id          = ""vpc-0c33dc8cd64f408c4""
}

resource ""aws_vpc_dhcp_options"" ""pike"" {
  domain_name_servers = [""8.8.8.8"", ""8.8.4.4""]
  tags = {
    pike = ""permissions""
  }
}","Has one ""aws_vpc_dhcp_options"" resource:
Specifies Google's DNS servers
Tagged with ""permissions"" to categorize or specify its role or access levels within the infrastructure

Has one 
""aws_vpc_dhcp_options_association"" resource:
Associates the ""aws_vpc_dhcp_options"" with a specified ""aws_vpc"" resource
Tagged with ""permissions"" to ensure consistent labeling and access management across related resources","I'll help you create a Terraform configuration that creates DHCP options with Google's DNS servers and associates them with a VPC. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""pike"" {
  domain_name_servers = [""8.8.8.8"", ""8.8.4.4""]  # Google DNS servers

  tags = {
    Name        = ""pike""
    permissions = ""true""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""pike_association"" {
  vpc_id          = var.vpc_id  # Reference to existing VPC ID
  dhcp_options_id = aws_vpc_dhcp_options.pike.id

  tags = {
    permissions = ""true""
  }
}

# Variable for VPC ID
variable ""vpc_id"" {
  type        = string
  description = ""The ID of the VPC to associate DHCP options with""
}
```

This configuration:
1. Creates a DHCP options set named ""pike"" with Google's DNS servers (8.8.8.8 and 8.8.4.4)
2. Tags the DHCP options with ""permissions""
3. Associates the DHCP options with a specified VPC (provided via variable)
4. Tags the association with ""permissions""
5. Includes the required AWS provider configuration

To use this, you'll need to provide the VPC ID when applying the configuration. Make sure you have the proper AWS credentials configured.",,,,,,"I'll help you create a Terraform configuration that creates DHCP options with Google's DNS servers and associates them with a VPC. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""pike"" {
  domain_name_servers = [""8.8.8.8"", ""8.8.4.4""]  # Google DNS servers

  tags = {
    Name        = ""pike""
    permissions = ""true""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""pike_dhcp_association"" {
  vpc_id          = var.vpc_id  # Reference to existing VPC ID
  dhcp_options_id = aws_vpc_dhcp_options.pike.id

  tags = {
    permissions = ""true""
  }
}

# Variable for VPC ID
variable ""vpc_id"" {
  type        = string
  description = ""ID of the VPC to associate DHCP options with""
}
```

This configuration:
1. Sets up the AWS provider requirement
2. Creates a DHCP options set named ""pike"" with Google's DNS servers (8.8.8.8 and 8.8.4.4)
3. Associates the DHCP options with a specified VPC
4. Includes appropriate tags with ""permissions""
5. Uses a variable for the VPC ID to make the configuration reusable

You'll need to provide the VPC ID when applying this configuration, either through a tfvars file or command line variable.",,,,,,"I'll help you create a Terraform configuration that creates DHCP options with Google's DNS servers and associates them with a VPC. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""pike"" {
  domain_name_servers = [""8.8.8.8"", ""8.8.4.4""]  # Google DNS servers

  tags = {
    Name        = ""pike""
    permissions = ""true""
  }
}

# Associate DHCP Options Set with VPC
resource ""aws_vpc_dhcp_options_association"" ""pike_association"" {
  vpc_id          = var.vpc_id  # Reference to existing VPC ID
  dhcp_options_id = aws_vpc_dhcp_options.pike.id

  tags = {
    permissions = ""true""
  }
}

# Variable for VPC ID
variable ""vpc_id"" {
  type        = string
  description = ""ID of the VPC to associate DHCP options with""
}
```

This configuration:
1. Creates a DHCP options set named ""pike"" using Google's DNS servers (8.8.8.8 and 8.8.4.4)
2. Tags the DHCP options with ""permissions""
3. Associates the DHCP options with a specified VPC (which should be provided as a variable)
4. Tags the association with ""permissions""
5. Includes the required AWS provider configuration

You'll need to provide the VPC ID when applying this configuration.",,,,,
"aws_vpc, aws_vpc, aws_vpc_peering_connection","establishes a VPC peering connection between two AWS Virtual Private Clouds (VPCs). The first VPC, named ""peer,"" is configured with a CIDR block of 10.0.0.0/24, and the second VPC, named ""base,"" has a CIDR block of 10.1.0.0/24. The peering connection, named ""pike,"" connects these two VPCs, allowing them to communicate with each other as if they were in the same network","package terraform.validation

# Set default validation states
default is_valid_vpcs = false
default is_valid_vpc_peering_connection = false

# Validate aws_vpc resources
is_valid_vpcs {
    # Validate the first VPC named ""peer""
    peer_vpc := input.configuration.root_module.resources[_]
    peer_vpc.type == ""aws_vpc""
    peer_vpc.name == ""peer""
    peer_vpc.expressions.cidr_block.constant_value == ""10.0.0.0/24""

    # Validate the second VPC named ""base""
    base_vpc := input.configuration.root_module.resources[_]
    base_vpc.type == ""aws_vpc""
    base_vpc.name == ""base""
    base_vpc.expressions.cidr_block.constant_value == ""10.1.0.0/24""

    # Ensure different VPCs
    peer_vpc != base_vpc
}

# Validate aws_vpc_peering_connection resource
is_valid_vpc_peering_connection {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_vpc_peering_connection""
    resource.name == ""pike""
    # Ensure connection between ""peer"" and ""base"" VPCs
    resource.expressions.peer_vpc_id.references[0] == ""aws_vpc.peer.id""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.base.id""
    
}",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id

}

resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
}

resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
}","Has two ""aws_vpc"" resources:
The first VPC is named ""peer"" and is configured with a ""cidr_block"" of ""10.0.0.0/24"".
The second VPC is named ""base"" and is configured with a ""cidr_block"" of ""10.1.0.0/24"".

Has one ""aws_vpc_peering_connection"" resource:
Named ""pike"" to connect the two specified VPCs (""peer"" and ""base"").
Configured to allow the VPCs to communicate with each other as if they were in the same network.","I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create the peer VPC
resource ""aws_vpc"" ""peer"" {
  cidr_block           = ""10.0.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""peer-vpc""
  }
}

# Create the base VPC
resource ""aws_vpc"" ""base"" {
  cidr_block           = ""10.1.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""base-vpc""
  }
}

# Create VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    Name = ""pike-peering-connection""
  }
}

# Update route table for peer VPC
resource ""aws_route"" ""peer_to_base"" {
  route_table_id            = aws_vpc.peer.main_route_table_id
  destination_cidr_block    = aws_vpc.base.cidr_block
  vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
}

# Update route table for base VPC
resource ""aws_route"" ""base_to_peer"" {
  route_table_id            = aws_vpc.base.main_route_table_id
  destination_cidr_block    = aws_vpc.peer.cidr_block
  vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two VPCs (""peer"" and ""base"") with the specified CIDR blocks
3. Establishes a VPC peering connection named ""pike"" between them
4. Updates the route tables of both VPCs to allow traffic to flow between them

Note that:
- The `auto_accept = true` parameter automatically accepts the peering connection (only works when both VPCs are in the same AWS account)
- Route table entries are created in both VPCs to enable bi-directional communication
- DNS hostname support is enabled for both VPCs
- You should change the region to match your requirements",,,,,,"I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create the peer VPC
resource ""aws_vpc"" ""peer_vpc"" {
  cidr_block           = ""10.0.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""peer-vpc""
  }
}

# Create the base VPC
resource ""aws_vpc"" ""base_vpc"" {
  cidr_block           = ""10.1.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""base-vpc""
  }
}

# Create VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer_vpc.id
  vpc_id      = aws_vpc.base_vpc.id
  auto_accept = true

  tags = {
    Name = ""pike-peering-connection""
  }
}

# Create route table for peer VPC
resource ""aws_route_table"" ""peer_route_table"" {
  vpc_id = aws_vpc.peer_vpc.id

  route {
    cidr_block                = aws_vpc.base_vpc.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""peer-route-table""
  }
}

# Create route table for base VPC
resource ""aws_route_table"" ""base_route_table"" {
  vpc_id = aws_vpc.base_vpc.id

  route {
    cidr_block                = aws_vpc.peer_vpc.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""base-route-table""
  }
}

# Associate route tables with VPCs
resource ""aws_main_route_table_association"" ""peer_route_association"" {
  vpc_id         = aws_vpc.peer_vpc.id
  route_table_id = aws_route_table.peer_route_table.id
}

resource ""aws_main_route_table_association"" ""base_route_association"" {
  vpc_id         = aws_vpc.base_vpc.id
  route_table_id = aws_route_table.base_route_table.id
}
```

This configuration:
1. Creates two VPCs (""peer"" and ""base"") with specified CIDR blocks
2. Establishes a VPC peering connection named ""pike"" between them
3. Creates and configures route tables for both VPCs to enable traffic flow
4. Associates the route tables with their respective VPCs

The configuration includes:
- Required AWS provider block
- VPC resources with specified CIDR blocks
- VPC peering connection with auto-accept enabled
- Route tables with necessary routes for the peering connection
- Route table associations

Make sure to replace the region in the provider block with your desired AWS region. This configuration will create a fully functional VPC peering setup allowing communication between the two VPCs.",,,,,,"I'll help you create a Terraform configuration for VPC peering between two VPCs. Here's the solution that establishes the VPCs and their peering connection:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create the peer VPC
resource ""aws_vpc"" ""peer"" {
  cidr_block           = ""10.0.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""peer-vpc""
  }
}

# Create the base VPC
resource ""aws_vpc"" ""base"" {
  cidr_block           = ""10.1.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""base-vpc""
  }
}

# Create the VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    Name = ""pike-peering-connection""
  }
}

# Add route table entry for peer VPC
resource ""aws_route_table"" ""peer_route_table"" {
  vpc_id = aws_vpc.peer.id

  route {
    cidr_block                = aws_vpc.base.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""peer-route-table""
  }
}

# Add route table entry for base VPC
resource ""aws_route_table"" ""base_route_table"" {
  vpc_id = aws_vpc.base.id

  route {
    cidr_block                = aws_vpc.peer.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""base-route-table""
  }
}

# Associate route tables with respective VPCs
resource ""aws_main_route_table_association"" ""peer_route_association"" {
  vpc_id         = aws_vpc.peer.id
  route_table_id = aws_route_table.peer_route_table.id
}

resource ""aws_main_route_table_association"" ""base_route_association"" {
  vpc_id         = aws_vpc.base.id
  route_table_id = aws_route_table.base_route_table.id
}
```

This configuration:
1. Creates two VPCs (""peer"" and ""base"") with specified CIDR blocks
2. Establishes a VPC peering connection named ""pike"" between them
3. Creates route tables for both VPCs
4. Adds necessary routes to allow traffic between the VPCs
5. Associates the route tables with their respective VPCs

The `auto_accept = true` parameter automatically accepts the peering connection request since both VPCs are in the same AWS account. If you're peering VPCs across different AWS accounts, you'll need to handle the acceptance separately.

Make sure to adjust the region in the provider block according to your needs. Also, consider adding additional security groups and network ACLs to control traffic between the VPCs as needed.",,,,,
"aws_elasticache_user, aws_elasticache_user_group",Provide an ElastiCache user group resource.,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_group_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user_group""
}

# Aggregate all checks
allow {
    aws_elasticache_user_group_valid(input.resource_changes)
}",2,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""default""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}

resource ""aws_elasticache_user_group"" ""test"" {
  engine        = ""REDIS""
  user_group_id = ""userGroupId""
  user_ids      = [aws_elasticache_user.test.user_id]
}","creates elasticache user resources(requires access string, engine = REDIS, user id and user name) and sets engine , creates group resource using the user ids","I'll help you create a Terraform configuration for an AWS ElastiCache user group resource. This configuration will include the required AWS provider and the ElastiCache user group resource.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_elasticache_user_group"" ""example"" {
  engine        = ""redis""
  user_group_id = ""example-user-group""
  user_ids      = [""example-user""]

  tags = {
    Name = ""example-elasticache-user-group""
    Environment = ""production""
  }
}

# Note: You'll need to have the referenced user(s) created first
resource ""aws_elasticache_user"" ""example"" {
  user_id       = ""example-user""
  user_name     = ""example-user""
  access_string = ""on ~* +@all""
  engine        = ""redis""
  passwords     = [""example-password123""]
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an ElastiCache user group resource with:
   - A Redis engine
   - A user group ID
   - Associated user IDs
   - Tags for better resource management
3. Includes a sample ElastiCache user resource that would be referenced by the user group

Note that you'll need to adjust the values (user_group_id, user_ids, passwords, etc.) according to your specific requirements. Also ensure that any referenced users exist before creating the user group.",,,,,,"I'll help you create a Terraform configuration for an AWS ElastiCache user group resource. This will include the required AWS provider and the ElastiCache user group resource.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_elasticache_user_group"" ""example"" {
  engine        = ""redis""
  user_group_id = ""example-user-group""
  user_ids      = [""example-user""]

  tags = {
    Name = ""example-elasticache-user-group""
    Environment = ""production""
  }
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an ElastiCache user group resource with:
   - Redis as the engine
   - A unique user group ID
   - A list of user IDs that belong to this group
   - Tags for better resource management

You can customize the `user_group_id`, `user_ids`, and tags according to your specific needs. Note that you'll need to have already created the ElastiCache users referenced in the `user_ids` list.",,,,,,"I'll help you create a Terraform configuration for an AWS ElastiCache user group resource. Here's a complete example with the required provider and an ElastiCache user group configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_elasticache_user_group"" ""example"" {
  engine        = ""redis""
  user_group_id = ""example-user-group""
  user_ids      = [""example-user""]

  tags = {
    Name = ""example-user-group""
    Environment = ""production""
  }
}

# Optional: Create a user that can be associated with the user group
resource ""aws_elasticache_user"" ""example"" {
  user_id       = ""example-user""
  user_name     = ""example-user""
  access_string = ""on ~* +@all""
  engine        = ""redis""
  passwords     = [""password123""]  # It's recommended to use variables or secrets management
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates an ElastiCache user group with:
   - A specified Redis engine
   - A unique user group ID
   - Associated user IDs
   - Resource tags
3. Includes an optional ElastiCache user resource that can be associated with the group

Remember to:
- Replace the placeholder values with your actual desired configuration
- Use secure password management practices
- Adjust the access permissions according to your security requirements
- Configure additional parameters as needed for your specific use case",,,,,
"aws_eks_addon, aws_eks_cluster, aws_iam_role, aws_subnet, aws_vpc","Generate a Terraform script to add an addon with name ""example"", to an AWS EKS cluster. Define a resource that specifies the cluster's name and sets the addon name to 'vpc-cni'.","package terraform.validation

default has_aws_eks_addon_example = false

has_aws_eks_addon_example {
    addon := input.planned_values.root_module.resources[_]
    addon.type == ""aws_eks_addon""
    addon.name == ""example""
    addon.values.cluster_name == input.planned_values.root_module.resources[_].values.name  # Ensures it references aws_eks_cluster.example correctly
    addon.values.addon_name == ""vpc-cni""
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_iam_role"" ""example"" {
  name = ""eks-cluster-1""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""eks.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_eks_cluster"" ""example"" {
  name     = ""test""
  version  = ""test-version""
  role_arn = aws_iam_role.example.arn

  vpc_config {
    subnet_ids = [
      aws_subnet.private-us-east-1a.id,
    ]
  }
}

resource ""aws_eks_addon"" ""example"" {
  cluster_name = aws_eks_cluster.example.name
  addon_name   = ""vpc-cni""
}","has one ""aws_eks_addon"" resource
with one ""cluster_name""
with one ""addon_name""","I'll help you create a Terraform configuration to add a VPC-CNI addon to an EKS cluster. Here's the code:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_eks_addon"" ""example"" {
  cluster_name = ""example""
  addon_name   = ""vpc-cni""
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an EKS addon resource named ""example""
3. Specifies the cluster name as ""example""
4. Sets the addon name to ""vpc-cni"" which is the Amazon VPC CNI plugin for Kubernetes

The `aws_eks_addon` resource allows you to manage EKS add-ons in your cluster. The VPC-CNI addon is essential for networking in EKS clusters as it's responsible for providing networking capabilities to your pods.

Note: Make sure the EKS cluster exists before applying this configuration, as the addon resource depends on an existing cluster.",,,,,,"I'll help you create a Terraform configuration to add an addon to an AWS EKS cluster. The configuration will include the required AWS provider and the EKS addon resource.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

resource ""aws_eks_addon"" ""example"" {
  cluster_name = ""example-cluster""  # Replace with your cluster name
  addon_name   = ""vpc-cni""
  
  # Optional: Specify addon version if needed
  # addon_version = ""v1.12.0-eksbuild.1""
  
  # Optional: Configure resolve conflicts
  # resolve_conflicts = ""OVERWRITE""
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an EKS addon resource named ""example""
3. Specifies the cluster name and sets the addon name to 'vpc-cni'
4. Includes commented optional parameters that you can uncomment and configure as needed

Make sure to replace ""example-cluster"" with your actual EKS cluster name. The vpc-cni addon is commonly used for networking in EKS clusters.",,,,,,"I'll help you create a Terraform configuration to add an addon to an AWS EKS cluster. Here's a solution that creates an EKS addon for the VPC CNI:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

resource ""aws_eks_addon"" ""example"" {
  cluster_name = ""your-eks-cluster-name""  # Replace with your cluster name
  addon_name   = ""vpc-cni""
  
  # Optional: Specify addon version if needed
  # addon_version = ""v1.12.0-eksbuild.1""
  
  # Optional: Configure resolve conflicts
  # resolve_conflicts = ""OVERWRITE""
  
  tags = {
    Environment = ""production""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an EKS addon resource named ""example""
3. Specifies the EKS cluster name where the addon should be installed
4. Sets the addon name to 'vpc-cni'
5. Includes optional commented configurations for version control and conflict resolution
6. Adds tags for better resource management

Remember to replace ""your-eks-cluster-name"" with your actual EKS cluster name. The vpc-cni addon is essential for networking in EKS clusters.",,,,,
"aws_iam_policy, aws_iam_role, aws_iam_role_policy_attachment, aws_kinesis_analytics_application, aws_kinesis_stream, aws_iam_policy_document, aws_iam_policy_document",Generate a basic Amazon Kinesis Analytics Application with an input kinesis stream,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kinesisanalytics.amazonaws.com"") 
}

has_valid_kinesis_stream {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_stream""
    resource.values.name
}

has_valid_kinesis_analytics_application {
        some i
        resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_analytics_application""
    resource.values.name
    resource.values.inputs[_].kinesis_stream
}

has_valid_resources {
        has_valid_iam_role(input.resource_changes)
        has_valid_kinesis_stream
    has_valid_kinesis_analytics_application
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}


data ""aws_iam_policy_document"" ""StreamReaderWriter"" {
  statement {
    effect = ""Allow""
    actions = [""kinesis:DescribeStream"",
               ""kinesis:GetShardIterator"",
               ""kinesis:GetRecords""]
    resources = [aws_kinesis_stream.test_stream.arn]
  }
}

data ""aws_iam_policy_document"" ""KinesisAssume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}

resource ""aws_iam_policy"" ""StreamReaderWriter"" {
  name   = ""KinesisStreamReaderWriter""
  policy = data.aws_iam_policy_document.StreamReaderWriter.json
}

resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_role""
  assume_role_policy = data.aws_iam_policy_document.KinesisAssume.json
}

resource ""aws_iam_role_policy_attachment"" ""attach_kinesis_role"" {
  role       = aws_iam_role.kinesis_role.name
  policy_arn = aws_iam_policy.StreamReaderWriter.arn
}

resource ""aws_kinesis_stream"" ""test_stream"" {
  name             = ""events""
  shard_count      = 1
  retention_period = 168
}


resource ""aws_kinesis_analytics_application"" ""test_application"" {
  name = ""kinesis-analytics-application-test""

  inputs {
    name_prefix = ""test_prefix""

    kinesis_stream {
      resource_arn = aws_kinesis_stream.test_stream.arn
      role_arn     = aws_iam_role.kinesis_role.arn
    }

    parallelism {
      count = 1
    }

    schema {
      record_columns {
        mapping  = ""$.test""
        name     = ""test""
        sql_type = ""VARCHAR(8)""
      }

      record_encoding = ""UTF-8""

      record_format {
        mapping_parameters {
          json {
            record_row_path = ""$""
          }
        }
      }
    }
  }
}","Has one ""aws_kinesis_analytics_application"" resource with one ""aws_kinesis_stream"" resource as an input","I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with an input Kinesis stream. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create a Kinesis stream as input
resource ""aws_kinesis_stream"" ""input_stream"" {
  name             = ""input-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""development""
  }
}

# Create an IAM role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-application-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Create IAM policy for Kinesis Analytics to read from input stream
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-application-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kinesis:GetShardIterator"",
          ""kinesis:GetRecords"",
          ""kinesis:DescribeStream"",
          ""kinesis:ListShards""
        ]
        Resource = [aws_kinesis_stream.input_stream.arn]
      }
    ]
  })
}

# Create Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""example-kinesis-analytics-app""

  inputs {
    name_prefix = ""SOURCE_SQL_STREAM""

    kinesis_stream {
      resource_arn = aws_kinesis_stream.input_stream.arn
      role_arn     = aws_iam_role.kinesis_analytics_role.arn
    }

    schema_version = ""1""

    schema {
      record_columns {
        name     = ""example_column""
        sql_type = ""VARCHAR(8)""
        mapping  = ""$.example""
      }

      record_format {
        mapping_parameters {
          json {
            record_row_path = ""$""
          }
        }
      }
    }
  }
}
```

This configuration creates:

1. A Kinesis input stream with 1 shard and 24-hour retention
2. An IAM role and policy for the Kinesis Analytics application to read from the input stream
3. A Kinesis Analytics application that reads from the input stream

The configuration includes:
- Required AWS provider block
- Input Kinesis stream configuration
- IAM role and policy for permissions
- Kinesis Analytics application with basic JSON schema configuration

You can customize the configuration by:
- Adjusting the region
- Modifying the stream name, shard count, and retention period
- Updating the schema configuration to match your data structure
- Adding more record columns as needed
- Adjusting IAM permissions based on your requirements

Remember to replace placeholder values with your actual requirements before applying the configuration.",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with an input Kinesis stream. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create a Kinesis stream as input
resource ""aws_kinesis_stream"" ""input_stream"" {
  name             = ""input-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""production""
  }
}

# Create IAM role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Kinesis Analytics to read from input stream
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kinesis:GetShardIterator"",
          ""kinesis:GetRecords"",
          ""kinesis:DescribeStream"",
          ""kinesis:ListShards""
        ]
        Resource = aws_kinesis_stream.input_stream.arn
      }
    ]
  })
}

# Create Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""example-analytics-application""

  inputs {
    name_prefix = ""INPUT_""

    kinesis_stream {
      resource_arn = aws_kinesis_stream.input_stream.arn
      role_arn    = aws_iam_role.kinesis_analytics_role.arn
    }

    schema_version = ""1""

    schema {
      record_columns {
        name     = ""example_column""
        sql_type = ""VARCHAR(8)""
        mapping  = ""$.example""
      }

      record_format {
        mapping_parameters {
          json {
            record_row_path = ""$""
          }
        }
      }
    }
  }
}
```

This configuration creates:
1. A Kinesis input stream
2. An IAM role and policy for Kinesis Analytics to access the input stream
3. A Kinesis Analytics application that reads from the input stream

The application is configured to:
- Read JSON data from the input stream
- Process a single column named ""example_column""
- Use JSON format for record parsing

You can customize the configuration by:
- Adjusting the shard count and retention period of the input stream
- Adding more record columns in the schema
- Modifying the IAM permissions as needed
- Changing the region, names, and tags according to your requirements

Remember to replace the placeholder values with your actual requirements before applying this configuration.",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with an input Kinesis stream. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create an IAM role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-application-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Create an IAM role policy for Kinesis Analytics
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-application-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kinesis:GetShardIterator"",
          ""kinesis:GetRecords"",
          ""kinesis:DescribeStream"",
          ""kinesis:ListShards""
        ]
        Resource = [aws_kinesis_stream.input_stream.arn]
      }
    ]
  })
}

# Create the input Kinesis stream
resource ""aws_kinesis_stream"" ""input_stream"" {
  name             = ""kinesis-input-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""dev""
  }
}

# Create the Kinesis Analytics application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""example-kinesis-analytics-app""

  inputs {
    name_prefix = ""PREFIX_""

    kinesis_stream {
      resource_arn = aws_kinesis_stream.input_stream.arn
      role_arn     = aws_iam_role.kinesis_analytics_role.arn
    }

    schema_version = ""1""

    schema {
      record_columns {
        name     = ""example_column""
        sql_type = ""VARCHAR(8)""
        mapping  = ""$.example""
      }

      record_format {
        mapping_parameters {
          json {
            record_row_path = ""$""
          }
        }
      }
    }
  }
}
```

This configuration creates:
1. An IAM role and policy for the Kinesis Analytics application
2. An input Kinesis stream
3. A Kinesis Analytics application with the input stream configured

The configuration includes:
- Required AWS provider block
- IAM role with necessary permissions to read from Kinesis streams
- A basic Kinesis stream with one shard
- A Kinesis Analytics application with JSON input format and a sample schema

You can customize the configuration by:
- Adjusting the region
- Modifying the stream settings (shard count, retention period)
- Updating the schema to match your data structure
- Adding more record columns as needed
- Adjusting the IAM permissions based on your requirements

Remember to replace placeholder values with your actual requirements before applying the configuration.",,,,,
aws_lambda_layer_version,"Create a Lambda Layer Version resource from ""lambda_layer_payload.zip""","package main

import future.keywords.in

default allow = false

# Check if the AWS Lambda Layer Version is valid
aws_lambda_layer_version_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_layer_version""
    resource.change.after.filename == ""lambda_layer_payload.zip""
}

# Aggregate all checks
allow {
    aws_lambda_layer_version_valid(input.resource_changes)
}",1,"resource ""aws_lambda_layer_version"" ""lambda_layer"" {
  filename   = ""lambda_layer_payload.zip""
  layer_name = ""lambda_layer_name""

  compatible_runtimes = [""nodejs16.x""]
}","Create a aws_lambda_layer_version by using file_name = ""lambda_layer_payload.zip"". Layer name can be any.","I'll help you create a Terraform configuration for an AWS Lambda Layer Version resource using the specified ZIP file. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_lambda_layer_version"" ""example_layer"" {
  filename            = ""lambda_layer_payload.zip""
  layer_name         = ""example_lambda_layer""
  description        = ""Example Lambda Layer from ZIP file""
  compatible_runtimes = [""python3.8"", ""python3.9""] # Adjust runtimes as needed

  # Optional: Set compatible architectures
  compatible_architectures = [""x86_64""]
}
```

This configuration:
1. Defines the required AWS provider
2. Creates a Lambda Layer Version using the specified ZIP file
3. Sets basic properties like layer name, description, and compatible runtimes
4. Includes compatible architectures (optional)

You can customize the `compatible_runtimes` and other parameters according to your specific needs. The `lambda_layer_payload.zip` file should be present in your working directory or specify the full path to its location.",,,,,,"I'll help you create a Terraform configuration for an AWS Lambda Layer Version resource that uses a ""lambda_layer_payload.zip"" file.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_lambda_layer_version"" ""lambda_layer"" {
  filename            = ""lambda_layer_payload.zip""
  layer_name         = ""my_lambda_layer""
  description        = ""Lambda layer from payload zip""
  compatible_runtimes = [""python3.8"", ""python3.9""]  # Adjust runtimes as needed

  # Optional: specify compatible architectures
  compatible_architectures = [""x86_64""]
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Lambda Layer Version resource using the specified zip file
3. Includes basic configuration options like layer name, description, and compatible runtimes
4. Specifies compatible architecture

You can adjust the compatible runtimes and other parameters according to your specific needs. The `lambda_layer_payload.zip` file should be present in your working directory when applying this configuration.

Make sure the zip file contains the appropriate layer content structured according to AWS Lambda Layer requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS Lambda Layer Version resource using the specified zip file. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

resource ""aws_lambda_layer_version"" ""example_layer"" {
  filename            = ""lambda_layer_payload.zip""
  layer_name         = ""example_lambda_layer""
  description        = ""Example Lambda Layer from payload.zip""
  compatible_runtimes = [""python3.8"", ""python3.9""]  # Specify compatible runtimes

  # Optional: specify compatible architectures
  compatible_architectures = [""x86_64""]
}
```

This configuration:
1. Declares the required AWS provider
2. Creates a Lambda Layer Version using the specified zip file
3. Sets basic configuration like layer name, description, and compatible runtimes
4. Specifies compatible architecture

You can customize the `compatible_runtimes` and other parameters according to your specific needs. The layer will be created from the ""lambda_layer_payload.zip"" file in your working directory.",,,,,
