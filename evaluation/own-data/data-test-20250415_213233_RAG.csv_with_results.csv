Resource,Prompt,Rego intent,Difficulty,Reference output,Intent,LLM Output #0,LLM Plannable? #0,LLM Correct? #0,LLM Plan Phase Error #0,LLM OPA match phase Error #0,LLM Notes #0,LLM Output #1,LLM Plannable? #1,LLM Correct? #1,LLM Plan Phase Error #1,LLM OPA match phase Error #1,LLM Notes #1,LLM Output #2,LLM Plannable? #2,LLM Correct? #2,LLM Plan Phase Error #2,LLM OPA match phase Error #2,LLM Notes #2
"aws_api_gateway_integration, aws_api_gateway_integration, aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_dynamodb_table, aws_iam_role, aws_iam_role, aws_lambda_function, aws_lambda_function, aws_lambda_permission, aws_lambda_permission, aws_s3_bucket","An AWS service that holds a web server which allows you to upload cat pictures and provides random cat pictures on demand. Accomplish this using the following resources: AWS DynamoDB table, AWS S3 bucket, AWS Lambda function, AWS Lambda permission, AWS API Gateway rest API, AWS API Gateway resource, AWS API Gateway method. Add any necessary resources.","package caas_middle

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
	api.address in cat.expressions.parent_id.references
	api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
	method.expressions.http_method.constant_value == method_string
	resource.address in method.expressions.resource_id.references
	api.address in method.expressions.rest_api_id.references
}

table_valid(table) if {
	some attribute in table.expressions.attribute
	attribute.name
	attribute.type

	table.expressions.hash_key
}

lambda_valid(lambda, bucket) if {
	some env in lambda.expressions.environment
	bucket.address in env.variables.references
}

permission_valid(permission, lambda, api) if {
	permission.expressions.action.constant_value == ""lambda:InvokeFunction""
	lambda.address in permission.expressions.function_name.references
	permission.expressions.principal.constant_value == ""apigateway.amazonaws.com""
	api.address in permission.expressions.source_arn.references
}

integration_valid(integration, lambda, method, resource, api, integration_method) if {
	method.address in integration.expressions.http_method.references
	resource.address in integration.expressions.resource_id.references
	api.address in integration.expressions.rest_api_id.references
	integration.expressions.integration_http_method.constant_value == integration_method
	integration.expressions.type.constant_value == ""AWS_PROXY""
	lambda.address in integration.expressions.uri.references
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some api in resources
	api.type == ""aws_api_gateway_rest_api""

	some cat in resources
	cat.type == ""aws_api_gateway_resource""

	some method_get in resources
	method_get.type == ""aws_api_gateway_method""

	some method_put in resources
	method_put.type == ""aws_api_gateway_method""

	some table in resources
	table.type == ""aws_dynamodb_table""

	some lambda_get in resources
	lambda_get.type == ""aws_lambda_function""

	some lambda_put in resources
	lambda_put.type == ""aws_lambda_function""

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	some permission_get in resources
	permission_get.type == ""aws_lambda_permission""

	some permission_put in resources
	permission_put.type == ""aws_lambda_permission""

	some integration_get in resources
	integration_get.type == ""aws_api_gateway_integration""

	some integration_put in resources
	integration_put.type == ""aws_api_gateway_integration""

	api_valid(api)
	cat_valid(cat, api)
	method_valid(method_get, ""GET"", cat, api)
	method_valid(method_put, ""PUT"", cat, api)
	lambda_valid(lambda_get, bucket, archive_get)
	lambda_valid(lambda_put, bucket, archive_put)
	permission_valid(permission_get, lambda_get, api)
	permission_valid(permission_put, lambda_put, api)
	integration_valid(integration_get, lambda_get, method_get, cat, api, ""GET"")
	integration_valid(integration_put, lambda_put, method_put, cat, api, ""PUT"")
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_api resource id
    with ""resource_id"" linking to aws_api_gateway_resource resource
    with ""authorization"" argument
    with ""http_method"" argument with values ""GET"" and ""PUT""
have one ""aws_s3_bucket"" resource
have one ""aws_dynamodb_table"" resource
    with ""name"" argument
    with ""hash_key"" argument
    with ""attribute"" argument block
        with ""name"" argument
        with ""type"" argument
with two ""aws_lambda_function"" resources
    with ""function_name"" argument
    with ""role"" argument
    with ""environment"" argument block
        with ""variables"" argument block linking to the aws_s3_bucket
with two ""aws_lambda_permission"" resources
    with ""action"" argument with value ""lambda:InvokeFunction""
    with ""principal"" argument with value ""apigateway.amazonaws.com""
    with ""function_name"" argument linking to aws_lambda_function resource
    with ""source_arn"" argument linking to aws_api_gateway_rest_api (two different end points)
with two ""aws_api_gateway_integration"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource resource
    with ""http_method"" argument linking to aws_api_gateway_method resource
    with ""type"" argument with value ""AWS_PROXY""
    with ""integration_http_method"" argument with values ""GET"" and ""PUT""","I'll help you create a Terraform configuration for a cat picture service using AWS services. Here's a solution that includes all the requested components:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-storage-${random_string.suffix.result}""
}

resource ""random_string"" ""suffix"" {
  length  = 8
  special = false
  upper   = false
}

# DynamoDB table for metadata
resource ""aws_dynamodb_table"" ""cat_metadata"" {
  name           = ""cat-pictures-metadata""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""picture_id""
  attribute {
    name = ""picture_id""
    type = ""S""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket"",
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*"",
          aws_dynamodb_table.cat_metadata.arn
        ]
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_pictures_handler"" {
  filename         = ""lambda_function.zip""
  function_name    = ""cat_pictures_handler""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME = aws_s3_bucket.cat_pictures.id
      TABLE_NAME  = aws_dynamodb_table.cat_metadata.name
    }
  }
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name        = ""cat-pictures-api""
  description = ""API for cat pictures service""
}

# API Gateway resource for cat pictures
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway GET method
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# API Gateway POST method
resource ""aws_api_gateway_method"" ""post_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat",,,,,,"I'll help you create a Terraform configuration for a cat picture service using AWS services. Here's a solution that includes all the requested components:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-storage-${random_string.suffix.result}""
}

# Random string for unique naming
resource ""random_string"" ""suffix"" {
  length  = 8
  special = false
}

# DynamoDB table for metadata
resource ""aws_dynamodb_table"" ""cat_metadata"" {
  name           = ""cat-pictures-metadata""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""picture_id""
  
  attribute {
    name = ""picture_id""
    type = ""S""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket"",
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*"",
          aws_dynamodb_table.cat_metadata.arn
        ]
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_pictures_handler"" {
  filename         = ""lambda_function.zip""
  function_name    = ""cat_pictures_handler""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME = aws_s3_bucket.cat_pictures.id
      TABLE_NAME  = aws_dynamodb_table.cat_metadata.name
    }
  }
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat-pictures-api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway methods
resource ""aws_api_gateway_method"" ""get_cats"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""post_cats"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_",,,,,,"I'll help you create a Terraform configuration for a cat picture service using AWS services. Here's a solution that includes all the requested components:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-bucket""
}

resource ""aws_s3_bucket_public_access_block"" ""cat_pictures"" {
  bucket = aws_s3_bucket.cat_pictures.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# DynamoDB table to store picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures"" {
  name           = ""cat-pictures""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""picture_id""
  attribute {
    name = ""picture_id""
    type = ""S""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket"",
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*"",
          aws_dynamodb_table.cat_pictures.arn
        ]
      },
      {
        Effect = ""Allow""
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = [""arn:aws:logs:*:*:*""]
      }
    ]
  })
}

# Lambda function for handling cat pictures
resource ""aws_lambda_function"" ""cat_pictures"" {
  filename         = ""lambda_function.zip""
  function_name    = ""cat_pictures_handler""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME    = aws_s3_bucket.cat_pictures.id
      DYNAMODB_TABLE = aws_dynamodb_table.cat_pictures.name
    }
  }
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat-pictures-api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""pictures""
}

# API Gateway methods
resource ""aws_api_gateway_method"" ""get_pictures"" {
  rest_api_id   = aws_api",,,,,
"aws_codebuild_project, aws_iam_role, aws_s3_bucket, aws_s3_bucket",create a AWS codebuild project resource with example iam role and secondary artifiacts,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for aws_iam_role resource
has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""codebuild.amazonaws.com"") 
} 

has_valid_codebuild {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_codebuild_project""
        resource.values.artifacts
    resource.values.environment
    resource.values.name
    resource.values.source[_].type
    resource.values.secondary_artifacts[_].type
    role := input.configuration.root_module.resources[i]
    role.expressions.service_role
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    
}


has_valid_resources {
        has_valid_iam_role(input.resource_changes)
    has_valid_codebuild
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""test_role8"" {
  name = ""test_role8""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Sid    = """"
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      },
    ]
  })
}

resource ""aws_s3_bucket"" ""apriltwentyninth"" {
  bucket = ""apriltwentyninth""
}

resource ""aws_s3_bucket"" ""apriltwentyninth2"" {
  bucket = ""apriltwentyninth2""
}


resource ""aws_codebuild_project"" ""example7"" {
  name          = ""test-project8""
  service_role  = aws_iam_role.test_role8.arn

  artifacts {
    location  = aws_s3_bucket.apriltwentyninth.bucket
    type      = ""S3""
    name     = ""results.zip""
    path      = ""/""
    packaging = ""ZIP""
  }

  secondary_artifacts {
    artifact_identifier =  ""SecondaryArtifact""
    type = ""S3""
    location  = aws_s3_bucket.apriltwentyninth2.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:7.0-24.10.29""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  source_version = ""master""

  secondary_sources {
    source_identifier = ""source2""
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  secondary_source_version {
    source_identifier = ""source2""
    source_version = ""master""
  }
}","Has one ""aws_iam_role"" resource, at least one ""aws_s3_bucket"" resource, and one ""aws_codebuild_project"" resource with a ""secondary_artifacts"" block","I'll help you create an AWS CodeBuild project resource with an IAM role and secondary artifacts using Terraform. Here's a comprehensive example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""example-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""example-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
      }
    ]
  })
}

# S3 Bucket for artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket = ""example-codebuild-artifacts""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""example"" {
  name          = ""example-project""
  description   = ""Example CodeBuild project""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  secondary_artifacts {
    artifact_identifier = ""secondary_artifact1""
    type               = ""S3""
    location           = aws_s3_bucket.artifact_bucket.id
    name               = ""secondary_output""
    packaging          = ""ZIP""
    path              = ""secondary/""
  }

  secondary_artifacts {
    artifact_identifier = ""secondary_artifact2""
    type               = ""S3""
    location           = aws_s3_bucket.artifact_bucket.id
    name               = ""other_output""
    packaging          = ""ZIP""
    path              = ""other/""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""ENVIRONMENT""
      value = ""development""
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""example-log-group""
      stream_name = ""example-log-stream""
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration includes:

1. An IAM role and policy for CodeBuild with basic permissions
2. An S3 bucket for storing artifacts
3. A CodeBuild project with:
   - Multiple secondary artifacts configurations
   - Environment configuration
   - Source configuration (GitHub in this example)
   - Logs configuration
   - Build timeout and other basic settings

The CodeBuild project includes two secondary artifacts that will be stored in the specified S3 bucket. Each secondary artifact has its own configuration for packaging and path.

Remember",,,,,,"I'll help you create an AWS CodeBuild project with an IAM role and secondary artifacts configuration. Here's a complete Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-service-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""codebuild-service-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
      }
    ]
  })
}

# S3 Bucket for artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""example"" {
  name          = ""example-project""
  description   = ""Example CodeBuild project""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  secondary_artifacts {
    type                = ""S3""
    artifact_identifier = ""secondary_artifact""
    location           = aws_s3_bucket.artifact_bucket.id
    packaging          = ""ZIP""
    encryption_disabled = false
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:4.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""example-log-group""
      stream_name = ""example-log-stream""
    }

    s3_logs {
      status   = ""ENABLED""
      location = ""${aws_s3_bucket.artifact_bucket.id}/build-logs""
    }
  }

  tags = {
    Environment = ""Test""
  }
}
```

This configuration includes:

1. The required AWS provider configuration
2. An IAM role and policy for CodeBuild with basic permissions
3. An S3 bucket for artifacts
4. A CodeBuild project with:
   - Basic project settings
   - Secondary artifacts configuration pointing to an S3 bucket
   - Environment configuration
   - Source configuration (using GitHub as an example)
   - Logs configuration
   - Tags

Key features:
- The IAM role has basic permissions for CloudWatch Logs and S3
- Secondary artifacts are configured to store in S3 with ZIP packaging
- The build environment uses the standard AWS CodeBuild image
- Logging is configured for both CloudWatch and S3

You can customize the values (bucket names, project name, GitHub repository, etc.) according to your specific needs. Remember to adjust",,,,,,"I'll help you create an AWS CodeBuild project with an IAM role and secondary artifacts. Here's a comprehensive Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-project-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""codebuild-project-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:*"",
          ""codecommit:*""
        ]
      }
    ]
  })
}

# S3 Bucket for artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""project"" {
  name          = ""example-project""
  description   = ""Example CodeBuild project""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  secondary_artifacts {
    type                = ""S3""
    artifact_identifier = ""secondary_artifact""
    location           = aws_s3_bucket.artifact_bucket.id
    path               = ""secondary/""
    packaging          = ""ZIP""
    namespace_type     = ""BUILD_ID""
  }

  secondary_artifacts {
    type                = ""S3""
    artifact_identifier = ""logs_artifact""
    location           = aws_s3_bucket.artifact_bucket.id
    path               = ""logs/""
    packaging          = ""ZIP""
    namespace_type     = ""BUILD_ID""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""ENVIRONMENT""
      value = ""development""
    }
  }

  source {
    type            = ""CODECOMMIT""
    location        = ""https://git-codecommit.region.amazonaws.com/v1/repos/example-repo""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""codebuild-log-group""
      stream_name = ""codebuild-log-stream""
      status      = ""ENABLED""
    }

    s3_logs {
      status   = ""ENABLED""
      location = ""${aws_s3_bucket.artifact_bucket.id}/build-logs""
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration includes:

1. A CodeBuild project with secondary artifacts configuration
2. An IAM role and policy for CodeBuild permissions
3. An S3 bucket for storing artifacts
4. Environment configuration for the build
5. Source configuration (using CodeCommit as an example)
6. Logs",,,,,
"aws_db_instance, aws_db_subnet_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc","sets up an AWS environment with a main VPC, two subnets across different availability zones, and a security group to allow database access. It also configures a 20 GB 15.3 PostgresSQL RDS instance with public accessibility, linked to the defined subnets and security group, outputting the database endpoint for connectivity.","package terraform.validation

default is_valid_db_instance = false

default is_valid_vpc = false

default is_valid_subnet = false

default is_valid_security_group = false

default is_valid_db_subnet_group = false

# Validate aws_db_instance resource
is_valid_db_instance {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_instance""
        resource.expressions.allocated_storage.constant_value == 20
        resource.expressions.engine.constant_value == ""postgres""
        resource.expressions.engine_version.constant_value == ""15.3""
        resource.expressions.instance_class.constant_value != null
        resource.expressions.password.constant_value != null
        resource.expressions.username.constant_value != null
        resource.expressions.publicly_accessible.constant_value == true
        resource.expressions.db_subnet_group_name.references != null
        resource.expressions.vpc_security_group_ids.references != null
}

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# Validate aws_subnet resource (more than one)
is_valid_subnet {
        count(subnets) > 1
}

subnets[resource] {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
}

# Validate aws_security_group resource
is_valid_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""
        # Additional checks for specific ingress/egress rules can be added here
}

# Validate aws_db_subnet_group resource
is_valid_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}
",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type           = ""gp2""
  engine                 = ""postgres""
  engine_version         = ""17.2""
  instance_class         = ""db.t4g.micro""
  skip_final_snapshot    = true
  db_name                = ""mydbname""
  username               = ""username""
  password               = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name   = aws_db_subnet_group.main.id
  publicly_accessible    = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""
    with one publicly_accessible = true

has one aws_vpc

has more than one aws_subnet

has one security_group

has one db_subnet_group","I'll help you create a Terraform configuration that sets up an AWS VPC, subnets, security group, and a PostgreSQL RDS instance. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""subnet-2""
  }
}

# Security Group
resource ""aws_security_group"" ""db_sg"" {
  name        = ""database-sg""
  description = ""Security group for PostgreSQL database""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""database-sg""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""main-db-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""Main DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgresql-db""
  engine              = ""postgres""
  engine_version      = ""15.3""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""your-password-here""
  
  publicly_accessible = true
  
  db_subnet_group_name   = aws_db_subnet_group.db_subnet_group.name
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  
  skip_final_snapshot = true

  tags = {
    Name = ""postgresql-db""
  }
}

# Output
output ""db_endpoint"" {
  description = ""The endpoint of the database""
  value       = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with CIDR block 10.0.0.0/16
2. Creates two subnets in different availability zones
3. Creates a security group allowing PostgreSQL traffic (port 5432)
4. Sets up a DB subnet group with both subnets
5. Creates a PostgreSQL 15.3 RDS instance with:
   - 20GB storage
   - Public accessibility
   - Connections through the security group
   - Placement in the defined subnets
6. Outputs the database endpoint for connection

Remember to:
- Replace ""your-password-here"" with a secure password
- Consider your security requirements before allowing public access (0.0.0.0",,,,,,"I'll help you create a Terraform configuration that sets up an AWS environment with VPC, subnets, security group, and a PostgreSQL RDS instance. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""subnet-2""
  }
}

# Security Group
resource ""aws_security_group"" ""db_sg"" {
  name        = ""database-sg""
  description = ""Security group for PostgreSQL database""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""database-sg""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""main-db-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""Main DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgresql-db""
  engine              = ""postgres""
  engine_version      = ""15.3""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""yourpasswordhere""
  
  publicly_accessible = true
  
  db_subnet_group_name   = aws_db_subnet_group.db_subnet_group.name
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  
  skip_final_snapshot = true

  tags = {
    Name = ""postgresql-db""
  }
}

# Output
output ""db_endpoint"" {
  description = ""The connection endpoint for the database""
  value       = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Creates a VPC with CIDR block 10.0.0.0/16
2. Creates two subnets in different availability zones
3. Sets up a security group allowing PostgreSQL access (port 5432)
4. Creates a DB subnet group using both subnets
5. Launches a PostgreSQL 15.3 RDS instance with:
   - 20GB GP2 storage
   - Public accessibility
   - DB instance class: db.t3.micro
   - Connected to the created VPC, subnets, and security group
6. Outputs the database endpoint for connection

Remember to:
- Replace ""yourpasswordhere"" with a secure password
- Consider your",,,,,,"I'll help you create a Terraform configuration that sets up an AWS VPC, subnets, security group, and a PostgreSQL RDS instance. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""subnet-2""
  }
}

# Security Group
resource ""aws_security_group"" ""db_sg"" {
  name        = ""database-sg""
  description = ""Security group for PostgreSQL database""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""database-sg""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""main-db-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""Main DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgresql-db""
  engine              = ""postgres""
  engine_version      = ""15.3""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""yourpasswordhere""
  
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  db_subnet_group_name   = aws_db_subnet_group.db_subnet_group.name
  
  publicly_accessible    = true
  skip_final_snapshot    = true

  tags = {
    Name = ""postgresql-db""
  }
}

# Output
output ""db_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with CIDR block 10.0.0.0/16
2. Creates two subnets in different availability zones
3. Creates a security group allowing PostgreSQL access (port 5432)
4. Sets up a DB subnet group with both subnets
5. Creates a PostgreSQL 15.3 RDS instance with:
   - 20GB GP2 storage
   - Public accessibility
   - Micro instance class
   - Connected to the created security group and subnet group
6. Outputs the database endpoint for connection

Remember to:
- Replace ""yourpasswordhere"" with a secure password
- Adjust the region if needed
- Consider your security requirements before keeping public accessibility
- Review the instance class based on your performance needs",,,,,
"aws_rds_cluster, aws_vpc","Create an mySQL cluster with 100 GB allocated to us-east-1a, us-east-1b, us-east-1c.","package main

import future.keywords.in

default allow = false

# Required AZs
required_azs := [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]

# Check for RDS cluster creation
rds_cluster_created(resources) {
some resource in resources
resource.type == ""aws_rds_cluster""
resource.change.actions[_] == ""create""
}

# Check if engine is MySQL
rds_engine_mysql(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.engine == ""mysql""
}

# Check if AZs are correct and storage is 100GB
azs_and_storage_valid(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.availability_zones == required_azs
resource.change.after.allocated_storage == 100
}

# Aggregate checks for RDS cluster
allow {
rds_cluster_created(input.resource_changes)
some resource in input.resource_changes
rds_engine_mysql(resource)
azs_and_storage_valid(resource)
}",3,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

# Configure the AWS Provider
provider ""aws"" {
region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
cidr_block = ""192.168.0.0/22""
}

resource ""aws_rds_cluster"" ""example"" {
cluster_identifier = ""example""
availability_zones = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
engine = ""mysql""
db_cluster_instance_class = ""db.r6gd.large""
storage_type = ""io1""
allocated_storage = 100
iops = 1000
master_username = ""test""
master_password = ""mustbeeightcharaters""
skip_final_snapshot = true
}
","Has one ""aws_rds_cluster"" resource
with one ""allocated_storage"" = 100
with one ""availability_zones"" = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
with one ""engine"" = ""mysql""","I'll help you create a Terraform configuration for an MySQL RDS cluster with 100 GB storage spread across three availability zones in us-east-1. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier      = ""mysql-cluster""
  engine                  = ""aurora-mysql""
  engine_version          = ""5.7.mysql_aurora.2.10.2""
  database_name          = ""mydb""
  master_username        = ""admin""
  master_password        = ""your-secure-password""
  skip_final_snapshot    = true
  storage_encrypted      = true
  
  availability_zones = [
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ]
}

resource ""aws_rds_cluster_instance"" ""cluster_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r5.large""
  engine              = ""aurora-mysql""
  engine_version      = ""5.7.mysql_aurora.2.10.2""
  publicly_accessible = false
  
  availability_zone = element([
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ], count.index)
}

resource ""aws_rds_cluster_parameter_group"" ""cluster_pg"" {
  family = ""aurora-mysql5.7""
  name   = ""mysql-cluster-pg""

  parameter {
    name  = ""character_set_server""
    value = ""utf8""
  }

  parameter {
    name  = ""character_set_client""
    value = ""utf8""
  }
}
```

This configuration creates:
1. An Aurora MySQL cluster spread across three AZs in us-east-1
2. Three cluster instances, one in each specified availability zone
3. A parameter group for the cluster with some basic configurations
4. Each instance will have access to the 100GB shared storage (Aurora storage automatically scales)

Note: 
- Replace ""your-secure-password"" with a secure password
- The instance class can be modified based on your requirements
- Additional security groups, subnet groups, and VPC configurations might be needed depending on your network setup
- Aurora storage automatically scales as needed, so the 100GB requirement will be handled automatically by the service

You might want to add additional configurations like backup retention, maintenance windows, or security groups based on your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an Amazon RDS MySQL cluster with multi-AZ deployment across three availability zones, each with 100GB storage allocation. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_subnet_group"" ""mysql_subnet_group"" {
  name       = ""mysql-subnet-group""
  subnet_ids = [""subnet-xxxxx"", ""subnet-yyyyy"", ""subnet-zzzzz""] # Replace with your subnet IDs
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier     = ""mysql-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.10.2""
  database_name         = ""mydb""
  master_username       = ""admin""
  master_password       = ""your-secure-password""
  storage_encrypted     = true
  skip_final_snapshot   = true
  db_subnet_group_name  = aws_db_subnet_group.mysql_subnet_group.name
  
  availability_zones = [
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ]
}

resource ""aws_rds_cluster_instance"" ""mysql_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r5.large""
  engine              = ""aurora-mysql""
  engine_version      = ""5.7.mysql_aurora.2.10.2""
  
  # Allocate 100GB to each instance
  allocated_storage   = 100
  
  availability_zone   = element([
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ], count.index)
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a DB subnet group for the cluster
3. Creates an Aurora MySQL cluster across three AZs
4. Creates three DB instances, one in each AZ with 100GB storage each
5. Uses proper encryption and security settings

Note: 
- Replace the subnet IDs with your actual subnet IDs
- Choose a secure master password
- You may want to adjust the instance class based on your performance needs
- Storage in Aurora is automatically managed, but we've specified 100GB as requested

Make sure you have the appropriate VPC, subnets, and security groups set up before deploying this configuration.",,,,,,"I'll help you create a Terraform configuration for an Aurora MySQL cluster with specified storage and availability zones. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier     = ""mysql-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.11.2""
  database_name         = ""mydb""
  master_username       = ""admin""
  master_password       = ""your-secure-password""
  skip_final_snapshot   = true
  
  availability_zones = [
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ]
}

resource ""aws_rds_cluster_instance"" ""cluster_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r5.large""
  engine              = aws_rds_cluster.mysql_cluster.engine
  engine_version      = aws_rds_cluster.mysql_cluster.engine_version
  publicly_accessible = false

  allocated_storage = 100
}
```

This configuration:
1. Creates an Aurora MySQL cluster across three availability zones (us-east-1a, us-east-1b, us-east-1c)
2. Allocates 100GB storage for each instance
3. Uses the Aurora MySQL engine
4. Creates three instances (one in each AZ)
5. Uses db.r5.large instance class which is suitable for production workloads

Note: Remember to replace ""your-secure-password"" with a strong password that meets RDS requirements. Also, consider storing sensitive information like passwords in AWS Secrets Manager or using variables.",,,,,
"aws_cloudwatch_event_rule, aws_cloudwatch_event_target, aws_iam_role, aws_lambda_function, aws_lambda_permission, archive_file, aws_iam_policy_document",Create a Lambda function and invoke it every 15 minutes,"package main

import future.keywords.in

default allow = false

# Check if AWS Lambda function is being created
aws_lambda_function_created(resources) {
    some resource in resources
    resource.type == ""aws_lambda_function""
}

# Check if CloudWatch Event Rule is set to invoke Lambda every 15 minutes
cloudwatch_event_rule_for_lambda_valid(resources) {
    some resource in resources
    resource.type == ""aws_cloudwatch_event_rule""
    resource.change.after.schedule_expression == ""rate(15 minutes)""
}

# Aggregate all checks
allow {
    aws_lambda_function_created(input.resource_changes)
    cloudwatch_event_rule_for_lambda_valid(input.resource_changes)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_cloudwatch_event_rule"" ""lambda_schedule"" {
  name        = ""lambda-schedule-rule""
  description = ""Invoke Lambda function every 15 minutes""
  schedule_expression = ""rate(15 minutes)""
  role_arn = aws_iam_role.cron.arn
}

# Create a CloudWatch Events Target to associate with the Lambda function
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule = aws_cloudwatch_event_rule.lambda_schedule.name
  arn = aws_lambda_function.cron.arn
}

data ""archive_file"" ""lambda-func"" {
  type        = ""zip""
  source_file = ""./supplement/lambda_func.py""
  output_path = ""./supplement/lambda_func.zip""
}

resource ""aws_lambda_function"" ""cron"" {
  function_name = ""cron-lambda-function""
  role          = aws_iam_role.cron.arn
  filename      = data.archive_file.lambda-func.output_path
  source_code_hash = data.archive_file.lambda-func.output_base64sha256
  handler       = ""lambda_func.handler""
  runtime       = ""python3.12""
}

resource ""aws_lambda_permission"" ""cron"" {
  function_name = aws_lambda_function.cron.function_name
  action        = ""lambda:InvokeFunction""
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.lambda_schedule.arn
}

data ""aws_iam_policy_document"" ""cron_assume_role"" {
  statement {
    actions = [""sts:AssumeRole""]
    principals {
      type = ""Service""
      identifiers = [
        ""lambda.amazonaws.com"",
        ""events.amazonaws.com""
      ]
    }
  }
}

resource ""aws_iam_role"" ""cron"" {
  name               = ""cron_assume_role""
  assume_role_policy = data.aws_iam_policy_document.cron_assume_role.json
}

resource ""aws_iam_role_policy_attachment"" ""cron"" {
  role       = aws_iam_role.cron.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}
",Needs to have an aws_lambda_function defined. Create a aws_cloudwatch_event_rule to trigger every 15 minutes and aws_lambda_permission to have permission to run lambda function. Create a target to trigger the lambda function.,"I'll help you create a Terraform configuration for an AWS Lambda function that gets invoked every 15 minutes using CloudWatch Events (EventBridge). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""scheduled_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Basic Lambda execution policy
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Lambda function
resource ""aws_lambda_function"" ""scheduled_lambda"" {
  filename      = ""lambda_function.zip""  # Make sure this zip file exists with your Lambda code
  function_name = ""scheduled_function""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs16.x""

  environment {
    variables = {
      ENVIRONMENT = ""production""
    }
  }
}

# CloudWatch Event Rule (runs every 15 minutes)
resource ""aws_cloudwatch_event_rule"" ""every_fifteen_minutes"" {
  name                = ""every-fifteen-minutes""
  description         = ""Triggers every fifteen minutes""
  schedule_expression = ""rate(15 minutes)""
}

# CloudWatch Event Target
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule      = aws_cloudwatch_event_rule.every_fifteen_minutes.name
  target_id = ""SendToLambda""
  arn       = aws_lambda_function.scheduled_lambda.arn
}

# Lambda permission to allow CloudWatch Events to invoke the function
resource ""aws_lambda_permission"" ""allow_cloudwatch"" {
  statement_id  = ""AllowExecutionFromCloudWatch""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.scheduled_lambda.function_name
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.every_fifteen_minutes.arn
}
```

This configuration:
1. Creates an IAM role for the Lambda function
2. Attaches basic Lambda execution permissions to the role
3. Creates a Lambda function (note: you need to provide the actual Lambda function code in a zip file)
4. Creates a CloudWatch Event Rule that triggers every 15 minutes
5. Sets up the Lambda function as a target for the CloudWatch Event
6. Adds necessary permissions for CloudWatch Events to invoke the Lambda function

Remember to:
- Replace the Lambda function runtime if you're using a different programming language
- Provide the actual Lambda function code in a zip file named ""lambda_function.zip""
- Adjust the region in the provider block if needed
- Modify the handler name to match your actual Lambda function handler

The function will automatically be triggered every 15 minutes after deployment.",,,,,,"I'll help you create a Terraform configuration for a Lambda function that gets invoked every 15 minutes using CloudWatch Events (EventBridge). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""scheduled_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Basic Lambda execution policy
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Lambda function
resource ""aws_lambda_function"" ""scheduled_lambda"" {
  filename         = ""lambda_function.zip""  # Make sure this file exists
  function_name    = ""scheduled_function""
  role             = aws_iam_role.lambda_role.arn
  handler          = ""index.handler""
  runtime          = ""nodejs14.x""  # Change according to your needs

  environment {
    variables = {
      Environment = ""production""
    }
  }
}

# CloudWatch Event Rule (runs every 15 minutes)
resource ""aws_cloudwatch_event_rule"" ""every_fifteen_minutes"" {
  name                = ""every-fifteen-minutes""
  description         = ""Triggers every fifteen minutes""
  schedule_expression = ""rate(15 minutes)""
}

# CloudWatch Event Target
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule      = aws_cloudwatch_event_rule.every_fifteen_minutes.name
  target_id = ""SendToLambda""
  arn       = aws_lambda_function.scheduled_lambda.arn
}

# Lambda permission to allow CloudWatch Events to invoke the function
resource ""aws_lambda_permission"" ""allow_cloudwatch"" {
  statement_id  = ""AllowCloudWatchInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.scheduled_lambda.function_name
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.every_fifteen_minutes.arn
}
```

This configuration:
1. Creates an IAM role for the Lambda function
2. Attaches basic Lambda execution permissions to the role
3. Creates a Lambda function (note: you need to provide the actual Lambda function code in a ZIP file)
4. Creates a CloudWatch Event Rule that triggers every 15 minutes
5. Sets up the Lambda function as a target for the CloudWatch Event
6. Adds the necessary permissions for CloudWatch Events to invoke the Lambda function

Remember to:
- Replace the region with your desired AWS region
- Provide the actual Lambda function code in a ZIP file named ""lambda_function.zip""
- Adjust the runtime if you're using a different programming language
- Modify the handler name according to your function's entry point

The Lambda function will be automatically triggered every 15 minutes after deployment.",,,,,,"I'll help you create a Terraform configuration for an AWS Lambda function that gets invoked every 15 minutes using CloudWatch Events (EventBridge). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""scheduled_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Basic Lambda execution policy
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Lambda function
resource ""aws_lambda_function"" ""scheduled_lambda"" {
  filename         = ""lambda_function.zip""  # Make sure this file exists
  function_name    = ""scheduled_lambda""
  role             = aws_iam_role.lambda_role.arn
  handler          = ""index.handler""
  runtime          = ""nodejs14.x""  # Change as needed

  environment {
    variables = {
      EXAMPLE_VAR = ""example_value""
    }
  }
}

# CloudWatch Event Rule
resource ""aws_cloudwatch_event_rule"" ""lambda_schedule"" {
  name                = ""every-15-minutes""
  description         = ""Triggers Lambda function every 15 minutes""
  schedule_expression = ""rate(15 minutes)""
}

# CloudWatch Event Target
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule      = aws_cloudwatch_event_rule.lambda_schedule.name
  target_id = ""TriggerLambda""
  arn       = aws_lambda_function.scheduled_lambda.arn
}

# Lambda permission to allow CloudWatch Events to invoke the function
resource ""aws_lambda_permission"" ""allow_cloudwatch"" {
  statement_id  = ""AllowExecutionFromCloudWatch""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.scheduled_lambda.function_name
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.lambda_schedule.arn
}
```

This configuration:
1. Creates an IAM role for the Lambda function
2. Attaches basic Lambda execution permissions to the role
3. Creates the Lambda function (note: you need to provide the actual Lambda function code in a ZIP file)
4. Creates a CloudWatch Event rule that triggers every 15 minutes
5. Sets up the event target to point to the Lambda function
6. Adds necessary permissions for CloudWatch Events to invoke the Lambda function

Make sure to:
1. Replace the region with your desired AWS region
2. Provide the actual Lambda function code in a ZIP file named ""lambda_function.zip""
3. Adjust the runtime if you're using a different programming language
4. Modify the environment variables as needed for your specific use case

The Lambda function will be automatically triggered every 15 minutes after applying this configuration.",,,,,
"aws_dax_cluster, aws_iam_role",Create a cluster with dax.r4.large and replication factor = 1.,"package main

import future.keywords.in

default allow = false

# Check if the AWS DynamoDB table is being created in us-east-1
aws_dax_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_dax_cluster""
    resource.change.after.node_type == ""dax.r4.large""
    resource.change.after.replication_factor == 1
}

# Aggregate all checks
allow {
    aws_dax_cluster_valid(input.resource_changes)
}",3,"resource ""aws_iam_role"" ""example"" {
  name = ""redshift_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""redshift.amazonaws.com""
        },
      },
    ],
  })
}

resource ""aws_dax_cluster"" ""bar"" {
  cluster_name       = ""cluster-example""
  iam_role_arn       = aws_iam_role.example.arn
  node_type          = ""dax.r4.large""
  replication_factor = 1
}","require a cluster name, iam_role_arn, node_type, replication factor","I'll help you create a Terraform configuration for an AWS DAX (DynamoDB Accelerator) cluster using dax.r4.large node type and a replication factor of 1.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""dax-example-cluster""
  node_type         = ""dax.r4.large""
  replication_factor = 1

  iam_role_arn      = aws_iam_role.dax_role.arn
  subnet_group_name = aws_dax_subnet_group.example.name

  # Required security group
  security_group_ids = [aws_security_group.dax.id]

  # Cluster settings
  server_side_encryption {
    enabled = true
  }

  tags = {
    Environment = ""production""
    Name        = ""dax-cluster""
  }
}

# IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax-service-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Attach the required DAX policy to the role
resource ""aws_iam_role_policy_attachment"" ""dax_policy"" {
  role       = aws_iam_role.dax_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSDAXFullAccess""
}

# DAX subnet group
resource ""aws_dax_subnet_group"" ""example"" {
  name       = ""dax-subnet-group""
  subnet_ids = [""subnet-12345678"", ""subnet-87654321""] # Replace with your subnet IDs
}

# Security group for DAX
resource ""aws_security_group"" ""dax"" {
  name        = ""dax-security-group""
  description = ""Security group for DAX cluster""
  vpc_id      = ""vpc-12345678"" # Replace with your VPC ID

  ingress {
    from_port   = 8111
    to_port     = 8111
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""] # Replace with your CIDR block
  }
}
```

This configuration creates:
1. A DAX cluster with dax.r4.large node type and replication factor of 1
2. The required IAM role and policy attachment for DAX
3. A subnet group for the DAX cluster
4. A security group with the necessary ingress rules

Note: You'll need to replace the following placeholder values with your actual values:
- subnet IDs in the subnet group
- VPC ID in the security group
- CIDR blocks in the security group ingress rules

The configuration includes server-side encryption enabled by default for security best practices. Remember to adjust the tags and names according to your naming conventions.",,,,,,"I'll help you create a Terraform configuration for an AWS DAX cluster with dax.r4.large node type and a replication factor of 1.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""dax-example-cluster""
  node_type         = ""dax.r4.large""
  replication_factor = 1

  iam_role_arn      = aws_iam_role.dax_role.arn
  subnet_group_name = aws_dax_subnet_group.example.name

  tags = {
    Name = ""example-dax-cluster""
  }
}

resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""dax_policy"" {
  role       = aws_iam_role.dax_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSDAXServiceRole""
}

resource ""aws_dax_subnet_group"" ""example"" {
  name = ""dax-subnet-group""
  subnet_ids = [
    ""subnet-xxxxxxxx"",  # Replace with your subnet IDs
    ""subnet-yyyyyyyy""
  ]
}
```

This configuration:
1. Creates a DAX cluster with the specified node type (dax.r4.large)
2. Sets replication factor to 1
3. Creates necessary IAM role and attaches the required DAX service policy
4. Creates a subnet group for the DAX cluster

Remember to:
- Replace the subnet IDs with your actual subnet IDs
- Adjust the region as needed
- Update the cluster name and other names as per your requirements
- Ensure your VPC and subnets are properly configured for DAX

The configuration includes all necessary components for a working DAX cluster while following AWS best practices for permissions and networking.",,,,,,"I'll help you create a Terraform configuration for an AWS DAX cluster with the specified instance type and replication factor. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""dax-example-cluster""
  node_type         = ""dax.r4.large""
  replication_factor = 1
  
  # IAM role for DAX cluster
  iam_role_arn      = aws_iam_role.dax_role.arn

  # Basic cluster settings
  description       = ""DAX cluster example""
  subnet_group_name = aws_dax_subnet_group.example.name
  security_group_ids = [aws_security_group.dax.id]

  # Tags
  tags = {
    Environment = ""production""
    Name        = ""dax-cluster""
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax-cluster-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Attach the required policy to the IAM role
resource ""aws_iam_role_policy_attachment"" ""dax_policy"" {
  role       = aws_iam_role.dax_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSDAXServiceRole""
}

# Create a subnet group for DAX
resource ""aws_dax_subnet_group"" ""example"" {
  name       = ""dax-subnet-group""
  subnet_ids = [""subnet-12345678"", ""subnet-87654321""]  # Replace with your subnet IDs
}

# Create a security group for DAX
resource ""aws_security_group"" ""dax"" {
  name        = ""dax-security-group""
  description = ""Security group for DAX cluster""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID

  ingress {
    from_port   = 8111
    to_port     = 8111
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]  # Replace with your CIDR block
  }
}
```

This configuration:
1. Creates a DAX cluster with dax.r4.large instance type and replication factor of 1
2. Sets up the required IAM role with appropriate permissions
3. Creates a subnet group for the DAX cluster
4. Sets up a security group with basic inbound rules
5. Includes all necessary supporting resources

Make sure to:
- Replace the subnet IDs with your actual subnet IDs
- Replace the VPC ID with your actual VPC ID
- Adjust the CIDR blocks in the security group to match your network architecture
- Change the region if needed
- Update the cluster name and other identifiers as per your naming conventions

The configuration includes the required AWS provider and creates all the necessary components for a functioning DAX cluster.",,,,,
"aws_iam_role, aws_sagemaker_domain, aws_subnet, aws_vpc, aws_iam_policy_document",Create a SageMaker Domain resource.,"package main

import future.keywords.in

default allow = false


aws_sagemaker_domain_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_domain""
}

aws_iam_role_valid(resources) {
    some resource in resources
    resource.type == ""aws_iam_role""
}

aws_vpc_valid(resources) {
    some resource in resources
    resource.type == ""aws_vpc""
}

aws_subnet_valid(resources) {
    some resource in resources
    resource.type == ""aws_subnet""
}

# Aggregate all checks
allow {
    aws_sagemaker_domain_valid(input.resource_changes)
    aws_iam_role_valid(input.resource_changes)
    aws_vpc_valid(input.resource_changes)
    aws_subnet_valid(input.resource_changes)
}
",3,"resource ""aws_vpc"" ""example"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""example"" {
  vpc_id     = aws_vpc.example.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_sagemaker_domain"" ""example"" {
  domain_name = ""example""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.example.id
  subnet_ids  = [aws_subnet.example.id]

  default_user_settings {
    execution_role = aws_iam_role.example.arn
  }
}

resource ""aws_iam_role"" ""example"" {
  name               = ""example""
  path               = ""/""
  assume_role_policy = data.aws_iam_policy_document.example.json
}

data ""aws_iam_policy_document"" ""example"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""sagemaker.amazonaws.com""]
    }
  }
}",Has a sagemaker domain resources which requires a an iam role and a subnet,"I'll help you create a Terraform configuration for an AWS SageMaker Domain resource. Here's a complete example with the required provider and necessary configurations:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-domain-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""sagemaker_policy"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

resource ""aws_vpc"" ""sagemaker_vpc"" {
  cidr_block = ""10.0.0.0/16""

  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""sagemaker-vpc""
  }
}

resource ""aws_subnet"" ""sagemaker_subnet"" {
  vpc_id     = aws_vpc.sagemaker_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-subnet""
  }
}

resource ""aws_sagemaker_domain"" ""example"" {
  domain_name = ""example-domain""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.sagemaker_vpc.id
  subnet_ids  = [aws_subnet.sagemaker_subnet.id]

  default_user_settings {
    execution_role = aws_iam_role.sagemaker_role.arn
  }

  default_space_settings {
    execution_role = aws_iam_role.sagemaker_role.arn
  }

  retention_policy {
    home_efs_file_system = ""Delete""
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration:

1. Sets up the AWS provider
2. Creates an IAM role for SageMaker with appropriate permissions
3. Creates a VPC and subnet for the SageMaker domain
4. Creates the SageMaker domain with:
   - IAM authentication mode
   - VPC and subnet configuration
   - Default user and space settings
   - Retention policy for the EFS file system
   - Tags for resource management

You can customize the values (domain name, CIDR blocks, tags, etc.) according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS SageMaker Domain resource. This will include the required provider and necessary configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create IAM role for SageMaker Domain
resource ""aws_iam_role"" ""sagemaker_domain_role"" {
  name = ""sagemaker-domain-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the role
resource ""aws_iam_role_policy_attachment"" ""sagemaker_full_access"" {
  role       = aws_iam_role.sagemaker_domain_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

# Create VPC for SageMaker Domain
resource ""aws_vpc"" ""sagemaker_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""sagemaker-vpc""
  }
}

# Create subnet for SageMaker Domain
resource ""aws_subnet"" ""sagemaker_subnet"" {
  vpc_id     = aws_vpc.sagemaker_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-subnet""
  }
}

# Create SageMaker Domain
resource ""aws_sagemaker_domain"" ""example"" {
  domain_name = ""example-domain""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.sagemaker_vpc.id
  subnet_ids  = [aws_subnet.sagemaker_subnet.id]

  default_user_settings {
    execution_role = aws_iam_role.sagemaker_domain_role.arn
  }

  default_space_settings {
    execution_role = aws_iam_role.sagemaker_domain_role.arn
  }

  retention_policy {
    home_efs_file_system = ""Delete""
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration creates:
1. The required AWS provider
2. An IAM role for the SageMaker Domain with necessary permissions
3. A VPC and subnet for the SageMaker Domain
4. The SageMaker Domain itself with basic settings

The SageMaker Domain is configured with:
- IAM authentication mode
- A default user settings configuration
- A default space settings configuration
- A retention policy for the EFS file system
- Basic tagging

You can customize this configuration further by:
- Adding additional subnet_ids
- Modifying the VPC CIDR ranges
- Adding more specific IAM policies
- Adjusting the domain settings based on your requirements
- Adding additional tags

Remember to configure your AWS credentials and region before applying this configuration.",,,,,,"I'll help you create a SageMaker Domain resource using Terraform. This will include the required AWS provider and necessary configurations for a SageMaker Domain.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_iam_role"" ""sagemaker_execution_role"" {
  name = ""sagemaker-domain-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""sagemaker_full_access"" {
  role       = aws_iam_role.sagemaker_execution_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

resource ""aws_vpc"" ""sagemaker_vpc"" {
  cidr_block = ""10.0.0.0/16""

  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""sagemaker-domain-vpc""
  }
}

resource ""aws_subnet"" ""sagemaker_subnet"" {
  vpc_id     = aws_vpc.sagemaker_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-domain-subnet""
  }
}

resource ""aws_sagemaker_domain"" ""example"" {
  domain_name = ""example-domain""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.sagemaker_vpc.id
  subnet_ids  = [aws_subnet.sagemaker_subnet.id]

  default_user_settings {
    execution_role = aws_iam_role.sagemaker_execution_role.arn
  }

  default_space_settings {
    execution_role = aws_iam_role.sagemaker_execution_role.arn
  }

  retention_policy {
    home_efs_file_system = ""Delete""
  }

  tags = {
    Environment = ""Development""
    Project     = ""SageMaker""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role for SageMaker execution
3. Attaches the necessary SageMaker policy to the IAM role
4. Creates a VPC and subnet for the SageMaker domain
5. Creates the SageMaker domain with:
   - IAM authentication
   - VPC and subnet configuration
   - Default user and space settings
   - Retention policy for the EFS file system
   - Resource tags

You can customize the values (domain name, CIDR blocks, tags, etc.) according to your specific requirements.",,,,,
"aws_eks_cluster, aws_iam_role, aws_iam_role_policy_attachment, aws_subnet, aws_subnet, aws_vpc","Generate a Terraform configuration to create an AWS IAM role called ""demo"" with attribute name set to ""eks-cluster-demo"", for an Amazon EKS cluster. The role should allow assuming roles for EKS services and have the AmazonEKSClusterPolicy attached. Additionally, create an AWS EKS cluster with the name ""demo"" and configure its VPC with specified subnet IDs from in the us-east-1 region. Ensure dependency handling for the IAM role's policy attachment during EKS cluster management.","package terraform.analysis

default has_aws_iam_role = false
default has_aws_iam_role_policy_attachment = false
default has_aws_eks_cluster = false

find_resource(rtype, rname) = resource {
    resource = input.configuration.root_module.resources[_]
    resource.type == rtype
    resource.name == rname
}

has_aws_iam_role {
    r := find_resource(""aws_iam_role"", ""demo"")
    r.expressions.name.constant_value == ""eks-cluster-demo""
    r.expressions.assume_role_policy
}

has_aws_iam_role_policy_attachment {
    r := input.configuration.root_module.resources[_]
    r.type == ""aws_iam_role_policy_attachment""
    r.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
    r.expressions.role.references[_] == ""aws_iam_role.demo.name""
}

has_aws_eks_cluster {
    r := find_resource(""aws_eks_cluster"", ""demo"")
    r.expressions.role_arn.references[_] == ""aws_iam_role.demo.arn""
    count(r.expressions.vpc_config[_].subnet_ids.references) >= 4
    count(r.depends_on) == 1
}

valid_config {
    has_aws_iam_role
    has_aws_iam_role_policy_attachment
    has_aws_eks_cluster
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_subnet"" ""public-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
}

resource ""aws_iam_role"" ""example"" {
  name = ""eks-cluster-demo""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""eks.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_eks_cluster"" ""demo"" {
  name     = ""cluster-name""
  role_arn = aws_iam_role.example.arn

  vpc_config {
    subnet_ids = [
      aws_subnet.private-us-east-1a.id,
      aws_subnet.public-us-east-1a.id,
    ]
  }

  depends_on = [aws_iam_role_policy_attachment.eks_cluster_policy,
                aws_iam_role_policy_attachment.eks_service_policy]
}","has one ""aws_iam_role""
with one ""name""
with one ""assume_role_policy"" 

has one ""aws_iam_role_policy_attachment"" 
with one ""policy_arn""
with one ""role"" referencing the ""aws_iam_role.demo.name""

has one ""variable"" block for ""cluster_name""
with one ""default""
with one ""type""
with one ""description""
with one ""nullable""

has one ""aws_eks_cluster"" 
with one ""name""
with one ""role_arn"" referencing the ""aws_iam_role.demo.arn""
with one ""vpc_config"" containing ""subnet_ids""
with dependencies on one IAM role policy attachment","I'll help you create a Terraform configuration for an EKS cluster with the required IAM role. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM role for EKS cluster
resource ""aws_iam_role"" ""eks_cluster_role"" {
  name = ""demo""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""eks.amazonaws.com""
        }
      }
    ]
  })

  tags = {
    Name = ""eks-cluster-demo""
  }
}

# Attach the AmazonEKSClusterPolicy to the role
resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.eks_cluster_role.name
}

# Create EKS cluster
resource ""aws_eks_cluster"" ""demo"" {
  name     = ""demo""
  role_arn = aws_iam_role.eks_cluster_role.arn

  vpc_config {
    subnet_ids = [
      ""subnet-12345678"", # Replace with actual subnet IDs
      ""subnet-87654321""  # Replace with actual subnet IDs
    ]
  }

  # Ensure that IAM Role permissions are created before and deleted after
  # the EKS Cluster. Otherwise, EKS will not be able to properly delete
  # EKS managed EC2 infrastructure such as Security Groups.
  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy
  ]
}

output ""endpoint"" {
  value = aws_eks_cluster.demo.endpoint
}

output ""kubeconfig-certificate-authority-data"" {
  value = aws_eks_cluster.demo.certificate_authority[0].data
}
```

This configuration:
1. Sets up the AWS provider for us-east-1 region
2. Creates an IAM role named ""demo"" with the necessary assume role policy for EKS
3. Attaches the AmazonEKSClusterPolicy to the IAM role
4. Creates an EKS cluster named ""demo"" using the IAM role
5. Configures the VPC settings with subnet IDs (you'll need to replace the placeholder subnet IDs with your actual subnet IDs)
6. Handles dependencies using depends_on to ensure proper resource creation order
7. Includes useful outputs for cluster endpoint and certificate authority data

Note: Make sure to replace the subnet IDs with your actual subnet IDs from your VPC in the us-east-1 region.",,,,,,"I'll help you create a Terraform configuration for an AWS EKS cluster with the required IAM role and policy attachments. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM role for EKS cluster
resource ""aws_iam_role"" ""eks_cluster_role"" {
  name = ""demo""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""eks.amazonaws.com""
        }
      }
    ]
  })

  tags = {
    Name = ""eks-cluster-demo""
  }
}

# Attach the AmazonEKSClusterPolicy to the role
resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.eks_cluster_role.name
}

# Create EKS cluster
resource ""aws_eks_cluster"" ""demo"" {
  name     = ""demo""
  role_arn = aws_iam_role.eks_cluster_role.arn

  vpc_config {
    subnet_ids = [
      ""subnet-xxxxxxxx"", # Replace with actual subnet IDs
      ""subnet-yyyyyyyy""
    ]
  }

  # Ensure that IAM Role permissions are created before and deleted after
  # the EKS Cluster. Otherwise, EKS will not be able to properly delete
  # EKS managed EC2 infrastructure such as Security Groups.
  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy
  ]
}

output ""endpoint"" {
  value = aws_eks_cluster.demo.endpoint
}

output ""kubeconfig-certificate-authority-data"" {
  value = aws_eks_cluster.demo.certificate_authority[0].data
}
```

This configuration:
1. Sets up the AWS provider for us-east-1 region
2. Creates an IAM role named ""demo"" with the necessary assume role policy for EKS
3. Attaches the AmazonEKSClusterPolicy to the IAM role
4. Creates an EKS cluster named ""demo"" using the IAM role
5. Configures the VPC with specified subnet IDs (you'll need to replace the placeholder subnet IDs)
6. Handles dependencies properly using depends_on
7. Outputs the cluster endpoint and certificate authority data

Note: Remember to replace the subnet IDs (subnet-xxxxxxxx and subnet-yyyyyyyy) with your actual subnet IDs in the VPC configuration.",,,,,,"I'll help you create a Terraform configuration for an EKS cluster with the required IAM role. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM Role for EKS Cluster
resource ""aws_iam_role"" ""eks_cluster_role"" {
  name = ""demo""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""eks.amazonaws.com""
        }
      }
    ]
  })

  tags = {
    Name = ""eks-cluster-demo""
  }
}

# Attach the AmazonEKSClusterPolicy to the role
resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.eks_cluster_role.name
}

# Create EKS Cluster
resource ""aws_eks_cluster"" ""demo"" {
  name     = ""demo""
  role_arn = aws_iam_role.eks_cluster_role.arn

  vpc_config {
    subnet_ids = [
      ""subnet-12345678"", # Replace with actual subnet IDs
      ""subnet-87654321""  # Replace with actual subnet IDs
    ]
  }

  # Ensure that IAM Role permissions are created before and deleted after
  # the EKS Cluster. Otherwise, EKS will not be able to properly delete
  # EKS managed EC2 infrastructure such as Security Groups.
  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy
  ]
}

output ""endpoint"" {
  value = aws_eks_cluster.demo.endpoint
}

output ""kubeconfig-certificate-authority-data"" {
  value = aws_eks_cluster.demo.certificate_authority[0].data
}
```

This configuration:

1. Sets up the AWS provider for us-east-1 region
2. Creates an IAM role named ""demo"" with the necessary assume role policy for EKS
3. Attaches the AmazonEKSClusterPolicy to the IAM role
4. Creates an EKS cluster named ""demo"" using the IAM role
5. Configures the VPC settings with subnet IDs (replace the example subnet IDs with your actual subnet IDs)
6. Uses depends_on to ensure proper dependency handling
7. Includes useful outputs for cluster endpoint and certificate authority data

Note: Make sure to replace the subnet IDs with your actual subnet IDs from your VPC in the us-east-1 region.

The configuration handles all the dependencies correctly and follows AWS best practices for EKS cluster creation. The IAM role is properly configured with the necessary permissions, and the cluster will be created in the specified VPC subnets.",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer with ALB Target Group,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.values.subnet_mapping != null
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.ipv4_ipam_pool_id.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.ipv6_cider_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value != ""forward""
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    resources.values.target_type == ""alb""
    
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group""
        resource.expressions.port != null
        resource.expressions.protocol != null
        resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
        is_valid_lb
        is_valid_lb_listener
        is_valid_lb_target_group_attachment
        is_valid_lb_target_group
        is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.public_subnets[0]
  security_groups = [ aws_security_group.instance-sg.id ]
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.id
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id
    with target_type = ""alb""

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a target group. This configuration will include the required AWS provider, the ALB itself, and its associated target group.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create the ALB
resource ""aws_lb"" ""example_alb"" {
  name               = ""example-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [""subnet-12345678"", ""subnet-87654321""] # Replace with your subnet IDs

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
    Name        = ""example-alb""
  }
}

# Create ALB Target Group
resource ""aws_lb_target_group"" ""example_tg"" {
  name        = ""example-target-group""
  port        = 80
  protocol    = ""HTTP""
  vpc_id      = ""vpc-12345678"" # Replace with your VPC ID
  target_type = ""instance""

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }

  tags = {
    Name = ""example-target-group""
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example_alb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.example_tg.arn
  }
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = ""vpc-12345678"" # Replace with your VPC ID

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""alb-security-group""
  }
}
```

This configuration creates:
1. An Application Load Balancer (ALB)
2. A target group for the ALB
3. A listener that forwards traffic to the target group
4. A security group for the ALB

Note: You'll need to replace the following placeholder values with your actual values:
- subnet IDs in the ALB resource
- VPC ID in the target group and security group resources

The ALB is configured to:
- Be internet-facing (not internal)
- Listen on port 80
- Use HTTP protocol
- Perform health checks on the target group
- Allow inbound traffic on port 80 and all outbound traffic

The target group is configured to:
- Accept HTTP traffic on port 80
- Use standard health check settings
- Target EC2 instances

Remember to adjust the security group rules, health check settings, and other parameters according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Target Group. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create the ALB
resource ""aws_lb"" ""example"" {
  name               = ""example-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [""subnet-12345678"", ""subnet-87654321""]  # Replace with your subnet IDs

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
  }
}

# Create Target Group
resource ""aws_lb_target_group"" ""example"" {
  name        = ""example-target-group""
  port        = 80
  protocol    = ""HTTP""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID
  target_type = ""instance""

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.example.arn
  }
}

# Create Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""allow_http""
  description = ""Allow HTTP inbound traffic""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID

  ingress {
    description = ""HTTP from anywhere""
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""allow_http""
  }
}
```

This configuration creates:
1. An Application Load Balancer (ALB)
2. A Target Group for the ALB
3. A Listener that forwards traffic to the Target Group
4. A Security Group for the ALB

Note: You'll need to replace the following placeholder values with your actual values:
- subnet IDs in the ALB resource
- VPC ID in the Target Group and Security Group resources
- Adjust the region in the provider block
- Modify security group rules according to your requirements
- Adjust health check settings in the target group as needed

The ALB is set up as an internet-facing load balancer (internal = false) and listens on port 80. The target group is configured for HTTP traffic and includes basic health check settings.",,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Target Group. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create ALB
resource ""aws_lb"" ""main"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [""subnet-12345678"", ""subnet-87654321""]  # Replace with your subnet IDs

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
    Name        = ""my-application-lb""
  }
}

# Create Target Group
resource ""aws_lb_target_group"" ""main"" {
  name        = ""my-target-group""
  port        = 80
  protocol    = ""HTTP""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID
  target_type = ""instance""

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }

  tags = {
    Environment = ""production""
    Name        = ""my-target-group""
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.main.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.main.arn
  }
}

# Create Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""alb-security-group""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. A Target Group
3. A Listener that forwards traffic to the Target Group
4. A Security Group for the ALB

Make sure to:
- Replace the subnet IDs with your actual subnet IDs
- Replace the VPC ID with your actual VPC ID
- Adjust the security group rules according to your needs
- Modify the health check settings if needed
- Update the tags to match your requirements

The ALB is configured as an internet-facing load balancer (internal = false) and listens on port 80. The target group is configured for HTTP traffic and includes basic health check settings.",,,,,
aws_elastic_beanstalk_application,Provision a resource to deploy and scale a web application that was developed with supported programming languages.,"package terraform.validation

default is_configuration_valid = false

default is_valid_eb_app = false


is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}


is_configuration_valid {
    is_valid_eb_app
}",1,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_elastic_beanstalk_application"" ""tftest"" {
  name        = ""tf-test-name""
  description = ""tf-test-desc""
}","Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""","I'll help you create a Terraform configuration to deploy an AWS Elastic Beanstalk application, which is ideal for deploying and scaling web applications. Here's a solution that includes the basic components needed:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""web_app"" {
  name        = ""my-web-application""
  description = ""Web application deployed using Elastic Beanstalk""
}

# Create Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""web_app_env"" {
  name                = ""my-web-app-environment""
  application         = aws_elastic_beanstalk_application.web_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.4 running Node.js 18""  # Example stack, can be changed based on your needs

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""InstanceType""
    value     = ""t2.micro""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }
}

# Output the environment URL
output ""environment_url"" {
  value = aws_elastic_beanstalk_environment.web_app_env.endpoint_url
}
```

This configuration:
1. Creates an Elastic Beanstalk application
2. Sets up an environment with auto-scaling capabilities
3. Uses a Node.js platform (can be changed to other supported platforms like Java, Python, Ruby, etc.)
4. Configures basic auto-scaling settings (min 1, max 4 instances)
5. Sets up a load-balanced environment
6. Outputs the application URL

You can customize the solution stack name based on your application's requirements and adjust the auto-scaling settings according to your needs.",,,,,,"I'll help you create a Terraform configuration to deploy an AWS Elastic Beanstalk application, which is a service designed to deploy and scale web applications. Here's a solution that includes the necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""web_app"" {
  name        = ""my-web-application""
  description = ""Web application deployed using Elastic Beanstalk""
}

# Create Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""web_app_env"" {
  name                = ""my-web-app-environment""
  application         = aws_elastic_beanstalk_application.web_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.0 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""InstanceType""
    value     = ""t2.micro""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""3""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an Elastic Beanstalk application
3. Creates an Elastic Beanstalk environment with:
   - Auto-scaling configuration (min 1, max 3 instances)
   - t2.micro instance type
   - Load balanced environment
   - Node.js 18 platform (you can change the solution stack name for other programming languages)

The environment will automatically handle deployment, capacity provisioning, load balancing, auto-scaling, and application health monitoring.",,,,,,"I'll help you create a Terraform configuration to deploy a web application using AWS Elastic Beanstalk, which is a service designed to deploy and scale web applications. Here's a solution that includes the necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""web_app"" {
  name        = ""my-web-application""
  description = ""Web application deployed with Elastic Beanstalk""
}

# Create Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""web_app_env"" {
  name                = ""my-web-app-environment""
  application         = aws_elastic_beanstalk_application.web_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.4 running Node.js 18""  # Example stack, adjust as needed

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }

  setting {
    namespace = ""aws:ec2:instances""
    name      = ""InstanceTypes""
    value     = ""t3.micro""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }
}

# Output the environment URL
output ""environment_url"" {
  value = aws_elastic_beanstalk_environment.web_app_env.endpoint_url
}
```

This configuration:
1. Creates an Elastic Beanstalk application
2. Sets up an environment with auto-scaling capabilities (1-4 instances)
3. Uses a load-balanced environment type
4. Specifies t3.micro instances for cost-effectiveness
5. Uses Node.js 18 platform (you can change the solution stack name for other languages)
6. Outputs the environment URL for easy access

The solution supports various programming languages through different solution stacks (Java, .NET, PHP, Python, Ruby, Go, Docker, etc.). You can modify the `solution_stack_name` to match your application's requirements.",,,,,
"aws_iam_role, aws_kinesisanalyticsv2_application, aws_iam_policy_document",Generate a basic Amazon Kinesis V2 Analytics Application to manage a SQL application,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
	some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kinesisanalytics.amazonaws.com"") 
}


has_valid_kinesisanalyticsv2_application {
	some i
	resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesisanalyticsv2_application""
    resource.values.name
    resource.values.runtime_environment == ""SQL-1_0""
   	role := input.configuration.root_module.resources[i]
    role.expressions.service_execution_role
}

has_valid_resources {
	has_valid_iam_role(input.resource_changes)
    has_valid_kinesisanalyticsv2_application
}",3,"provider ""aws"" {
  region = ""us-west-2""
}

data ""aws_iam_policy_document"" ""KinesisAssume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}


resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_role""
  assume_role_policy = data.aws_iam_policy_document.KinesisAssume.json
}

resource ""aws_kinesisanalyticsv2_application"" ""flink_app"" {
  name                 = ""example-application1""
  runtime_environment  = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_role.arn
}","Has one ""aws_iam_role"" resource, and one ""aws_kinesisanalyticsv2_application"" with argument runtime_environment == ""SQL-1_0""","I'll help you create a Terraform configuration for an Amazon Kinesis Analytics V2 Application that manages a SQL application. I'll include the required provider and necessary configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Modify as per your region requirement
}

# Create a Kinesis Analytics V2 Application
resource ""aws_kinesisanalyticsv2_application"" ""example"" {
  name                   = ""example-kinesis-analytics-sql-app""
  application_configuration {
    sql_application_configuration {
      input {
        name_prefix = ""PREFIX_""
        
        input_schema {
          record_format {
            record_format_type = ""JSON""
            
            mapping_parameters {
              json_mapping_parameters {
                record_row_path = ""$""
              }
            }
          }

          record_column {
            name     = ""example_column""
            sql_type = ""VARCHAR(8)""
            mapping  = ""$.example_field""
          }
        }

        kinesis_streams_input {
          resource_arn = ""arn:aws:kinesis:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:stream/example-stream""
        }
      }
    }
  }

  runtime_environment = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_analytics_role.arn

  tags = {
    Environment = ""Production""
    Application = ""Analytics""
  }
}

# IAM Role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for Kinesis Analytics
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kinesis:GetShardIterator"",
          ""kinesis:GetRecords"",
          ""kinesis:DescribeStream"",
          ""kinesis:ListShards""
        ]
        Resource = ""*""
      }
    ]
  })
}

# Data sources for current region and account ID
data ""aws_region"" ""current"" {}
data ""aws_caller_identity"" ""current"" {}
```

This configuration creates:
1. A Kinesis Analytics V2 Application configured for SQL processing
2. An IAM role with necessary permissions for the Kinesis Analytics application
3. An IAM policy attached to the role
4. Uses data sources to get the current region and account ID

Key features of this configuration:
- SQL runtime environment
- JSON input format configuration
- Basic input schema definition
- Required IAM roles and policies
- Tags for resource management

You can customize this further by:
- Modifying the input schema
- Adding output configurations
- Adjusting the IAM permissions
- Adding more application-specific configurations
- Modifying the tags

Make sure to replace the example values with your actual requirements and ensure you have the necessary permissions to create these resources in your AWS account.",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics V2 Application that manages a SQL application. Here's a complete example with the necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create an IAM role for the Kinesis Analytics application
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-application-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Create the Kinesis Analytics v2 Application
resource ""aws_kinesisanalyticsv2_application"" ""example"" {
  name                   = ""example-sql-application""
  runtime_environment    = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_analytics_role.arn

  application_configuration {
    sql_application_configuration {
      input {
        name_prefix = ""PREFIX""
        
        schema_version = ""1""

        kinesis_streams_input {
          resource_arn = ""arn:aws:kinesis:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:stream/example-stream""
          
          input_schema {
            record_format {
              record_format_type = ""JSON""
              mapping_parameters {
                json_mapping_parameters {
                  record_row_path = ""$""
                }
              }
            }

            record_columns {
              name     = ""example_column""
              sql_type = ""VARCHAR(8)""
              mapping  = ""$.example""
            }
          }
        }
      }
    }
  }
}

# Data sources to get current region and account ID
data ""aws_region"" ""current"" {}
data ""aws_caller_identity"" ""current"" {}

# Add IAM role policy for Kinesis access
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kinesis:GetShardIterator"",
          ""kinesis:GetRecords"",
          ""kinesis:DescribeStream"",
          ""kinesis:ListShards""
        ]
        Resource = ""*""
      }
    ]
  })
}
```

This configuration creates:

1. An IAM role and policy for the Kinesis Analytics application
2. A Kinesis Analytics V2 application configured for SQL processing
3. Basic input configuration expecting JSON data from a Kinesis stream
4. Necessary permissions for the application to read from Kinesis streams

Key features of this configuration:
- Uses SQL-1_0 runtime environment
- Configures JSON input processing
- Sets up basic IAM permissions
- Includes required provider configuration
- Uses data sources to get current region and account ID

Remember to:
1. Update the region in the provider block to match your desired region
2. Modify the input configuration according to your specific data structure
3. Adjust the IAM permissions based on your security requirements
4. Update the Kinesis stream ARN to point to your actual stream

This is a basic configuration that you can build upon based on your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics V2 Application that manages a SQL application. Here's a basic configuration that includes the required provider and the Kinesis Analytics application:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

resource ""aws_kinesisanalyticsv2_application"" ""example"" {
  name                   = ""example-kinesis-analytics-sql-app""
  application_configuration {
    sql_application_configuration {
      input {
        name_prefix = ""PREFIX_""
        
        kinesis_streams_input {
          resource_arn = aws_kinesis_stream.example.arn
          role_arn     = aws_iam_role.example.arn
        }

        input_schema {
          record_format {
            record_format_type = ""JSON""
            
            mapping_parameters {
              json_mapping_parameters {
                record_row_path = ""$""
              }
            }
          }

          record_column {
            name     = ""example_column""
            sql_type = ""VARCHAR(8)""
            mapping  = ""$.example""
          }
        }
      }
    }
  }

  runtime_environment = ""SQL-1_0""
  
  service_execution_role_arn = aws_iam_role.example.arn
}

# Example IAM role for the Kinesis Analytics application
resource ""aws_iam_role"" ""example"" {
  name = ""kinesis-analytics-example-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Example Kinesis stream as input
resource ""aws_kinesis_stream"" ""example"" {
  name             = ""example-input-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""test""
  }
}
```

This configuration creates:
1. A Kinesis Analytics V2 application configured for SQL processing
2. An IAM role for the application to execute
3. A sample Kinesis stream as an input source

The application is configured with:
- SQL runtime environment
- JSON input format
- Basic input schema configuration
- Input stream configuration with necessary IAM roles
- Name prefix for input processing

You can customize this further by:
- Adding output configurations
- Modifying the input schema
- Adding reference data sources
- Configuring CloudWatch logging
- Adding application code
- Adjusting the IAM permissions according to your needs

Remember to adjust the region, names, and other parameters according to your specific requirements.",,,,,
"aws_dax_cluster, aws_iam_role",Create a DAX cluster with node type = dax.r4.large and replication factor = 1.,"package main

import future.keywords.in

default allow = false

# Check if the AWS DynamoDB table is being created in us-east-1
aws_dax_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_dax_cluster""
    resource.change.after.node_type == ""dax.r4.large""
    resource.change.after.replication_factor == 1
}

# Aggregate all checks
allow {
    aws_dax_cluster_valid(input.resource_changes)
}",3,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""example"" {
  name = ""test""

  assume_role_policy = jsonencode({
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""eks.amazonaws.com""
      }
    }]
    Version = ""2012-10-17""
  })
}


resource ""aws_dax_cluster"" ""bar"" {
  cluster_name       = ""cluster-example""
  iam_role_arn       = aws_iam_role.example.arn
  node_type          = ""dax.r4.large""
  replication_factor = 1
}","require a cluster name, iam_role_arn, node_type, replication factor","I'll help you create a Terraform configuration for an AWS DAX (DynamoDB Accelerator) cluster with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""dax-example-cluster""
  node_type         = ""dax.r4.large""
  replication_factor = 1

  iam_role_arn      = aws_iam_role.dax_role.arn

  # Required security configurations
  security_group_ids = [aws_security_group.dax_sg.id]
  subnet_group_name  = aws_dax_subnet_group.example.name

  # Maintenance window and other optional settings
  maintenance_window = ""sun:05:00-sun:09:00""
  description       = ""DAX cluster example""

  tags = {
    Name = ""example-dax-cluster""
  }
}

resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""dax_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSDAXServiceRole""
  role       = aws_iam_role.dax_role.name
}

resource ""aws_dax_subnet_group"" ""example"" {
  name       = ""dax-subnet-group""
  subnet_ids = [aws_subnet.example.id] # Replace with your subnet IDs
}

resource ""aws_security_group"" ""dax_sg"" {
  name        = ""dax-security-group""
  description = ""Security group for DAX cluster""
  vpc_id      = aws_vpc.example.id # Replace with your VPC ID

  ingress {
    from_port   = 8111
    to_port     = 8111
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }
}
```

This configuration creates:
1. A DAX cluster with the specified node type (dax.r4.large) and replication factor (1)
2. Required IAM role and policy attachment for DAX
3. A subnet group for the DAX cluster
4. A security group for the DAX cluster

Note: You'll need to:
1. Replace the subnet IDs and VPC ID with your actual values
2. Adjust the security group rules according to your needs
3. Modify the maintenance window and other optional parameters as required
4. Consider adding additional tags or configuration options specific to your use case

The configuration includes all necessary components for a working DAX cluster while following AWS best practices for security and networking.",,,,,,"I'll help you create a Terraform configuration for an AWS DAX (DynamoDB Accelerator) cluster with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""dax-example-cluster""
  node_type         = ""dax.r4.large""
  replication_factor = 1

  # IAM role for the DAX cluster
  iam_role_arn      = aws_iam_role.dax_role.arn

  # Basic settings
  description       = ""DAX cluster example""
  maintenance_window = ""sun:05:00-sun:09:00""
  notification_topic_arn = null  # Optional: SNS topic ARN for notifications

  # Network settings
  subnet_group_name = aws_dax_subnet_group.example.name
  security_group_ids = [aws_security_group.dax.id]

  tags = {
    Environment = ""production""
    Name        = ""dax-cluster""
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax-cluster-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Attach the required policy for DAX
resource ""aws_iam_role_policy_attachment"" ""dax_policy"" {
  role       = aws_iam_role.dax_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSDAXFullAccess""
}

# Create DAX subnet group
resource ""aws_dax_subnet_group"" ""example"" {
  name       = ""dax-subnet-group""
  subnet_ids = [""subnet-12345678"", ""subnet-87654321""]  # Replace with your subnet IDs
}

# Create security group for DAX
resource ""aws_security_group"" ""dax"" {
  name        = ""dax-security-group""
  description = ""Security group for DAX cluster""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID

  ingress {
    from_port   = 8111
    to_port     = 8111
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]  # Replace with your CIDR block
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""dax-security-group""
  }
}
```

This configuration:
1. Creates a DAX cluster with the specified node type (dax.r4.large) and replication factor (1)
2. Sets up the necessary IAM role with appropriate permissions
3. Creates a subnet group for the DAX cluster
4. Creates a security group with basic network access rules
5. Includes all required supporting resources

Remember to:
- Replace the subnet IDs, VPC ID, and CIDR blocks with your actual values
- Adjust the region as needed
- Modify the maintenance window and other optional parameters according to your requirements
- Update the tags to match your organization's tagging strategy

This configuration provides a complete and secure setup for a DAX cluster in AWS using Terraform.",,,,,,"I'll help you create a Terraform configuration for an AWS DAX (DynamoDB Accelerator) cluster with the specified parameters. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""dax-example-cluster""
  node_type         = ""dax.r4.large""
  replication_factor = 1

  iam_role_arn      = aws_iam_role.dax_role.arn

  # Required network configuration
  subnet_group_name = aws_dax_subnet_group.example.name
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax-cluster-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Attach required policy to the IAM role
resource ""aws_iam_role_policy_attachment"" ""dax_policy"" {
  role       = aws_iam_role.dax_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSDAXServiceRole""
}

# Create subnet group for DAX
resource ""aws_dax_subnet_group"" ""example"" {
  name        = ""dax-subnet-group""
  subnet_ids  = [""subnet-xxxxxxxx""]  # Replace with your subnet IDs
}
```

This configuration:
1. Creates a DAX cluster with the specified node type (dax.r4.large) and replication factor (1)
2. Sets up the required IAM role with appropriate permissions
3. Creates a subnet group for the DAX cluster

Make sure to:
1. Replace the region with your desired AWS region
2. Replace the subnet IDs with your actual subnet IDs
3. Adjust the cluster name and other identifiers as needed for your environment

The configuration includes all necessary components to create a functioning DAX cluster in your AWS environment.",,,,,
"aws_redshift_cluster, aws_redshift_usage_limit",Create a 2 node RedShift cluster and limit the concurrency scaling to 60 minutes,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 2
}

aws_redshift_usage_limit_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_usage_limit""
    resource.change.after.feature_type == ""concurrency-scaling""
    resource.change.after.limit_type == ""time""
    resource.change.after.amount == 60
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
    aws_redshift_usage_limit_valid(input.resource_changes)
}
",2,"resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type          = ""dc2.large""
  number_of_nodes    = 2

  database_name = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  skip_final_snapshot = true
}

resource ""aws_redshift_usage_limit"" ""example"" {
  cluster_identifier = aws_redshift_cluster.example.id
  feature_type       = ""concurrency-scaling""
  limit_type         = ""time""
  amount             = 60
}","Has an aws_redshift_cluster resource and check cluster_type is ""single_node"" or number of nodes is 2, check there is a aws_redshift_usage_limit resouce where the feature type is concurrency_scaling, limit_type is ""time"", and amount is 60","I'll help you create a Terraform configuration for a 2-node Amazon RedShift cluster with concurrency scaling limited to 60 minutes. I'll include the required provider and necessary configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier     = ""redshift-cluster-example""
  database_name         = ""mydb""
  master_username       = ""admin""
  master_password      = ""MyStrongPassword123!""  # Change this to a secure password
  node_type            = ""dc2.large""  # Choose appropriate node type
  cluster_type         = ""multi-node""
  number_of_nodes      = 2
  skip_final_snapshot  = true

  # Concurrency scaling configuration
  concurrency_scaling {
    max_concurrency_scaling_clusters = 1
    auto_scaling {
      max_concurrency_scaling_minutes = 60
    }
  }

  # Additional recommended settings
  encrypted           = true
  port               = 5439
  publicly_accessible = false

  tags = {
    Environment = ""production""
    Name        = ""redshift-cluster""
  }
}

# Security group for the RedShift cluster
resource ""aws_security_group"" ""redshift"" {
  name        = ""redshift-security-group""
  description = ""Security group for RedShift cluster""

  ingress {
    from_port   = 5439
    to_port     = 5439
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]  # Modify with your appropriate CIDR
  }

  tags = {
    Name = ""redshift-security-group""
  }
}
```

This configuration:
1. Creates a 2-node RedShift cluster
2. Sets concurrency scaling to a maximum of 60 minutes
3. Includes basic security configurations (encryption, security group)
4. Uses the required AWS provider
5. Sets up necessary cluster parameters (identifier, database name, credentials)

Remember to:
- Change the region to your desired AWS region
- Modify the master password to a secure value
- Adjust the CIDR blocks in the security group to match your network requirements
- Choose an appropriate node type based on your workload requirements
- Store sensitive information like passwords in a secure manner (consider using AWS Secrets Manager or SSM Parameter Store)

The concurrency scaling is configured to limit usage to 60 minutes as requested, and the cluster is set up with 2 nodes as specified.",,,,,,"I'll help you create a Terraform configuration for a 2-node Amazon RedShift cluster with concurrency scaling limited to 60 minutes. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier     = ""example-cluster""
  database_name         = ""exampledb""
  master_username      = ""admin""
  master_password      = ""YourStrongPassword123!""  # Change this to a secure password
  node_type            = ""dc2.large""  # Choose appropriate node type
  cluster_type         = ""multi-node""
  number_of_nodes      = 2
  
  # Concurrency scaling configuration
  concurrency_scaling {
    max_concurrency_scaling_clusters = 1
    auto_concurrency_scaling_enabled = true
  }

  # Parameter group to set concurrency scaling timeout
  parameter_group_name = aws_redshift_parameter_group.example.name

  skip_final_snapshot  = true
  
  tags = {
    Environment = ""production""
    Name        = ""example-redshift-cluster""
  }
}

resource ""aws_redshift_parameter_group"" ""example"" {
  family = ""redshift-1.0""
  name   = ""example-parameter-group""

  parameter {
    name  = ""max_concurrency_scaling_minutes""
    value = ""60""
  }
}

# Optional: Create a security group for the cluster
resource ""aws_security_group"" ""redshift"" {
  name        = ""redshift-security-group""
  description = ""Security group for Redshift cluster""
  vpc_id      = ""vpc-xxxxx""  # Replace with your VPC ID

  ingress {
    from_port   = 5439
    to_port     = 5439
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]  # Adjust according to your security requirements
  }
}
```

This configuration:

1. Creates a 2-node RedShift cluster
2. Sets up concurrency scaling with a maximum runtime of 60 minutes through the parameter group
3. Includes basic security group configuration
4. Uses the dc2.large node type (adjust according to your needs)

Key features:
- The cluster is configured with 2 nodes using `number_of_nodes = 2`
- Concurrency scaling is enabled and configured through the parameter group
- The `max_concurrency_scaling_minutes` parameter is set to 60 minutes
- Basic security settings are included

Remember to:
1. Replace the master password with a secure value
2. Update the VPC ID in the security group
3. Adjust the security group ingress rules according to your security requirements
4. Choose an appropriate node type based on your workload requirements
5. Modify the region as needed

Note: Make sure you have appropriate IAM permissions to create these resources in your AWS account.",,,,,,"I'll help you create a Terraform configuration for a 2-node Amazon RedShift cluster with concurrency scaling limited to 60 minutes. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier  = ""example-cluster""
  node_type          = ""dc2.large""  # Choose appropriate node type
  number_of_nodes    = 2
  
  database_name      = ""exampledb""
  master_username    = ""admin""
  master_password    = ""YourStrongPassword123!""  # Change this to a secure password
  
  # Enable concurrency scaling
  cluster_parameter_group_name = aws_redshift_parameter_group.example.name
  
  # Additional recommended settings
  encrypted          = true
  skip_final_snapshot = true
  
  tags = {
    Environment = ""production""
    Name        = ""example-redshift-cluster""
  }
}

resource ""aws_redshift_parameter_group"" ""example"" {
  name   = ""example-parameter-group""
  family = ""redshift-1.0""
  
  parameter {
    name  = ""max_concurrency_scaling_clusters""
    value = ""1""
  }
  
  parameter {
    name  = ""concurrency_scaling_max_time""
    value = ""60""  # 60 minutes limit
  }
}

# Optional: Create a security group for the cluster
resource ""aws_security_group"" ""redshift"" {
  name        = ""redshift-security-group""
  description = ""Security group for Redshift cluster""
  
  ingress {
    from_port   = 5439
    to_port     = 5439
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]  # Modify according to your security requirements
  }
}
```

This configuration:
1. Creates a 2-node RedShift cluster
2. Sets up a parameter group that limits concurrency scaling to 60 minutes
3. Enables encryption
4. Creates a basic security group for the cluster
5. Uses the parameter group to control concurrency scaling

Important notes:
- Remember to replace the master password with a secure value
- Adjust the node type according to your needs
- Modify the security group ingress rules according to your security requirements
- The region can be changed to match your requirements
- The parameter group family should match your RedShift version

The concurrency scaling is controlled through the parameter group settings, where we set:
- `max_concurrency_scaling_clusters` to 1 (enables concurrency scaling)
- `concurrency_scaling_max_time` to 60 (limits to 60 minutes)",,,,,
"aws_db_proxy, aws_db_subnet_group, aws_iam_role, aws_rds_cluster, aws_secretsmanager_secret, aws_secretsmanager_secret_version, aws_security_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc","Set up an AWS RDS Aurora MySQL cluster with a proxy for enhanced connection management, including VPC, subnets, security groups, IAM roles, and Secrets Manager for secure credential storage. The backup retention period for rds cluster should be 5, and the preffered backup window is 7 am to 9 am.","package terraform.validation

default is_valid_aws_db_proxy = false

default is_valid_aws_rds_cluster = false

default is_valid_aws_vpc = false

default is_valid_aws_subnet = false

default is_valid_aws_security_group = false

default is_valid_aws_db_subnet_group = false

default is_valid_aws_secretsmanager_secret = false

default is_valid_aws_iam_role = false

# Validate aws_db_proxy resource
is_valid_aws_db_proxy {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_proxy""
        resource.expressions.engine_family.constant_value == ""MYSQL""
        resource.expressions.require_tls.constant_value == true
        auth := resource.expressions.auth[_]
        auth.auth_scheme.constant_value == ""SECRETS""
}

# Validate aws_rds_cluster resource
is_valid_aws_rds_cluster {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_rds_cluster""
        resource.expressions.engine.constant_value == ""aurora-mysql""
        resource.expressions.master_username != null
        resource.expressions.master_password != null
    resource.expressions.preferred_backup_window.constant_value == ""07:00-09:00""
    resource.expressions.backup_retention_period.constant_value == 5
}

# Validate aws_vpc resource
is_valid_aws_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block != null
}

# Validate aws_subnet resource
is_valid_aws_subnet {
        count([x |
                resource := input.configuration.root_module.resources[x]
                resource.type == ""aws_subnet""
        ]) == 2 # Ensure there are exactly two subnet instances
}

# Validate aws_security_group resource
is_valid_aws_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""

        # Checks for at least one ingress and one egress rule, more specific validation can be added
        count(resource.expressions.ingress) > 0
        count(resource.expressions.egress) > 0
}

# Validate aws_db_subnet_group resource
is_valid_aws_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}

# Validate aws_secretsmanager_secret resource
is_valid_aws_secretsmanager_secret {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_secretsmanager_secret""
        resource.expressions.name != null
}

# Validate aws_iam_role resource
is_valid_aws_iam_role {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_iam_role""
        resource.expressions.assume_role_policy != null
}

",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port       = 3306
  to_port         = 3306
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port       = 3306
  to_port         = 3306
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

resource ""aws_rds_cluster"" ""example"" {
  cluster_identifier      = ""example-cluster""
  engine                  = ""aurora-mysql""
  engine_version          = ""8.0.mysql_aurora.3.08.0""
  master_username         = ""myusername""
  master_password         = ""password123""
  database_name           = ""exampledb""
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot     = true
  vpc_security_group_ids  = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name    = aws_db_subnet_group.main.name
}

resource ""aws_secretsmanager_secret"" ""db-credentials"" {
  name_prefix = ""db-credentials-""
}

resource ""aws_secretsmanager_secret_version"" ""db-credentials-version"" {
  secret_id     = aws_secretsmanager_secret.db-credentials.id
  secret_string = jsonencode({
    username = aws_rds_cluster.example.master_username
    password = aws_rds_cluster.example.master_password
  })
}

resource ""aws_iam_role"" ""rds-proxy-role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy"" ""rds-proxy-policy"" {
  name        = ""rds-proxy-policy""

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""secretsmanager:GetSecretValue"",
        ]
        Resource = [
          ""${aws_secretsmanager_secret_version.db-credentials-version.arn}"",
        ]
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""attach-proxy-policy"" {
  role       = aws_iam_role.rds-proxy-role.name
  policy_arn = aws_iam_policy.rds-proxy-policy.arn
}

resource ""aws_db_proxy"" ""example"" {
  name                   = ""example-proxy""
  debug_logging          = false
  engine_family          = ""MYSQL""
  idle_client_timeout    = 1800
  require_tls            = true
  role_arn               = aws_iam_role.rds-proxy-role.arn
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  vpc_subnet_ids         = module.vpc.private_subnets

  auth {
    auth_scheme  = ""SECRETS""
    secret_arn   = aws_secretsmanager_secret.db-credentials.arn
    iam_auth     = ""DISABLED""
  }
}

resource ""aws_db_proxy_default_target_group"" ""example"" {
  db_proxy_name = aws_db_proxy.example.name

  connection_pool_config {
    connection_borrow_timeout    = 120
    init_query                   = ""SET x=1, y=2""
    max_connections_percent      = 100
    max_idle_connections_percent = 50
    session_pinning_filters      = [""EXCLUDE_VARIABLE_SETS""]
  }
}

resource ""aws_db_proxy_target"" ""example"" {
  db_cluster_identifier = aws_rds_cluster.example.cluster_identifier
  db_proxy_name          = aws_db_proxy.example.name
  target_group_name      = aws_db_proxy_default_target_group.example.name
}
","has one ""aws_db_proxy"" resource
with ""engine_family"" = ""MYSQL""
with ""require_tls"" = true
with ""auth_scheme"" = ""SECRETS""

has one ""aws_rds_cluster"" resource
with ""engine"" = ""aurora-mysql""
with ""master_username"" and ""master_password"" from variables

has one ""aws_vpc"" resource
with a ""cidr_block""

has one ""aws_subnet"" resource
with two instances for different availability zones

has one ""aws_security_group"" resource
with ingress and egress rules for RDS and EC2 instances

has one ""aws_db_subnet_group"" resource
with subnet IDs

has one ""aws_secretsmanager_secret"" resource
with DB credentials

has one ""aws_iam_role"" resource
for the RDS proxy with an assume role policy","I'll help you create a Terraform configuration for an AWS RDS Aurora MySQL cluster with a proxy. Here's a comprehensive solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""aurora-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""private_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""aurora-private-1""
  }
}

resource ""aws_subnet"" ""private_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""aurora-private-2""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""aurora"" {
  name       = ""aurora-subnet-group""
  subnet_ids = [aws_subnet.private_1.id, aws_subnet.private_2.id]

  tags = {
    Name = ""Aurora DB subnet group""
  }
}

# Security Group
resource ""aws_security_group"" ""aurora"" {
  name        = ""aurora-security-group""
  description = ""Security group for Aurora cluster""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}

# Secrets Manager for DB credentials
resource ""aws_secretsmanager_secret"" ""aurora_credentials"" {
  name = ""aurora-credentials""
}

resource ""aws_secretsmanager_secret_version"" ""aurora_credentials"" {
  secret_id = aws_secretsmanager_secret.aurora_credentials.id
  secret_string = jsonencode({
    username = ""admin""
    password = ""yourpasswordhere""
  })
}

# IAM Role for RDS Proxy
resource ""aws_iam_role"" ""rds_proxy"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

# Aurora Cluster
resource ""aws_rds_cluster"" ""aurora"" {
  cluster_identifier      = ""aurora-cluster""
  engine                  = ""aurora-mysql""
  engine_version         = ""5.7.mysql_aurora.2.10.2""
  database_name          = ""mydb""
  master_username        = jsondecode(aws_secretsmanager_secret_version.aurora_credentials.secret_string)[""username""]
  master_password        = jsondecode(aws_secretsmanager_secret_version.aurora_credentials.secret_string)[""password""]
  db_subnet_group_name   = aws_db_subnet_group.aurora.name
  vpc_security_group_ids = [aws_security_group.aurora.id]
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot    = true

  tags = {
    Name = ""aurora-cluster""
  }
}

# Aurora Instance
resource ""aws_rds_cluster_instance"" ""aurora_instance"" {
  ",,,,,,"I'll help you create a Terraform configuration for an AWS RDS Aurora MySQL cluster with a proxy setup. This will include all the necessary networking components and security configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""aurora_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""aurora-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""aurora_subnet_1"" {
  vpc_id            = aws_vpc.aurora_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""aurora-subnet-1""
  }
}

resource ""aws_subnet"" ""aurora_subnet_2"" {
  vpc_id            = aws_vpc.aurora_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""aurora-subnet-2""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""aurora_subnet_group"" {
  name       = ""aurora-subnet-group""
  subnet_ids = [aws_subnet.aurora_subnet_1.id, aws_subnet.aurora_subnet_2.id]

  tags = {
    Name = ""Aurora DB subnet group""
  }
}

# Security Group
resource ""aws_security_group"" ""aurora_sg"" {
  name        = ""aurora-security-group""
  description = ""Security group for Aurora cluster""
  vpc_id      = aws_vpc.aurora_vpc.id

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}

# Secrets Manager for DB credentials
resource ""aws_secretsmanager_secret"" ""aurora_secret"" {
  name = ""aurora-secret""
}

resource ""aws_secretsmanager_secret_version"" ""aurora_secret_version"" {
  secret_id = aws_secretsmanager_secret.aurora_secret.id
  secret_string = jsonencode({
    username = ""admin""
    password = ""yourpasswordhere""
  })
}

# IAM Role for RDS Proxy
resource ""aws_iam_role"" ""rds_proxy_role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

# Aurora Cluster
resource ""aws_rds_cluster"" ""aurora_cluster"" {
  cluster_identifier     = ""aurora-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.10.2""
  database_name         = ""mydb""
  master_username       = jsondecode(aws_secretsmanager_secret_version.aurora_secret_version.secret_string)[""username""]
  master_password       = jsondecode(aws_secretsmanager_secret_version.aurora_secret_version.secret_string)[""password""]
  db_subnet_group_name  = aws_db_subnet_group.aurora_subnet_group.name
  vpc_security_group_ids = [aws_security_group.aurora_sg.id]
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot    =",,,,,,"I'll help you create a Terraform configuration for an AWS RDS Aurora MySQL cluster with a proxy setup. This will include all the necessary networking components, security, and secrets management.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""aurora_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""aurora-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""aurora_subnet_1"" {
  vpc_id            = aws_vpc.aurora_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""aurora-subnet-1""
  }
}

resource ""aws_subnet"" ""aurora_subnet_2"" {
  vpc_id            = aws_vpc.aurora_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""aurora-subnet-2""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""aurora_subnet_group"" {
  name       = ""aurora-subnet-group""
  subnet_ids = [aws_subnet.aurora_subnet_1.id, aws_subnet.aurora_subnet_2.id]

  tags = {
    Name = ""Aurora DB subnet group""
  }
}

# Security Group
resource ""aws_security_group"" ""aurora_sg"" {
  name        = ""aurora-security-group""
  description = ""Security group for Aurora cluster""
  vpc_id      = aws_vpc.aurora_vpc.id

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}

# Secrets Manager for DB credentials
resource ""aws_secretsmanager_secret"" ""aurora_secret"" {
  name = ""aurora-cluster-secret""
}

resource ""aws_secretsmanager_secret_version"" ""aurora_secret_version"" {
  secret_id = aws_secretsmanager_secret.aurora_secret.id
  secret_string = jsonencode({
    username = ""admin""
    password = ""YOUR_PASSWORD_HERE""
  })
}

# IAM Role for RDS Proxy
resource ""aws_iam_role"" ""rds_proxy_role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

# Aurora Cluster
resource ""aws_rds_cluster"" ""aurora_cluster"" {
  cluster_identifier     = ""aurora-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.10.2""
  database_name         = ""mydb""
  master_username       = jsondecode(aws_secretsmanager_secret_version.aurora_secret_version.secret_string)[""username""]
  master_password       = jsondecode(aws_secretsmanager_secret_version.aurora_secret_version.secret_string)[""password""]
  db_subnet_group_name  = aws_db_subnet_group.aurora_subnet_group.name
  vpc_security_group_ids = [aws_security_group.aurora_sg.id]
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip",,,,,
"aws_dlm_lifecycle_policy, aws_glacier_vault",generate an S3 Glacier vault and use it with AWS Data Lifecycle Manager,"package terraform.validation

default is_valid_terraform_plan = false

# Rule to check for an AWS Glacier Vault resource with a 'name' attribute
is_valid_glacier_vault {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_glacier_vault""
        not is_null(resource.values.name)
}

# Rule to check for an AWS DLM Lifecycle Policy with required attributes
is_valid_dlm_lifecycle_policy {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_dlm_lifecycle_policy""
        not is_null(resource.values.description)
#         not is_null(resource.values.execution_role_arn)
        not is_null(resource.values.policy_details)
    
    execution_role_arn_configured(resource)
}

execution_role_arn_configured(_) {
    role_resource := input.configuration.root_module.resources[_]
    role_resource.type == ""aws_dlm_lifecycle_policy""
#     role_resource.name == ""dlm_lifecycle_role""
    not is_null(role_resource.expressions.execution_role_arn)
}

# Combined rule to validate the entire Terraform plan
is_valid_terraform_plan {
        is_valid_glacier_vault
        is_valid_dlm_lifecycle_policy
}",3,"resource ""aws_glacier_vault"" ""example"" {
  name = ""my-glacier-vault""
}

resource ""aws_dlm_lifecycle_policy"" ""example"" {
  description = ""Automated archiving policy""

  execution_role_arn = ""arn:aws:iam::123456789012:role/DLMServiceRole""  # Replace with your DLM execution role ARN

  policy_details {
    resource_types = [""VOLUME""]
    target_tags = {
      ""archive"" = ""true""
    }
    schedule {
      name = ""2 weeks of daily snapshots""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times         = [""23:45""]
      }

      retain_rule {
        count = 14
      }

      tags_to_add = {
        SnapshotCreator = ""DLM""
      }

      copy_tags = false
    }
  }
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute

has one ""aws_dlm_lifecycle_policy"" resource
    with ""description"" attribute
    with ""execution_role_arn""
    with ""policy_details""","I'll help you create a Terraform configuration for an S3 Glacier vault with AWS Data Lifecycle Manager. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  tags = {
    Environment = ""production""
  }

  notification {
    sns_topic = aws_sns_topic.glacier_updates.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  access_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""add-read-only-perm""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""glacier:InitiateJob"",
          ""glacier:GetJobOutput""
        ]
        Resource = ""arn:aws:glacier:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:vaults/${var.vault_name}""
      }
    ]
  })
}

# Create SNS Topic for Glacier notifications
resource ""aws_sns_topic"" ""glacier_updates"" {
  name = ""glacier-vault-updates""
}

# Create DLM Lifecycle Policy
resource ""aws_dlm_lifecycle_policy"" ""example"" {
  description        = ""Example DLM lifecycle policy for Glacier""
  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn
  state             = ""ENABLED""

  policy_details {
    resource_types = [""VOLUME""]

    schedule {
      name = ""2 weeks retention then move to glacier""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times        = [""23:45""]
      }

      retain_rule {
        count = 14
      }

      tags_to_add = {
        SnapshotCreator = ""DLM""
      }

      copy_tags = true
    }

    target_tags = {
      Backup = ""true""
    }
  }
}

# Create IAM role for DLM
resource ""aws_iam_role"" ""dlm_lifecycle_role"" {
  name = ""dlm-lifecycle-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dlm.amazonaws.com""
        }
      }
    ]
  })
}

# Attach policy to DLM role
resource ""aws_iam_role_policy"" ""dlm_lifecycle"" {
  name = ""dlm-lifecycle-policy""
  role = aws_iam_role.dlm_lifecycle_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""ec2:CreateSnapshot"",
          ""ec2:DeleteSnapshot"",
          ""ec2:DescribeVolumes"",
          ""ec2:DescribeSnapshots""
        ]
        Resource = ""*""
      },
      {
        Effect = ""Allow""
        Action = [
          ""glacier:CreateVaultLock"",
          ""glacier:InitiateJob"",
          ""glacier:AbortMultipartUpload"",
          ""glacier:CreateVault"",
          ""glacier:DeleteArchive"",
          ""glacier:DeleteVault"",
          ""glacier:ListParts"",
          ""glacier:ListVaults"",
          ""glacier:UploadArchive"",
          ""glacier:UploadMultipartPart""
        ]
        Resource = aws_",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault with AWS Data Lifecycle Manager. Here's a solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  tags = {
    Environment = ""Production""
  }
}

# Create SNS Topic for Glacier notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-vault-notifications""
}

# Create DLM Lifecycle Policy
resource ""aws_dlm_lifecycle_policy"" ""example"" {
  description        = ""example DLM lifecycle policy""
  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn
  state             = ""ENABLED""

  policy_details {
    resource_types = [""VOLUME""]

    schedule {
      name = ""2 weeks of daily snapshots""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times        = [""23:45""]
      }

      retain_rule {
        count = 14
      }

      copy_tags = true
    }

    target_tags = {
      Snapshot = ""true""
    }
  }
}

# Create IAM Role for DLM
resource ""aws_iam_role"" ""dlm_lifecycle_role"" {
  name = ""dlm-lifecycle-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dlm.amazonaws.com""
        }
      }
    ]
  })
}

# Attach policy to DLM role
resource ""aws_iam_role_policy"" ""dlm_lifecycle"" {
  name = ""dlm-lifecycle-policy""
  role = aws_iam_role.dlm_lifecycle_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""ec2:CreateSnapshot"",
          ""ec2:DeleteSnapshot"",
          ""ec2:DescribeSnapshots"",
          ""ec2:DescribeVolumes"",
          ""glacier:CreateVault"",
          ""glacier:DeleteArchive"",
          ""glacier:DescribeVault""
        ]
        Resource = ""*""
      }
    ]
  })
}

# Add vault access policy
resource ""aws_glacier_vault_lock"" ""example"" {
  vault_name = aws_glacier_vault.example_vault.name
  complete_lock = false
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""glacier:DescribeVault"",
          ""glacier:ListTagsForVault""
        ],
        Resource = aws_glacier_vault.example_vault.arn
      }
    ]
  })
}
```

This configuration:
1. Creates a Glacier vault with notification settings
2. Sets up an SNS topic for vault notifications
3. Creates a DLM lifecycle policy that takes daily snapshots and retains them for 2 weeks
4. Creates necessary IAM roles and policies for DLM to manage the lifecycle
5. Adds a vault access policy to control access to the Glacier vault

Make sure to:
- Adjust the region in the provider block
- Modify the retention periods and schedule according to your needs
- Review and adjust the",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault with AWS Data Lifecycle Manager. The configuration will include the necessary resources to set up both the Glacier vault and the lifecycle policy.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  tags = {
    Environment = ""Production""
  }

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }
}

# Create SNS Topic for Glacier notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-vault-notifications""
}

# Create IAM Role for DLM
resource ""aws_iam_role"" ""dlm_lifecycle_role"" {
  name = ""dlm-lifecycle-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dlm.amazonaws.com""
        }
      }
    ]
  })
}

# Create IAM Policy for DLM
resource ""aws_iam_role_policy"" ""dlm_lifecycle"" {
  name = ""dlm-lifecycle-policy""
  role = aws_iam_role.dlm_lifecycle_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""glacier:CreateVaultLock"",
          ""glacier:InitiateJob"",
          ""glacier:AbortMultipartUpload"",
          ""glacier:CompleteMultipartUpload"",
          ""glacier:DeleteArchive"",
          ""glacier:DeleteVault"",
          ""glacier:UploadArchive"",
          ""glacier:InitiateMultipartUpload""
        ]
        Resource = [aws_glacier_vault.example_vault.arn]
      }
    ]
  })
}

# Create DLM Lifecycle Policy
resource ""aws_dlm_lifecycle_policy"" ""glacier_policy"" {
  description        = ""Glacier lifecycle policy""
  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn
  state             = ""ENABLED""

  policy_details {
    resource_types = [""VOLUME""]

    schedule {
      name = ""Move to Glacier after 90 days""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times        = [""23:45""]
      }

      retain_rule {
        count = 90
      }

      transition_to_cold {
        days = 90
      }

      tags_to_add = {
        LifecyclePolicy = ""ToGlacier""
      }
    }

    target_tags = {
      Archive = ""true""
    }
  }
}

# Optional: Add Vault Lock Policy
resource ""aws_glacier_vault_lock"" ""example"" {
  vault_name = aws_glacier_vault.example_vault.name
  complete_lock = true

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Deny""
        Principal = ""*""
        Action = [
          ""glacier:DeleteArchive""
        ]
        Resource = [aws_glacier_vault.example_vault.arn]
        Condition = {
          NumericLessThan = {
            ""glacier:ArchiveAgeInDays"": 365
          }
        }
      }
    ]
  })
}
```

This configuration:

1. Creates a Glacier vault with",,,,,
"aws_connect_bot_association, aws_connect_instance, aws_lex_bot, aws_lex_intent, aws_region",create an aws_connect_bot_association resource with a SAML aws_connect_instance,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for multiple aws_lex_intent resources
has_valid_lex_intents {
    count([intent | intent := input.planned_values.root_module.resources[_]; intent.type == ""aws_lex_intent""; intent.values.name; intent.values.fulfillment_activity])
}

has_valid_connect_instance {
	count([instance | instance := input.planned_values.root_module.resources[_]; instance.type == ""aws_connect_instance""; instance.values.identity_management_type == ""SAML""; instance.values.inbound_calls_enabled; instance.values.outbound_calls_enabled])
}

# Rule for aws_lex_bot resource with specific arguments
has_valid_lex_bot_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lex_bot""
    resource.values.child_directed == true
    resource.values.clarification_prompt
    resource.values.abort_statement
    resource.values.name
    # is_boolean(resource.values.create_version)
    # resource.values.description
    # is_boolean(resource.values.detect_sentiment)
    # resource.values.idle_session_ttl_in_seconds
    # resource.values.process_behavior
    # resource.values.voice_id
    # resource.values.locale
    # is_boolean(resource.values.enable_model_improvements)
    # resource.values.nlu_intent_confidence_threshold
}

has_valid_connect_bot_association {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_connect_bot_association""
    resource.values.lex_bot[_].name
    instance := input.configuration.root_module.resources[i]
    instance.expressions.instance_id
    
    
}



# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lex_intents
    has_valid_lex_bot_instance
    has_valid_connect_instance
    has_valid_connect_bot_association
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_region"" ""current"" {}

resource ""aws_connect_instance"" ""example"" {
  identity_management_type = ""CONNECT_MANAGED""
  inbound_calls_enabled    = true
  instance_alias           = ""friendly-name-connect-13""
  outbound_calls_enabled   = true
}

resource ""aws_lex_intent"" ""example"" {
  create_version = true
  name           = ""connect_lex_intent""
  
  sample_utterances = [
    ""I would like to pick up flowers."",
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

resource ""aws_lex_bot"" ""example13"" {
  name             = ""connect_lex_bot""
  process_behavior = ""BUILD""
  child_directed   = false

  abort_statement {
    message {
      content      = ""Sorry, I am not able to assist at this time.""
      content_type = ""PlainText""
    }
  }
  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, what would you like to do?""
      content_type = ""PlainText""
    }
  }
  intent {
    intent_name    = aws_lex_intent.example.name
    intent_version = ""1""
  }  
}

resource ""aws_connect_bot_association"" ""example"" {
  instance_id = aws_connect_instance.example.id
  lex_bot {
    lex_region = data.aws_region.current.name
    name       = aws_lex_bot.example13.name
  }
}

","Has one resource ""aws_connect_instance"" with argument ""identity_management_type"" = SAML, at least one ""aws_lex_intent"" resource, one ""aws_lex_bot"" resource, and one ""aws_connnect_bot_association"" resource","I'll help you create a Terraform configuration for an AWS Connect bot association with a SAML-based Connect instance. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create the SAML-based Connect instance
resource ""aws_connect_instance"" ""example"" {
  identity_management_type = ""SAML""
  instance_alias          = ""example-connect-instance""
  inbound_calls_enabled   = true
  outbound_calls_enabled  = true
}

# Create the Connect bot association
resource ""aws_connect_bot_association"" ""example"" {
  instance_id = aws_connect_instance.example.id
  lex_bot {
    lex_region = ""us-west-2""  # Specify your desired region
    name       = ""ExampleBot""
  }
}

# Output the instance ID and bot association ID
output ""connect_instance_id"" {
  value = aws_connect_instance.example.id
}

output ""bot_association_id"" {
  value = aws_connect_bot_association.example.id
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a SAML-based Connect instance
3. Creates a bot association linking a Lex bot to the Connect instance
4. Includes outputs for the instance and bot association IDs

Make sure to:
- Replace ""us-west-2"" with your desired AWS region
- Update the ""ExampleBot"" name with your actual Lex bot name
- Adjust the instance_alias to your preferred name
- Configure your SAML settings as needed for the Connect instance

The bot association will be created after the Connect instance is successfully provisioned.",,,,,,"I'll help you create a Terraform configuration for an AWS Connect bot association with a SAML-based Connect instance. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# AWS Connect Instance with SAML
resource ""aws_connect_instance"" ""example"" {
  identity_management_type = ""SAML""
  instance_alias          = ""example-connect-instance""
  inbound_calls_enabled   = true
  outbound_calls_enabled  = true
}

# Lex Bot V2
resource ""aws_lexv2_bot"" ""example"" {
  name = ""example_bot""
  
  data_privacy {
    child_directed = false
  }
  
  idle_session_ttl_in_seconds = 300
}

# Lex Bot Version
resource ""aws_lexv2_bot_version"" ""example"" {
  bot_id = aws_lexv2_bot.example.id
}

# Bot Association
resource ""aws_connect_bot_association"" ""example"" {
  instance_id = aws_connect_instance.example.id
  
  lex_bot {
    lex_region = ""us-east-1""
    name       = aws_lexv2_bot.example.name
  }
}
```

This configuration includes:

1. The required AWS provider declaration
2. An AWS Connect instance configured with SAML authentication
3. A Lex V2 bot (required for the association)
4. A bot version
5. The bot association resource linking the Connect instance with the Lex bot

The `aws_connect_bot_association` resource connects the Lex bot to your Connect instance, allowing the bot to be used within your contact center. Make sure to:

- Replace ""us-east-1"" with your desired AWS region
- Adjust the instance_alias and bot name as needed
- Configure any additional settings required for your specific use case

All resources are properly linked using resource references to ensure proper dependency management.",,,,,,"I'll help you create a Terraform configuration for an AWS Connect bot association with a SAML-enabled Connect instance. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create SAML Connect Instance
resource ""aws_connect_instance"" ""example"" {
  identity_management_type = ""SAML""
  instance_alias          = ""example-connect-instance""
  inbound_calls_enabled   = true
  outbound_calls_enabled  = true
}

# Create Bot Association
resource ""aws_connect_bot_association"" ""example"" {
  instance_id = aws_connect_instance.example.id
  lex_bot {
    lex_region = ""us-west-2""  # Specify the region where your Lex bot is deployed
    name       = ""ExampleBot""
  }
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates a SAML-enabled Connect instance
3. Creates a bot association linking a Lex bot to the Connect instance

Note that you'll need to:
- Ensure the Lex bot exists in the specified region
- Have proper IAM permissions to create these resources
- Replace the `lex_region` and bot `name` values with your actual values

The bot association will be created after the Connect instance is successfully provisioned due to the implicit dependency created by referencing the instance ID.",,,,,
aws_lightsail_database,"AWS Lightsail that creates a postgres database, which enable creating a final snapshot of your database on deletion","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.blueprint_id == ""postgres_12""
    resource.values.bundle_id
    resource.values.final_snapshot_name
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name     = ""test""
  availability_zone            = ""us-east-1a""
  master_database_name         = ""testdatabasename""
  master_password              = ""testdatabasepassword""
  master_username              = ""test""
  blueprint_id                 = ""postgres_12""
  bundle_id                    = ""micro_1_0""
  final_snapshot_name          = ""MyFinalSnapshot""
}","""have one """"aws_lightsail_database"""" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id = postgres_12
    with bundle_id
    with final_snapshot_name","I'll help you create a Terraform configuration for an AWS Lightsail PostgreSQL database with final snapshot enabled. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

resource ""aws_lightsail_database"" ""postgres_db"" {
  relational_database_name = ""my-postgres-db""
  availability_zone       = ""us-east-1a""  # Change to match your region
  master_database_name   = ""maindb""
  master_password       = ""YourSecurePassword123!""  # Replace with secure password
  master_username       = ""dbadmin""
  blueprint_id          = ""postgres_13""  # PostgreSQL version
  bundle_id            = ""micro_2_0""     # Database instance size

  # Enable final snapshot on deletion
  skip_final_snapshot  = false
  final_snapshot_name = ""final-snapshot-${formatdate(""YYYYMMDDhhmmss"", timestamp())}""

  # Additional optional configurations
  preferred_backup_window = ""03:00-04:00""
  preferred_maintenance_window = ""Mon:05:00-Mon:06:00""
  publicly_accessible = false

  tags = {
    Environment = ""production""
    ManagedBy   = ""terraform""
  }
}
```

This configuration:
1. Creates a PostgreSQL database in AWS Lightsail
2. Enables final snapshot creation when the database is deleted
3. Uses dynamic naming for the final snapshot with timestamp
4. Includes basic security and maintenance configurations
5. Adds relevant tags for resource management

Key features:
- `skip_final_snapshot = false`: Ensures a final snapshot is created before deletion
- `final_snapshot_name`: Generates a unique name using timestamp
- Configurable backup and maintenance windows
- Security settings with `publicly_accessible = false`

Remember to:
- Replace the password with a secure value
- Adjust the region and availability zone as needed
- Choose appropriate bundle_id based on your needs
- Update the PostgreSQL version (blueprint_id) if needed
- Modify the database name and other parameters according to your requirements",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail PostgreSQL database with a final snapshot enabled on deletion. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_lightsail_database"" ""postgres_db"" {
  relational_database_name = ""my-postgres-db""
  availability_zone       = ""us-west-2a""  # Change to your desired AZ
  master_database_name   = ""main_database""
  master_username       = ""dbadmin""
  master_password       = ""your-secure-password""  # Replace with a secure password
  blueprint_id          = ""postgres_14""  # PostgreSQL version
  bundle_id            = ""micro_2_0""     # Database instance size

  # Enable final snapshot on deletion
  skip_final_snapshot  = false
  final_snapshot_name = ""final-snapshot-${formatdate(""YYYYMMDDhhmmss"", timestamp())}""

  # Additional optional configurations
  preferred_backup_window = ""03:00-04:00""
  preferred_maintenance_window = ""sun:05:00-sun:06:00""
  backup_retention_enabled = true
  
  tags = {
    Environment = ""production""
    Name        = ""postgres-database""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail PostgreSQL database
3. Enables final snapshot creation when the database is deleted (skip_final_snapshot = false)
4. Generates a unique final snapshot name using timestamp
5. Includes basic backup and maintenance configurations
6. Adds tags for better resource management

Key features:
- The database will create a final snapshot before deletion
- The final snapshot name includes a timestamp to ensure uniqueness
- Basic backup retention is enabled
- Maintenance windows are configured
- You can modify the bundle_id to choose different instance sizes
- You can change the blueprint_id for different PostgreSQL versions

Remember to:
- Replace the region and availability zone with your desired values
- Set a strong master_password
- Adjust the bundle_id based on your performance needs
- Modify backup and maintenance windows according to your requirements",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail PostgreSQL database with the final snapshot enabled on deletion. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change this to your desired region
}

resource ""aws_lightsail_database"" ""postgres_db"" {
  relational_database_name = ""my-postgres-db""
  availability_zone       = ""us-east-1a""  # Change this to your desired AZ
  master_database_name   = ""main_database""
  master_username        = ""dbadmin""
  master_password        = ""YourStrongPassword123!""  # Replace with a secure password
  blueprint_id           = ""postgres_12""
  bundle_id             = ""micro_2_0""  # Smallest instance, adjust as needed
  
  # Enable final snapshot on deletion
  skip_final_snapshot    = false
  final_snapshot_name    = ""final-snapshot-${aws_lightsail_database.postgres_db.relational_database_name}""
  
  # Additional optional configurations
  preferred_backup_window      = ""03:00-04:00""
  preferred_maintenance_window = ""sun:05:00-sun:06:00""
  
  tags = {
    Environment = ""production""
    ManagedBy   = ""terraform""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail PostgreSQL database
3. Enables final snapshot creation on deletion with `skip_final_snapshot = false`
4. Specifies a name for the final snapshot using the database name
5. Includes basic backup and maintenance window configurations
6. Adds some example tags

Key features:
- Uses PostgreSQL 12 as the database engine
- Creates a micro instance (bundle_id = ""micro_2_0"")
- Sets up backup and maintenance windows
- Creates a final snapshot when the database is deleted
- Includes basic security with master username and password

Remember to:
- Change the region and availability zone as needed
- Use a strong password for master_password
- Adjust the bundle_id based on your performance needs
- Modify backup and maintenance windows according to your requirements
- Store sensitive information like passwords in variables or secrets management",,,,,
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket and an example payment configuration resource for the S3 bucket.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""

        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""Requester""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name","I'll help you create a Terraform configuration for an S3 bucket with a payment configuration. Here's a complete solution that includes the required AWS provider and the necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""my-example-payment-bucket""
}

# Add payment configuration for the bucket
resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example_bucket.id
  payer  = ""Requester""  # Can be either ""BucketOwner"" or ""Requester""
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an S3 bucket named ""my-example-payment-bucket""
3. Configures the request payment settings for the bucket to charge the requester for data transfer costs

The payment configuration allows you to specify who pays for the requests and data transfer costs. In this example, it's set to ""Requester"", meaning the person making requests to the bucket pays for the data transfer costs. You can change it to ""BucketOwner"" if you want the bucket owner to pay instead.

Remember to replace the bucket name with a globally unique name as S3 bucket names must be unique across all AWS accounts.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a payment configuration. Here's a solution that includes both resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""my-example-payment-bucket""
}

# Create payment configuration for the S3 bucket
resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example_bucket.id
  payer  = ""Requester""  # Can be either ""BucketOwner"" or ""Requester""
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an S3 bucket named ""my-example-payment-bucket""
3. Configures the request payment settings for the bucket, setting the payer to ""Requester"" (meaning the person requesting the data pays for the transfer costs)

Make sure to:
- Change the bucket name to something unique as S3 bucket names must be globally unique
- Adjust the region as needed
- Consider whether you want the bucket owner (""BucketOwner"") or the requester (""Requester"") to pay for data transfer costs

The payment configuration allows you to specify who pays for data transfer and request costs when accessing the bucket's contents.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a payment configuration. Here's a solution that includes both resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""my-example-payment-bucket""

  tags = {
    Name        = ""Payment Bucket""
    Environment = ""Production""
  }
}

# Configure payment configuration for the bucket
resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example_bucket.id
  payer  = ""Requester""  # Can be either ""BucketOwner"" or ""Requester""
}
```

This configuration does the following:

1. Sets up the required AWS provider
2. Creates an S3 bucket with a specified name and tags
3. Configures the request payment configuration for the bucket, setting the payer to ""Requester""

The payment configuration resource allows you to specify who pays for the requests and data transfer costs associated with the bucket. The `payer` parameter can be set to either:
- ""BucketOwner"" (default): The bucket owner pays for all requests and data transfer costs
- ""Requester"": The requester pays for the costs of requests and data transfer, while the bucket owner pays for the storage costs

Make sure to replace the bucket name with your desired name, as S3 bucket names must be globally unique across all AWS accounts.",,,,,
"aws_internet_gateway, aws_vpc","creates a VPC with a CIDR block, and an internet gateway in the AWS us-east-2 region, both tagged with names ""vpc"" and ""ig"" respectively.","package terraform.validation

# Set default validation states
default is_valid_vpc = false

default is_valid_internet_gateway = false

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
        resource.expressions.enable_dns_hostnames.constant_value == true
        resource.expressions.tags != null
}

# Validate aws_internet_gateway resource
is_valid_internet_gateway {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_internet_gateway""
        resource.expressions.vpc_id.references[0] == ""aws_vpc._.id""
        resource.expressions.tags != null
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""_"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  tags = merge({
    ""Name"" = ""vpc""
  })
}

resource ""aws_internet_gateway"" ""_"" {
  vpc_id = aws_vpc._.id
  tags = merge({
    ""Name"" = ""ig""
  })
}","Has one ""aws_vpc"" resource:
with a specified ""cidr_block""
with ""enable_dns_hostnames"" set to true

Has one ""aws_internet_gateway"" resource:
with ""vpc_id"" referencing the ""aws_vpc"" resource by its ID
with a ""tags"" map that is not null, including at least a ""Name"" key with a value","I'll help you create a Terraform configuration that creates a VPC with a CIDR block and an Internet Gateway in the AWS us-east-2 region with appropriate tags.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""vpc""
  }
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""ig""
  }
}
```

This configuration:
1. Sets up the AWS provider for the us-east-2 region
2. Creates a VPC with CIDR block 10.0.0.0/16 and tags it with the name ""vpc""
3. Creates an Internet Gateway attached to the VPC and tags it with the name ""ig""
4. Includes DNS support and hostname features for the VPC

You can apply this configuration using `terraform init` followed by `terraform apply`.",,,,,,"I'll help you create a Terraform configuration that creates a VPC with a CIDR block and an Internet Gateway in the AWS us-east-2 region, including the specified tags.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""vpc""
  }
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""ig""
  }
}
```

This configuration will:
1. Set up the AWS provider for the us-east-2 region
2. Create a VPC with CIDR block 10.0.0.0/16 and tag it with name ""vpc""
3. Create an Internet Gateway attached to the VPC and tag it with name ""ig""

The VPC includes DNS support and DNS hostnames enabled by default, which is a common best practice. The Internet Gateway is automatically attached to the VPC using the `vpc_id` reference to the created VPC.",,,,,,"I'll help you create a Terraform configuration that sets up a VPC with a CIDR block and an Internet Gateway in the us-east-2 region, including the required tags.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""vpc""
  }
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""ig""
  }
}
```

This configuration:
1. Sets up the AWS provider for us-east-2 region
2. Creates a VPC with CIDR block 10.0.0.0/16
3. Creates an Internet Gateway and attaches it to the VPC
4. Tags both resources with their respective names as specified
5. Enables DNS support and DNS hostnames for the VPC

The VPC and Internet Gateway are properly associated, and both have the required name tags. The configuration follows AWS best practices and includes all necessary components.",,,,,
"aws_route53_health_check, aws_route53_health_check, aws_route53_record, aws_route53_record, aws_route53_zone","Configure Active-Active Failover policies using Route 53 resources. Name the health check resources ""primary_health_check"" and ""secondary_health_check"" and use a fqdn for both. Call the zone ""main""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record_primary = false

default is_valid_r53_record_secondary = false

default is_valid_r53_health_check_primary = false

default is_valid_r53_health_check_secondary = false



# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""

}

# Validate aws_route53_record
is_valid_r53_record_primary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.failover_routing_policy[0].type.constant_value == ""PRIMARY""
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.set_identifier
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.main.zone_id""
    resource.expressions.health_check_id.references[0] == ""aws_route53_health_check.primary_health_check.id""
}

is_valid_r53_record_secondary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.failover_routing_policy[0].type.constant_value == ""SECONDARY""
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.set_identifier
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.main.zone_id""
    resource.expressions.health_check_id.references[0] == ""aws_route53_health_check.secondary_health_check.id""

}

# Validate aws_route53_health_check
is_valid_r53_health_check_primary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_health_check""
    resource.name == ""primary_health_check""
    resource.expressions.fqdn
    resource.expressions.request_interval
    resource.expressions.type
}

is_valid_r53_health_check_secondary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_health_check""
    resource.name == ""secondary_health_check""
    resource.expressions.fqdn
    resource.expressions.request_interval
    resource.expressions.type
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_health_check_primary
    is_valid_r53_health_check_secondary
    is_valid_r53_zone
    is_valid_r53_record_primary
    is_valid_r53_record_secondary
}




",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
}

resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example53.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
}

resource ""aws_route53_record"" ""primary_record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""60""
  records = [""192.0.2.101""]
  set_identifier = ""primary-endpoint""

  failover_routing_policy {
    type = ""PRIMARY""
  }

  health_check_id = aws_route53_health_check.primary_health_check.id
}

resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example53.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
}

resource ""aws_route53_record"" ""secondary_record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""60""
  records = [""192.0.2.102""] 
  set_identifier = ""secondary-endpoint""
  
  failover_routing_policy {
    type = ""SECONDARY""
  }

  health_check_id = aws_route53_health_check.secondary_health_check.id
}","Has one ""aws_route53_zone"" resource
    with ""name""

Has two ""aws_route53_record"" resources
    The first ""aws_route53_record"" 
        with ""name""
        with ""type""
        with ""ttl""
        with ""records""
        with ""zone_id"" referencing the ""aws_route53_zone"" resource
        with ""health_check_id"" referencing one of the ""aws_health_check"" resource
        with ""set_identifier""
        with ""failover_routing_policy"" block that contains ""type"" = ""PRIMARY""
    The second ""aws_route53_record""
        with ""name""
        with ""type""
        with ""ttl""
        with ""records""
        with ""zone_id"" referencing the ""aws_route53_zone"" resource
        with ""health_check_id"" referencing the other ""aws_health_check"" resource
        with ""set_identifier""
        with ""failover_routing_policy"" block that contains ""type"" = ""SECONDARY""

Has two ""aws_health_check"" resources
    with ""fqdn"" 
    with ""request_interval""
    with ""type""

","I'll help you create a Terraform configuration for Active-Active Failover using Route 53 with health checks. This configuration will include health checks for both primary and secondary endpoints and the associated Route 53 records.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Health Check for Primary Endpoint
resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = ""3""
  request_interval  = ""30""

  tags = {
    Name = ""primary-health-check""
  }
}

# Health Check for Secondary Endpoint
resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = ""3""
  request_interval  = ""30""

  tags = {
    Name = ""secondary-health-check""
  }
}

# Route 53 Record for Primary Endpoint
resource ""aws_route53_record"" ""primary"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""service.example.com""
  type    = ""A""
  
  failover_routing_policy {
    type = ""PRIMARY""
  }

  set_identifier = ""primary""
  
  alias {
    name                   = ""primary.example.com""
    zone_id                = aws_route53_zone.main.zone_id
    evaluate_target_health = true
  }

  health_check_id = aws_route53_health_check.primary_health_check.id
}

# Route 53 Record for Secondary Endpoint
resource ""aws_route53_record"" ""secondary"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""service.example.com""
  type    = ""A""
  
  failover_routing_policy {
    type = ""SECONDARY""
  }

  set_identifier = ""secondary""
  
  alias {
    name                   = ""secondary.example.com""
    zone_id                = aws_route53_zone.main.zone_id
    evaluate_target_health = true
  }

  health_check_id = aws_route53_health_check.secondary_health_check.id
}
```

This configuration:
1. Creates a Route 53 hosted zone named ""example.com""
2. Sets up two health checks (primary and secondary) using FQDNs
3. Creates two Route 53 records with failover routing policies
4. Configures the records as aliases with health check associations

The health checks will monitor the specified FQDNs every 30 seconds, and if a failure is detected after 3 consecutive failed checks, Route 53 will route traffic to the healthy endpoint.

Remember to replace ""example.com"" with your actual domain name and adjust the FQDNs and other parameters according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration for Active-Active Failover using Route 53 with health checks. Here's a solution that implements the required health checks and DNS routing:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Primary Health Check
resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = ""3""
  request_interval  = ""30""

  tags = {
    Name = ""primary-health-check""
  }
}

# Secondary Health Check
resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = ""3""
  request_interval  = ""30""

  tags = {
    Name = ""secondary-health-check""
  }
}

# Route 53 Active-Active Record Set
resource ""aws_route53_record"" ""active_active"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""
  type    = ""A""

  failover_routing_policy {
    type = ""PRIMARY""
  }

  set_identifier = ""primary""
  
  alias {
    name                   = ""primary.example.com""
    zone_id                = aws_route53_zone.main.zone_id
    evaluate_target_health = true
  }

  health_check_id = aws_route53_health_check.primary_health_check.id
}

resource ""aws_route53_record"" ""active_active_secondary"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""
  type    = ""A""

  failover_routing_policy {
    type = ""SECONDARY""
  }

  set_identifier = ""secondary""

  alias {
    name                   = ""secondary.example.com""
    zone_id                = aws_route53_zone.main.zone_id
    evaluate_target_health = true
  }

  health_check_id = aws_route53_health_check.secondary_health_check.id
}
```

This configuration creates:
1. A Route 53 hosted zone named ""main""
2. Two health checks (""primary_health_check"" and ""secondary_health_check"") using FQDNs
3. Two Route 53 record sets implementing active-active failover routing policy
4. Both record sets are configured with their respective health checks
5. Uses alias records pointing to the primary and secondary endpoints

The health checks monitor the specified FQDNs every 30 seconds and will fail after 3 consecutive failed checks. The failover routing policy will automatically route traffic to the healthy endpoint if one fails.

Make sure to replace ""example.com"" with your actual domain name and adjust the FQDNs, ports, and paths according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration for Active-Active Failover using Route 53 with health checks. This will include a Route 53 zone, health checks for both primary and secondary endpoints, and DNS records with failover routing policy.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Primary Health Check
resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = ""3""
  request_interval  = ""30""

  tags = {
    Name = ""primary-health-check""
  }
}

# Secondary Health Check
resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = ""3""
  request_interval  = ""30""

  tags = {
    Name = ""secondary-health-check""
  }
}

# Primary Record
resource ""aws_route53_record"" ""primary"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www""
  type    = ""A""
  ttl     = ""300""
  
  failover_routing_policy {
    type = ""PRIMARY""
  }

  set_identifier = ""primary""
  records        = [""10.0.0.1""]  # Replace with your primary endpoint IP

  health_check_id = aws_route53_health_check.primary_health_check.id
}

# Secondary Record
resource ""aws_route53_record"" ""secondary"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www""
  type    = ""A""
  ttl     = ""300""
  
  failover_routing_policy {
    type = ""SECONDARY""
  }

  set_identifier = ""secondary""
  records        = [""10.0.0.2""]  # Replace with your secondary endpoint IP

  health_check_id = aws_route53_health_check.secondary_health_check.id
}
```

This configuration creates:
1. A Route 53 hosted zone named ""example.com""
2. Two health checks monitoring primary and secondary endpoints
3. Two A records with failover routing policy (Primary and Secondary)
4. Both records are associated with their respective health checks

Note: Replace ""example.com"" with your actual domain name and the IP addresses (10.0.0.1 and 10.0.0.2) with your actual endpoint IPs.

The health checks will monitor the specified FQDNs, and Route 53 will automatically failover to the secondary endpoint if the primary health check fails.",,,,,
"aws_iam_role, aws_iam_role_policy, aws_kendra_index",Generate a basic Kendra index resource With user group resolution configuration,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
    some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kendra.amazonaws.com"") 
}

has_valid_kendra_index {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_index""
    resource.values.name
    resource.values.user_group_resolution_configuration
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
}


has_valid_resources {
    has_valid_iam_role(input.resource_changes)
    has_valid_kendra_index
}
",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""role"" {
  name = ""Kendra-Role""
  assume_role_policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": ""sts:AssumeRole"",
      ""Principal"": {
        ""Service"": ""kendra.amazonaws.com""
      },
      ""Effect"": ""Allow"",
      ""Sid"": """"
    }
  ]
}
EOF
}

resource ""aws_iam_role_policy"" ""policy"" {
  name = ""Kendra-Policy""
  role = aws_iam_role.role.id

  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:GetObject"",
        ""s3:ListBucket""
      ],
      ""Resource"": ""*""
    }
  ]
}
EOF
}

resource ""aws_kendra_index"" ""example"" {
  name     = ""example""
  role_arn = aws_iam_role.role.arn

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""
  }
}","Has one ""aws_iam_role"" resource and one ""aws_kendra_index"" resource with the ""user_group_resolution_configuration"" block","I'll help you create a Terraform configuration for an AWS Kendra index resource with user group resolution configuration. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra Index with user group resolution""
  role_arn    = aws_iam_role.kendra_role.arn

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""  # Can be AWS_SSO or NONE
  }

  edition = ""ENTERPRISE_EDITION""  # Required for user group resolution
  
  tags = {
    Environment = ""production""
    Project     = ""search""
  }
}

resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra-index-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra-index-policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kendra:*"",
          ""sso:*"",
          ""identitystore:*""
        ]
        Resource = ""*""
      }
    ]
  })
}
```

This configuration creates:
1. A Kendra index with user group resolution configured for AWS SSO
2. The necessary IAM role and policy for Kendra to function
3. Uses the Enterprise Edition which is required for user group resolution features
4. Includes basic tagging for resource management

Key features of this configuration:
- Uses the ENTERPRISE_EDITION which is required for user group resolution
- Configures user group resolution for AWS SSO integration
- Creates the required IAM role with necessary permissions
- Includes proper provider configuration
- Includes necessary permissions for SSO and identity store access

Make sure to adjust the region, names, and tags according to your specific needs.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra index resource with user group resolution configuration. Here's a solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra Index with user group resolution""
  role_arn    = aws_iam_role.kendra_role.arn

  edition = ""ENTERPRISE_EDITION""

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""
  }

  # Optional: Configure user token field mappings
  user_token_configurations {
    json_token_type_configuration {
      group_attribute_field = ""groups""
      user_name_attribute_field = ""username""
    }
  }

  tags = {
    Environment = ""production""
    Purpose     = ""search""
  }
}

# IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra_index_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""kendra_policy"" {
  role       = aws_iam_role.kendra_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSKendraServiceRole""
}
```

This configuration:
1. Creates a Kendra index with user group resolution configured for AWS SSO
2. Sets up the necessary IAM role with appropriate permissions
3. Includes user token configurations for group and username attributes
4. Uses the Enterprise Edition of Kendra
5. Adds tags for better resource management

The index is configured with user group resolution mode set to AWS SSO, which allows for integration with AWS Single Sign-On for user group resolution. The configuration includes the necessary IAM role and policy attachments for Kendra to function properly.

Note: Make sure you have the appropriate permissions and AWS SSO configured in your AWS account before using this configuration.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra index resource with user group resolution configuration. Here's a solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra Index with user group resolution""
  edition     = ""ENTERPRISE_EDITION""
  role_arn    = aws_iam_role.kendra_role.arn

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""
  }

  tags = {
    Environment = ""production""
    Purpose     = ""search""
  }
}

resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra_index_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra_index_policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kendra:*"",
          ""sso:*""
        ]
        Resource = ""*""
      }
    ]
  })
}
```

This configuration creates:
1. A Kendra index using the Enterprise Edition
2. User group resolution configuration set to use AWS SSO
3. Required IAM role and policy for Kendra to function
4. Basic tagging for resource management

Key features:
- The index uses AWS SSO for user group resolution
- Appropriate IAM permissions are set up
- The configuration includes necessary role and policy attachments
- Basic tags are included for resource management

Note: You may need to adjust the IAM permissions based on your specific requirements and security policies. The example provides basic permissions for demonstration purposes.",,,,,
aws_elasticache_cluster,Provide a Memcached Cluster,"package main

import future.keywords.in

default allow = false

aws_elasticache_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_cluster""
    resource.change.after.engine == ""memcached""
}

# Aggregate all checks
allow {
    aws_elasticache_cluster_valid(input.resource_changes)
}",2,"provider ""aws"" {
  region=""us-east-1""
}
resource ""aws_elasticache_cluster"" ""example"" {
  cluster_id           = ""cluster-example""
  engine               = ""memcached""
  node_type            = ""cache.m4.large""
  num_cache_nodes      = 2
  parameter_group_name = ""default.memcached1.4""
  port                 = 11211
}","requires cluster id, engine=memcached, node_type, num_cache_nodes, parameter_group_name","I'll help you create a Terraform configuration for an AWS ElastiCache Memcached cluster. Here's a complete configuration that includes the required provider and sets up a basic Memcached cluster:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_elasticache_cluster"" ""memcached_cluster"" {
  cluster_id           = ""memcached-cluster""
  engine              = ""memcached""
  node_type           = ""cache.t3.micro""
  num_cache_nodes     = 2
  parameter_group_name = ""default.memcached1.6""
  port                = 11211
  
  # Recommended settings for production
  az_mode             = ""cross-az""
  maintenance_window  = ""sun:05:00-sun:09:00""
  
  # Network settings
  security_group_ids = [aws_security_group.memcached_sg.id]
  
  tags = {
    Environment = ""production""
    Service     = ""memcached""
  }
}

resource ""aws_security_group"" ""memcached_sg"" {
  name        = ""memcached-security-group""
  description = ""Security group for Memcached cluster""
  
  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""] # Adjust this to your VPC CIDR
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Memcached cluster with 2 nodes
3. Uses t3.micro instances (adjust according to your needs)
4. Creates a security group for the cluster
5. Sets up basic network security rules
6. Includes standard tags for resource management

You can customize the configuration by adjusting:
- node_type
- num_cache_nodes
- maintenance_window
- security_group rules
- tags
- other parameters as needed for your specific use case

Remember to adjust the CIDR blocks in the security group to match your network architecture.",,,,,,"I'll help you create a Terraform configuration for an AWS ElastiCache Memcached cluster. Here's a complete configuration that includes the required provider and sets up a Memcached cluster with basic settings:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_elasticache_cluster"" ""memcached_cluster"" {
  cluster_id           = ""memcached-cluster""
  engine              = ""memcached""
  node_type           = ""cache.t3.micro""
  num_cache_nodes     = 2
  parameter_group_name = ""default.memcached1.6""
  port                = 11211
  
  az_mode             = ""cross-az""
  preferred_availability_zones = [""us-west-2a"", ""us-west-2b""]
  
  maintenance_window  = ""sun:05:00-sun:06:00""
  
  tags = {
    Environment = ""production""
    Name        = ""memcached-cluster""
  }
}

resource ""aws_elasticache_subnet_group"" ""memcached_subnet_group"" {
  name       = ""memcached-subnet-group""
  subnet_ids = [""subnet-12345678"", ""subnet-87654321""]  # Replace with your subnet IDs
}

resource ""aws_security_group"" ""memcached_security_group"" {
  name        = ""memcached-security-group""
  description = ""Security group for Memcached cluster""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID

  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]  # Replace with your CIDR block
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""memcached-security-group""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Memcached cluster with 2 nodes
3. Creates a subnet group for the cluster
4. Sets up a security group for the cluster with appropriate rules
5. Uses the t3.micro instance type (modify as needed for production)
6. Configures cross-AZ deployment for high availability
7. Sets up maintenance window and other basic parameters

Remember to:
- Replace the subnet IDs with your actual subnet IDs
- Replace the VPC ID with your actual VPC ID
- Adjust the CIDR blocks in the security group to match your network
- Modify the node type and number of nodes based on your requirements
- Update the region to match your desired AWS region

This is a basic configuration that you can build upon based on your specific needs.",,,,,,"I'll help you create a Terraform configuration for an AWS ElastiCache Memcached cluster. Here's a complete configuration with the required provider and necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_elasticache_cluster"" ""memcached_cluster"" {
  cluster_id           = ""memcached-cluster""
  engine              = ""memcached""
  node_type           = ""cache.t3.micro""
  num_cache_nodes     = 2
  parameter_group_name = ""default.memcached1.6""
  port                = 11211
  
  # Network configuration
  az_mode            = ""cross-az""
  preferred_availability_zones = [""us-west-2a"", ""us-west-2b""]
  
  # Maintenance window
  maintenance_window = ""sun:05:00-sun:06:00""
  
  # Tags
  tags = {
    Environment = ""Production""
    Service     = ""MemcachedCluster""
  }
}

resource ""aws_elasticache_subnet_group"" ""memcached_subnet"" {
  name       = ""memcached-subnet-group""
  subnet_ids = [""subnet-12345678"", ""subnet-87654321""] # Replace with your subnet IDs
}

resource ""aws_elasticache_parameter_group"" ""memcached_params"" {
  family = ""memcached1.6""
  name   = ""memcached-parameters""

  parameter {
    name  = ""max_item_size""
    value = ""10485760""  # 10MB
  }
}

resource ""aws_security_group"" ""memcached_sg"" {
  name        = ""memcached-security-group""
  description = ""Security group for Memcached cluster""
  vpc_id      = ""vpc-12345678"" # Replace with your VPC ID

  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""] # Replace with your CIDR block
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""memcached-sg""
  }
}
```

This configuration creates:
1. A Memcached cluster with 2 nodes
2. A subnet group for the cluster
3. A parameter group with custom settings
4. A security group for controlling access to the cluster

Key features of this configuration:
- Uses t3.micro instances (modify node_type as needed)
- Runs across two availability zones
- Uses Memcached version 1.6
- Sets up basic security group rules
- Includes maintenance window
- Configures custom parameters

Remember to:
1. Replace the subnet IDs with your actual subnet IDs
2. Replace the VPC ID with your actual VPC ID
3. Adjust the CIDR blocks in the security group to match your network
4. Modify the node type and number of nodes based on your requirements
5. Adjust the maintenance window to your preferred time
6. Update the tags to match your organization's tagging strategy",,,,,
aws_dax_parameter_group,Create a DAX parameter group.,"package main

import future.keywords.in

default allow = false

# Check for DAX parameter group creation
dax_parameter_group_created(resources) {
some resource in resources
resource.type == ""aws_dax_parameter_group""
resource.change.actions[_] == ""create""
}

# Allow DAX parameter group creation with specific parameters
allow {
dax_parameter_group_created(input.resource_changes)
}",2,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

resource ""aws_dax_parameter_group"" ""example"" {
name = ""example""

parameters {
name = ""query-ttl-millis""
value = ""100000""
}

parameters {
name = ""record-ttl-millis""
value = ""100000""
}
}","Has one ""aws_dax_parameter_group""","I'll help you create a DAX parameter group using Terraform. A DAX parameter group allows you to specify runtime parameters for DAX clusters.

Here's the Terraform configuration to create a DAX parameter group:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Replace with your desired region
}

resource ""aws_dax_parameter_group"" ""example"" {
  name        = ""example-dax-parameter-group""
  description = ""Example DAX parameter group""

  parameters {
    name  = ""query-ttl-millis""
    value = ""100000""
  }

  parameters {
    name  = ""record-ttl-millis""
    value = ""200000""
  }
}
```

This configuration:
1. Declares the required AWS provider
2. Creates a DAX parameter group with the name ""example-dax-parameter-group""
3. Sets two common parameters:
   - query-ttl-millis: The time-to-live (TTL) for query results
   - record-ttl-millis: The TTL for individual records

You can modify the parameter values according to your requirements. Other available parameters include:
- ""max-item-size""
- ""max-stale-time-millis""
- ""max-ttl-millis""

Remember that the parameter group needs to be associated with a DAX cluster to take effect.",,,,,,"I'll help you create a DAX parameter group using Terraform. Here's the code to create an AWS DAX parameter group with the required configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_dax_parameter_group"" ""example"" {
  name        = ""example-dax-parameter-group""
  description = ""Example DAX parameter group""

  parameters {
    name  = ""query-ttl-millis""
    value = ""100000""
  }

  parameters {
    name  = ""record-ttl-millis""
    value = ""200000""
  }
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates a DAX parameter group with a name and description
3. Sets two common DAX parameters as examples:
   - query-ttl-millis: Time-to-live for query results
   - record-ttl-millis: Time-to-live for individual records

You can modify the parameter values and add additional parameters according to your specific needs. The parameter group can then be associated with a DAX cluster.

Note that DAX parameter groups are used to specify runtime parameters for DAX clusters, similar to how DynamoDB works with caching parameters.",,,,,,"I'll help you create a DAX (DynamoDB Accelerator) parameter group using Terraform. Here's a code solution that creates a DAX parameter group with some common parameters:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_dax_parameter_group"" ""example"" {
  name        = ""example-dax-parameter-group""
  description = ""Example DAX parameter group""

  parameters {
    name  = ""query-ttl-millis""
    value = ""100000""
  }

  parameters {
    name  = ""record-ttl-millis""
    value = ""100000""
  }
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates a DAX parameter group with a name and description
3. Configures two common parameters:
   - query-ttl-millis: Time-to-live for query results
   - record-ttl-millis: Time-to-live for individual records

You can modify the parameter values according to your specific needs. The parameter group can then be associated with a DAX cluster when you create one.",,,,,
"aws_s3_bucket, aws_s3_bucket_notification, aws_sns_topic, aws_iam_policy_document",Create an IAM policy document that allows the 's3.amazonaws.com' service to publish to an SNS topic for S3 event notifications. Specify the actions as 'SNS:Publish' and set the resource to the ARN of the SNS topic 's3-event-notification-topic'. Apply a condition to allow this action only if the source ARN is similar to the ARN of the specified S3 bucket. Create an SNS topic named 's3-event-notification-topic' with the generated IAM policy. Set up an S3 bucket named 'your-bucket-name' and configure it to send notifications to the SNS topic for 's3:ObjectCreated:*' events with a filter for '.log' suffix.,"package terraform.validation

default has_aws_iam_policy_document_topic = false
default has_aws_sns_topic = false
default has_aws_s3_bucket = false
default has_aws_s3_bucket_notification = false

has_aws_iam_policy_document_topic {
    policy := input.planned_values.root_module.resources[_]
    policy.type == ""aws_iam_policy_document""
}

has_aws_sns_topic {
    topic := input.planned_values.root_module.resources[_]
    topic.type == ""aws_sns_topic""
    topic.name == ""topic""
    topic.values.name == ""s3-event-notification-topic""
    topic.values.policy == input.planned_values.root_module.resources[_].values.json  # Validate policy is correctly used from data source
}

has_aws_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""bucket""
    bucket.values.bucket == ""your-bucket-name""
}

has_aws_s3_bucket_notification {
    notification := input.planned_values.root_module.resources[_]
    notification.type == ""aws_s3_bucket_notification""
    notification.name == ""bucket_notification""
    notification.values.bucket == input.planned_values.root_module.resources[_].values.id  # Correct bucket ID reference
    notification.values.topic.topic_arn == input.planned_values.root_module.resources[_].values.arn  # Correct topic ARN reference
    notification.values.topic.events[0] == ""s3:ObjectCreated:*""
    notification.values.topic.filter_suffix == "".log""
}

valid_configuration {
    has_aws_iam_policy_document_topic
    has_aws_sns_topic
    has_aws_s3_bucket
    has_aws_s3_bucket_notification
}
",3,"provider ""aws"" {
    region = ""us-west-1""
}

data ""aws_iam_policy_document"" ""topic"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""s3.amazonaws.com""]
    }

    actions   = [""SNS:Publish""]
    resources = [""arn:aws:sns:*:*:s3-event-notification-topic""]

    condition {
      test     = ""ArnLike""
      variable = ""aws:SourceArn""
      values   = [aws_s3_bucket.bucket.arn]
    }
  }
}
resource ""aws_sns_topic"" ""topic"" {
  name   = ""s3-event-notification-topic""
  policy = data.aws_iam_policy_document.topic.json
}

resource ""aws_s3_bucket"" ""bucket"" {
  bucket = ""your-bucket-name""
}

resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.bucket.id

  topic {
    topic_arn     = aws_sns_topic.topic.arn
    events        = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }
}","Data ""aws_iam_policy_document""
has one ""effect""
has one ""principals"" 
has one ""actions""
has one ""resources""

Resource ""aws_sns_topic"" 
has one ""name""
uses the JSON output

Resource ""aws_s3_bucket"" 
has one ""bucket""

Resource ""aws_s3_bucket_notification"" 
has one ""bucket""
configures a ""topic"" notification with:
""topic_arn""
""events""
""filter_suffix""","I'll help you create a Terraform configuration that sets up an S3 bucket with SNS notifications and the necessary IAM policies. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create the SNS Topic
resource ""aws_sns_topic"" ""s3_notification_topic"" {
  name = ""s3-event-notification-topic""
}

# Create IAM policy document for SNS
data ""aws_iam_policy_document"" ""sns_topic_policy"" {
  statement {
    effect = ""Allow""
    
    principals {
      type        = ""Service""
      identifiers = [""s3.amazonaws.com""]
    }

    actions = [""SNS:Publish""]
    
    resources = [aws_sns_topic.s3_notification_topic.arn]

    condition {
      test     = ""ArnLike""
      variable = ""aws:SourceArn""
      values   = [""arn:aws:s3:*:*:your-bucket-name""]
    }
  }
}

# Attach the policy to the SNS topic
resource ""aws_sns_topic_policy"" ""default"" {
  arn    = aws_sns_topic.s3_notification_topic.arn
  policy = data.aws_iam_policy_document.sns_topic_policy.json
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""notification_bucket"" {
  bucket = ""your-bucket-name""
}

# Enable versioning for the bucket
resource ""aws_s3_bucket_versioning"" ""bucket_versioning"" {
  bucket = aws_s3_bucket.notification_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Configure S3 bucket notification
resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.notification_bucket.id

  topic {
    topic_arn = aws_sns_topic.s3_notification_topic.arn
    events    = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }

  depends_on = [aws_sns_topic_policy.default]
}
```

This configuration:
1. Creates an SNS topic named ""s3-event-notification-topic""
2. Creates an IAM policy document that allows S3 to publish to the SNS topic
3. Attaches the policy to the SNS topic
4. Creates an S3 bucket
5. Enables versioning on the bucket
6. Configures the bucket to send notifications to the SNS topic for object creation events with .log suffix

Make sure to:
- Replace ""your-bucket-name"" with your desired bucket name
- Adjust the region in the provider block as needed
- The depends_on in the bucket notification ensures the SNS topic policy is created before configuring the notification

This configuration follows AWS best practices and includes all necessary permissions and relationships between the services.",,,,,,"I'll help you create a Terraform configuration that sets up an S3 notification system with SNS topic and the necessary IAM policies. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create SNS Topic
resource ""aws_sns_topic"" ""s3_notification_topic"" {
  name = ""s3-event-notification-topic""
}

# Create IAM Policy Document for SNS
data ""aws_iam_policy_document"" ""sns_topic_policy"" {
  statement {
    effect = ""Allow""
    
    principals {
      type        = ""Service""
      identifiers = [""s3.amazonaws.com""]
    }

    actions = [""SNS:Publish""]
    
    resources = [aws_sns_topic.s3_notification_topic.arn]

    condition {
      test     = ""ArnLike""
      variable = ""aws:SourceArn""
      values   = [""arn:aws:s3:::your-bucket-name""]
    }
  }
}

# Attach the policy to the SNS topic
resource ""aws_sns_topic_policy"" ""default"" {
  arn    = aws_sns_topic.s3_notification_topic.arn
  policy = data.aws_iam_policy_document.sns_topic_policy.json
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""notification_bucket"" {
  bucket = ""your-bucket-name""
}

# Enable versioning for the bucket
resource ""aws_s3_bucket_versioning"" ""versioning"" {
  bucket = aws_s3_bucket.notification_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Configure S3 bucket notification
resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.notification_bucket.id

  topic {
    topic_arn = aws_sns_topic.s3_notification_topic.arn
    events    = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }

  depends_on = [aws_sns_topic_policy.default]
}
```

This configuration:
1. Creates an SNS topic named 's3-event-notification-topic'
2. Creates an IAM policy document that allows S3 to publish to the SNS topic
3. Attaches the policy to the SNS topic
4. Creates an S3 bucket
5. Configures the S3 bucket to send notifications to the SNS topic for object creation events with '.log' suffix
6. Includes proper dependencies and permissions

The configuration includes all necessary resources and proper security settings. The S3 bucket will only send notifications for files ending in '.log', and the SNS topic will only accept publications from the specified S3 bucket.",,,,,,"I'll help you create a Terraform configuration that sets up an S3 bucket with SNS notifications and the necessary IAM policies. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create the SNS Topic
resource ""aws_sns_topic"" ""s3_event_notification"" {
  name = ""s3-event-notification-topic""
}

# Create IAM policy document for SNS
data ""aws_iam_policy_document"" ""sns_topic_policy"" {
  statement {
    effect = ""Allow""
    
    principals {
      type        = ""Service""
      identifiers = [""s3.amazonaws.com""]
    }

    actions = [""SNS:Publish""]
    
    resources = [aws_sns_topic.s3_event_notification.arn]

    condition {
      test     = ""ArnLike""
      variable = ""aws:SourceArn""
      values   = [""arn:aws:s3:*:*:your-bucket-name""]
    }
  }
}

# Attach the policy to the SNS topic
resource ""aws_sns_topic_policy"" ""default"" {
  arn    = aws_sns_topic.s3_event_notification.arn
  policy = data.aws_iam_policy_document.sns_topic_policy.json
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""notification_bucket"" {
  bucket = ""your-bucket-name""
}

# Configure S3 bucket notification
resource ""aws_s3_bucket_notification"" ""bucket_notification"" {
  bucket = aws_s3_bucket.notification_bucket.id

  topic {
    topic_arn = aws_sns_topic.s3_event_notification.arn
    events    = [""s3:ObjectCreated:*""]
    filter_suffix = "".log""
  }

  depends_on = [aws_sns_topic_policy.default]
}
```

This configuration:
1. Creates an SNS topic named ""s3-event-notification-topic""
2. Creates an IAM policy document that allows S3 to publish to the SNS topic with the specified condition
3. Attaches the policy to the SNS topic
4. Creates an S3 bucket
5. Configures the S3 bucket to send notifications to the SNS topic for object creation events with .log files

The configuration includes all necessary dependencies and follows AWS best practices for setting up S3 event notifications with SNS. The `depends_on` ensures that the topic policy is created before configuring the bucket notification.

Remember to replace ""your-bucket-name"" with your desired bucket name, and note that S3 bucket names must be globally unique across all AWS accounts.",,,,,
aws_db_instance,Create a PostgresQL instance.,"package main

import future.keywords.in

default allow = false

# Check for DB instance creation
db_instance_created(resources) {
some resource in resources
resource.type == ""aws_db_instance""
resource.change.actions[_] == ""create""
}

# Check if engine is MySQL
db_engine_mysql(resource) {
resource.type == ""aws_db_instance""
resource.change.after.engine == ""postgres""
}

# Aggregate checks for DB instance
allow {
db_instance_created(input.resource_changes)
some resource in input.resource_changes
db_engine_mysql(resource)
}",3,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

# Configure the AWS Provider
provider ""aws"" {
region = ""us-east-1""
}

resource ""aws_db_instance"" ""default"" {
allocated_storage = 10
db_name = ""mydb""
engine = ""postgres""
instance_class = ""db.t3.micro""
username = ""USERNAME""
password = ""PASSWORD""
skip_final_snapshot = true
}","Has one ""aws_db_instance"" resource
with one ""engine"" = ""postgres""","I'll help you create a PostgreSQL RDS instance using Terraform. Here's a configuration that sets up a basic PostgreSQL RDS instance with common settings:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_db_instance"" ""postgresql"" {
  identifier           = ""my-postgresql-db""
  engine              = ""postgres""
  engine_version      = ""13.7""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  # Database credentials
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""your-secure-password""  # Consider using secrets management
  
  # Network settings
  publicly_accessible = false
  
  # Backup settings
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  
  # Maintenance settings
  maintenance_window     = ""Mon:04:00-Mon:05:00""
  
  # Enhanced monitoring
  monitoring_interval    = 0
  
  # Skip final snapshot when destroying
  skip_final_snapshot    = true
  
  # Enable deletion protection
  deletion_protection    = true
  
  # Performance insights
  performance_insights_enabled = true
  
  tags = {
    Name        = ""PostgreSQL-Instance""
    Environment = ""Production""
  }
}
```

This configuration:
1. Creates a PostgreSQL RDS instance
2. Uses GP2 storage with 20GB allocated
3. Sets up basic backup and maintenance windows
4. Enables performance insights
5. Implements deletion protection
6. Configures basic security settings

Remember to:
- Change the password to a secure value
- Adjust the instance size based on your needs
- Consider adding a security group
- Modify the backup and maintenance windows according to your requirements
- Consider enabling encryption at rest
- Adjust the region as needed

You might want to add additional security groups, subnet groups, and parameter groups depending on your specific requirements.",,,,,,"I'll help you create a PostgreSQL RDS instance using Terraform. I'll include a basic configuration with common settings.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_db_instance"" ""postgresql"" {
  identifier           = ""my-postgresql-db""
  engine              = ""postgres""
  engine_version      = ""13.7""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username           = ""dbadmin""
  password           = ""your-secure-password""  # Consider using variables or secrets management
  
  skip_final_snapshot = true
  
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""
  
  vpc_security_group_ids = []  # Add your security group IDs
  db_subnet_group_name   = """"  # Add your subnet group name if using VPC
  
  # Enable encryption at rest
  storage_encrypted = true
  
  # Enable automated backups
  deletion_protection = true
  
  tags = {
    Name        = ""postgresql-instance""
    Environment = ""production""
  }
}
```

This configuration creates a PostgreSQL RDS instance with:
- A db.t3.micro instance type (suitable for dev/test, adjust for production)
- 20GB of GP2 storage
- Automated backups with 7-day retention
- Encryption at rest enabled
- Deletion protection enabled
- Basic maintenance and backup windows

Remember to:
1. Replace the password with a secure value
2. Add appropriate security group IDs and subnet group if using VPC
3. Adjust the instance size and storage based on your needs
4. Modify the backup and maintenance windows according to your requirements
5. Consider using variables for sensitive information

You can customize this further by adding parameters like multi-AZ deployment, performance insights, or additional storage options based on your specific requirements.",,,,,,"I'll help you create a PostgreSQL RDS instance using Terraform. Here's a configuration that sets up a basic PostgreSQL RDS instance with common best practices:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust the region as needed
}

resource ""aws_db_instance"" ""postgresql"" {
  identifier           = ""postgresql-instance""
  engine              = ""postgres""
  engine_version      = ""13.7""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  # Database credentials
  username            = ""dbadmin""
  password            = ""yourpasswordhere""  # Consider using secrets management
  
  # Network settings
  publicly_accessible = false
  
  # Backup settings
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  
  # Maintenance settings
  maintenance_window     = ""Mon:04:00-Mon:05:00""
  
  # Enhanced monitoring
  monitoring_interval    = 0
  
  # Database settings
  db_name               = ""mydb""
  port                  = 5432
  
  # Security settings
  storage_encrypted     = true
  
  # Skip final snapshot when destroying
  skip_final_snapshot   = true
  
  # Parameter group
  parameter_group_name  = ""default.postgres13""
  
  tags = {
    Name        = ""postgresql-instance""
    Environment = ""production""
  }
}
```

This configuration:
1. Creates a PostgreSQL RDS instance
2. Uses GP2 storage with 20GB allocated
3. Sets up basic backup and maintenance windows
4. Enables storage encryption
5. Uses the t3.micro instance class (suitable for development; adjust for production)
6. Sets up basic security and monitoring configurations

Remember to:
- Change the password to a secure value
- Consider using AWS Secrets Manager for credential management
- Adjust the instance class based on your workload requirements
- Configure appropriate VPC and security group settings for production use
- Modify backup and maintenance windows according to your needs
- Consider enabling enhanced monitoring for production workloads

You can enhance this configuration by adding VPC configurations, security groups, and other advanced settings based on your specific requirements.",,,,,
"aws_glacier_vault, aws_sns_topic, aws_iam_policy_document",generate an S3 Glacier vault with notification service,"package terraform.validation

default is_valid_terraform_plan = false

# Rule to check for an AWS Glacier Vault resource with 'name' and 'notification' attributes
is_valid_glacier_vault {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_glacier_vault""
	not is_null(resource.name)
	not is_null(resource.expressions)
	not is_null(resource.expressions.notification[0].sns_topic)
	not is_null(resource.expressions.notification[0].events)
}


# Rule to check for the existence of an AWS SNS Topic resource
is_valid_sns_topic {
	sns_topic_resource := input.planned_values.root_module.resources[_]
	sns_topic_resource.type == ""aws_sns_topic""
}

# Combined rule to validate the entire Terraform plan
is_valid_terraform_plan {
	is_valid_glacier_vault
	is_valid_sns_topic
}",3,"resource ""aws_sns_topic"" ""aws_sns_topic"" {
  name = ""glacier-sns-topic""
}

data ""aws_iam_policy_document"" ""my_archive"" {
  statement {
    sid    = ""add-read-only-perm""
    effect = ""Allow""

    principals {
      type        = ""*""
      identifiers = [""*""]
    }

    actions = [
      ""glacier:InitiateJob"",
      ""glacier:GetJobOutput"",
    ]

    resources = [""arn:aws:glacier:eu-west-1:432981146916:vaults/MyArchive""]
  }
}

resource ""aws_glacier_vault"" ""my_archive"" {
  name = ""MyArchive""

  notification {
    sns_topic = aws_sns_topic.aws_sns_topic.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  access_policy = data.aws_iam_policy_document.my_archive.json

  tags = {
    Test = ""MyArchive""
  }
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute
     with ""notification"" attribute
          with ""event"" attribute
          with ""sns_topic"" attribute

with ""aws_sns_topic"" resource","I'll help you create a Terraform configuration for an S3 Glacier vault with notification service. This will include the AWS Glacier vault and its notification configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Configure the AWS Provider
provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  tags = {
    Environment = ""Production""
    Purpose     = ""LongTermStorage""
  }
}

# Create SNS Topic for notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-vault-notifications""
}

# SNS Topic Policy
resource ""aws_sns_topic_policy"" ""glacier_notification_policy"" {
  arn = aws_sns_topic.glacier_notifications.arn

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""GlacierNotificationPolicy""
        Effect = ""Allow""
        Principal = {
          Service = ""glacier.amazonaws.com""
        }
        Action   = ""SNS:Publish""
        Resource = aws_sns_topic.glacier_notifications.arn
      }
    ]
  })
}
```

This configuration includes:

1. Required AWS provider configuration
2. A Glacier vault with notification settings
3. An SNS topic to receive notifications
4. An SNS topic policy that allows Glacier to publish notifications

The Glacier vault is configured to send notifications for archive and inventory retrieval completion events to the SNS topic. You can modify the `events` list to include other notification events as needed.

The supported events for Glacier notifications are:
- ArchiveRetrievalCompleted
- InventoryRetrievalCompleted
- VaultLockComplete

You can customize the tags, vault name, and region according to your requirements. Make sure to subscribe to the SNS topic separately to receive the notifications through your preferred endpoint (email, Lambda, etc.).",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault with notification service. This will include the AWS Glacier vault and its notification configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  tags = {
    Environment = ""Production""
    Purpose     = ""LongTermStorage""
  }
}

# Create SNS Topic for notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-vault-notifications""
}

# SNS Topic Policy
resource ""aws_sns_topic_policy"" ""glacier_notification_policy"" {
  arn = aws_sns_topic.glacier_notifications.arn

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""GlacierNotificationPolicy""
        Effect = ""Allow""
        Principal = {
          Service = ""glacier.amazonaws.com""
        }
        Action   = ""SNS:Publish""
        Resource = aws_sns_topic.glacier_notifications.arn
      }
    ]
  })
}

# Optional: Create an SNS topic subscription (uncomment and modify as needed)
/*
resource ""aws_sns_topic_subscription"" ""glacier_notification_subscription"" {
  topic_arn = aws_sns_topic.glacier_notifications.arn
  protocol  = ""email""  # Can be: email, sms, sqs, http, https, lambda
  endpoint  = ""your-email@example.com""
}
*/
```

This configuration:

1. Creates a Glacier vault with notification settings
2. Sets up an SNS topic to receive the notifications
3. Configures the necessary SNS topic policy to allow Glacier to publish notifications
4. Includes an optional (commented) SNS subscription configuration

The notification configuration is set up to trigger on vault events like archive and inventory retrieval completions. You can modify the `events` list in the vault notification block to include other events you want to monitor.

Key features:
- Vault with notifications enabled
- SNS topic for receiving notifications
- Proper IAM permissions through SNS topic policy
- Tags for better resource management
- Optional subscription configuration (commented out)

Remember to:
1. Replace the region with your desired AWS region
2. Modify the vault name as needed
3. Adjust the notification events based on your requirements
4. If using the subscription, uncomment and configure the endpoint appropriately",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault with notification service. Here's a solution that includes the required provider and creates a Glacier vault with SNS notifications:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create SNS Topic for notifications
resource ""aws_sns_topic"" ""glacier_notifications"" {
  name = ""glacier-vault-notifications""
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  notification {
    sns_topic = aws_sns_topic.glacier_notifications.arn
    events    = [""ArchiveRetrievalCompleted"", ""InventoryRetrievalCompleted""]
  }

  tags = {
    Environment = ""Production""
    Purpose     = ""LongTermStorage""
  }
}

# SNS Topic Policy
resource ""aws_sns_topic_policy"" ""glacier_notification_policy"" {
  arn = aws_sns_topic.glacier_notifications.arn

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""GlacierNotificationPolicy""
        Effect = ""Allow""
        Principal = {
          Service = ""glacier.amazonaws.com""
        }
        Action   = ""SNS:Publish""
        Resource = aws_sns_topic.glacier_notifications.arn
      }
    ]
  })
}

# Optional: Add Vault Access Policy
resource ""aws_glacier_vault_lock"" ""vault_lock"" {
  vault_name = aws_glacier_vault.example_vault.name
  complete_lock = false  # Set to true for immutable policy

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""EnforceArchiveLock""
        Effect = ""Deny""
        Principal = {
          AWS = ""*""
        }
        Action   = [""glacier:DeleteArchive""]
        Resource = aws_glacier_vault.example_vault.arn
        Condition = {
          NumericLessThan = {
            ""glacier:ArchiveAgeInDays"" = 365
          }
        }
      }
    ]
  })
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates an SNS topic for notifications
3. Creates a Glacier vault with notification configuration
4. Configures the SNS topic policy to allow Glacier to publish notifications
5. Adds an optional vault lock policy that prevents deletion of archives less than 365 days old

The notification configuration will send notifications for archive and inventory retrieval completion events. You can modify the `events` list in the notification block to include other events such as:
- `ArchiveRetrievalCompleted`
- `InventoryRetrievalCompleted`
- `ArchiveRetrievalFailed`
- `InventoryRetrievalFailed`

Remember to:
- Adjust the region in the provider block
- Modify the vault name and SNS topic name as needed
- Customize the tags and policies according to your requirements
- Consider whether you want a complete lock on the vault (setting `complete_lock = true` makes the policy immutable)",,,,,
"aws_route53_record, aws_route53_zone","Set up a non-alias aws_route_53 record that is linked to an aws_route53_zone resource named ""example53.com""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
	some i
	resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}

# Validate aws_route53_record
is_valid_r53_record {
	some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.example53.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""example53"" {
  name = ""example53.com""
}

resource ""aws_route53_record"" ""example53_A"" {
  zone_id = aws_route53_zone.example53.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""300""
  records = [""192.0.2.1""]  
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
","I'll help you create a Terraform configuration for an AWS Route 53 record that links to a Route 53 zone for ""example53.com"". Here's a solution that includes both the zone and record resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create the Route 53 Zone
resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""
}

# Create the Route 53 Record
resource ""aws_route53_record"" ""primary"" {
  zone_id = aws_route53_zone.primary.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = 300
  records = [""10.0.0.1""]  # Replace with your actual IP address
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates a Route 53 hosted zone for ""example53.com""
3. Creates an A record in that zone
   - Links to the zone using the zone_id reference
   - Sets a TTL of 300 seconds
   - Points to an example IP (10.0.0.1 - you should replace this with your actual IP address)

Note that you can modify the record type (currently ""A"") and other parameters based on your specific needs.",,,,,,"I'll help you create a Terraform configuration for setting up an AWS Route 53 record that's linked to a Route 53 zone for ""example53.com"". Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

# Create the Route 53 zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
}

# Create the Route 53 record
resource ""aws_route53_record"" ""example"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = 300
  records = [""10.0.0.1""]  # Replace with your actual IP address
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Route 53 zone for ""example53.com""
3. Creates an A record in that zone
4. Links the record to the zone using the zone_id reference

You can modify the record type, TTL, and record values according to your specific needs. The example shows an A record, but you could change it to CNAME, MX, TXT, or other supported record types.",,,,,,"I'll help you create a Terraform configuration for an AWS Route 53 record that links to a Route 53 zone for ""example53.com"". I'll include both the zone and record resources, along with the required provider block.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

# Create the Route 53 zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
}

# Create the Route 53 record
resource ""aws_route53_record"" ""main"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = 300
  records = [""10.0.0.1""]  # Replace with your actual IP address
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Route 53 hosted zone for ""example53.com""
3. Creates an A record in that zone (you can modify the record type, TTL, and target IP as needed)

The record is directly linked to the zone using the `zone_id` reference from the zone resource. Note that this is a simple A record example; you can adjust the record type and values based on your specific needs.",,,,,
"aws_vpc, aws_vpc_dhcp_options, aws_vpc_dhcp_options_association","sets up a basic AWS network infrastructure consisting of a Virtual Private Cloud (VPC) with a custom DHCP options set. The VPC is configured with a CIDR block of 192.168.0.0/16. The DHCP options include a domain name (windomain.local), a combination of custom and Google's DNS servers (192.168.56.102 and 8.8.8.8), and a NetBIOS name server. These DHCP options are then associated with the created VPC, customizing its DHCP settings.






","package terraform.validation

# Set default validation states
default is_valid_vpc = false

default is_valid_dhcp_options = false

default is_valid_dhcp_options_association = false

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value == ""192.168.0.0/16""
}

# Validate aws_vpc_dhcp_options resource
is_valid_dhcp_options {
        some i
        resource := input.planned_values.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options""
        resource.values.domain_name == ""windomain.local""
    resource.values.domain_name_servers == [""192.168.56.102"", ""8.8.8.8""]
        resource.values.netbios_name_servers != null
}

# Validate aws_vpc_dhcp_options_association resource
is_valid_dhcp_options_association {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options_association""
        resource.expressions.dhcp_options_id.references[0] == ""aws_vpc_dhcp_options.default.id""
        resource.expressions.vpc_id.references[0] == ""aws_vpc.default.id""
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""default"" {
  cidr_block = ""192.168.0.0/16""
}

resource ""aws_vpc_dhcp_options"" ""default"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = concat([""192.168.56.102""], [""8.8.8.8""])
  netbios_name_servers = [""192.168.56.102""]
}

resource ""aws_vpc_dhcp_options_association"" ""default"" {
  vpc_id          = aws_vpc.default.id
  dhcp_options_id = aws_vpc_dhcp_options.default.id
}","Has one ""aws_vpc"" resource:
Configured with a ""cidr_block"" of ""192.168.0.0/16""
Includes associated custom DHCP options

Has one ""aws_vpc_dhcp_options"" resource:
Specifies a ""domain_name"" of ""windomain.local""
Includes DNS servers with a combination of a custom DNS server (""192.168.56.102"") and Google's DNS server (""8.8.8.8"")
Configures a ""NetBIOS"" name server

Has one ""aws_vpc_dhcp_options_association"" resource:
Associates the ""aws_vpc_dhcp_options"" with the created ""aws_vpc""
Customizes DHCP settings specifically for the created VPC environment","I'll help you create a Terraform configuration that sets up a VPC with custom DHCP options according to your requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""dns_resolver"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a custom DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The VPC has DNS hostnames and DNS support enabled by default. All resources are properly tagged for better resource management. You can modify the region in the provider block to match your requirements.",,,,,,"I'll help you create a Terraform configuration that sets up a VPC with custom DHCP options according to your requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""dhcp_association"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with CIDR block 192.168.0.0/16 and enables DNS support
3. Creates a custom DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The configuration includes proper tagging for better resource management and identification. You can modify the region in the provider block to match your desired AWS region.",,,,,,"I'll help you create a Terraform configuration that sets up a VPC with custom DHCP options as specified. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # You can change this to your desired region
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""dhcp_association"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a custom DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The configuration enables both DNS hostnames and DNS support in the VPC for proper name resolution. All resources are properly tagged for better resource management.",,,,,
"aws_elastic_beanstalk_application, aws_elastic_beanstalk_application_version, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment, aws_s3_bucket, aws_s3_object, aws_sqs_queue","Create an elastic beanstalk worker environment for running batch processing jobs. Name the iam_role ""eb_ec2_role"", the instance profile ""eb_ec2_profile"", the elastic beanstalk application ""batch_job_app"", the bucket ""sampleapril26426"", the object ""examplebucket_object"", the sqs queue ""batch_job_queue"", and the application version ""version"".","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_s3_bucket = false

default is_valid_s3_object = false

default is_valid_eb_app = false

default is_valid_eb_env = false

is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0] == ""aws_iam_role.eb_ec2_role.name""
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0] == ""aws_iam_role.eb_ec2_role.name""
}

is_valid_s3_bucket {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    resource.name
    resource.expressions.bucket.constant_value == ""sampleapril26426""
}

is_valid_s3_object {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_s3_object""
    resource.expressions.bucket.references[0] == ""aws_s3_bucket.sampleapril26426.id""
    resource.expressions.key
    resource.expressions.source
    
}

is_valid_sqs_queue {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_sqs_queue""
    resource.expressions.name
}

# Validate aws_eb_app
is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}

is_valid_eb_app_version {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application_version""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.batch_job_app.name""
    resource.expressions.bucket.references[0] == ""aws_s3_object.examplebucket_object.bucket""
    resource.expressions.key.references[0] == ""aws_s3_object.examplebucket_object.key""
}
# Validate aws_eb_env
is_valid_eb_env {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.batch_job_app.name""
    resource.expressions.solution_stack_name
    resource.expressions.tier.constant_value == ""Worker""
    resource.expressions.version_label.references[0] == ""aws_elastic_beanstalk_application_version.version.name""
            some a, b
    resource.expressions.setting[a].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource.expressions.setting[b].value.references[0] == ""aws_sqs_queue.batch_job_queue.id""
}


# Combine all checks into a final rule
is_configuration_valid {
        is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
    is_valid_s3_bucket
    is_valid_s3_object 
    is_valid_sqs_queue
    is_valid_eb_app_version
    is_valid_eb_app
    is_valid_eb_env
}
",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region = ""us-east-1"" 
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}


resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}


# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}


resource ""aws_s3_bucket"" ""sample_bucket"" {
  bucket_prefix = ""sample-""
}

resource ""aws_s3_object"" ""examplebucket_object"" {
  key    = ""randofile""
  bucket = aws_s3_bucket.sample_bucket.id
  source = ""./supplement/app.zip""
}

# Define the Elastic Beanstalk application
resource ""aws_elastic_beanstalk_application"" ""batch_job_app"" {
  name        = ""batch-job-application""
}

# Define the Elastic Beanstalk application version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""batch-job-app-version""
  application = aws_elastic_beanstalk_application.batch_job_app.name
  bucket = aws_s3_object.examplebucket_object.bucket
  key    = aws_s3_object.examplebucket_object.key
}

# Create an Elastic Beanstalk environment in the worker tier
resource ""aws_elastic_beanstalk_environment"" ""batch_job_env"" {
  name                = ""batch-job-environment""
  application         = aws_elastic_beanstalk_application.batch_job_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  tier                = ""Worker""
  version_label       = aws_elastic_beanstalk_application_version.version.name

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""SingleInstance"" 
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""WorkerQueueURL""
    value     = aws_sqs_queue.batch_job_queue.id 
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""HttpPath""
    value     = ""/""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""MimeType""
    value     = ""application/json"" 
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Define the SQS queue
resource ""aws_sqs_queue"" ""batch_job_queue"" {
  name = ""batch-job-queue""
}
","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has one ""aws_elastic_beanstalk_application"" resource
    with ""name"" 

Has one ""aws_s3_bucket"" resource
    with ""bucket""

Has one ""aws_s3_object"" resource
    with ""key""
    with ""source""
    with ""bucket"" referencing the ""aws_s3_bucket"" resource

Has one ""aws_elastic_beanstalk_application_version"" resource
    with ""name
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""bucket"" referencing the ""bucket"" from the ""aws_s3_object"" resource
    with ""key"" referencing the ""key"" from the ""aws_s3_object"" resource

Has one ""aws_sqs_queue"" resource
    with ""name""

Has one ""aws_elastic_beanstalk_environment"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with ""tier"" set to ""Worker""
    with ""version_label"" referencing the ""aws_elastic_beanstalk_application_version"" resource
    with several ""setting"" blocks that have ""namespaces"" set to either ""aws:elasticbeanstalk:environment"" or ""aws:elasticbeanstalk:sqsd"", 
    with one ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource
","I'll help you create a Terraform configuration for an Elastic Beanstalk worker environment with the specified components. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# Instance Profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# S3 Bucket
resource ""aws_s3_bucket"" ""app_bucket"" {
  bucket = ""sampleapril26426""
}

# S3 Bucket Object
resource ""aws_s3_bucket_object"" ""app_version"" {
  bucket = aws_s3_bucket.app_bucket.id
  key    = ""examplebucket_object""
  source = ""application.zip""  # Make sure this file exists in your local directory
}

# SQS Queue
resource ""aws_sqs_queue"" ""batch_queue"" {
  name = ""batch_job_queue""
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""batch_app"" {
  name        = ""batch_job_app""
  description = ""Batch processing application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""batch_app_version"" {
  name        = ""version""
  application = aws_elastic_beanstalk_application.batch_app.name
  description = ""application version created by terraform""
  bucket      = aws_s3_bucket.app_bucket.id
  key         = aws_s3_bucket_object.app_version.key
}

# Elastic Beanstalk Worker Environment
resource ""aws_elastic_beanstalk_environment"" ""worker_environment"" {
  name                = ""worker-environment""
  application         = aws_elastic_beanstalk_application.batch_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v3.5.3 running Python 3.8""
  tier                = ""Worker""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""WorkerQueueURL""
    value     = aws_sqs_queue.batch_queue.id
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }
}
```

This configuration creates:
1. An IAM role and instance profile for EC2 instances
2. An S3 bucket and object for application versions
3. An SQS queue for worker environment
4. An Elastic Beanstalk application and version
5. A worker environment with auto-scaling configuration

Note: 
- Make sure to replace the `source = ""application.zip""` with your actual application source file
- The solution stack name might need to be updated based on your requirements
- You might want to add additional IAM policies to the EC2 role based on your application's needs
- Consider adding",,,,,,"I'll help you create a Terraform configuration for an Elastic Beanstalk worker environment with the specified components. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# IAM instance profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# S3 bucket for application versions
resource ""aws_s3_bucket"" ""app_bucket"" {
  bucket = ""sampleapril26426""
}

# S3 bucket object
resource ""aws_s3_object"" ""app_version"" {
  bucket = aws_s3_bucket.app_bucket.id
  key    = ""examplebucket_object""
  source = ""path/to/your/application.zip""  # Replace with actual application source
}

# SQS Queue for worker environment
resource ""aws_sqs_queue"" ""worker_queue"" {
  name = ""batch_job_queue""
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""batch_app"" {
  name        = ""batch_job_app""
  description = ""Batch processing application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""batch_app_version"" {
  name        = ""version""
  application = aws_elastic_beanstalk_application.batch_app.name
  description = ""application version created by terraform""
  bucket      = aws_s3_bucket.app_bucket.id
  key         = aws_s3_object.app_version.key
}

# Elastic Beanstalk Worker Environment
resource ""aws_elastic_beanstalk_environment"" ""worker_environment"" {
  name                = ""worker-environment""
  application         = aws_elastic_beanstalk_application.batch_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v3.5.4 running Docker""  # Update with desired solution stack
  tier                = ""Worker""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""WorkerQueueURL""
    value     = aws_sqs_queue.worker_queue.id
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }
}
```

This configuration creates:
1. An IAM role and instance profile for EC2 instances
2. An S3 bucket and object for application versions
3. An SQS queue for worker tasks
4. An Elastic Beanstalk application and version
5. A worker environment configured with the specified components

Note: You'll need to:
1. Replace the region if needed
2. Update the solution_stack_name to match your desired platform
3. Provide the actual path to your application source file
4. Add any additional environment-specific settings as needed

The worker environment will",,,,,,"I'll help you create a Terraform configuration for an Elastic Beanstalk worker environment with the specified components. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# Instance Profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# S3 Bucket
resource ""aws_s3_bucket"" ""application_bucket"" {
  bucket = ""sampleapril26426""
}

# S3 Bucket Object
resource ""aws_s3_object"" ""app_version"" {
  bucket = aws_s3_bucket.application_bucket.id
  key    = ""examplebucket_object""
  source = ""application.zip""  # Make sure this file exists
}

# SQS Queue
resource ""aws_sqs_queue"" ""batch_queue"" {
  name = ""batch_job_queue""
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""batch_app"" {
  name        = ""batch_job_app""
  description = ""Batch processing application""
}

# Application Version
resource ""aws_elastic_beanstalk_application_version"" ""batch_app_version"" {
  name        = ""version""
  application = aws_elastic_beanstalk_application.batch_app.name
  description = ""application version created by terraform""
  bucket      = aws_s3_bucket.application_bucket.id
  key         = aws_s3_object.app_version.key
}

# Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""worker_environment"" {
  name                = ""worker-environment""
  application         = aws_elastic_beanstalk_application.batch_app.name
  solution_stack_name = ""64bit Amazon Linux 2 v3.5.3 running Docker""
  tier                = ""Worker""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:sqsd""
    name      = ""WorkerQueueURL""
    value     = aws_sqs_queue.batch_queue.id
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }
}
```

This configuration creates:
1. An IAM role and instance profile for EC2 instances
2. An S3 bucket and object for application versions
3. An SQS queue for batch processing
4. An Elastic Beanstalk application and version
5. A worker environment configured with the specified components

Note: Make sure to:
- Replace the region if needed
- Ensure the `application.zip` file exists in your working directory
- Adjust the solution stack name according to your needs
- Add any additional environment-specific settings as required

The worker environment will automatically process messages from the specified SQS queue using the deployed application version.",,,,,
"aws_s3_bucket, aws_s3_bucket_metric","Create a S3 bucket with bucket name = ""mybucket"" and a bucket metric resource that adds metrics configuration for the entire bucket.","package s3_bucket_metric
import future.keywords.in

default valid := false

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some metrics in input.configuration.root_module.resources
    metrics.type == ""aws_s3_bucket_metric""
    bucket.address in metrics.expressions.bucket.references
}

valid {
    some bucket in input.configuration.root_module.resources
    bucket.type == ""aws_s3_bucket""
    bucket.expressions.bucket.constant_value == ""mybucket""
    
    some metrics in input.configuration.root_module.resources
    metrics.type == ""aws_s3_bucket_metric""
    metrics.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""example""
}

resource ""aws_s3_bucket_metric"" ""example-entire-bucket"" {
  bucket = aws_s3_bucket.example.id
  name   = ""EntireBucket""
}","Has an aws_s3_bucket resource

Has an aws_s3_bucket_metric
with bucket = bucked id OR name","I'll help you create a Terraform configuration for an S3 bucket with a bucket metric configuration. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

# Create S3 bucket metrics configuration
resource ""aws_s3_bucket_metric"" ""bucket_metrics"" {
  bucket = aws_s3_bucket.mybucket.id
  name   = ""EntireBucketMetrics""

  filter {
    prefix = """"  # Empty prefix means entire bucket
  }
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates an S3 bucket named ""mybucket""
3. Creates a metric configuration for the entire bucket
   - The empty prefix in the filter means it will collect metrics for the entire bucket
   - The metrics configuration is named ""EntireBucketMetrics""

The metric configuration will allow you to monitor various metrics for the entire bucket through CloudWatch metrics.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a bucket metric configuration. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_metrics"" ""example_bucket_metrics"" {
  bucket = aws_s3_bucket.example_bucket.id
  name   = ""EntireBucketMetrics""

  filter {
    prefix = """"  # Empty prefix means entire bucket
  }
}
```

This configuration:

1. Declares the AWS provider requirement
2. Creates an S3 bucket named ""mybucket""
3. Creates a bucket metrics configuration that:
   - References the created bucket
   - Names the metrics configuration ""EntireBucketMetrics""
   - Uses an empty prefix filter to measure metrics for the entire bucket

The metrics will be available in CloudWatch after the configuration is applied. The metrics will track the bucket's storage usage, request counts, and other S3-related metrics.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a bucket metric configuration that covers the entire bucket. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""my_bucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_metrics"" ""entire_bucket"" {
  bucket = aws_s3_bucket.my_bucket.id
  name   = ""EntireBucket""

  filter {
    prefix = """"  # Empty prefix means entire bucket
  }
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates an S3 bucket named ""mybucket""
3. Creates a bucket metrics configuration that:
   - References the created bucket
   - Uses name ""EntireBucket""
   - Configures metrics for the entire bucket by using an empty prefix in the filter

The metrics configuration will track all objects in the bucket, and you can view these metrics in Amazon CloudWatch.",,,,,
"aws_glacier_vault, aws_iam_policy_document",generage an S3 Glacier vault with a access policy,"package terraform.validation

default is_valid_terraform_plan = false

# Rule to check for an AWS Glacier Vault resource with 'name' and 'access_policy' attributes
is_valid_glacier_vault {
	resource := input.planned_values.root_module.resources[_]
	resource.type == ""aws_glacier_vault""
	not is_null(resource.values.name)
	not is_null(resource.values.access_policy)
}

# Combined rule to validate the entire Terraform plan
is_valid_terraform_plan {
	is_valid_glacier_vault
}",3,"provider ""aws"" {
  region     = ""us-east-1""
}

data ""aws_iam_policy_document"" ""my_archive"" {
  statement {
    sid    = ""add-read-only-perm""
    effect = ""Allow""

    principals {
      type        = ""*""
      identifiers = [""*""]
    }

    actions = [
      ""glacier:InitiateJob"",
      ""glacier:GetJobOutput"",
    ]

    resources = [""arn:aws:glacier:eu-west-1:432981146916:vaults/MyArchive""]
  }
}

resource ""aws_glacier_vault"" ""my_archive"" {
  name = ""MyArchive""
  access_policy = data.aws_iam_policy_document.my_archive.json

  tags = {
    Test = ""MyArchive""
  }
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute
     with ""access_policy"" attribute","I'll help you create a Terraform configuration for an S3 Glacier vault with an access policy. Here's a solution that includes the required provider and creates a Glacier vault with an associated access policy:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  access_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""allow-glacier-access""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""glacier:ListVaults"",
          ""glacier:DescribeVault"",
          ""glacier:UploadArchive"",
          ""glacier:InitiateMultipartUpload"",
          ""glacier:AbortMultipartUpload"",
          ""glacier:CompleteMultipartUpload""
        ]
        Resource = ""arn:aws:glacier:*:*:vaults/example-glacier-vault""
      }
    ]
  })

  tags = {
    Environment = ""production""
    Purpose     = ""long-term-storage""
  }
}

resource ""aws_glacier_vault_lock"" ""vault_lock"" {
  complete_lock = true
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""enforce-secure-transport""
        Effect = ""Deny""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""glacier:*""
        ]
        Resource = aws_glacier_vault.example_vault.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"" = ""false""
          }
        }
      }
    ]
  })
  vault_name = aws_glacier_vault.example_vault.name
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Glacier vault with an access policy that allows basic operations
3. Includes a vault lock policy that enforces secure transport (HTTPS)
4. Adds tags for better resource management
5. The access policy allows specific Glacier operations while maintaining security
6. Uses the vault lock to enforce additional security measures

The vault lock policy is set to `complete_lock = true`, which means it cannot be changed once applied. Make sure this is what you want before applying the configuration.

Remember to adjust the vault name, permissions, and other parameters according to your specific needs.",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault with an access policy. Here's a solution that includes both the vault creation and its associated access policy:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  tags = {
    Environment = ""Production""
    Purpose     = ""LongTermBackup""
  }
}

# Create Glacier Vault Access Policy
resource ""aws_glacier_vault_policy"" ""example_vault_policy"" {
  vault_name = aws_glacier_vault.example_vault.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""allow-glacier-access""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""glacier:DescribeVault"",
          ""glacier:ListVaults"",
          ""glacier:InitiateJob"",
          ""glacier:GetJobOutput"",
          ""glacier:UploadArchive"",
          ""glacier:InitiateMultipartUpload"",
          ""glacier:AbortMultipartUpload"",
          ""glacier:CompleteMultipartUpload""
        ]
        Resource = aws_glacier_vault.example_vault.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"" = ""true""
          }
        }
      }
    ]
  })
}
```

This configuration:
1. Creates a Glacier vault named ""example-glacier-vault""
2. Applies tags for better resource management
3. Creates an access policy that:
   - Allows specific Glacier operations
   - Requires secure transport (HTTPS)
   - References the vault using its ARN
   - Includes common Glacier operations like upload, describe, and job management

You can customize the policy by modifying the allowed actions, principals, and conditions according to your security requirements.",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault with an access policy. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Glacier Vault
resource ""aws_glacier_vault"" ""example_vault"" {
  name = ""example-glacier-vault""

  tags = {
    Environment = ""production""
  }
}

# Create Glacier Vault Access Policy
resource ""aws_glacier_vault_lock"" ""example_lock"" {
  vault_name = aws_glacier_vault.example_vault.name
  complete_lock = true

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""ReadWriteAccess""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""glacier:InitiateJob"",
          ""glacier:GetJobOutput"",
          ""glacier:UploadArchive"",
          ""glacier:DeleteArchive""
        ]
        Resource = aws_glacier_vault.example_vault.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"": ""true""
          }
        }
      }
    ]
  })
}

# Optional: Add notification configuration
resource ""aws_glacier_vault_notification"" ""example_notification"" {
  vault_name = aws_glacier_vault.example_vault.name
  sns_topic = ""arn:aws:sns:us-west-2:123456789012:example-topic""  # Replace with your SNS topic ARN

  events = [
    ""ArchiveRetrievalCompleted"",
    ""InventoryRetrievalCompleted""
  ]
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a Glacier vault named ""example-glacier-vault""
3. Applies a vault access policy that:
   - Allows specific Glacier operations
   - Requires secure transport (HTTPS)
   - Can be customized based on your specific access requirements
4. Optionally adds a notification configuration for vault events

You can customize the policy, vault name, and other parameters according to your specific requirements. The policy shown is a basic example that allows certain operations while requiring secure transport. Make sure to adjust the region and any ARNs to match your AWS environment.",,,,,
"aws_chime_voice_connector, aws_chime_voice_connector, aws_chime_voice_connector_group","Create two AWS Chime Voice Connectors, both with encryption. Then create an aws_chime_voice_connector_group resource with both of these connectors. One connector should have priority=1 and the other should have priority=3","package aws_chime_voice_connector
import future.keywords.in

default valid := false
default connectors_valid := false

valid {
    connectors_valid
}

connectors_valid {
    some vc1 in input.configuration.root_module.resources
    vc1.type == ""aws_chime_voice_connector""
    vc1.expressions.require_encryption.constant_value == true

    some vc2 in input.configuration.root_module.resources
    vc2.type == ""aws_chime_voice_connector""
    vc2.expressions.require_encryption.constant_value == true

    not vc1 == vc2
    
    some vcgroup in input.configuration.root_module.resources
    vcgroup.type == ""aws_chime_voice_connector_group""

    some connector1 in vcgroup.expressions.connector
    vc1.address in connector1.voice_connector_id.references
    some connector2 in vcgroup.expressions.connector
    vc2.address in connector2.voice_connector_id.references
    
    not connector1 == connector2
    
    connector1.priority.constant_value == 1
    connector2.priority.constant_value == 3
}

connectors_valid {
        some vc1 in input.configuration.root_module.resources
    vc1.type == ""aws_chime_voice_connector""
    vc1.expressions.require_encryption.constant_value == true

    some vc2 in input.configuration.root_module.resources
    vc2.type == ""aws_chime_voice_connector""
    vc2.expressions.require_encryption.constant_value == true

    not vc1 == vc2
    
    some vcgroup in input.configuration.root_module.resources
    vcgroup.type == ""aws_chime_voice_connector_group""

    some connector1 in vcgroup.expressions.connector
    vc1.address in connector1.voice_connector_id.references
    some connector2 in vcgroup.expressions.connector
    vc2.address in connector2.voice_connector_id.references
    
    not connector1 == connector2
    
    connector1.priority.constant_value == 3
    connector2.priority.constant_value == 1
}",3,"resource ""aws_chime_voice_connector"" ""vc1"" {
  name               = ""connector-test-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""vc2"" {
  name               = ""connector-test-2""
  require_encryption = true
  aws_region         = ""us-west-2""
}

resource ""aws_chime_voice_connector_group"" ""group"" {
  name = ""test-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.vc1.id
    priority           = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.vc2.id
    priority           = 3
  }
}","Has one resource ""aws_chime_voice_connector""
with require_encryption=""true""

Has another resource ""aws_chime_voice_connector""
with require_encryption=""true""

Has one resource ""aws_chime_voice_connector_group""
with a connector for each voice connector with the correct id
one connector should have priority=1 and the other priority=3","I'll help you create a Terraform configuration for two AWS Chime Voice Connectors with encryption and group them together with different priorities. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Ensure you're using a region that supports Chime Voice Connectors
}

# First Voice Connector
resource ""aws_chime_voice_connector"" ""connector_1"" {
  name               = ""voice-connector-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

# Second Voice Connector
resource ""aws_chime_voice_connector"" ""connector_2"" {
  name               = ""voice-connector-2""
  require_encryption = true
  aws_region         = ""us-east-1""
}

# Voice Connector Group
resource ""aws_chime_voice_connector_group"" ""connector_group"" {
  name = ""voice-connector-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_2.id
    priority          = 3
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two voice connectors with encryption enabled
3. Creates a voice connector group that includes both connectors with their specified priorities (1 and 3)
4. Both voice connectors are configured with encryption as required
5. The voice connector group references both connectors using their IDs and assigns the requested priorities

The voice connectors will be created first, and then the group will be created with references to both connectors.",,,,,,"I'll help you create a Terraform configuration for two AWS Chime Voice Connectors with encryption and group them together in a voice connector group with different priorities.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Note: Chime Voice Connector is only available in specific regions
}

# First Voice Connector
resource ""aws_chime_voice_connector"" ""connector_1"" {
  name               = ""voice-connector-1""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Second Voice Connector
resource ""aws_chime_voice_connector"" ""connector_2"" {
  name               = ""voice-connector-2""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Voice Connector Group
resource ""aws_chime_voice_connector_group"" ""connector_group"" {
  name = ""voice-connector-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_2.id
    priority          = 3
  }
}
```

This configuration:
1. Creates two voice connectors with encryption enabled
2. Creates a voice connector group that includes both connectors
3. Sets priority 1 for the first connector and priority 3 for the second connector
4. Uses proper resource references to link the connectors to the group

The voice connectors are both configured with encryption required, and they're grouped together with different priorities as requested. The group can be used for routing and failover purposes in your AWS Chime implementation.",,,,,,"I'll help you create a Terraform configuration for two AWS Chime Voice Connectors with encryption and group them together in a voice connector group with different priorities.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # AWS Chime Voice Connector is only available in us-east-1 and us-west-2
}

# First Voice Connector
resource ""aws_chime_voice_connector"" ""connector_1"" {
  name               = ""voice-connector-1""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Second Voice Connector
resource ""aws_chime_voice_connector"" ""connector_2"" {
  name               = ""voice-connector-2""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Voice Connector Group
resource ""aws_chime_voice_connector_group"" ""connector_group"" {
  name = ""voice-connector-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_2.id
    priority          = 3
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two voice connectors (connector_1 and connector_2) with encryption enabled
3. Creates a voice connector group that includes both connectors with their respective priorities (1 and 3)
4. Both voice connectors have require_encryption set to true
5. The voice connector group references both connectors using their IDs and assigns the requested priorities

Note that AWS Chime Voice Connector is only available in us-east-1 and us-west-2 regions, so make sure to deploy in one of these regions.",,,,,
"aws_efs_file_system, aws_efs_file_system_policy, aws_iam_policy_document",create a AWS EFS with a system policy,"package terraform.validation

default is_valid_efs_setup = false

# Rule to check if a valid aws_efs_file_system and aws_efs_file_system_policy exists
is_valid_efs_setup {
    has_valid_efs_file_system
    has_valid_efs_file_system_policy
}

# Helper rule to check if a valid aws_efs_file_system exists
has_valid_efs_file_system {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_file_system""
}

# Helper rule to check if a valid aws_efs_file_system_policy exists with required arguments
has_valid_efs_file_system_policy {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_efs_file_system_policy""
    resource.expressions.file_system_id
    resource.expressions.policy
}
",3,"resource ""aws_efs_file_system"" ""fs"" {
  creation_token = ""my-product""
}

data ""aws_iam_policy_document"" ""policy"" {
  statement {
    sid    = ""ExampleStatement01""
    effect = ""Allow""

    principals {
      type        = ""AWS""
      identifiers = [""*""]
    }

    actions = [
      ""elasticfilesystem:ClientMount"",
      ""elasticfilesystem:ClientWrite"",
    ]

    resources = [aws_efs_file_system.fs.arn]

    condition {
      test     = ""Bool""
      variable = ""aws:SecureTransport""
      values   = [""true""]
    }
  }
}

resource ""aws_efs_file_system_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.fs.id
  policy         = data.aws_iam_policy_document.policy.json
}","Have one ""aws_efs_file_system"" resource

Have one ""aws_efs_file_system_policy"" resource
with one ""file_system_id""
with one ""policy""","I'll help you create an AWS EFS (Elastic File System) with a system policy using Terraform. Here's a code solution that includes the required provider, EFS creation, and the associated system policy:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create the EFS file system
resource ""aws_efs_file_system"" ""example"" {
  creation_token = ""my-efs""
  encrypted      = true

  tags = {
    Name = ""MyEFS""
  }
}

# Create EFS system policy
resource ""aws_efs_file_system_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.example.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""AllowAccessToEFS""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""elasticfilesystem:ClientMount"",
          ""elasticfilesystem:ClientWrite""
        ]
        Resource = aws_efs_file_system.example.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"" = ""true""
          }
        }
      }
    ]
  })
}

# Optional: Create mount targets in your VPC
resource ""aws_efs_mount_target"" ""example"" {
  file_system_id  = aws_efs_file_system.example.id
  subnet_id       = ""subnet-xxxxxx"" # Replace with your subnet ID
  security_groups = [""sg-xxxxxx""]   # Replace with your security group ID
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an EFS file system with encryption enabled
3. Attaches a system policy that:
   - Allows client mount and write operations
   - Requires secure transport (TLS)
   - Can be accessed by AWS principals (you may want to restrict this further)
4. Includes an optional mount target configuration (you'll need to specify your subnet and security group IDs)

Remember to replace the subnet_id and security_groups values with your actual VPC subnet and security group IDs.

The system policy can be customized further based on your specific security requirements by modifying the IAM policy statement in the `aws_efs_file_system_policy` resource.",,,,,,"I'll help you create an AWS EFS (Elastic File System) with a system policy using Terraform. Here's a complete solution that includes the required provider and creates an EFS with appropriate system policies:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# EFS File System
resource ""aws_efs_file_system"" ""efs"" {
  creation_token = ""my-efs""
  encrypted      = true

  tags = {
    Name = ""MyEFS""
  }

  lifecycle_policy {
    transition_to_ia = ""AFTER_30_DAYS""
  }
}

# EFS Policy
resource ""aws_efs_file_system_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.efs.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""AllowEFSAccess""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""elasticfilesystem:ClientMount"",
          ""elasticfilesystem:ClientWrite""
        ]
        Resource = aws_efs_file_system.efs.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"" = ""true""
          }
        }
      }
    ]
  })
}

# EFS Mount Target (you'll need to specify your VPC and subnet)
resource ""aws_efs_mount_target"" ""mount"" {
  file_system_id  = aws_efs_file_system.efs.id
  subnet_id       = ""subnet-xxxxxxxx"" # Replace with your subnet ID
  security_groups = [""sg-xxxxxxxx""]   # Replace with your security group ID
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an EFS file system with encryption enabled
3. Implements a lifecycle policy to transition files to Infrequent Access after 30 days
4. Creates a system policy that:
   - Allows client mount and write operations
   - Requires secure transport (HTTPS)
   - Can be accessed by AWS principals
5. Creates a mount target (you'll need to specify your VPC subnet and security group)

Remember to replace the subnet_id and security_groups with your actual values. You may also want to adjust the policy according to your specific security requirements.",,,,,,"I'll help you create an AWS EFS (Elastic File System) with a system policy using Terraform. Here's a complete solution that includes the required provider and creates an EFS with an associated system policy:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create EFS File System
resource ""aws_efs_file_system"" ""efs"" {
  creation_token = ""my-efs""
  encrypted      = true

  tags = {
    Name = ""MyEFS""
  }
}

# Create EFS System Policy
resource ""aws_efs_file_system_policy"" ""policy"" {
  file_system_id = aws_efs_file_system.efs.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""AllowRootAccess""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action = [
          ""elasticfilesystem:ClientMount"",
          ""elasticfilesystem:ClientWrite""
        ]
        Resource = aws_efs_file_system.efs.arn
        Condition = {
          Bool = {
            ""aws:SecureTransport"" = ""true""
          }
        }
      }
    ]
  })
}

# Create Mount Target (optional, but commonly needed)
resource ""aws_efs_mount_target"" ""mount"" {
  file_system_id = aws_efs_file_system.efs.id
  subnet_id      = ""subnet-xxxxxx""  # Replace with your subnet ID
  
  security_groups = [""sg-xxxxxx""]   # Replace with your security group ID
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an EFS file system with encryption enabled
3. Attaches a system policy that allows root access through secure transport
4. Creates a mount target (you'll need to specify your subnet ID and security group)

The system policy allows:
- Client mount and write operations
- Only through secure transport (TLS)
- Access to the specific EFS resource

Remember to:
1. Replace the subnet ID and security group ID with your actual values
2. Adjust the region as needed
3. Modify the policy according to your specific security requirements
4. Update the tags as needed for your environment

The policy can be customized further based on your specific access requirements and security needs.",,,,,
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration","create a bucket 'pike-680235478471'. Set up request payment configuration for an AWS S3 bucket named 'pike-680235478471', specifying the payer as 'Requester'.","package terraform.validation

default has_s3_bucket = false

default has_s3_bucket_request_payment_configuration = false

default valid_configuration = false

has_s3_bucket {
    some i
    bucket := input.planned_values.root_module.resources[i]
    bucket.type == ""aws_s3_bucket""
    bucket.values.bucket == ""pike-680235478471""
}

has_s3_bucket_request_payment_configuration {
    some i
    payment_config := input.planned_values.root_module.resources[i]
    payment_config.type == ""aws_s3_bucket_request_payment_configuration""
    payment_config.values.bucket == ""pike-680235478471""
    payment_config.values.payer == ""Requester""
}

valid_configuration {
    has_s3_bucket
    has_s3_bucket_request_payment_configuration
}
",2,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""a"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""pike"" {
  bucket = ""pike-680235478471""
  payer  = ""Requester""
}","has one aws_s3_bucket resource
with bucket

has one aws_s3_bucket_request_payment_configuration resource
with bucket
with payer (BucketOwner or Requester)","I'll help you create a Terraform configuration for an S3 bucket with request payment configuration set to 'Requester'. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""Requester""
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates an S3 bucket named ""pike-680235478471""
3. Sets up the request payment configuration for the bucket, making the requester pay for requests and data transfer costs

The `aws_s3_bucket_request_payment_configuration` resource explicitly sets the payer to ""Requester"" instead of the default ""BucketOwner"" setting.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with request payment configuration set to 'Requester'. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""Requester""
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates an S3 bucket named ""pike-680235478471""
3. Sets up the request payment configuration for the bucket, specifying the payer as ""Requester""

The `aws_s3_bucket_request_payment_configuration` resource configures who pays for the data transfer and request costs: either the bucket owner or the requester. In this case, it's set to ""Requester"".",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with request payment configuration set to 'Requester'. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""pike_bucket"" {
  bucket = ""pike-680235478471""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""payment_config"" {
  bucket = aws_s3_bucket.pike_bucket.id
  payer  = ""Requester""
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates an S3 bucket named ""pike-680235478471""
3. Sets up the request payment configuration for the bucket, specifying the payer as ""Requester""

The `aws_s3_bucket_request_payment_configuration` resource configures who pays for the data transfer and request costs: either the bucket owner (""BucketOwner"") or the person requesting the download (""Requester""). In this case, it's set to ""Requester"" as specified in your requirements.",,,,,
"aws_vpc, aws_vpc, aws_vpc_peering_connection","establishes a VPC peering connection between two AWS Virtual Private Clouds (VPCs). The first VPC, named ""peer,"" is configured with a CIDR block of 10.0.0.0/24, and the second VPC, named ""base,"" has a CIDR block of 10.1.0.0/24. The peering connection, named ""pike,"" connects these two VPCs, allowing them to communicate with each other as if they were in the same network, and is tagged with pike = ""permissions"" to indicate its purpose or management policy.","package terraform.validation

# Set default validation states
default is_valid_vpcs = false
default is_valid_vpc_peering_connection = false

# Validate aws_vpc resources
is_valid_vpcs {
    # Validate the first VPC named ""peer""
    peer_vpc := input.configuration.root_module.resources[_]
    peer_vpc.type == ""aws_vpc""
    peer_vpc.name == ""peer""
    peer_vpc.expressions.cidr_block.constant_value == ""10.0.0.0/24""

    # Validate the second VPC named ""base""
    base_vpc := input.configuration.root_module.resources[_]
    base_vpc.type == ""aws_vpc""
    base_vpc.name == ""base""
    base_vpc.expressions.cidr_block.constant_value == ""10.1.0.0/24""

    # Ensure different VPCs
    peer_vpc != base_vpc
}

# Validate aws_vpc_peering_connection resource
is_valid_vpc_peering_connection {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_vpc_peering_connection""
    resource.name == ""pike""
    # Ensure connection between ""peer"" and ""base"" VPCs
    resource.expressions.peer_vpc_id.references[0] == ""aws_vpc.peer.id""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.base.id""
    # Check for the specific tag indicating its purpose
    resource.expressions.tags.constant_value[""pike""] == ""permissions""
}",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id

  tags = {
    pike = ""permissions""
  }
}

resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
}

resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
}","Has two ""aws_vpc"" resources:
The first VPC is named ""peer"" and is configured with a ""cidr_block"" of ""10.0.0.0/24"".
The second VPC is named ""base"" and is configured with a ""cidr_block"" of ""10.1.0.0/24"".

Has one ""aws_vpc_peering_connection"" resource:
Named ""pike"" to connect the two specified VPCs (""peer"" and ""base"").
Configured to allow the VPCs to communicate with each other as if they were in the same network.
Tagged with ""pike"" set to ""permissions"", indicating its purpose or management policy within the infrastructure.","I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Update with your desired region
}

# Create the peer VPC
resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
  
  tags = {
    Name = ""peer-vpc""
  }
}

# Create the base VPC
resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
  
  tags = {
    Name = ""base-vpc""
  }
}

# Create VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    pike = ""permissions""
    Name = ""pike-peering-connection""
  }
}

# Update route tables for peer VPC
resource ""aws_route"" ""peer_to_base"" {
  route_table_id            = aws_vpc.peer.main_route_table_id
  destination_cidr_block    = aws_vpc.base.cidr_block
  vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
}

# Update route tables for base VPC
resource ""aws_route"" ""base_to_peer"" {
  route_table_id            = aws_vpc.base.main_route_table_id
  destination_cidr_block    = aws_vpc.peer.cidr_block
  vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two VPCs (""peer"" and ""base"") with the specified CIDR blocks
3. Establishes a VPC peering connection named ""pike"" between them
4. Creates the necessary route table entries to enable communication between the VPCs
5. Includes appropriate tagging for the peering connection

The `auto_accept = true` parameter automatically accepts the peering connection request since both VPCs are in the same AWS account. If the VPCs were in different accounts, you would need to handle the acceptance separately.

The route table entries ensure that traffic can flow between the VPCs through the peering connection. Each VPC needs a route to the other VPC's CIDR block via the peering connection.",,,,,,"I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create the peer VPC
resource ""aws_vpc"" ""peer"" {
  cidr_block           = ""10.0.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""peer-vpc""
  }
}

# Create the base VPC
resource ""aws_vpc"" ""base"" {
  cidr_block           = ""10.1.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""base-vpc""
  }
}

# Create VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    pike = ""permissions""
    Name = ""pike-peering-connection""
  }
}

# Add route table entries for peer VPC
resource ""aws_route_table"" ""peer_route_table"" {
  vpc_id = aws_vpc.peer.id

  route {
    cidr_block                = aws_vpc.base.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""peer-route-table""
  }
}

# Add route table entries for base VPC
resource ""aws_route_table"" ""base_route_table"" {
  vpc_id = aws_vpc.base.id

  route {
    cidr_block                = aws_vpc.peer.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""base-route-table""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two VPCs (""peer"" and ""base"") with specified CIDR blocks
3. Establishes a VPC peering connection named ""pike"" between the VPCs
4. Creates route tables for both VPCs to enable traffic flow between them
5. Adds appropriate tags including the requested ""pike = permissions"" tag
6. Enables DNS support and hostnames for both VPCs
7. Configures auto-accept for the peering connection

The configuration includes all necessary components for a functioning VPC peering setup. Make sure to adjust the region in the provider block according to your needs.",,,,,,"I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # You can change this to your desired region
}

# Create the peer VPC
resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
  
  tags = {
    Name = ""peer-vpc""
  }
}

# Create the base VPC
resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
  
  tags = {
    Name = ""base-vpc""
  }
}

# Create VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    pike = ""permissions""
    Name = ""pike-peering-connection""
  }
}

# Add route table entries for peer VPC
resource ""aws_route_table"" ""peer_route_table"" {
  vpc_id = aws_vpc.peer.id

  route {
    cidr_block                = aws_vpc.base.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""peer-route-table""
  }
}

# Add route table entries for base VPC
resource ""aws_route_table"" ""base_route_table"" {
  vpc_id = aws_vpc.base.id

  route {
    cidr_block                = aws_vpc.peer.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""base-route-table""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two VPCs (""peer"" and ""base"") with their specified CIDR blocks
3. Establishes a VPC peering connection named ""pike"" between them
4. Creates route tables for both VPCs to enable traffic flow
5. Includes appropriate tags for identification and management

The peering connection is auto-accepted (assuming both VPCs are in the same account), and the necessary routes are added to allow traffic between the VPCs. Make sure to adjust the region in the provider block according to your needs.",,,,,
"aws_iam_role, aws_iam_role_policy, aws_kendra_index",Generate a basic Kendra index resource with the default Document Metadata Configuration Updates,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
    some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kendra.amazonaws.com"") 
}

has_valid_kendra_index {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_index""
    resource.values.name
    resource.values.document_metadata_configuration_updates
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
}


has_valid_resources {
    has_valid_iam_role(input.resource_changes)
    has_valid_kendra_index
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""role"" {
  name = ""Kendra-Role""
  assume_role_policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": ""sts:AssumeRole"",
      ""Principal"": {
        ""Service"": ""kendra.amazonaws.com""
      },
      ""Effect"": ""Allow"",
      ""Sid"": """"
    }
  ]
}
EOF
}

resource ""aws_iam_role_policy"" ""policy"" {
  name = ""Kendra-Policy""
  role = aws_iam_role.role.id

  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:GetObject"",
        ""s3:ListBucket""
      ],
      ""Resource"": ""*""
    }
  ]
}
EOF
}

resource ""aws_kendra_index"" ""example"" {
  name     = ""example""
  role_arn = aws_iam_role.role.arn

  document_metadata_configuration_updates {
    name = ""_authors""
    type = ""STRING_LIST_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = false
    }
    relevance {
      importance = 1
    }
  }

  document_metadata_configuration_updates {
    name = ""_category""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_created_at""
    type = ""DATE_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      freshness  = false
      importance = 1
      duration   = ""25920000s""
      rank_order = ""ASCENDING""
    }
  }

  document_metadata_configuration_updates {
    name = ""_data_source_id""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_document_title""
    type = ""STRING_VALUE""
    search {
      displayable = true
      facetable   = false
      searchable  = true
      sortable    = true
    }
    relevance {
      importance            = 2
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_excerpt_page_number""
    type = ""LONG_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = false
    }
    relevance {
      importance = 2
      rank_order = ""ASCENDING""
    }
  }

  document_metadata_configuration_updates {
    name = ""_faq_id""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_file_type""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_language_code""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_last_updated_at""
    type = ""DATE_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      freshness  = false
      importance = 1
      duration   = ""25920000s""
      rank_order = ""ASCENDING""
    }
  }

  document_metadata_configuration_updates {
    name = ""_source_uri""
    type = ""STRING_VALUE""
    search {
      displayable = true
      facetable   = false
      searchable  = false
      sortable    = false
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_tenant_id""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_version""
    type = ""STRING_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance            = 1
      values_importance_map = {}
    }
  }

  document_metadata_configuration_updates {
    name = ""_view_count""
    type = ""LONG_VALUE""
    search {
      displayable = false
      facetable   = false
      searchable  = false
      sortable    = true
    }
    relevance {
      importance = 1
      rank_order = ""ASCENDING""
    }
  }
}","Has one ""aws_iam_role"" resource and one ""aws_kendra_index"" resource with all default ""document_metadata_configuration_updates"" set","I'll help you create a Terraform configuration for an AWS Kendra index resource with default document metadata configuration updates. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra Index with default document metadata configuration""
  role_arn    = aws_iam_role.kendra_role.arn

  document_metadata_configuration_updates {
    name = ""_created_at""
    type = ""DATE_VALUE""
    relevance {
      freshness = true
    }
  }

  document_metadata_configuration_updates {
    name = ""_last_updated_at""
    type = ""DATE_VALUE""
    relevance {
      freshness = true
    }
  }

  document_metadata_configuration_updates {
    name = ""_source_uri""
    type = ""STRING_VALUE""
    relevance {
      importance = 1
    }
  }

  tags = {
    Environment = ""production""
    Name        = ""example-kendra-index""
  }
}

# IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra_index_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""kendra_policy"" {
  role       = aws_iam_role.kendra_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSKendraServiceRole""
}
```

This configuration creates:
1. A Kendra index with default document metadata configurations
2. The required IAM role and policy attachments for Kendra to function
3. Three default document metadata configuration updates for created_at, last_updated_at, and source_uri fields

Key features of this configuration:
- The index includes basic metadata configurations that are commonly used
- Proper IAM roles and permissions are set up
- Tags are included for better resource management
- The configuration follows AWS best practices for Kendra setup

Remember to:
- Adjust the region in the provider block to match your requirements
- Modify the tags and names according to your naming conventions
- Ensure you have the necessary permissions to create these resources
- Consider any additional metadata configurations specific to your use case",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra index resource with default document metadata configuration updates. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra Index with default document metadata configuration""
  role_arn    = aws_iam_role.kendra_role.arn # You'll need to create this IAM role separately

  document_metadata_configuration_updates {
    name = ""document_title""
    type = ""STRING_VALUE""
    relevance {
      importance = ""HIGH""
    }
    search {
      displayable = true
      facetable  = false
      searchable = true
      sortable   = true
    }
  }

  server_side_encryption_configuration {
    kms_key_id = ""aws/kendra"" # Uses AWS managed key by default
  }

  tags = {
    Environment = ""production""
    Name        = ""example-kendra-index""
  }
}

# Note: You'll need to ensure proper IAM roles and permissions are set up
# The following IAM role is required for Kendra to function properly
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra_index_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""kendra_policy"" {
  role       = aws_iam_role.kendra_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSKendraFullAccess""
}
```

This configuration creates:
1. A Kendra index with a basic document metadata configuration for the document title
2. Default server-side encryption using AWS managed KMS key
3. The necessary IAM role and policy attachment for Kendra to function
4. Basic tagging for resource management

The document metadata configuration is set up with:
- A STRING_VALUE type field for document_title
- HIGH importance for relevance
- Standard search configuration with displayable, searchable, and sortable options enabled

Remember that:
- Kendra indexes can take several minutes to create
- There are costs associated with running a Kendra index
- You may need to adjust the IAM permissions based on your specific use case
- Additional document metadata configurations can be added as needed",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra index resource with default document metadata configuration updates. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra Index with default document metadata configuration""
  role_arn    = aws_iam_role.kendra_role.arn

  document_metadata_configuration_updates {
    name          = ""example-metadata""
    type          = ""STRING_VALUE""
    relevance {
      importance = 1
    }
    search {
      displayable = true
      facetable   = true
      searchable  = true
      sortable    = true
    }
  }

  edition = ""ENTERPRISE_EDITION""
  
  tags = {
    Name = ""example-kendra-index""
    Environment = ""production""
  }
}

# IAM role for Kendra
resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""kendra_policy"" {
  role       = aws_iam_role.kendra_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSKendraFullAccess""
}
```

This configuration creates:
1. A Kendra index with a basic document metadata configuration
2. The required IAM role and policy attachment for Kendra to function
3. Uses the Enterprise Edition of Kendra
4. Includes basic tags for resource management

The document metadata configuration allows for:
- String value type metadata
- Basic relevance settings
- Search capabilities including displayable, facetable, searchable, and sortable options

Note that you might want to adjust the IAM roles and policies according to your specific security requirements and use case.",,,,,
"aws_kms_key, aws_s3_bucket, aws_s3_bucket_server_side_encryption_configuration",Create an AWS S3 bucket and apply KMS server side encryption that uses a defined KMS key resource.,"package aws_s3_bucket_sse
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some kms_key in input.configuration.root_module.resources
        kms_key.type == ""aws_kms_key""

        some sse in input.configuration.root_module.resources
        sse.type = ""aws_s3_bucket_server_side_encryption_configuration""
        bucket.address in sse.expressions.bucket.references
        some rule in sse.expressions.rule
        some rule_args in rule.apply_server_side_encryption_by_default
        kms_key.address in rule_args.kms_master_key_id.references
    rule_args.sse_algorithm.constant_value == ""aws:kms""
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some kms_key in input.configuration.root_module.resources
        kms_key.type == ""aws_kms_key""

        some sse in input.configuration.root_module.resources
        sse.type = ""aws_s3_bucket_server_side_encryption_configuration""
        bucket.expressions.bucket.constant_value == sse.expressions.bucket.constant_value
        some rule in sse.expressions.rule
        some rule_args in rule.apply_server_side_encryption_by_default
        kms_key.address in rule_args.kms_master_key_id.references
    rule_args.sse_algorithm.constant_value == ""aws:kms""
    
}",2,"resource ""aws_kms_key"" ""mykey"" {
  description             = ""This key is used to encrypt bucket objects""
  deletion_window_in_days = 10
}

resource ""aws_s3_bucket"" ""mybucket"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_server_side_encryption_configuration"" ""example"" {
  bucket = aws_s3_bucket.mybucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.mykey.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}","Has one resource ""aws_s3_bucket""

Has one resource ""aws_kms_key""

Has one resource ""aws_s3_bucket_server_side_encryption""
with bucket = bucket id OR name
with rule
    with apply_server_side_encryption_by_default
        with kms_master_key_id = kms key id
        and sse_algorithm = ""aws:kms""","I'll help you create a Terraform configuration for an S3 bucket with KMS server-side encryption using a defined KMS key resource. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# KMS key for S3 bucket encryption
resource ""aws_kms_key"" ""s3_key"" {
  description             = ""KMS key for S3 bucket encryption""
  deletion_window_in_days = 7
  enable_key_rotation    = true
  
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid    = ""Enable IAM User Permissions""
        Effect = ""Allow""
        Principal = {
          AWS = ""*""
        }
        Action   = ""kms:*""
        Resource = ""*""
      }
    ]
  })
}

# KMS key alias
resource ""aws_kms_alias"" ""s3_key_alias"" {
  name          = ""alias/s3-encryption-key""
  target_key_id = aws_kms_key.s3_key.key_id
}

# S3 bucket with KMS encryption
resource ""aws_s3_bucket"" ""encrypted_bucket"" {
  bucket = ""my-encrypted-bucket-name""
}

# S3 bucket server-side encryption configuration
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""bucket_encryption"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.s3_key.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}

# S3 bucket versioning
resource ""aws_s3_bucket_versioning"" ""bucket_versioning"" {
  bucket = aws_s3_bucket.encrypted_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# S3 bucket public access block
resource ""aws_s3_bucket_public_access_block"" ""bucket_public_access_block"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
```

This configuration:
1. Creates a KMS key with a basic policy and key rotation enabled
2. Creates an alias for the KMS key
3. Creates an S3 bucket
4. Configures server-side encryption using the created KMS key
5. Enables versioning on the bucket
6. Blocks all public access to the bucket

Make sure to:
- Replace ""my-encrypted-bucket-name"" with your desired bucket name
- Adjust the KMS key policy according to your security requirements
- Consider adding additional S3 bucket configurations like lifecycle rules or logging as needed

The bucket will be encrypted using the specified KMS key, and all objects uploaded to the bucket will be automatically encrypted using this key.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with KMS server-side encryption using a defined KMS key resource. Here's the solution:

```hcl
# Configure AWS Provider
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create KMS key
resource ""aws_kms_key"" ""s3_key"" {
  description             = ""KMS key for S3 bucket encryption""
  deletion_window_in_days = 7
  enable_key_rotation    = true
  
  tags = {
    Environment = ""Production""
    Purpose     = ""S3-Encryption""
  }
}

# Create KMS key alias
resource ""aws_kms_alias"" ""s3_key_alias"" {
  name          = ""alias/s3-encryption-key""
  target_key_id = aws_kms_key.s3_key.key_id
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""encrypted_bucket"" {
  bucket = ""my-encrypted-bucket-name""
}

# Enable versioning
resource ""aws_s3_bucket_versioning"" ""versioning"" {
  bucket = aws_s3_bucket.encrypted_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Configure server-side encryption
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""encryption"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.s3_key.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}

# Block public access
resource ""aws_s3_bucket_public_access_block"" ""block_public_access"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
```

This configuration:
1. Creates a KMS key with key rotation enabled
2. Creates a friendly alias for the KMS key
3. Creates an S3 bucket
4. Enables versioning on the bucket
5. Configures server-side encryption using the created KMS key
6. Blocks all public access to the bucket as a security best practice

The bucket will use the specified KMS key for encryption of all objects stored within it. Remember to replace ""my-encrypted-bucket-name"" with your desired bucket name, as S3 bucket names must be globally unique.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with KMS server-side encryption using a defined KMS key resource. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create KMS key for S3 bucket encryption
resource ""aws_kms_key"" ""s3_key"" {
  description             = ""KMS key for S3 bucket encryption""
  deletion_window_in_days = 7
  enable_key_rotation    = true
  
  tags = {
    Name = ""s3-bucket-key""
  }
}

# Create KMS key alias
resource ""aws_kms_alias"" ""s3_key_alias"" {
  name          = ""alias/s3-bucket-key""
  target_key_id = aws_kms_key.s3_key.key_id
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""encrypted_bucket"" {
  bucket = ""my-encrypted-bucket-name""
}

# Enable server-side encryption for the bucket
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""bucket_encryption"" {
  bucket = aws_s3_bucket.encrypted_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      kms_master_key_id = aws_kms_key.s3_key.arn
      sse_algorithm     = ""aws:kms""
    }
  }
}

# Enable versioning for the bucket
resource ""aws_s3_bucket_versioning"" ""bucket_versioning"" {
  bucket = aws_s3_bucket.encrypted_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}
```

This configuration:
1. Creates a KMS key specifically for S3 bucket encryption
2. Creates an alias for the KMS key for easier reference
3. Creates an S3 bucket
4. Configures server-side encryption using the created KMS key
5. Enables versioning on the bucket for additional data protection

Make sure to replace ""my-encrypted-bucket-name"" with your desired bucket name. Remember that S3 bucket names must be globally unique across all AWS accounts.

The encryption configuration uses the aws:kms algorithm with the specified KMS key for server-side encryption of all objects stored in the bucket.",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_route53_record, aws_route53_zone, aws_s3_bucket, aws_security_group, aws_subnet, aws_subnet, aws_vpc, aws_ami","The infrastructure for a video streaming site that has multiple servers to balance load. Accomplish this using the following resources: AWS Load Balancing, AWS Load Balancing target group, AWS Route53 zone, AWS Route53 record, and AWS VPC module, AWS VPC subnet, AWS VPC security group, AWS EC2 instance. Add any necessary resources.","package load_balancer_middle

import rego.v1

default valid := false

instance_valid(instance, ami) if {
	instance.expressions.instance_type
	ami.address in instance.expressions.ami.references
}

lb_valid(lb, security_group, subnets) if {
	every subnet in subnets {
		subnet.address in lb.expressions.subnets.references
	}
	security_group.address in lb.expressions.security_groups.references
}

lb_listener_valid(lb_listener, lb, target_group) if {
	some default_action in lb_listener.expressions.default_action
	target_group.address in default_action.target_group_arn.references
	default_action.type.constant_value == ""forward""
	lb.address in lb_listener.expressions.load_balancer_arn.references
}

lb_target_group_valid(target_group, vpc) if {
	vpc.address in target_group.expressions.vpc_id.references
	target_group.expressions.port
	target_group.expressions.protocol
}

lb_target_group_attachment_valid(target_group_attachment, target_group, instance) if {
	target_group.address in target_group_attachment.expressions.target_group_arn.references
	instance.address in target_group_attachment.expressions.target_id.references
}

lb_resources_valid(lb, lb_listener, target_group, target_group_attachment, instance, vpc, subnets, security_group) if {
	lb_valid(lb, security_group, subnets)
	lb_listener_valid(lb_listener, lb, target_group)
	lb_target_group_valid(target_group, vpc)
	lb_target_group_attachment_valid(target_group_attachment, target_group, instance)
}

route53_record_valid(record, zone, lb) if {
	zone.address in record.expressions.zone_id.references
	record.expressions.name
	record.expressions.type
	some alias in record.expressions.alias
	lb.address in alias.zone_id.references
	lb.address in alias.name.references
	alias.evaluate_target_health
}

route53_records_valid(records, zone, lb) if {
	some record_ipv4 in records
	route53_record_valid(record_ipv4, zone, lb)
}

route53_zone_valid(zone) if {
	zone.expressions.name
}

security_group_valid(security_group, vpc) if {
	vpc.address in security_group.expressions.vpc_id.references
}

subnet_valid(subnet, vpc) if {
	vpc.address in subnet.expressions.vpc_id.references
	subnet.expressions.cidr_block
}

subnets_valid(subnets, vpc) if {
	every subnet in subnets {
		subnet_valid(subnet, vpc)
	}
}

vpc_valid(vpc) if {
	vpc.expressions.cidr_block
}

valid if {
	resources := input.configuration.root_module.resources

	# ec2
	some instance in resources
	instance.type == ""aws_instance""
	some ami in resources
	ami.type == ""aws_ami""

	some vpc in resources
	vpc.type == ""aws_vpc""

	subnets := [subnet | subnet := resources[_]; subnet.type == ""aws_subnet""]
	count(subnets) > 1
	some security_group in resources
	security_group.type == ""aws_security_group""

	# lb resources
	some lb in resources
	lb.type == ""aws_lb""
	some lb_listener in resources
	lb_listener.type == ""aws_lb_listener""
	some target_group in resources
	target_group.type == ""aws_lb_target_group""
	some target_group_attachment in resources
	target_group_attachment.type == ""aws_lb_target_group_attachment""

	# route53
	records := [record | record := resources[_]; record.type == ""aws_route53_record""]
	some zone in resources
	zone.type == ""aws_route53_zone""

	# s3
	some s3_bucket in resources
	s3_bucket.type == ""aws_s3_bucket""

	instance_valid(instance, ami)
	lb_resources_valid(lb, lb_listener, target_group, target_group_attachment, instance, vpc, subnets, security_group)
	route53_records_valid(records, zone, lb)
	route53_zone_valid(zone)

	security_group_valid(security_group, vpc)

	subnets_valid(subnets, vpc)
	vpc_valid(vpc)
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_launch_configuration"" ""launch-config"" {
  name_prefix     = ""aws-asg-launch-config-""
  image_id        = data.aws_ami.ubuntu.id
  instance_type   = ""t2.micro""
  # user_data       = file(""user-data.sh"")  # load your script if needed
  security_groups = [aws_security_group.instance-sg.id]

  lifecycle {
    create_before_destroy = true
  }
}

resource ""aws_autoscaling_group"" ""asg"" {
  name                 = ""asg""
  min_size             = 1
  max_size             = 3
  desired_capacity     = 1
  launch_configuration = aws_launch_configuration.launch-config.name
  vpc_zone_identifier  = module.vpc.public_subnets

  lifecycle { 
    ignore_changes = [desired_capacity, target_group_arns]
  }

  health_check_type    = ""ELB""
}

resource ""aws_autoscaling_policy"" ""scale-down"" {
  name                   = ""scale-down""
  autoscaling_group_name = aws_autoscaling_group.asg.name
  adjustment_type        = ""ChangeInCapacity""
  scaling_adjustment     = -1
  cooldown               = 120
}

resource ""aws_cloudwatch_metric_alarm"" ""scale-down"" {
  alarm_description   = ""Monitors CPU utilization for Terramino ASG""
  alarm_actions       = [aws_autoscaling_policy.scale-down.arn]
  alarm_name          = ""scale-down""
  comparison_operator = ""LessThanOrEqualToThreshold""
  namespace           = ""AWS/EC2""
  metric_name         = ""CPUUtilization""
  threshold           = ""10""
  evaluation_periods  = ""2""
  period              = ""120""
  statistic           = ""Average""

  dimensions = {
    AutoScalingGroupName = aws_autoscaling_group.asg.name
  }
}

resource ""aws_autoscaling_policy"" ""scale-up"" {
  name                   = ""scale-up""
  autoscaling_group_name = aws_autoscaling_group.asg.name
  adjustment_type        = ""ChangeInCapacity""
  scaling_adjustment     = 1
  cooldown               = 120
}

resource ""aws_cloudwatch_metric_alarm"" ""scale-up"" {
  alarm_description   = ""Monitors CPU utilization for Terramino ASG""
  alarm_actions       = [aws_autoscaling_policy.scale-up.arn]
  alarm_name          = ""scale-up""
  comparison_operator = ""GreaterThanOrEqualToThreshold""
  namespace           = ""AWS/EC2""
  metric_name         = ""CPUUtilization""
  threshold           = ""80""
  evaluation_periods  = ""2""
  period              = ""120""
  statistic           = ""Average""

  dimensions = {
    AutoScalingGroupName = aws_autoscaling_group.asg.name
  }
}

resource ""aws_lb"" ""lb"" {
  name               = ""my-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
  subnets            = module.vpc.public_subnets
}

resource ""aws_lb_listener"" ""lb-listener"" {
  load_balancer_arn = aws_lb.lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target-group.arn
  }
}

resource ""aws_lb_target_group"" ""target-group"" {
  name     = ""my-lb-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_autoscaling_attachment"" ""as-attachment"" {
  autoscaling_group_name = aws_autoscaling_group.asg.id
  lb_target_group_arn   = aws_lb_target_group.target-group.arn
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_route53_zone"" ""test-video-stream"" {
  name = ""test-video-stream.com""
}

resource ""aws_route53_record"" ""lb_ipv4"" {
  type    = ""A""
  name    = ""lb""
  zone_id = aws_route53_zone.test-video-stream.zone_id

  alias {
    name                   = aws_lb.lb.dns_name
    zone_id                = aws_lb.lb.zone_id
    evaluate_target_health = true
  }
}

output ""lb_endpoint"" {
  value = ""http://${aws_lb.lb.dns_name}""
}

output ""application_endpoint"" {
  value = ""http://${aws_lb.lb.dns_name}/index.php""
}

output ""asg_name"" {
  value = aws_autoscaling_group.asg.name
}
","have one ""aws_s3_bucket"" resource
have one ""aws_lb"" resource
  with ""subnets"" argument linking to aws_subnet resources
  with ""security_groups"" argument linking aws_security_group resource
have one ""aws_lb_listener"" resource
  with ""load_balancer_arn"" argument linking to the aws_lb resource
  with ""default_action"" argument block
    with ""type"" argument with value ""forward""
    with ""target_group_arn"" argument linking to aws_lb_target_group resource
with one ""aws_lb_target_group"" resource
  with ""vpc_id"" argument linking to the aws_vpc resource
with one ""aws_lb_target_group_attachment"" resource
  with ""target_group_arn"" argument linking to the aws_lb_target_group resource
  with ""target_id"" argument linking to the aws_instance resource
have one ""aws_ami"" data resource
have one ""aws_instance"" resource
  with ""instance_type"" argument
  with ""ami"" argument linking to the aws_ami data resource
have one ""aws_vpc"" resource
  with ""cidr_block"" argument
have two ""aws_subnet"" resources
  with ""vpc_id"" argument linking to the aws_vpc resource
  with ""cidr_block"" argument
have one ""aws_security_group"" resource
  with ""vpc_id"" argument linking to the aws_vpc resource
have one ""aws_route53_zone"" resource
  with ""name"" argument
have one ""aws_route53_record"" resources
  with ""type"" argument
  with ""name"" argument
  with ""zone_id"" argument linking to the aws_route53_zone resource
  with ""alias"" argument block
    with ""name"" argument linking to the aws_lb dns_name
    with ""zone_id"" argument linking to aws_lb zone_id
    with ""evaluate_target_health"" argument","I'll help you create a Terraform configuration for a video streaming site with load balancing. Here's a comprehensive solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""video-streaming-vpc""
  }
}

# Create public subnets
resource ""aws_subnet"" ""public"" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.${count.index + 1}.0/24""
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = ""public-subnet-${count.index + 1}""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""main-igw""
  }
}

# Route Table
resource ""aws_route_table"" ""public"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""public-rt""
  }
}

# Security Group
resource ""aws_security_group"" ""web"" {
  name        = ""web-sg""
  description = ""Security group for web servers""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# EC2 Instances
resource ""aws_instance"" ""web"" {
  count         = 2
  ami           = ""ami-0c55b159cbfafe1f0""
  instance_type = ""t2.micro""
  subnet_id     = aws_subnet.public[count.index].id

  vpc_security_group_ids = [aws_security_group.web.id]

  tags = {
    Name = ""web-server-${count.index + 1}""
  }
}

# Application Load Balancer
resource ""aws_lb"" ""web"" {
  name               = ""video-streaming-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.web.id]
  subnets            = aws_subnet.public[*].id

  tags = {
    Name = ""video-streaming-alb""
  }
}

# Target Group
resource ""aws_lb_target_group"" ""web"" {
  name     = ""video-streaming-tg""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = aws_vpc.main.id

  health_check {
    path                = ""/""
    healthy_threshold   = 2
    unhealthy_threshold = 10
  }
}

# Target Group Attachment
resource ""aws_lb_target_group_attachment"" ""web"" {
  count            = 2
  target_group_arn = aws_lb_target_group.web.arn
  target_id        = aws_instance.web[count.",,,,,,"I'll help create a Terraform configuration for a video streaming site with load balancing. This solution will include all the requested components with proper networking and security configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""streaming-vpc""
  }
}

# Create public subnets
resource ""aws_subnet"" ""public"" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.${count.index + 1}.0/24""
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = ""public-subnet-${count.index + 1}""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""main-igw""
  }
}

# Route Table
resource ""aws_route_table"" ""public"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""public-rt""
  }
}

# Security Group
resource ""aws_security_group"" ""web"" {
  name        = ""web-sg""
  description = ""Security group for web servers""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# EC2 Instances
resource ""aws_instance"" ""web"" {
  count         = 2
  ami           = ""ami-0c55b159cbfafe1f0""
  instance_type = ""t2.micro""
  subnet_id     = aws_subnet.public[count.index].id

  vpc_security_group_ids = [aws_security_group.web.id]

  tags = {
    Name = ""web-server-${count.index + 1}""
  }
}

# Application Load Balancer
resource ""aws_lb"" ""web"" {
  name               = ""web-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.web.id]
  subnets            = aws_subnet.public[*].id

  tags = {
    Name = ""web-alb""
  }
}

# ALB Target Group
resource ""aws_lb_target_group"" ""web"" {
  name     = ""web-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = aws_vpc.main.id

  health_check {
    path                = ""/""
    healthy_threshold   = 2
    unhealthy_threshold = 10
  }
}

# ALB Target Group Attachment
resource ""aws_lb_target_group_attachment"" ""web"" {
  count            = 2
  target_group_arn = aws_lb_target_group.web.arn
  target_id        = aws_instance",,,,,,"I'll help you create a Terraform configuration for a video streaming site with load balancing. Here's a comprehensive solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""streaming-vpc""
  }
}

# Public Subnets
resource ""aws_subnet"" ""public"" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.${count.index + 1}.0/24""
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = ""public-subnet-${count.index + 1}""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""main-igw""
  }
}

# Route Table
resource ""aws_route_table"" ""public"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""public-rt""
  }
}

# Route Table Association
resource ""aws_route_table_association"" ""public"" {
  count          = 2
  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

# Security Group
resource ""aws_security_group"" ""web"" {
  name        = ""web-sg""
  description = ""Security group for web servers""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# EC2 Instances
resource ""aws_instance"" ""web"" {
  count         = 2
  ami           = ""ami-0c55b159cbfafe1f0"" # Replace with desired AMI
  instance_type = ""t2.micro""
  subnet_id     = aws_subnet.public[count.index].id

  vpc_security_group_ids = [aws_security_group.web.id]

  tags = {
    Name = ""web-server-${count.index + 1}""
  }
}

# Application Load Balancer
resource ""aws_lb"" ""web"" {
  name               = ""streaming-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.web.id]
  subnets            = aws_subnet.public[*].id

  tags = {
    Name = ""streaming-alb""
  }
}

# ALB Target Group
resource ""aws_lb_target_group"" ""web"" {
  name     = ""streaming-tg""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = aws_vpc.main.id

  health_check {
    path                = ""/""
    healthy_threshold   = 2
    unhealthy_threshold = 10
  }
}

#",,,,,
aws_ami,Create an AWS aws_ami resource for the latest Amazon Linux 2 AMI,"package terraform

default ami_latest_amazon_linux_2 = false

ami_latest_amazon_linux_2 {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_ami""
    resource.name == ""latest_amazon_linux_2""
    resource.expressions.filter[_].name.constant_value == ""name""
    resource.expressions.filter[_].values.constant_value[_] == ""amzn2-ami-hvm-*-x86_64-gp2""
    resource.expressions.most_recent.constant_value == true
    resource.expressions.owners.constant_value[_] == ""amazon""
}",2,"provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_ami"" ""latest_amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }
}",create aws_ami resource with name = any,"I'll help you create a Terraform configuration to get the latest Amazon Linux 2 AMI using the `aws_ami` data source. This is a common requirement when launching EC2 instances with the latest Amazon Linux 2 image.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }

  filter {
    name   = ""virtualization-type""
    values = [""hvm""]
  }

  filter {
    name   = ""root-device-type""
    values = [""ebs""]
  }

  filter {
    name   = ""state""
    values = [""available""]
  }
}

# Output the AMI ID for reference
output ""amazon_linux_2_ami_id"" {
  value = data.aws_ami.amazon_linux_2.id
}
```

This configuration:
1. Specifies the required AWS provider
2. Uses the `aws_ami` data source to find the latest Amazon Linux 2 AMI
3. Filters for:
   - The Amazon Linux 2 AMI pattern
   - HVM virtualization
   - EBS root device
   - Available state
4. Includes an output to display the AMI ID

You can reference this AMI ID in other resources using: `data.aws_ami.amazon_linux_2.id`

The AMI will be the latest version available in the specified region at the time of execution.",,,,,,"I'll help you create a data source to fetch the latest Amazon Linux 2 AMI. This is a common pattern in Terraform for getting the most recent Amazon Linux 2 AMI.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }

  filter {
    name   = ""virtualization-type""
    values = [""hvm""]
  }

  filter {
    name   = ""root-device-type""
    values = [""ebs""]
  }

  filter {
    name   = ""state""
    values = [""available""]
  }
}
```

This configuration:
1. Uses the aws_ami data source to find the AMI
2. Specifies we want the most recent AMI
3. Filters for Amazon Linux 2 specifically using the AMI name pattern
4. Ensures we get an HVM AMI with EBS as root device
5. Only looks for available AMIs
6. Restricts to AMIs owned by Amazon

You can reference this AMI in other resources using: `data.aws_ami.amazon_linux_2.id`",,,,,,"I'll help you create a Terraform configuration to find the latest Amazon Linux 2 AMI using the `aws_ami` data source. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

data ""aws_ami"" ""amazon_linux_2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""amzn2-ami-hvm-*-x86_64-gp2""]
  }

  filter {
    name   = ""virtualization-type""
    values = [""hvm""]
  }

  filter {
    name   = ""root-device-type""
    values = [""ebs""]
  }

  filter {
    name   = ""state""
    values = [""available""]
  }
}

# Output the AMI ID (optional)
output ""amazon_linux_2_ami_id"" {
  value = data.aws_ami.amazon_linux_2.id
}
```

This configuration:
1. Declares the AWS provider requirement
2. Uses the `aws_ami` data source to find the latest Amazon Linux 2 AMI
3. Sets filters to ensure we get:
   - The most recent version
   - Only Amazon-owned AMIs
   - Amazon Linux 2 HVM AMIs
   - EBS-backed instances
   - Only available AMIs
4. Includes an optional output to display the AMI ID

You can reference this AMI in other resources using `data.aws_ami.amazon_linux_2.id`.",,,,,
aws_route53_zone,"Set up an aws_route_53 zone named ""example53.com""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false


# Validate aws_route53_zone resource
is_valid_r53_zone {
	some i
	resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""example53.com""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
}
",1,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""
}","Has one ""aws_route53_zone"" resource
    with ""name"" set to ""example53.com""","I'll help you create a Terraform configuration for an AWS Route 53 hosted zone named ""example53.com"". Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create Route 53 hosted zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
  
  tags = {
    Environment = ""production""
    Name        = ""example53.com""
  }
}
```

This configuration:
1. Declares the required AWS provider
2. Creates a Route 53 hosted zone with the domain name ""example53.com""
3. Includes some basic tags for better resource management

After applying this configuration, AWS will create a hosted zone and provide you with a set of name servers that you'll need to configure with your domain registrar to make the zone functional.

Note: If you need the name servers after creation, you can reference them using `aws_route53_zone.main.name_servers`.",,,,,,"I'll help you create a Terraform configuration for an AWS Route 53 hosted zone named ""example53.com"". Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""

  tags = {
    Environment = ""production""
    Name        = ""example53_zone""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Route 53 hosted zone for the domain ""example53.com""
3. Includes some basic tags for better resource management

After applying this configuration, AWS will create a hosted zone and provide you with a set of name servers that you can use to configure your domain registrar.

Note: If you want to view the name servers assigned to your zone, you can use the following output:

```hcl
output ""name_servers"" {
  value = aws_route53_zone.primary.name_servers
}
```",,,,,,"I'll help you create a Terraform configuration for setting up an AWS Route 53 hosted zone named ""example53.com"". Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired AWS region
}

# Create Route 53 hosted zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""

  tags = {
    Environment = ""production""
    Name        = ""example53.com""
  }
}
```

This configuration will:
1. Set up the required AWS provider
2. Create a public Route 53 hosted zone for the domain ""example53.com""
3. Add some basic tags for better resource management

After applying this configuration, AWS will create a hosted zone and provide you with a set of name servers that you can use to configure your domain registrar.

Note: You'll need to update the name servers at your domain registrar to point to the AWS name servers for the domain to work properly with Route 53.",,,,,
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket where the bucket owner pays for fees.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""
        payment_config.expressions.payer.constant_value == ""BucketOwner""
        
        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""BucketOwner""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name
with payer = ""BucketOwner""","I'll help you create a Terraform configuration for an S3 bucket where the bucket owner pays for all associated fees. This is done using the `request_payer` argument set to ""BucketOwner"".

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  # Configure the AWS Provider
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_s3_bucket"" ""owner_pays_bucket"" {
  bucket = ""my-owner-pays-bucket""  # Change this to your desired bucket name
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id
  payer  = ""BucketOwner""
}

# Optional: Add versioning
resource ""aws_s3_bucket_versioning"" ""versioning"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Recommended: Add server-side encryption
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""encryption"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = ""AES256""
    }
  }
}
```

This configuration:
1. Creates an S3 bucket
2. Sets the request payment configuration to ""BucketOwner""
3. Enables versioning (optional but recommended)
4. Enables server-side encryption (recommended for security)

The `request_payer` setting ensures that the bucket owner pays for all data transfer and request fees, rather than the requester. This is particularly useful when sharing data with other AWS accounts while maintaining control over the associated costs.

Remember to:
- Change the bucket name to something unique
- Adjust the region as needed
- Consider adding additional security configurations like bucket policies or access points if needed",,,,,,"I'll help you create a Terraform configuration for an S3 bucket where the bucket owner pays for all fees (Requester Pays disabled). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_s3_bucket"" ""owner_pays_bucket"" {
  bucket = ""my-owner-pays-bucket""  # Change this to your desired bucket name
}

resource ""aws_s3_bucket_request_payment_configuration"" ""owner_pays_config"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id
  payer  = ""BucketOwner""  # BucketOwner means the bucket owner pays for all fees
}

resource ""aws_s3_bucket_ownership_controls"" ""bucket_ownership"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id
  
  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an S3 bucket
3. Configures the request payment settings to ensure the bucket owner pays for all fees (not the requester)
4. Sets up bucket ownership controls to ensure the bucket owner has full control

The key part is setting `payer = ""BucketOwner""` in the request payment configuration, which ensures that the bucket owner pays for all storage and data transfer costs associated with the bucket.

Remember to replace the bucket name with your desired name, keeping in mind that S3 bucket names must be globally unique across all AWS accounts.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket where the bucket owner pays for all fees (Requester Pays disabled). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""owner_pays_bucket"" {
  bucket = ""my-owner-pays-bucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id
  payer  = ""BucketOwner""  # BucketOwner means the bucket owner pays for all fees
}

resource ""aws_s3_bucket_ownership_controls"" ""example"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id
  
  rule {
    object_ownership = ""BucketOwnerPreferred""
  }
}

resource ""aws_s3_bucket_public_access_block"" ""example"" {
  bucket = aws_s3_bucket.owner_pays_bucket.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}
```

This configuration:
1. Creates an S3 bucket
2. Sets the request payment configuration to ""BucketOwner"" (default setting)
3. Sets bucket ownership controls to ""BucketOwnerPreferred""
4. Adds public access block settings as a security best practice

The bucket owner will be responsible for all storage and data transfer costs. Remember to replace ""my-owner-pays-bucket"" with your desired bucket name, as S3 bucket names must be globally unique.",,,,,
"aws_eks_cluster, aws_eks_fargate_profile, aws_iam_role, aws_iam_role, aws_iam_role_policy_attachment, aws_subnet, aws_subnet, aws_vpc, null_resource, aws_eks_cluster_auth","Create an IAM role for EKS Fargate profiles with the necessary policies for pod execution. Attach the AmazonEKSFargatePodExecutionRolePolicy to the IAM role. Define an AWS EKS Fargate profile for the 'kube-system' namespace, associating it with the EKS cluster and specifying the pod execution role ARN. Ensure that the specified subnets have the required resource tag for cluster association. Use a null_resource to patch the Kubernetes deployment 'coredns' in the 'kube-system' namespace, using the EKS cluster's endpoint, certificate authority, and authentication token for communication.","package terraform.validation

default has_aws_iam_role_fargate = false
default has_aws_iam_role_policy_attachment_fargate = false
default has_aws_eks_fargate_profile = false
default has_aws_eks_cluster_auth = false
default has_null_resource_k8s_patcher = false

has_aws_iam_role_fargate {
    role := input.planned_values.root_module.resources[_]
    role.type == ""aws_iam_role""
    role.name == ""eks-fargate-profile""
    role.values.name == ""eks-fargate-profile""
    role.values.assume_role_policy != null
}

has_aws_iam_role_policy_attachment_fargate {
    attachment := input.planned_values.root_module.resources[_]
    attachment.type == ""aws_iam_role_policy_attachment""
    attachment.name == ""eks-fargate-profile""
    attachment.values.policy_arn == ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
    attachment.values.role == input.planned_values.root_module.resources[_].values.name  # Ensure role is correctly referenced
}

has_aws_eks_fargate_profile {
    fargate_profile := input.configuration.root_module.resources[_]
    fargate_profile.type == ""aws_eks_fargate_profile""
    fargate_profile.name == ""kube-system""
    fargate_profile.expressions.cluster_name != null
    fargate_profile.expressions.fargate_profile_name.constant_value == ""kube-system""
    fargate_profile.expressions.pod_execution_role_arn != null
    count(fargate_profile.expressions.subnet_ids.references) == 4  # Ensure there are two subnet IDs
    fargate_profile.expressions.selector[_].namespace.constant_value == ""kube-system""
}

has_aws_eks_cluster_auth {
    cluster_auth := input.configuration.root_module.resources[_]
    cluster_auth.type == ""aws_eks_cluster_auth""
    cluster_auth.name != null  # Check for proper referencing
}

has_null_resource_k8s_patcher {
    k8s_patcher := input.configuration.root_module.resources[_]
    k8s_patcher.type == ""null_resource""
    k8s_patcher.name == ""k8s_patcher""
    k8s_patcher.depends_on != null
    count(k8s_patcher.expressions.triggers.references) == 8  # Check for three triggers
    k8s_patcher.provisioners[0].type == ""local-exec""
    k8s_patcher.provisioners[0].expressions.command != null
}

valid_configuration {
    has_aws_iam_role_fargate
    has_aws_iam_role_policy_attachment_fargate
    has_aws_eks_fargate_profile
    has_aws_eks_cluster_auth
    has_null_resource_k8s_patcher
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

resource ""aws_subnet"" ""test_eks_1"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  availability_zone = data.aws_availability_zones.available.names[0]

  tags = {
    ""kubernetes.io/cluster/test"" = ""shared""
  }
}

resource ""aws_subnet"" ""test_eks_2"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""

  availability_zone = data.aws_availability_zones.available.names[1]

  tags = {
    ""kubernetes.io/cluster/test"" = ""shared""
  }
}

resource ""aws_iam_role"" ""eks-cluster"" {
  name = ""eks-cluster-1""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""eks.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.eks-cluster.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.eks-cluster.name
}


resource ""aws_eks_cluster"" ""cluster"" {
  name     = ""test""
  role_arn = aws_iam_role.eks-cluster.arn

  vpc_config {
    subnet_ids = [
      aws_subnet.test_eks_1.id,
      aws_subnet.test_eks_2.id
    ]
  }

  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy,
    aws_iam_role_policy_attachment.eks_service_policy,
  ]
}

resource ""aws_iam_role"" ""eks-fargate-profile"" {
  name = ""eks-fargate-profile""

  assume_role_policy = jsonencode({
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""eks-fargate-pods.amazonaws.com""
      }
    }]
    Version = ""2012-10-17""
  })
}

resource ""aws_iam_role_policy_attachment"" ""eks-fargate-profile"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
  role       = aws_iam_role.eks-fargate-profile.name
}

resource ""aws_eks_fargate_profile"" ""kube-system"" {
  cluster_name           = aws_eks_cluster.cluster.name
  fargate_profile_name   = ""kube-system""
  pod_execution_role_arn = aws_iam_role.eks-fargate-profile.arn

  subnet_ids = [
    aws_subnet.test_eks_1.id,
    aws_subnet.test_eks_2.id
  ]

  selector {
    namespace = ""kube-system""
  }

  depends_on = [ aws_iam_role_policy_attachment.eks-fargate-profile ]
}

data ""aws_eks_cluster_auth"" ""eks"" {
  name = aws_eks_cluster.cluster.id
}

resource ""null_resource"" ""k8s_patcher"" {
  depends_on = [aws_eks_fargate_profile.kube-system]

  triggers = {
    endpoint = aws_eks_cluster.cluster.endpoint
    ca_crt   = base64decode(aws_eks_cluster.cluster.certificate_authority[0].data)
    token    = data.aws_eks_cluster_auth.eks.token
  }

  provisioner ""local-exec"" {
    command = <<EOH
      cat >/tmp/ca.crt <<EOF
      ${base64decode(aws_eks_cluster.cluster.certificate_authority[0].data)}
      EOF
      kubectl \
        --server=""${aws_eks_cluster.cluster.endpoint}"" \
        --certificate_authority=/tmp/ca.crt \
        --token=""${data.aws_eks_cluster_auth.eks.token}"" \
        patch deployment coredns \
        -n kube-system --type json \
        -p='[{""op"": ""remove"", ""path"": ""/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type""}]'
      EOH
  }

  lifecycle {
    ignore_changes = [triggers]
  }
}","Resource ""aws_iam_role"" ""eks-fargate-profile""
has one ""name""
has one ""assume_role_policy""
Resource ""aws_iam_role_policy_attachment"" ""eks-fargate-profile""
has one ""policy_arn""
has one ""role""
Resource ""aws_eks_fargate_profile"" ""kube-system""
has one ""cluster_name""
has one ""fargate_profile_name""
has one ""pod_execution_role_arn""
has one ""subnet_ids""
has one ""namespace"" in ""selector""
Data ""aws_eks_cluster_auth"" ""eks""
has one ""name""
Resource ""null_resource"" ""k8s_patcher""
has multiple ""triggers"" (endpoint, ca_crt, token)
uses a ""local-exec"" provisioner to execute a command that patches the Kubernetes deployment
has a ""lifecycle"" policy to ignore changes in ""triggers""","I'll help you create a Terraform configuration for an EKS Fargate profile with the necessary IAM role and CoreDNS patch. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
    null = {
      source  = ""hashicorp/null""
      version = ""~> 3.0""
    }
  }
}

# IAM Role for Fargate Profile
resource ""aws_iam_role"" ""eks_fargate_pod_execution_role"" {
  name = ""eks-fargate-pod-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""eks-fargate-pods.amazonaws.com""
        }
      }
    ]
  })
}

# Attach the required policy for Fargate pod execution
resource ""aws_iam_role_policy_attachment"" ""eks_fargate_pod_execution_role_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
  role       = aws_iam_role.eks_fargate_pod_execution_role.name
}

# EKS Fargate Profile
resource ""aws_eks_fargate_profile"" ""kube_system"" {
  cluster_name           = var.cluster_name
  fargate_profile_name   = ""kube-system-profile""
  pod_execution_role_arn = aws_iam_role.eks_fargate_pod_execution_role.arn
  subnet_ids             = var.private_subnet_ids

  selector {
    namespace = ""kube-system""
  }

  tags = {
    ""kubernetes.io/cluster/${var.cluster_name}"" = ""owned""
  }
}

# Data source to get EKS cluster auth token
data ""aws_eks_cluster"" ""cluster"" {
  name = var.cluster_name
}

data ""aws_eks_cluster_auth"" ""cluster"" {
  name = var.cluster_name
}

# Null resource to patch CoreDNS deployment
resource ""null_resource"" ""patch_coredns"" {
  depends_on = [aws_eks_fargate_profile.kube_system]

  provisioner ""local-exec"" {
    command = <<EOF
      aws eks get-token --cluster-name ${var.cluster_name} --region ${var.aws_region} | \
      kubectl patch deployment coredns \
        -n kube-system \
        --type json \
        -p='[{""op"": ""remove"", ""path"": ""/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type""},{""op"": ""replace"", ""path"": ""/spec/template/metadata/annotations"", ""value"": {""eks.amazonaws.com/compute-type"": ""fargate""}}]' \
        --kubeconfig <(echo '${data.aws_eks_cluster.cluster.certificate_authority[0].data}')
    EOF
  }
}

# Variables
variable ""cluster_name"" {
  description = ""Name of the EKS cluster""
  type        = string
}

variable ""private_subnet_ids"" {
  description = ""List of private subnet IDs for the Fargate profile""
  type        = list(string)
}

variable ""aws_region"" {
  description = ""AWS region""
  type        = string
}
```

This configuration:

1. Creates an IAM role with the necessary trust policy for Fargate pod execution
2. Attaches the AmazonEKSFargatePodExecutionRolePolicy to the IAM role
3. Creates a Fargate profile for the kube-system namespace
4. Uses data sources to get the EKS cluster information
5. Creates a null_resource to patch the CoreDNS deployment to use Fargate

Make sure to:
- Replace the variable values with your actual EKS cluster name, subnet IDs,",,,,,,"I'll help you create a Terraform configuration for an EKS Fargate profile with the necessary IAM role and CoreDNS patch. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
    kubernetes = {
      source  = ""hashicorp/kubernetes""
      version = ""~> 2.0""
    }
  }
}

# IAM Role for Fargate Profile
resource ""aws_iam_role"" ""eks_fargate_pod_execution_role"" {
  name = ""eks-fargate-pod-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""eks-fargate-pods.amazonaws.com""
        }
      }
    ]
  })
}

# Attach the AWS managed policy for Fargate pod execution
resource ""aws_iam_role_policy_attachment"" ""eks_fargate_pod_execution_role_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
  role       = aws_iam_role.eks_fargate_pod_execution_role.name
}

# EKS Fargate Profile
resource ""aws_eks_fargate_profile"" ""kube_system"" {
  cluster_name           = var.cluster_name
  fargate_profile_name   = ""kube-system-profile""
  pod_execution_role_arn = aws_iam_role.eks_fargate_pod_execution_role.arn
  subnet_ids             = var.private_subnet_ids

  selector {
    namespace = ""kube-system""
  }

  tags = {
    ""kubernetes.io/cluster/${var.cluster_name}"" = ""owned""
  }
}

# Data source to get EKS cluster auth token
data ""aws_eks_cluster"" ""cluster"" {
  name = var.cluster_name
}

data ""aws_eks_cluster_auth"" ""cluster"" {
  name = var.cluster_name
}

# Null resource to patch CoreDNS deployment
resource ""null_resource"" ""patch_coredns"" {
  depends_on = [aws_eks_fargate_profile.kube_system]

  provisioner ""local-exec"" {
    command = <<-EOF
      aws eks get-token --cluster-name ${var.cluster_name} | \
      kubectl --kubeconfig=<(echo ""$KUBECONFIG"") \
      patch deployment coredns \
      -n kube-system --type json \
      -p='[{""op"": ""remove"", ""path"": ""/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type""},{""op"": ""add"", ""path"": ""/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type"", ""value"": ""fargate""}]'
    EOF

    environment = {
      KUBECONFIG = yamlencode({
        apiVersion = ""v1""
        clusters = [{
          cluster = {
            server                     = data.aws_eks_cluster.cluster.endpoint
            certificate-authority-data = data.aws_eks_cluster.cluster.certificate_authority[0].data
          }
          name = var.cluster_name
        }]
        contexts = [{
          context = {
            cluster = var.cluster_name
            user    = var.cluster_name
          }
          name = var.cluster_name
        }]
        current-context = var.cluster_name
        kind           = ""Config""
        users = [{
          name = var.cluster_name
          user = {
            token = data.aws_eks_cluster_auth.cluster.token
          }
        }]
      })
    }
  }
}

# Variables
variable ""cluster_name"" {
  type        = string
  description = ""Name of the EKS cluster""
}

variable ""private_subnet_ids"" {",,,,,,"I'll help you create a Terraform configuration for an EKS Fargate profile with the necessary IAM role and CoreDNS patch. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
    null = {
      source  = ""hashicorp/null""
      version = ""~> 3.0""
    }
  }
}

# IAM Role for Fargate Profile
resource ""aws_iam_role"" ""eks_fargate_role"" {
  name = ""eks-fargate-pod-execution-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""eks-fargate-pods.amazonaws.com""
        }
      }
    ]
  })
}

# Attach the required policy for Fargate pod execution
resource ""aws_iam_role_policy_attachment"" ""eks_fargate_pod_execution_role_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy""
  role       = aws_iam_role.eks_fargate_role.name
}

# EKS Fargate Profile
resource ""aws_eks_fargate_profile"" ""kube_system"" {
  cluster_name           = var.cluster_name
  fargate_profile_name   = ""kube-system-profile""
  pod_execution_role_arn = aws_iam_role.eks_fargate_role.arn
  subnet_ids             = var.private_subnet_ids

  selector {
    namespace = ""kube-system""
  }

  depends_on = [aws_iam_role_policy_attachment.eks_fargate_pod_execution_role_policy]
}

# Data source to get EKS cluster auth token
data ""aws_eks_cluster"" ""cluster"" {
  name = var.cluster_name
}

data ""aws_eks_cluster_auth"" ""cluster"" {
  name = var.cluster_name
}

# Null resource to patch CoreDNS
resource ""null_resource"" ""patch_coredns"" {
  depends_on = [aws_eks_fargate_profile.kube_system]

  provisioner ""local-exec"" {
    command = <<EOF
      aws eks update-kubeconfig --name ${var.cluster_name} --region ${var.aws_region}
      kubectl patch deployment coredns \
        -n kube-system \
        --type json \
        -p='[{""op"": ""remove"", ""path"": ""/spec/template/metadata/annotations/eks.amazonaws.com~1compute-type""}]'
    EOF
  }
}

# Variables
variable ""cluster_name"" {
  type        = string
  description = ""Name of the EKS cluster""
}

variable ""aws_region"" {
  type        = string
  description = ""AWS region""
}

variable ""private_subnet_ids"" {
  type        = list(string)
  description = ""List of private subnet IDs for the Fargate profile""
}
```

This configuration:

1. Creates an IAM role for Fargate pod execution with the necessary trust relationship
2. Attaches the AmazonEKSFargatePodExecutionRolePolicy to the IAM role
3. Creates a Fargate profile for the kube-system namespace
4. Uses a null_resource to patch the CoreDNS deployment to work with Fargate

To use this configuration:

1. Make sure your subnets are properly tagged with `kubernetes.io/cluster/<cluster-name>` = `shared`
2. Provide values for the required variables:
   - cluster_name
   - aws_region
   - private_subnet_ids

The configuration will:
- Create the necessary IAM role and policy attachment
- Create the Fargate profile for the kube-system namespace
- Patch the CoreDNS deployment to work with Fargate

Note:",,,,,
"aws_iam_role, aws_lambda_function, archive_file, aws_iam_policy_document","Create an AWS Lambda function. The function should be triggered by a nodejs18 runtime,use a ""lambda.js"" as the source code and a ""index.js"" as the handler. Please declare all necessary resources.","package main

import future.keywords.in

default allow = false

# Check for IAM Role for Lambda
aws_iam_role_for_lambda_valid(resources) {
    some resource in resources
    resource.type == ""aws_iam_role""
}

# Check for AWS Lambda Function with specific configurations
aws_lambda_function_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_function""
    resource.change.after.runtime == ""nodejs18.x""
    resource.change.after.handler == ""index.test""
    resource.change.after.filename == ""lambda_function_payload.zip""
}

# Check for Archive File for Lambda code
archive_file_for_lambda_valid(resources) {
    some resource in resources
    resource.type == ""archive_file""
    resource.values.source_file == ""lambda.js""
    resource.values.type == ""zip""
}

# Aggregate all checks
allow {
    aws_iam_role_for_lambda_valid(input.resource_changes)
    aws_lambda_function_valid(input.resource_changes)
    archive_file_for_lambda_valid(input.prior_state.values.root_module.resources)
}
",3,"data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""lambda.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""iam_for_lambda"" {
  name               = ""iam_for_lambda""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

data ""archive_file"" ""lambda"" {
  type        = ""zip""
  source_file = ""lambda.js""
  output_path = ""lambda_function_payload.zip""
}

resource ""aws_lambda_function"" ""test_lambda"" {
  filename      = ""lambda_function_payload.zip""
  function_name = ""lambda_function_name""
  role          = aws_iam_role.iam_for_lambda.arn
  handler       = ""index.test""

  runtime = ""nodejs18.x""
}","Needs to have a defined IAM role. Data for a .zip file, with source file = lambda.js, function name can be anything. For the aws_lambda_function use the defined zip, function name, set handler to anything, and runtime to ""nodejs.18x""","I'll help you create an AWS Lambda function with the specified requirements. Here's a Terraform configuration that includes all necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_execution_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy attachment for basic Lambda execution
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Archive file for Lambda source code
data ""archive_file"" ""lambda_zip"" {
  type        = ""zip""
  source_file = ""lambda.js""
  output_path = ""lambda_function.zip""
}

# Lambda function
resource ""aws_lambda_function"" ""example_lambda"" {
  filename         = ""lambda_function.zip""
  function_name    = ""example_lambda_function""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  source_code_hash = data.archive_file.lambda_zip.output_base64sha256
  runtime         = ""nodejs18.x""
}
```

This configuration includes:

1. Required AWS provider configuration
2. An IAM role for Lambda execution
3. Basic Lambda execution policy attachment
4. Archive file data source to zip the Lambda source code
5. The Lambda function resource itself with the specified:
   - Node.js 18.x runtime
   - ""lambda.js"" as the source file
   - ""index.js"" handler
   - Necessary IAM role and permissions

Make sure you have the ""lambda.js"" file in your working directory before applying this configuration. The code will package it into a ZIP file and deploy it to AWS Lambda.",,,,,,"I'll help you create a Terraform configuration for an AWS Lambda function with the specified requirements. This will include the Lambda function itself, an IAM role, and necessary permissions.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_execution_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Basic Lambda execution policy attachment
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}

# Lambda function
resource ""aws_lambda_function"" ""example_lambda"" {
  filename         = ""lambda.js""
  function_name    = ""example_lambda_function""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.js""
  runtime         = ""nodejs18.x""

  environment {
    variables = {
      ENVIRONMENT = ""production""
    }
  }
}
```

This configuration includes:

1. The required AWS provider declaration
2. An IAM role for the Lambda function with the necessary assume role policy
3. Basic Lambda execution policy attachment to allow CloudWatch Logs
4. The Lambda function resource with the specified:
   - nodejs18.x runtime
   - lambda.js as the source code file
   - index.js as the handler
   - Basic environment variable setup

Make sure to have the `lambda.js` file in your working directory when applying this configuration. The IAM role and policy attachment ensure that your Lambda function has the basic permissions it needs to execute and write logs to CloudWatch.",,,,,,"I'll help you create a Terraform configuration for an AWS Lambda function with the specified requirements. This will include the necessary IAM role and policy for Lambda execution.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_execution_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for the Lambda role
resource ""aws_iam_role_policy_attachment"" ""lambda_policy"" {
  role       = aws_iam_role.lambda_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}

# Lambda function
resource ""aws_lambda_function"" ""example_lambda"" {
  filename         = ""lambda.js""
  function_name    = ""example_lambda_function""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.js""
  runtime         = ""nodejs18.x""

  environment {
    variables = {
      ENVIRONMENT = ""production""
    }
  }
}
```

This configuration includes:

1. The required AWS provider block
2. An IAM role for the Lambda function with the necessary assume role policy
3. An IAM policy attachment that gives the Lambda function basic execution permissions
4. The Lambda function resource itself with the specified:
   - nodejs18.x runtime
   - lambda.js as the source code file
   - index.js as the handler
   - Basic environment configuration

Make sure to have the `lambda.js` file in your working directory when applying this configuration. The IAM role and policy attachment ensure that your Lambda function has the necessary permissions to execute and write logs to CloudWatch.",,,,,
"aws_iam_role, aws_kinesisanalyticsv2_application, aws_iam_policy_document",Generate a basic Amazon Kinesis Analytics Application with a CloudWatch log stream,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kinesisanalytics.amazonaws.com"") 
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
}

has_valid_cloudwatch_log_group {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_cloudwatch_log_group""
    resource.values.name
}

has_valid_cloudwatch_log_stream {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_cloudwatch_log_stream""
    resource.values.name
    role := input.configuration.root_module.resources[i]
    role.expressions.log_group_name

}

has_valid_kinesis_analytics_application {
        some i
        resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_analytics_application""
    resource.values.name
    resource.values.cloudwatch_logging_options
    
}

has_valid_resources {
	has_valid_bucket
    has_valid_iam_role(input.resource_changes)
    has_valid_cloudwatch_log_group
    has_valid_cloudwatch_log_stream
    has_valid_kinesis_analytics_application
}",3,"provider ""aws"" {
  region = ""us-west-2""
}

data ""aws_iam_policy_document"" ""KinesisAssume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}


resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_role""
  assume_role_policy = data.aws_iam_policy_document.KinesisAssume.json
}

resource ""aws_kinesisanalyticsv2_application"" ""flink_app"" {
  name                 = ""example-application1""
  runtime_environment  = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_role.arn
}","Has one ""aws_cloudwatch_log_group"" resource, one ""aws_cloudwatch_log_stream"" resource, one ""aws_iam_role"" resource, one ""aws_s3_bucket"" resource, and one ""aws_kinesis_analytics_application"" resource with a ""cloudwatch_logging_options"" block
","I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with CloudWatch log stream integration. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""kinesis_analytics_logs"" {
  name              = ""/aws/kinesis-analytics/my-application""
  retention_in_days = 14
}

# Create CloudWatch Log Stream
resource ""aws_cloudwatch_log_stream"" ""kinesis_analytics_log_stream"" {
  name           = ""KinesisAnalyticsLogStream""
  log_group_name = aws_cloudwatch_log_group.kinesis_analytics_logs.name
}

# Create IAM Role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-application-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Create IAM Role Policy for CloudWatch Logs
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-cloudwatch-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:PutLogEvents"",
          ""logs:DescribeLogStreams"",
          ""logs:DescribeLogGroups"",
          ""logs:CreateLogStream""
        ]
        Resource = [
          aws_cloudwatch_log_group.kinesis_analytics_logs.arn,
          ""${aws_cloudwatch_log_group.kinesis_analytics_logs.arn}:*""
        ]
      }
    ]
  })
}

# Create Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""my-kinesis-analytics-app""

  # Configure CloudWatch logging
  cloudwatch_logging_options {
    log_stream_arn = aws_cloudwatch_log_stream.kinesis_analytics_log_stream.arn
    role_arn       = aws_iam_role.kinesis_analytics_role.arn
  }

  # Basic application configuration
  code = <<-EOF
    CREATE OR REPLACE STREAM ""DESTINATION_SQL_STREAM"" (
      column1 varchar(64),
      column2 varchar(64)
    );
  EOF

  inputs {
    name_prefix = ""SOURCE_SQL_STREAM""

    schema_version = ""1""

    kinesis_streams_input {
      resource_arn = ""arn:aws:kinesis:${data.aws_region.current.name}:${data.aws_caller_identity.current.account_id}:stream/example-stream""
      role_arn     = aws_iam_role.kinesis_analytics_role.arn
    }

    starting_position_configuration {
      starting_position = ""NOW""
    }
  }
}

# Data sources for current region and account ID
data ""aws_region"" ""current"" {}
data ""aws_caller_identity"" ""current"" {}
```

This configuration creates:
1. A CloudWatch Log Group and Log Stream for logging
2. An IAM role and policy for Kinesis Analytics to write to CloudWatch Logs
3. A basic Kinesis Analytics Application with CloudWatch logging enabled
4. Necessary permissions and configurations for the application to function

Key features:
- The application is configured with basic SQL code
- CloudWatch logging is properly configured with the necessary IAM permissions
- The configuration includes input stream configuration
- All resources",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with CloudWatch log stream integration. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""kinesis_analytics_logs"" {
  name              = ""/aws/kinesis-analytics/myapp""
  retention_in_days = 14
}

# Create CloudWatch Log Stream
resource ""aws_cloudwatch_log_stream"" ""kinesis_analytics_log_stream"" {
  name           = ""kinesis-analytics-log-stream""
  log_group_name = aws_cloudwatch_log_group.kinesis_analytics_logs.name
}

# Create IAM Role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Create IAM Role Policy for CloudWatch Logs
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:PutLogEvents"",
          ""logs:DescribeLogStreams"",
          ""logs:DescribeLogGroups"",
          ""logs:CreateLogStream""
        ]
        Resource = [
          aws_cloudwatch_log_group.kinesis_analytics_logs.arn,
          ""${aws_cloudwatch_log_group.kinesis_analytics_logs.arn}:*""
        ]
      }
    ]
  })
}

# Create Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""my-kinesis-analytics-app""

  cloudwatch_logging_options {
    log_stream_arn = aws_cloudwatch_log_stream.kinesis_analytics_log_stream.arn
    role_arn       = aws_iam_role.kinesis_analytics_role.arn
  }

  code = <<EOF
CREATE OR REPLACE STREAM ""DESTINATION_SQL_STREAM"" (
    column1 VARCHAR(64),
    column2 VARCHAR(64)
);

CREATE OR REPLACE PUMP ""STREAM_PUMP"" AS 
    INSERT INTO ""DESTINATION_SQL_STREAM""
    SELECT STREAM column1, column2
    FROM ""SOURCE_SQL_STREAM_001"";
EOF
}
```

This configuration creates:
1. A CloudWatch Log Group and Log Stream for logging
2. An IAM role with necessary permissions for Kinesis Analytics to write to CloudWatch Logs
3. A basic Kinesis Analytics Application with CloudWatch logging enabled
4. The necessary IAM policies to allow the application to write logs

The Kinesis Analytics Application includes a simple SQL code that creates a destination stream and pumps data from a source stream to it. You can modify the SQL code according to your specific requirements.

Remember to:
- Adjust the region in the provider block as needed
- Modify the retention period for logs if needed (currently set to 14 days)
- Update the names of resources to match your naming conventions
- Modify the SQL code in the Kinesis Analytics Application to match your use case",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with CloudWatch log stream integration. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""kinesis_analytics_logs"" {
  name              = ""/aws/kinesis-analytics/my-application""
  retention_in_days = 14
}

# Create CloudWatch Log Stream
resource ""aws_cloudwatch_log_stream"" ""kinesis_analytics_log_stream"" {
  name           = ""kinesis-analytics-log-stream""
  log_group_name = aws_cloudwatch_log_group.kinesis_analytics_logs.name
}

# Create IAM Role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-application-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Create IAM Role Policy for CloudWatch Logs
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-cloudwatch-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""logs:PutLogEvents"",
          ""logs:DescribeLogStreams"",
          ""logs:DescribeLogGroups"",
          ""logs:CreateLogStream""
        ]
        Resource = [
          aws_cloudwatch_log_group.kinesis_analytics_logs.arn,
          ""${aws_cloudwatch_log_group.kinesis_analytics_logs.arn}:*""
        ]
      }
    ]
  })
}

# Create Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""my-kinesis-analytics-app""

  cloudwatch_logging_options {
    log_stream_arn = aws_cloudwatch_log_stream.kinesis_analytics_log_stream.arn
    role_arn       = aws_iam_role.kinesis_analytics_role.arn
  }

  code = <<EOF
    CREATE OR REPLACE STREAM ""DESTINATION_SQL_STREAM"" (
        event_time TIMESTAMP,
        ticker_symbol VARCHAR(4),
        price DOUBLE);
        
    CREATE OR REPLACE PUMP ""STREAM_PUMP"" AS 
        INSERT INTO ""DESTINATION_SQL_STREAM""
        SELECT STREAM event_time, ticker_symbol, price
        FROM ""SOURCE_SQL_STREAM_001""
    EOF
}
```

This configuration creates:

1. A CloudWatch Log Group with a 14-day retention period
2. A CloudWatch Log Stream within the Log Group
3. An IAM Role with necessary permissions for Kinesis Analytics to write to CloudWatch Logs
4. An IAM Role Policy that grants the required CloudWatch Logs permissions
5. A Kinesis Analytics Application with:
   - CloudWatch logging configuration
   - Basic SQL code for stream processing

The application is configured with a simple SQL code that creates a destination stream and pumps data from the source stream to the destination stream. You can modify the SQL code according to your specific requirements.

Remember to:
- Adjust the region in the provider block as needed
- Modify the log retention period if 14 days isn't suitable
- Update the application name and other identifiers to match your naming conventions
- Customize the SQL code based on your actual stream processing needs",,,,,
"aws_dynamodb_table, aws_iam_role, aws_lambda_event_source_mapping, aws_lambda_function, archive_file, aws_iam_policy_document",Create a Lambda event source mapping from a DynamoDB,"package main

import future.keywords.in

default allow = false

aws_dynamodb_table_valid(resources) {
    some resource in resources
    resource.type == ""aws_dynamodb_table""
}

aws_lambda_function_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_function""
}

# Check for Lambda event source mapping from DynamoDB
aws_lambda_event_source_mapping_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_event_source_mapping""
}

# Aggregate all checks
allow {
    aws_dynamodb_table_valid(input.resource_changes)
    aws_lambda_function_valid(input.resource_changes)
    aws_lambda_event_source_mapping_valid(input.resource_changes)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""example_table"" {
  name           = ""example_table""
  hash_key       = ""id""
  read_capacity  = 10
  write_capacity = 10

  attribute {
    name = ""id""
    type = ""S""
  }

  # Enable DynamoDB Streams
  stream_enabled   = true
  stream_view_type = ""NEW_AND_OLD_IMAGES"" # Choose as per your requirement

  # Other configurations for your table
}

data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""lambda.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""iam_for_lambda"" {
  name               = ""iam_for_lambda""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

resource ""aws_iam_role_policy"" ""dynamodb_lambda_policy"" {
  name   = ""lambda-dynamodb-policy""
  role   = aws_iam_role.iam_for_lambda.id
  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
        ""Sid"": ""AllowLambdaFunctionToCreateLogs"",
        ""Action"": [ 
            ""logs:*"" 
        ],
        ""Effect"": ""Allow"",
        ""Resource"": [ 
            ""arn:aws:logs:*:*:*"" 
        ]
    },
    {
        ""Sid"": ""AllowLambdaFunctionInvocation"",
        ""Effect"": ""Allow"",
        ""Action"": [
            ""lambda:InvokeFunction""
        ],
        ""Resource"": [
            ""${aws_dynamodb_table.example_table.arn}/stream/*""
        ]
    },
    {
        ""Sid"": ""APIAccessForDynamoDBStreams"",
        ""Effect"": ""Allow"",
        ""Action"": [
            ""dynamodb:GetRecords"",
            ""dynamodb:GetShardIterator"",
            ""dynamodb:DescribeStream"",
            ""dynamodb:ListStreams""
        ],
        ""Resource"": ""${aws_dynamodb_table.example_table.arn}/stream/*""
    }
  ]
}
EOF
}

data ""archive_file"" ""lambda_zip_file"" {
  type        = ""zip""
  source_file = ""./supplement/app.js""
  output_path = ""./supplement/app.zip""
}

resource ""aws_lambda_function"" ""example_lambda"" {
  filename      = data.archive_file.lambda_zip_file.output_path
  function_name = ""lambda_app_function""
  source_code_hash = data.archive_file.lambda_zip_file.output_base64sha256
  role          = aws_iam_role.iam_for_lambda.arn
  handler       = ""app.handler""
  runtime       = ""nodejs18.x""
}

resource ""aws_lambda_event_source_mapping"" ""dynamodb_lambda_mapping"" {
  event_source_arn  = aws_dynamodb_table.example_table.stream_arn
  function_name     = aws_lambda_function.example_lambda.arn
  starting_position = ""LATEST"" # or ""TRIM_HORIZON"" as per your use case

}

output ""dynamodb_arn"" {
  value = aws_dynamodb_table.example_table.arn
  description = ""The ARN of the DynamoDB table""
}

output ""lambda_processing_arn"" {
  value = aws_lambda_function.example_lambda.arn
  description = ""The ARN of the Lambda function processing the DynamoDB stream""
}","Needs to have an aws_lambda_function defined (IAM role, zip). Create a DynamoDB table. Create lambda source event from an event source from DynamoDB table and function name.","I'll help you create a Terraform configuration for a Lambda event source mapping from a DynamoDB table. This will allow your Lambda function to process records from DynamoDB streams.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# DynamoDB table with streams enabled
resource ""aws_dynamodb_table"" ""example_table"" {
  name           = ""example-table""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  stream_enabled = true
  stream_view_type = ""NEW_AND_OLD_IMAGES""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_dynamodb_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for DynamoDB streams access
resource ""aws_iam_role_policy"" ""lambda_dynamodb_policy"" {
  name = ""lambda_dynamodb_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:GetRecords"",
          ""dynamodb:GetShardIterator"",
          ""dynamodb:DescribeStream"",
          ""dynamodb:ListStreams""
        ]
        Resource = ""${aws_dynamodb_table.example_table.stream_arn}""
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""example_lambda"" {
  filename         = ""lambda_function.zip""  # Your Lambda function code
  function_name    = ""dynamodb-stream-consumer""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""  # Change to your desired runtime

  environment {
    variables = {
      TABLE_NAME = aws_dynamodb_table.example_table.name
    }
  }
}

# Event source mapping
resource ""aws_lambda_event_source_mapping"" ""example_mapping"" {
  event_source_arn  = aws_dynamodb_table.example_table.stream_arn
  function_name     = aws_lambda_function.example_lambda.arn
  starting_position = ""LATEST""
  
  batch_size = 100
  enabled    = true

  depends_on = [
    aws_iam_role_policy.lambda_dynamodb_policy
  ]
}
```

This configuration:
1. Creates a DynamoDB table with streams enabled
2. Sets up an IAM role and policy for the Lambda function to access DynamoDB streams
3. Creates a Lambda function (you'll need to provide the actual function code in a ZIP file)
4. Creates the event source mapping between the DynamoDB stream and the Lambda function

Make sure to:
- Replace the region with your desired AWS region
- Provide your Lambda function code in a ZIP file
- Adjust the runtime according to your Lambda function's language
- Modify the batch size and other parameters as needed for your use case

The Lambda function will automatically receive records from the DynamoDB stream whenever changes occur in the table.",,,,,,"Error: Error code: 500 - {'type': 'error', 'error': {'type': 'api_error', 'message': 'Internal server error'}}",,,,,,"I'll help you create a Terraform configuration for a Lambda event source mapping from a DynamoDB table. This configuration will include the necessary AWS provider and the event source mapping resource.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Lambda event source mapping
resource ""aws_lambda_event_source_mapping"" ""dynamodb_trigger"" {
  event_source_arn  = aws_dynamodb_table.example.stream_arn
  function_name     = aws_lambda_function.example.function_name
  starting_position = ""LATEST""
  
  # Optional configurations
  batch_size        = 100
  enabled           = true
  
  # Maximum number of retry attempts
  maximum_retry_attempts = 3
  
  # Parallelization factor
  parallelization_factor = 1
}

# Example DynamoDB table with streams enabled
resource ""aws_dynamodb_table"" ""example"" {
  name           = ""example-table""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  stream_enabled = true
  stream_view_type = ""NEW_AND_OLD_IMAGES""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# Example Lambda function
resource ""aws_lambda_function"" ""example"" {
  filename         = ""lambda_function.zip""
  function_name    = ""example_lambda""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_dynamodb_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for DynamoDB streams access
resource ""aws_iam_role_policy"" ""lambda_dynamodb_policy"" {
  name = ""lambda_dynamodb_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:GetRecords"",
          ""dynamodb:GetShardIterator"",
          ""dynamodb:DescribeStream"",
          ""dynamodb:ListStreams""
        ]
        Resource = aws_dynamodb_table.example.stream_arn
      }
    ]
  })
}

# Basic Lambda execution policy
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a DynamoDB table with streams enabled
3. Creates a Lambda function
4. Sets up the necessary IAM roles and policies
5. Creates the event source mapping between DynamoDB and Lambda

Key features of the event source mapping:
- Uses `LATEST` as the starting position for reading the stream
- Sets a batch size of 100 records
- Includes retry attempts and parallelization settings
- Includes necessary IAM permissions for both Lambda and DynamoDB streams

Remember to:
1. Replace the region with your desired AWS region
2. Provide your actual Lambda function code in the `lambda_function.zip`
3. Adjust the function runtime and handler according to your Lambda function
4. Modify the DynamoDB table configuration according to your needs",,,,,
"aws_iam_role, aws_iam_role_policy, aws_kendra_data_source, aws_kendra_index",Generate a Kendra data source resource With URL Exclusion and Inclusion Patterns,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
    some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kendra.amazonaws.com"") 
}

has_valid_kendra_index {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_index""
    resource.values.name
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
}

has_valid_kendra_data_source{
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_data_source""
    resource.values.name
    resource.values.type
    resource.values.configuration[_].web_crawler_configuration[_].url_exclusion_patterns
    resource.values.configuration[_].web_crawler_configuration[_].url_inclusion_patterns
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
    role.expressions.index_id
}


has_valid_resources {
    has_valid_iam_role(input.resource_changes)
    has_valid_kendra_index
    has_valid_kendra_data_source
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""role"" {
  name = ""Kendra-Role""
  assume_role_policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": ""sts:AssumeRole"",
      ""Principal"": {
        ""Service"": ""kendra.amazonaws.com""
      },
      ""Effect"": ""Allow"",
      ""Sid"": """"
    }
  ]
}
EOF
}

resource ""aws_iam_role_policy"" ""policy"" {
  name = ""Kendra-Policy""
  role = aws_iam_role.role.id

  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:GetObject"",
        ""s3:ListBucket""
      ],
      ""Resource"": ""*""
    }
  ]
}
EOF
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example""
  description = ""example""
  edition     = ""DEVELOPER_EDITION""
  role_arn    = aws_iam_role.role.arn
}

resource ""aws_kendra_data_source"" ""example"" {
  index_id = aws_kendra_index.example.id
  name     = ""example""
  type     = ""WEBCRAWLER""
  role_arn = aws_iam_role.role.arn

  configuration {
    web_crawler_configuration {
      url_exclusion_patterns = [""example""]
      url_inclusion_patterns = [""hello""]

      urls {
        seed_url_configuration {
          seed_urls = [
            ""https://docs.aws.amazon.com/codedeploy/latest/userguide/getting-started-create-service-role.html""
          ]
        }
      }
    }
  }
}","Has one ""aws_iam_role"" resource, one ""aws_kendra_index"" resource, and one ""aws_kendra_data_source"" resource with ""url_exclusion_patterns"" and ""url_inclusion_patterns""","I'll help you create a Terraform configuration for an AWS Kendra data source resource with URL exclusion and inclusion patterns. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_kendra_data_source"" ""example"" {
  index_id = ""your-kendra-index-id""
  name     = ""example-web-crawler-datasource""
  type     = ""WEBCRAWLER""

  configuration {
    web_crawler_configuration {
      urls {
        seed_url_configuration {
          seed_urls = [""https://example.com""]
          
          web_crawler_mode = ""HOST_ONLY"" # or SUBDOMAINS
          
          url_inclusion_patterns = [
            ""https://example.com/included/*"",
            ""https://example.com/department/*""
          ]
          
          url_exclusion_patterns = [
            ""https://example.com/private/*"",
            ""https://example.com/internal/*"",
            ""*.pdf""
          ]
        }
      }

      crawl_depth = 2

      max_links_per_page = 100

      max_content_size_per_page_in_mega_bytes = 50

      url_filtering_configuration {
        allow_list_pattern = [""^https://example.com/""]
        deny_list_pattern  = [""^https://example.com/restricted/*""]
      }
    }
  }

  role_arn = aws_iam_role.kendra_datasource_role.arn

  schedule = ""cron(0 0 * * ? *)"" # Run at midnight every day

  language_code = ""en""
  
  description = ""Web crawler data source with URL patterns""
}

# Required IAM role for the Kendra data source
resource ""aws_iam_role"" ""kendra_datasource_role"" {
  name = ""kendra-datasource-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""kendra_datasource_policy"" {
  role       = aws_iam_role.kendra_datasource_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSKendraDataSourcePolicy""
}
```

This configuration creates a Kendra data source with the following features:

1. Web crawler configuration with seed URLs
2. URL inclusion patterns to specify which URLs should be crawled
3. URL exclusion patterns to specify which URLs should be skipped
4. URL filtering configuration with allow and deny lists
5. Crawl depth and page limits
6. Required IAM role and policy attachments
7. Scheduled crawling using a cron expression

Key components:
- The data source is of type ""WEBCRAWLER""
- URL inclusion and exclusion patterns are specified using wildcards
- The crawler is configured to run daily at midnight
- Maximum content size and links per page are specified
- Proper IAM roles and permissions are set up

Remember to replace:
- `your-kendra-index-id` with your actual Kendra index ID
- The seed URLs and patterns with your actual URLs
- Adjust the schedule, crawl depth, and other parameters according to your needs

This configuration provides a complete setup for a Kendra web crawler data source with URL pattern filtering.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra data source resource with URL exclusion and inclusion patterns. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_kendra_data_source"" ""example"" {
  index_id = ""your-kendra-index-id""
  name     = ""example-web-crawler-datasource""
  type     = ""WEBCRAWLER""

  configuration {
    web_crawler_configuration {
      urls {
        seed_url_configuration {
          seed_urls = [
            ""https://example.com""
          ]
          web_crawler_mode = ""HOST_ONLY""
        }

        url_inclusion_patterns = [
          ""https://example.com/included/*"",
          ""https://example.com/products/*""
        ]

        url_exclusion_patterns = [
          ""https://example.com/private/*"",
          ""https://example.com/internal/*"",
          ""*.pdf""
        ]
      }

      crawl_depth = 2
      max_links_per_page = 100
      max_content_size_per_page_in_mega_bytes = 5

      url_inclusion_patterns = [
        ""https://example.com/included/*"",
        ""https://example.com/products/*""
      ]

      url_exclusion_patterns = [
        ""https://example.com/private/*"",
        ""https://example.com/internal/*"",
        ""*.pdf""
      ]
    }
  }

  role_arn = aws_iam_role.kendra_datasource_role.arn

  tags = {
    Environment = ""production""
    Purpose     = ""web-crawler""
  }
}

# IAM role for Kendra data source
resource ""aws_iam_role"" ""kendra_datasource_role"" {
  name = ""kendra_datasource_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""kendra_datasource_policy"" {
  role       = aws_iam_role.kendra_datasource_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSKendraDataSourcePolicy""
}
```

This configuration creates:

1. A Kendra data source of type WEBCRAWLER with:
   - URL inclusion and exclusion patterns both at the URLs level and web crawler configuration level
   - Seed URL configuration with host-only crawling mode
   - Crawl depth and page limits
   - Content size restrictions

2. The necessary IAM role and policy attachment for the Kendra data source to function

Key features of this configuration:
- URL inclusion patterns to specify which URLs should be crawled
- URL exclusion patterns to specify which URLs should be skipped
- Crawl depth limitation of 2 levels
- Maximum of 100 links per page
- Maximum content size of 5MB per page
- Appropriate IAM roles and permissions

Remember to replace ""your-kendra-index-id"" with your actual Kendra index ID, and adjust the URLs, patterns, and other parameters according to your specific needs.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra data source resource with URL exclusion and inclusion patterns. Here's a solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_kendra_data_source"" ""example_datasource"" {
  index_id = ""your-kendra-index-id""
  name     = ""example-web-crawler-datasource""
  type     = ""WEBCRAWLER""

  configuration {
    web_crawler_configuration {
      urls {
        seed_url_configuration {
          seed_urls = [""https://example.com""]
          
          web_crawler_mode = ""HOST_ONLY""  # or ""SUBDOMAINS""
        }

        url_inclusion_patterns = [
          ""https://example.com/include/*"",
          ""https://example.com/products/*""
        ]

        url_exclusion_patterns = [
          ""https://example.com/exclude/*"",
          ""https://example.com/private/*"",
          ""https://example.com/internal/*""
        ]
      }

      crawl_depth = 2

      max_links_per_page = 100

      max_urls_per_minute = 300
    }
  }

  role_arn = aws_iam_role.kendra_datasource_role.arn

  schedule = ""cron(0 0 * * ? *)""  # Run daily at midnight

  depends_on = [
    aws_iam_role.kendra_datasource_role
  ]
}

# IAM role for the Kendra data source
resource ""aws_iam_role"" ""kendra_datasource_role"" {
  name = ""kendra-datasource-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""kendra_datasource_policy"" {
  role       = aws_iam_role.kendra_datasource_role.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSKendraDataSourcePolicy""
}
```

This configuration creates:
1. A Kendra data source of type WEBCRAWLER
2. Includes URL inclusion and exclusion patterns
3. Sets up basic crawler configuration including crawl depth and rate limits
4. Creates necessary IAM roles and policies

Key features:
- URL inclusion patterns to specify which URLs should be crawled
- URL exclusion patterns to specify which URLs should be skipped
- Web crawler configuration with customizable parameters
- IAM role and policy setup for the data source
- Scheduled crawling using cron expression

Remember to:
- Replace ""your-kendra-index-id"" with your actual Kendra index ID
- Adjust the seed URLs, inclusion patterns, and exclusion patterns according to your needs
- Modify the crawl parameters (depth, rate limits) based on your requirements
- Adjust the schedule expression to match your desired crawl frequency

This configuration provides a complete setup for a Kendra web crawler data source with URL filtering capabilities.",,,,,
"aws_iam_role, aws_kinesis_firehose_delivery_stream, aws_s3_bucket, aws_iam_policy_document",Generate a Kinesis Firehose Delivery Stream with an extended S3 Destination,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""firehose.amazonaws.com"") 
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
}

has_valid_firehose_delivery_stream {
        some i
        resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_firehose_delivery_stream""
    resource.values.name
    resource.values.destination == ""extended_s3""
    resource.values.extended_s3_configuration[_]
        role := input.configuration.root_module.resources[i]
    role.expressions.extended_s3_configuration[_].role_arn
}

has_valid_resources {
        has_valid_iam_role(input.resource_changes)
        has_valid_bucket
    has_valid_firehose_delivery_stream
}",3,"provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_s3_bucket"" ""januarysixteenth"" {
  bucket = ""januarysixteenth""
}

data ""aws_iam_policy_document"" ""firehose_assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""firehose.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""firehose_role"" {
  name               = ""firehose_test_role""
  assume_role_policy = data.aws_iam_policy_document.firehose_assume_role.json
}

resource ""aws_kinesis_firehose_delivery_stream"" ""extended_s3_stream"" {
  name        = ""terraform-kinesis-firehose-extended-s3-test-stream""
  destination = ""extended_s3""

  extended_s3_configuration {
    role_arn   = aws_iam_role.firehose_role.arn
    bucket_arn = aws_s3_bucket.januarysixteenth.arn
  }
}","Has one ""aws_iam_role"" resource, one ""aws_s3_bucket"" resource, and one ""aws_kinesis_firehose_delivery_stream"" resource with an ""extended_s3_configuration"" block 
","I'll help you create a Terraform configuration for an AWS Kinesis Firehose Delivery Stream with an extended S3 destination. This will include the necessary provider configuration and the Kinesis Firehose resource with S3 as the destination.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 Bucket for Firehose destination
resource ""aws_s3_bucket"" ""firehose_bucket"" {
  bucket = ""my-firehose-destination-bucket""
}

# IAM Role for Firehose
resource ""aws_iam_role"" ""firehose_role"" {
  name = ""firehose_delivery_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""firehose.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Policy for Firehose
resource ""aws_iam_role_policy"" ""firehose_policy"" {
  name = ""firehose_delivery_policy""
  role = aws_iam_role.firehose_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:AbortMultipartUpload"",
          ""s3:GetBucketLocation"",
          ""s3:GetObject"",
          ""s3:ListBucket"",
          ""s3:ListBucketMultipartUploads"",
          ""s3:PutObject""
        ]
        Resource = [
          aws_s3_bucket.firehose_bucket.arn,
          ""${aws_s3_bucket.firehose_bucket.arn}/*""
        ]
      }
    ]
  })
}

# Kinesis Firehose Delivery Stream
resource ""aws_kinesis_firehose_delivery_stream"" ""extended_s3_stream"" {
  name        = ""my-firehose-stream""
  destination = ""extended_s3""

  extended_s3_configuration {
    role_arn   = aws_iam_role.firehose_role.arn
    bucket_arn = aws_s3_bucket.firehose_bucket.arn

    buffering_size = 64
    buffering_interval = 60
    
    compression_format = ""GZIP""
    
    prefix = ""firehose/data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/""
    error_output_prefix = ""firehose/errors/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/!{firehose:error-output-type}/""

    processing_configuration {
      enabled = true

      processors {
        type = ""MetadataExtraction""
        parameters {
          parameter_name  = ""JsonParsingEngine""
          parameter_value = ""JQ-1.6""
        }
      }
    }

    cloudwatch_logging_options {
      enabled         = true
      log_group_name  = ""/aws/firehose/my-firehose-stream""
      log_stream_name = ""S3Delivery""
    }
  }
}
```

This configuration creates:
1. An S3 bucket as the destination
2. An IAM role and policy for Firehose to access S3
3. A Kinesis Firehose delivery stream with extended S3 configuration including:
   - GZIP compression
   - Dynamic prefixes for data organization
   - Error output handling
   - Data processing configuration
   - CloudWatch logging
   - Buffering settings

The Firehose will buffer data for 60 seconds or until it reaches 64MB before delivering to S3. The data will be",,,,,,"I'll help you create a Terraform configuration for an AWS Kinesis Firehose Delivery Stream with an S3 destination. Here's a complete solution with the necessary components:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# S3 bucket for Firehose destination
resource ""aws_s3_bucket"" ""firehose_bucket"" {
  bucket = ""my-firehose-destination-bucket""
}

# IAM role for Firehose
resource ""aws_iam_role"" ""firehose_role"" {
  name = ""firehose_delivery_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""firehose.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Firehose
resource ""aws_iam_role_policy"" ""firehose_policy"" {
  name = ""firehose_delivery_policy""
  role = aws_iam_role.firehose_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:AbortMultipartUpload"",
          ""s3:GetBucketLocation"",
          ""s3:GetObject"",
          ""s3:ListBucket"",
          ""s3:ListBucketMultipartUploads"",
          ""s3:PutObject""
        ]
        Resource = [
          aws_s3_bucket.firehose_bucket.arn,
          ""${aws_s3_bucket.firehose_bucket.arn}/*""
        ]
      }
    ]
  })
}

# Kinesis Firehose Delivery Stream
resource ""aws_kinesis_firehose_delivery_stream"" ""extended_s3_stream"" {
  name        = ""my-firehose-stream""
  destination = ""extended_s3""

  extended_s3_configuration {
    role_arn   = aws_iam_role.firehose_role.arn
    bucket_arn = aws_s3_bucket.firehose_bucket.arn
    
    buffer_size        = 5
    buffer_interval    = 300
    compression_format = ""GZIP""

    prefix = ""raw-data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/""
    error_output_prefix = ""errors/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/!{firehose:error-output-type}/""

    processing_configuration {
      enabled = true

      processors {
        type = ""MetadataExtraction""
        parameters {
          parameter_name  = ""JsonParsingEngine""
          parameter_value = ""JQ-1.6""
        }
      }
    }

    cloudwatch_logging_options {
      enabled         = true
      log_group_name  = ""/aws/firehose/my-firehose-stream""
      log_stream_name = ""S3Delivery""
    }
  }
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates an S3 bucket as the destination
3. Creates an IAM role and policy for Firehose to access S3
4. Creates a Kinesis Firehose Delivery Stream with:
   - Extended S3 destination configuration
   - Buffer settings (5MB or 300 seconds)
   - GZIP compression
   - Dynamic prefixing for data organization
   - Error output handling
   - Basic data processing configuration
   - CloudWatch logging enabled

The delivery stream will buffer",,,,,,"I'll help you create a Terraform configuration for an AWS Kinesis Firehose Delivery Stream with an S3 destination. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 Bucket for Destination
resource ""aws_s3_bucket"" ""destination"" {
  bucket = ""my-firehose-destination-bucket""
}

# IAM Role for Firehose
resource ""aws_iam_role"" ""firehose_role"" {
  name = ""firehose_delivery_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""firehose.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Policy for Firehose
resource ""aws_iam_role_policy"" ""firehose_policy"" {
  name = ""firehose_delivery_policy""
  role = aws_iam_role.firehose_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:AbortMultipartUpload"",
          ""s3:GetBucketLocation"",
          ""s3:GetObject"",
          ""s3:ListBucket"",
          ""s3:ListBucketMultipartUploads"",
          ""s3:PutObject""
        ]
        Resource = [
          aws_s3_bucket.destination.arn,
          ""${aws_s3_bucket.destination.arn}/*""
        ]
      }
    ]
  })
}

# Kinesis Firehose Delivery Stream
resource ""aws_kinesis_firehose_delivery_stream"" ""extended_s3_stream"" {
  name        = ""my-firehose-stream""
  destination = ""extended_s3""

  extended_s3_configuration {
    role_arn   = aws_iam_role.firehose_role.arn
    bucket_arn = aws_s3_bucket.destination.arn

    buffering_size = 64
    buffering_interval = 60

    prefix = ""raw-data/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/""
    error_output_prefix = ""errors/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/!{firehose:error-output-type}/""

    compression_format = ""GZIP""
  }
}
```

This configuration creates:
1. An S3 bucket as the destination
2. An IAM role and policy for Firehose to access S3
3. A Kinesis Firehose delivery stream with extended S3 configuration

The Firehose stream includes:
- GZIP compression
- 64MB buffering size
- 60-second buffering interval
- Dynamic prefixing based on timestamp
- Error output handling
- Proper IAM permissions for Firehose to write to S3

Remember to adjust the bucket name, region, and other parameters according to your specific needs.",,,,,
"aws_network_acl, aws_subnet, aws_vpc",Create an AWS VPC resource with a subnet and associated network ACLs to control both ingress and egress traffic,"package vpc_net_acl
import future.keywords.in

default valid := false

valid {
        some net_acl_resource in input.configuration.root_module.resources
        net_acl_resource.type == ""aws_network_acl""
        net_acl_resource.expressions.egress
        net_acl_resource.expressions.ingress
   
        some vpc_resource in input.configuration.root_module.resources
        vpc_resource.type == ""aws_vpc""
        vpc_resource.address in net_acl_resource.expressions.vpc_id.references

        some subnet in input.configuration.root_module.resources
        subnet.type == ""aws_subnet""
        vpc_resource.address in subnet.expressions.vpc_id.references
}",3,"resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""snet"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_network_acl"" ""ntwk_acl"" {
  vpc_id      = aws_vpc.main.id

  egress {
    protocol   = ""tcp""
    rule_no    = 200
    action     = ""allow""
    cidr_block = ""10.3.0.0/18""
    from_port  = 443
    to_port    = 443
   }

   ingress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""10.3.0.0/18""
    from_port  = 80
    to_port    = 80
  }
}","Has one resource ""aws_vpc""

Has one resource ""aws_subnet""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id""

Has one resource ""aws_network_acl""
with one ""vpc_id"" with value ""aws_vpc.{VPC NAME}.id""
with one ""egress"" with correct parameters
with one ""ingress"" with correct parameters","I'll help you create a Terraform configuration for an AWS VPC with a subnet and network ACLs to control both ingress and egress traffic. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create Subnet
resource ""aws_subnet"" ""main"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""main-subnet""
  }
}

# Create Network ACL
resource ""aws_network_acl"" ""main"" {
  vpc_id = aws_vpc.main.id

  # Ingress rules
  ingress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  ingress {
    protocol   = ""tcp""
    rule_no    = 110
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  ingress {
    protocol   = ""tcp""
    rule_no    = 120
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 22
    to_port    = 22
  }

  # Egress rules
  egress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  egress {
    protocol   = ""tcp""
    rule_no    = 110
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  egress {
    protocol   = ""-1""
    rule_no    = 120
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 0
    to_port    = 0
  }

  tags = {
    Name = ""main-nacl""
  }
}

# Associate Network ACL with Subnet
resource ""aws_network_acl_association"" ""main"" {
  network_acl_id = aws_network_acl.main.id
  subnet_id      = aws_subnet.main.id
}
```

This configuration:
1. Creates a VPC with CIDR block 10.0.0.0/16
2. Creates a subnet with CIDR block 10.0.1.0/24
3. Creates a Network ACL with:
   - Ingress rules allowing HTTP (80), HTTPS (443), and SSH (22)
   - Egress rules allowing HTTP (80), HTTPS (443), and all outbound traffic
4. Associates the Network ACL with the subnet

The Network ACL rules are configured to allow basic web traffic and SSH access while maintaining security. You can modify the rules according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS VPC with a subnet and network ACLs. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create Subnet
resource ""aws_subnet"" ""main"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""main-subnet""
  }
}

# Create Network ACL
resource ""aws_network_acl"" ""main"" {
  vpc_id = aws_vpc.main.id

  # Ingress rules
  ingress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  ingress {
    protocol   = ""tcp""
    rule_no    = 200
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  # SSH access
  ingress {
    protocol   = ""tcp""
    rule_no    = 300
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 22
    to_port    = 22
  }

  # Egress rules
  egress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  egress {
    protocol   = ""tcp""
    rule_no    = 200
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  # Allow all outbound responses
  egress {
    protocol   = ""tcp""
    rule_no    = 300
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 1024
    to_port    = 65535
  }

  tags = {
    Name = ""main-nacl""
  }
}

# Associate NACL with subnet
resource ""aws_network_acl_association"" ""main"" {
  network_acl_id = aws_network_acl.main.id
  subnet_id      = aws_subnet.main.id
}
```

This configuration:
1. Creates a VPC with CIDR block 10.0.0.0/16
2. Creates a subnet with CIDR block 10.0.1.0/24
3. Creates a Network ACL with:
   - Ingress rules allowing HTTP (80), HTTPS (443), and SSH (22)
   - Egress rules allowing HTTP (80), HTTPS (443), and ephemeral ports (1024-65535)
4. Associates the Network ACL with the subnet

The configuration includes proper tagging and follows AWS best practices for network security. You can customize the CIDR blocks and rules according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS VPC with a subnet and network ACLs. This solution will include both ingress and egress rules for network security.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create Subnet
resource ""aws_subnet"" ""main"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""main-subnet""
  }
}

# Create Network ACL
resource ""aws_network_acl"" ""main"" {
  vpc_id = aws_vpc.main.id

  # Ingress rules
  ingress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  ingress {
    protocol   = ""tcp""
    rule_no    = 200
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  # Allow SSH access
  ingress {
    protocol   = ""tcp""
    rule_no    = 300
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 22
    to_port    = 22
  }

  # Egress rules
  egress {
    protocol   = ""tcp""
    rule_no    = 100
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 80
    to_port    = 80
  }

  egress {
    protocol   = ""tcp""
    rule_no    = 200
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 443
    to_port    = 443
  }

  # Allow all outbound responses
  egress {
    protocol   = ""tcp""
    rule_no    = 300
    action     = ""allow""
    cidr_block = ""0.0.0.0/0""
    from_port  = 1024
    to_port    = 65535
  }

  tags = {
    Name = ""main-nacl""
  }
}

# Associate the NACL with the subnet
resource ""aws_network_acl_association"" ""main"" {
  network_acl_id = aws_network_acl.main.id
  subnet_id      = aws_subnet.main.id
}
```

This configuration creates:
1. A VPC with CIDR block 10.0.0.0/16
2. A subnet with CIDR block 10.0.1.0/24
3. A Network ACL with:
   - Ingress rules allowing HTTP (80), HTTPS (443), and SSH (22)
   - Egress rules allowing HTTP (80), HTTPS (443), and ephemeral ports (1024-65535)
4. Associates the Network ACL with the subnet

The Network ACL rules are stateless and control both incoming and outgoing traffic. The configuration includes common ports for web traffic and SSH access, while allowing return traffic through ephemeral ports.

Remember to adjust the CIDR blocks and rules according to your specific security requirements.",,,,,
aws_db_instance,"create a basic AWS RDS instance, with io1 storage type","package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
	resource := input.configuration.root_module.resources[_]
	resource.type == ""aws_db_instance""
	requirement1(resource.expressions)
	requirement2(resource.expressions)
    requirement3(resource.expressions)
    resource.expressions.storage_type.constant_value == ""io1""
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
	expressions.allocated_storage.constant_value >= 100
    expressions.allocated_storage.constant_value <= 65536
	expressions.engine
    expressions.username
	is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
	expressions.instance_class
	is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
	expressions.password
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

requirement3(expressions) {
	expressions.snapshot_identifier
}

requirement3(expressions) {
	expressions.replicate_source_db
}

requirement3(expressions) {
	expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
	engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
	engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
	instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
	startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 100
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
  storage_type        = ""io1""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""","I'll help you create a Terraform configuration for an AWS RDS instance with io1 (Provisioned IOPS) storage type. Here's a basic configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_db_instance"" ""example"" {
  identifier           = ""example-rds-instance""
  engine              = ""mysql""                   # Change as needed (e.g., postgresql, oracle-se2)
  engine_version      = ""8.0.28""                 # Specify your desired version
  instance_class      = ""db.t3.medium""           # Change as needed
  allocated_storage   = 100                      # Size in GB
  storage_type        = ""io1""                    # Specifying io1 storage type
  iops                = 3000                     # Required for io1. Value between 1000-80000
  
  db_name             = ""exampledb""
  username            = ""admin""
  password            = ""your-password-here""      # Consider using variables or secrets management
  
  skip_final_snapshot = true                     # Set to false in production
  
  # Basic network settings
  publicly_accessible    = false
  vpc_security_group_ids = []                    # Add your security group IDs
  db_subnet_group_name   = null                  # Specify your subnet group name if needed

  # Backup settings
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  tags = {
    Name        = ""example-rds""
    Environment = ""development""
  }
}
```

This configuration creates an RDS instance with the following key features:
- Uses io1 storage type with provisioned IOPS
- Sets up basic database configuration including engine, storage, and credentials
- Includes basic backup and maintenance windows
- Configurable network settings

Remember to:
1. Replace the password with a secure value
2. Adjust the storage size and IOPS according to your needs
3. Configure proper security groups and subnet groups
4. Modify the instance class based on your performance requirements
5. Update the region to match your deployment location
6. In production, set `skip_final_snapshot` to false and provide a `final_snapshot_identifier`

The io1 storage type is designed for I/O-intensive workloads that require consistent performance.",,,,,,"I'll help you create a Terraform configuration for an AWS RDS instance with io1 (Provisioned IOPS) storage type. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_db_instance"" ""main"" {
  identifier           = ""my-rds-instance""
  engine              = ""mysql""                  # You can change this to other supported engines
  engine_version      = ""8.0.28""
  instance_class      = ""db.t3.medium""
  allocated_storage   = 100
  storage_type        = ""io1""
  iops                = 3000                     # Must be between 1000-50000 for io1
  
  # Database credentials
  username            = ""admin""
  password            = ""your-secure-password""   # Better to use secrets management
  
  # Network settings
  publicly_accessible = false
  
  # Backup settings
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  
  # Maintenance settings
  maintenance_window     = ""Mon:04:00-Mon:05:00""
  
  # Performance Insights
  performance_insights_enabled = true
  
  # Enhanced monitoring
  monitoring_interval = 60
  
  # Skip final snapshot when destroying the database
  skip_final_snapshot = true
  
  tags = {
    Name        = ""my-rds-instance""
    Environment = ""production""
  }
}
```

This configuration creates an RDS instance with the following key features:
- Uses io1 storage type with 3000 IOPS
- Allocates 100GB of storage
- Uses MySQL 8.0.28 (you can modify the engine and version as needed)
- Includes basic backup and maintenance windows
- Enables Performance Insights and enhanced monitoring
- Sets up basic security and networking configurations

Remember to:
1. Replace the password with a secure value
2. Adjust the region as needed
3. Modify the instance class based on your requirements
4. Adjust the IOPS value based on your performance needs (must be between 1000-50000 for io1)
5. Consider adding additional security configurations like VPC settings, security groups, and encryption options based on your needs",,,,,,"I'll help you create a Terraform configuration for an AWS RDS instance with io1 (Provisioned IOPS) storage type. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_db_instance"" ""main"" {
  identifier        = ""my-rds-instance""
  engine            = ""mysql""
  engine_version    = ""8.0.28""
  instance_class    = ""db.t3.medium""
  allocated_storage = 100
  storage_type      = ""io1""
  iops             = 3000  # Must be between 1000-50000 for io1

  # Database settings
  db_name             = ""mydb""
  username            = ""admin""
  password            = ""your-secure-password""
  port                = 3306

  # Network settings
  publicly_accessible    = false
  multi_az              = false

  # Backup and maintenance
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  # Security
  storage_encrypted     = true
  skip_final_snapshot   = true

  tags = {
    Name        = ""my-rds-instance""
    Environment = ""production""
  }
}
```

This configuration creates an RDS instance with the following features:
- Uses io1 storage type with 3000 IOPS (you can adjust this based on your needs)
- 100GB of allocated storage
- MySQL 8.0.28 as the database engine
- Encrypted storage
- Basic backup configuration
- Standard maintenance windows
- Security settings

Remember to:
1. Replace the password with a secure value
2. Adjust the IOPS value based on your performance requirements
3. Modify the instance class based on your workload needs
4. Update the region if needed
5. Consider adding a security group and subnet group if needed

Also note that using io1 storage type will incur additional costs compared to standard storage (gp2).",,,,,
"aws_codebuild_project, aws_iam_policy, aws_iam_role, aws_iam_role_policy_attachment, aws_s3_bucket, aws_security_group, aws_subnet, aws_vpc, aws_iam_policy_document, aws_iam_policy_document",Infrastructure for an autograder for a CS class. The autograde needs to run students' code stored on GitHub in a specific environment and store the result. Make sure that the autograder prevents the students' code from reaching the internet. Accomplish this using AWS CodeBuild and AWS VPC.,"package autograder_high_jail

import rego.v1

codebuild_project_valid(codebuild_project, security_group, subnet, vpc) if {
	some artifact in codebuild_project.expressions.artifacts
	artifact.location
	artifact.type
	artifact.name

	some environment in codebuild_project.expressions.environment
	environment.compute_type
	environment.image
	environment.type

	some source in codebuild_project.expressions.source
	source.type.constant_value == ""GITHUB""
	source.location

	some vpc_config in codebuild_project.expressions.vpc_config
	security_group.address in vpc_config.security_group_ids.references
	subnet.address in vpc_config.subnets.references
	vpc.address in vpc_config.vpc_id.references
}

security_group_valid(security_group, vpc) if {
	vpc.address in security_group.expressions.vpc_id.references
}

subnet_valid(subnet, vpc) if {
	subnet.expressions.cidr_block
	vpc.address in subnet.expressions.vpc_id.references
}

vpc_valid(vpc) if {
	vpc.expressions.cidr_block
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources
	some codebuild_project in resources
	codebuild_project.type == ""aws_codebuild_project""
	some security_group in resources
	security_group.type == ""aws_security_group""
	some subnet in resources
	subnet.type == ""aws_subnet""
	some vpc in resources
	vpc.type == ""aws_vpc""
	codebuild_project_valid(codebuild_project, security_group, subnet, vpc)
	security_group_valid(security_group, vpc)
	subnet_valid(subnet, vpc)
	vpc_valid(vpc)
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""artifact-bucket-""
}

resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder_build""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip"" # include this
  }

  environment { # arguments required, exact value not specified
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1 # good to have, not required
    location        = ""github.com/source-location""
  }

  vpc_config {
    vpc_id             = aws_vpc.autograder_vpc.id
    subnets            = [aws_subnet.autograder_vpc_subnet.id]
    security_group_ids = [aws_security_group.autograder_vpc_securitygroup.id]
  }
}

resource ""aws_vpc"" ""autograder_vpc"" {
  cidr_block = ""10.0.0.0/16"" # extra value not specified
}

resource ""aws_subnet"" ""autograder_vpc_subnet"" {
  vpc_id     = aws_vpc.autograder_vpc.id
  cidr_block = ""10.0.0.0/24"" # include this
}

resource ""aws_security_group"" ""autograder_vpc_securitygroup"" {
  vpc_id = aws_vpc.autograder_vpc.id
}

resource ""aws_iam_role"" ""autograder_build_role"" {
  assume_role_policy = data.aws_iam_policy_document.autograder_build_policy_assume.json
}

data ""aws_iam_policy_document"" ""autograder_build_policy_assume"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }
  }
}

data ""aws_iam_policy_document"" ""autograder_build_policy"" {
  statement {
    effect = ""Allow""
    actions = [
      ""ec2:DescribeSubnets"",
      ""ec2:DescribeSecurityGroups"",
      ""ec2:DescribeVpcs""
    ]
    resources = [
      ""*""
    ]
  }

  statement {
    effect  = ""Allow""
    actions = [""s3:*""]
    resources = [
      aws_s3_bucket.artifact_bucket.arn,
      ""${aws_s3_bucket.artifact_bucket.arn}/*"",
    ]
  }
}

resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name        = ""lambda_policy""
  description = ""Grants permissions to Lambda to describe vpc, subnet, security group""

  policy = data.aws_iam_policy_document.autograder_build_policy.json
}

resource ""aws_iam_role_policy_attachment"" ""lambda_policy_attach"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}","have one ""aws_codebuild_project"" resource
  with ""artifacts"" argument block
    with ""type"" argument with value ""S3""
    with ""location"" argument
    with ""name"" argument
  with ""environment"" argument block
    with ""computer_type"" argument
    with ""image"" argument
    with ""type"" argument
  with ""source"" argument block
    with ""type"" argument with value ""GITHUB""
    with ""location"" argument
  with ""vpc_config"" argument block
    with ""vpc_id"" argument
    with ""subnets"" argument
    with ""security_group_ids"" argument
have one ""aws_vpc"" resource
  with ""cidr_block"" argument
have one ""aws_subnet"" argument
  with ""vpc_id"" argument
  with ""cidr_block"" argument
with one ""aws_security_group"" resource
  with ""vpc_id"" argument","I'll help you create a Terraform configuration for an autograder system using AWS CodeBuild in a VPC with restricted internet access. Here's a solution that includes a VPC, CodeBuild project, and necessary IAM roles:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""autograder_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""autograder-vpc""
  }
}

# Private Subnet
resource ""aws_subnet"" ""private_subnet"" {
  vpc_id            = aws_vpc.autograder_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""autograder-private-subnet""
  }
}

# VPC Endpoint for CodeBuild
resource ""aws_vpc_endpoint"" ""codebuild"" {
  vpc_id              = aws_vpc.autograder_vpc.id
  service_name        = ""com.amazonaws.us-west-2.codebuild""
  vpc_endpoint_type   = ""Interface""
  subnet_ids          = [aws_subnet.private_subnet.id]
  security_group_ids  = [aws_security_group.codebuild_sg.id]
  private_dns_enabled = true
}

# Security Group for CodeBuild
resource ""aws_security_group"" ""codebuild_sg"" {
  name        = ""codebuild-sg""
  description = ""Security group for CodeBuild""
  vpc_id      = aws_vpc.autograder_vpc.id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# S3 Bucket for build artifacts
resource ""aws_s3_bucket"" ""build_artifacts"" {
  bucket = ""autograder-build-artifacts""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""autograder-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.build_artifacts.arn}"",
          ""${aws_s3_bucket.build_artifacts.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = """,,,,,,"I'll help you create a Terraform configuration for an autograder system using AWS CodeBuild and VPC with proper network isolation. Here's a solution that will:
1. Create a VPC with private subnets (no internet access)
2. Set up AWS CodeBuild with VPC configuration
3. Configure necessary IAM roles and policies
4. Ensure the build environment can't access the internet

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""autograder_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""autograder-vpc""
  }
}

# Private Subnet
resource ""aws_subnet"" ""private_subnet"" {
  vpc_id            = aws_vpc.autograder_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""autograder-private-subnet""
  }
}

# Security Group for CodeBuild
resource ""aws_security_group"" ""codebuild_sg"" {
  name        = ""codebuild-sg""
  description = ""Security group for CodeBuild""
  vpc_id      = aws_vpc.autograder_vpc.id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""10.0.0.0/16""]  # Only allow internal VPC communication
  }
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""autograder-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""ecr:GetAuthorizationToken"",
          ""ecr:BatchCheckLayerAvailability"",
          ""ecr:GetDownloadUrlForLayer"",
          ""ecr:BatchGetImage""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""ec2:CreateNetworkInterface"",
          ""ec2:DescribeDhcpOptions"",
          ""ec2:DescribeNetworkInterfaces"",
          ""ec2:DeleteNetworkInterface"",
          ""ec2:DescribeSubnets"",
          ""ec2:DescribeSecurityGroups"",
          ""ec2:DescribeVpcs""
        ]
      }
    ]
  })
}

# S3 Bucket for storing results
resource ""aws_s3_bucket"" ""results_bucket"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""results_versioning"" {
  bucket = aws_s3_bucket.results_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""autograder"" {",,,,,,"I'll help you create a Terraform configuration for an autograder system using AWS CodeBuild in a VPC with restricted internet access. Here's a solution that includes a VPC with private subnets, CodeBuild project, and necessary IAM roles:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""autograder_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""autograder-vpc""
  }
}

# Private Subnet
resource ""aws_subnet"" ""private_subnet"" {
  vpc_id            = aws_vpc.autograder_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""autograder-private-subnet""
  }
}

# VPC Endpoint for CodeBuild
resource ""aws_vpc_endpoint"" ""codebuild"" {
  vpc_id              = aws_vpc.autograder_vpc.id
  service_name        = ""com.amazonaws.us-west-2.codebuild""
  vpc_endpoint_type   = ""Interface""
  subnet_ids          = [aws_subnet.private_subnet.id]
  private_dns_enabled = true

  security_group_ids = [aws_security_group.codebuild_sg.id]
}

# Security Group for CodeBuild
resource ""aws_security_group"" ""codebuild_sg"" {
  name        = ""codebuild-sg""
  description = ""Security group for CodeBuild""
  vpc_id      = aws_vpc.autograder_vpc.id

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""autograder-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:GetObjectVersion""
        ]
      }
    ]
  })
}

# S3 Bucket for storing results
resource ""aws_s3_bucket"" ""results_bucket"" {
  bucket = ""autograder-results-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class""
  service_role  = aws_iam_role.codebuild_role.arn
  build_timeout = ""30""

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codeb",,,,,
"aws_iam_group, aws_iam_group_membership, aws_iam_user, aws_iam_user",Create two basic AWS IAM users that are a part of a basic IAM group,"package iam_group_two_users
import future.keywords.in

default valid := false

valid {
    some user1 in input.configuration.root_module.resources
    user1.type == ""aws_iam_user""

    some user2 in input.configuration.root_module.resources
    user2.type == ""aws_iam_user""

    not user1 == user2

    some group in input.configuration.root_module.resources
    group.type == ""aws_iam_group""

    some group_membership in input.configuration.root_module.resources
    group_membership.type == ""aws_iam_group_membership""
    user1.address in group_membership.expressions.users.references
    user2.address in group_membership.expressions.users.references
    group.address in group_membership.expressions.group.references
}",3,"resource ""aws_iam_group_membership"" ""team"" {
  name = ""tf-testing-group-membership""

  users = [
    aws_iam_user.user_one.name,
    aws_iam_user.user_two.name,
  ]

  group = aws_iam_group.group.name
}

resource ""aws_iam_group"" ""group"" {
  name = ""test-group""
}

resource ""aws_iam_user"" ""user_one"" {
  name = ""test-user""
}

resource ""aws_iam_user"" ""user_two"" {
  name = ""test-user-two""
}","Has one ""aws_iam_group"" resource
with one ""name"" with any value

Has one ""aws_iam_user"" resource
with one ""name"" with any value

Has another ""aws_iam_user"" resource
with one ""name"" with any value

Has one ""aws_iam_group_membership"" resource
with one ""name"" with any value
with one ""users"" with the two user resource names
with one ""group"" with the group resource name","I'll help you create a Terraform configuration that creates two IAM users and adds them to an IAM group. Here's a solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create IAM group
resource ""aws_iam_group"" ""basic_group"" {
  name = ""basic_users_group""
}

# Create first IAM user
resource ""aws_iam_user"" ""user1"" {
  name = ""basic_user1""
  tags = {
    Description = ""Basic IAM User 1""
  }
}

# Create second IAM user
resource ""aws_iam_user"" ""user2"" {
  name = ""basic_user2""
  tags = {
    Description = ""Basic IAM User 2""
  }
}

# Add first user to the group
resource ""aws_iam_user_group_membership"" ""user1_membership"" {
  user = aws_iam_user.user1.name
  groups = [aws_iam_group.basic_group.name]
}

# Add second user to the group
resource ""aws_iam_user_group_membership"" ""user2_membership"" {
  user = aws_iam_user.user2.name
  groups = [aws_iam_group.basic_group.name]
}
```

This configuration will:
1. Create an IAM group named ""basic_users_group""
2. Create two IAM users: ""basic_user1"" and ""basic_user2""
3. Add both users to the created IAM group
4. Include basic tags for better resource management
5. Use the AWS provider with version 4.0 or higher

Each resource is properly defined and linked together using resource references. The membership resources ensure that both users are added to the group.",,,,,,"I'll help you create a Terraform configuration to create two IAM users and add them to a single IAM group.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM group
resource ""aws_iam_group"" ""basic_group"" {
  name = ""basic-users-group""
}

# Create first IAM user
resource ""aws_iam_user"" ""user1"" {
  name = ""basic-user-1""
  tags = {
    Description = ""Basic IAM User 1""
  }
}

# Create second IAM user
resource ""aws_iam_user"" ""user2"" {
  name = ""basic-user-2""
  tags = {
    Description = ""Basic IAM User 2""
  }
}

# Add users to the group
resource ""aws_iam_group_membership"" ""group_membership"" {
  name = ""basic-group-membership""

  users = [
    aws_iam_user.user1.name,
    aws_iam_user.user2.name
  ]

  group = aws_iam_group.basic_group.name
}
```

This configuration will:
1. Set up the AWS provider
2. Create a basic IAM group called ""basic-users-group""
3. Create two IAM users: ""basic-user-1"" and ""basic-user-2""
4. Add both users to the created IAM group using group membership resource

The configuration uses proper naming conventions and includes tags for better resource management. Each resource is properly referenced using the Terraform resource naming convention.",,,,,,"I'll help you create a Terraform configuration to create two IAM users and add them to an IAM group. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create an IAM group
resource ""aws_iam_group"" ""basic_group"" {
  name = ""basic_users_group""
  path = ""/""
}

# Create first IAM user
resource ""aws_iam_user"" ""user1"" {
  name = ""basic_user1""
  path = ""/""
}

# Create second IAM user
resource ""aws_iam_user"" ""user2"" {
  name = ""basic_user2""
  path = ""/""
}

# Add first user to the group
resource ""aws_iam_user_group_membership"" ""user1_membership"" {
  user   = aws_iam_user.user1.name
  groups = [aws_iam_group.basic_group.name]
}

# Add second user to the group
resource ""aws_iam_user_group_membership"" ""user2_membership"" {
  user   = aws_iam_user.user2.name
  groups = [aws_iam_group.basic_group.name]
}
```

This configuration will:
1. Create an IAM group named ""basic_users_group""
2. Create two IAM users: ""basic_user1"" and ""basic_user2""
3. Add both users to the created IAM group

The configuration includes the required AWS provider block and creates all necessary resources to establish the IAM users and group relationship. Each resource is properly defined with unique names and references to maintain dependencies.",,,,,
"aws_codebuild_project, aws_iam_role",create a Basic AWS codebuild project resource with example iam role and an example GITHUB source,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for aws_iam_role resource
has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""codebuild.amazonaws.com"") 
} 

has_valid_codebuild {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_codebuild_project""
        resource.values.artifacts
    resource.values.environment
    resource.values.name
    resource.values.source[_].type == ""GITHUB""
    role := input.configuration.root_module.resources[i]
    role.expressions.service_role
}

has_valid_resources {
        has_valid_iam_role(input.resource_changes)
    has_valid_codebuild
}
",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""test_role"" {
  name               = ""test_role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}


resource ""aws_codebuild_project"" ""example"" {
  name          = ""test-project""
  service_role  = aws_iam_role.test_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    location        = ""github.com/source-location""
    git_clone_depth = 1
  }
}","Has one ""aws_iam_role"" resource and one ""aws_codebuild_project"" resource with the ""type"" argument within the ""source"" block equal to ""GITHUB""","I'll help you create a Terraform configuration for an AWS CodeBuild project with a GitHub source and the necessary IAM role. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""example-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for the CodeBuild role
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""example-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:PutObject""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""example"" {
  name          = ""example-project""
  description   = ""Example CodeBuild project""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  source_version = ""main""

  vpc_config {
    vpc_id = ""vpc-12345678"" # Replace with your VPC ID

    subnets = [
      ""subnet-12345678"", # Replace with your subnet IDs
    ]

    security_group_ids = [
      ""sg-12345678"", # Replace with your security group IDs
    ]
  }

  tags = {
    Environment = ""Development""
  }
}

# GitHub webhook for automated builds
resource ""aws_codebuild_webhook"" ""example"" {
  project_name = aws_codebuild_project.example.name

  filter_group {
    filter {
      type    = ""EVENT""
      pattern = ""PUSH""
    }

    filter {
      type    = ""HEAD_REF""
      pattern = ""main""
    }
  }
}
```

This configuration includes:

1. Required AWS provider declaration
2. An IAM role for CodeBuild with basic permissions
3. A CodeBuild project configuration with:
   - GitHub as the source provider
   - Linux container environment
   - Basic VPC configuration
   - No artifacts (can be modified as needed)
4. A webhook configuration for automated builds on GitHub push events

Remember to:
- Replace the GitHub repository URL with your actual repository
- Update the VPC, subnet, and security group IDs with your actual values
- Adjust the IAM permissions based on your specific needs
- Modify the build environment and compute type if needed
- Update the source_version to match",,,,,,"I'll help you create a Terraform configuration for an AWS CodeBuild project with an IAM role and GitHub source. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""example-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""example-codebuild-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""example"" {
  name          = ""example-project""
  description   = ""Example CodeBuild project with GitHub source""
  build_timeout = ""30""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/amazonlinux2-x86_64-standard:3.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  source_version = ""main""

  vpc_config {
    vpc_id = ""vpc-12345678"" # Replace with your VPC ID

    subnets = [
      ""subnet-12345678"" # Replace with your subnet IDs
    ]

    security_group_ids = [
      ""sg-12345678"" # Replace with your security group IDs
    ]
  }

  tags = {
    Environment = ""Test""
  }
}

# GitHub Webhook for CodeBuild
resource ""aws_codebuild_webhook"" ""example"" {
  project_name = aws_codebuild_project.example.name
  
  filter_group {
    filter {
      type    = ""EVENT""
      pattern = ""PUSH""
    }

    filter {
      type    = ""HEAD_REF""
      pattern = ""main""
    }
  }
}
```

This configuration includes:

1. AWS provider configuration
2. IAM role and policy for CodeBuild
3. CodeBuild project with GitHub source configuration
4. Basic webhook configuration for GitHub integration
5. VPC configuration for the CodeBuild project
6. Common environment settings and build specifications

Key features:
- Uses Amazon Linux 2 as the build environment
- Sets up GitHub as the source repository
- Includes basic CloudWatch Logs permissions in the IAM role
- Configures webhook for automatic builds on GitHub push events
- Includes VPC configuration (replace the placeholder IDs with your actual VPC, subnet, and security group IDs)

Remember to:
1. Replace the GitHub repository URL with your actual repository
2. Update the VPC, subnet, and security group IDs
3. Adjust the IAM permissions based on your specific needs
4.",,,,,,"I'll help you create a Terraform configuration for an AWS CodeBuild project with a GitHub source and the necessary IAM role. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-service-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for the CodeBuild role
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""codebuild-service-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:*"",
          ""codecommit:*""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""project"" {
  name           = ""example-project""
  description    = ""Example CodeBuild project with GitHub source""
  build_timeout  = ""60""
  service_role   = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  source_version = ""main""

  logs_config {
    cloudwatch_logs {
      group_name  = ""example-codebuild-log-group""
      stream_name = ""example-codebuild-log-stream""
      status      = ""ENABLED""
    }
  }

  tags = {
    Environment = ""Development""
  }
}

# GitHub webhook for automated builds (optional)
resource ""aws_codebuild_webhook"" ""example"" {
  project_name = aws_codebuild_project.project.name
  
  filter_group {
    filter {
      type    = ""EVENT""
      pattern = ""PUSH""
    }

    filter {
      type    = ""HEAD_REF""
      pattern = ""main""
    }
  }
}
```

This configuration includes:

1. The required AWS provider configuration
2. An IAM role and policy for CodeBuild
3. A CodeBuild project with GitHub source configuration
4. Basic logging configuration
5. A webhook configuration for automated builds on GitHub push events

To use this configuration:

1. Replace the region with your desired AWS region
2. Update the GitHub repository URL in the source block
3. Adjust the build environment settings if needed
4. Modify the IAM policy permissions according to your security requirements
5. Update the tags as needed

Remember to have your GitHub credentials configured properly in AWS CodeBuild to access your repository. Also, ensure you have the necessary GitHub permissions to create webhooks if you're using the webhook resource.",,,,,
"aws_chime_voice_connector, aws_chime_voice_connector, aws_chime_voice_connector_group","Create two AWS Chime Voice Connectors, both with encryption. Then create an aws_chime_voice_connector_group resource with each connector as a connector argument","package aws_chime_voice_connector
import future.keywords.in

default valid := false

valid {
    some vc1 in input.configuration.root_module.resources
    vc1.type == ""aws_chime_voice_connector""
    vc1.expressions.require_encryption.constant_value == true

    some vc2 in input.configuration.root_module.resources
    vc2.type == ""aws_chime_voice_connector""
    vc2.expressions.require_encryption.constant_value == true

    not vc1 == vc2
    
    some vcgroup in input.configuration.root_module.resources
    vcgroup.type == ""aws_chime_voice_connector_group""

    some connector1 in vcgroup.expressions.connector
    vc1.address in connector1.voice_connector_id.references
    some connector2 in vcgroup.expressions.connector
    vc2.address in connector2.voice_connector_id.references
    
    not connector1 == connector2
}
",3,"resource ""aws_chime_voice_connector"" ""vc1"" {
  name               = ""connector-test-1""
  require_encryption = true
  aws_region         = ""us-east-1""
}

resource ""aws_chime_voice_connector"" ""vc2"" {
  name               = ""connector-test-2""
  require_encryption = true
  aws_region         = ""us-west-2""
}

resource ""aws_chime_voice_connector_group"" ""group"" {
  name = ""test-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.vc1.id
    priority           = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.vc2.id
    priority           = 3
  }
}","Has one resource ""aws_chime_voice_connector""
with require_encryption=""true""

Has another resource ""aws_chime_voice_connector""
with require_encryption=""true""

Has one resource ""aws_chime_voice_connector_group""
with a connector for each voice connector with the correct id","I'll help you create a Terraform configuration for two AWS Chime Voice Connectors with encryption and group them together in a voice connector group.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Specify your desired region
}

# First Voice Connector
resource ""aws_chime_voice_connector"" ""connector_1"" {
  name               = ""voice-connector-1""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Second Voice Connector
resource ""aws_chime_voice_connector"" ""connector_2"" {
  name               = ""voice-connector-2""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Voice Connector Group
resource ""aws_chime_voice_connector_group"" ""connector_group"" {
  name = ""voice-connector-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_2.id
    priority          = 2
  }
}
```

This configuration:
1. Creates two AWS Chime Voice Connectors with encryption enabled (`require_encryption = true`)
2. Creates a Voice Connector Group that includes both connectors
3. Assigns priorities to each connector in the group (1 and 2)

The voice connectors are referenced in the group using their IDs, and each connector in the group has a priority value that determines the order in which they are used.",,,,,,"I'll help you create two AWS Chime Voice Connectors with encryption and group them together using an aws_chime_voice_connector_group resource. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # AWS Chime resources are only available in us-east-1
}

# First Voice Connector
resource ""aws_chime_voice_connector"" ""connector_1"" {
  name               = ""voice-connector-1""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Second Voice Connector
resource ""aws_chime_voice_connector"" ""connector_2"" {
  name               = ""voice-connector-2""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Voice Connector Group
resource ""aws_chime_voice_connector_group"" ""connector_group"" {
  name = ""voice-connector-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_2.id
    priority          = 2
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two voice connectors with encryption enabled
3. Creates a voice connector group that includes both connectors with different priorities
4. Both voice connectors are created in us-east-1 as that's the only region that supports AWS Chime resources
5. The connectors are referenced in the group using their IDs
6. Priority values determine the order in which the connectors are used (lower number = higher priority)",,,,,,"I'll help you create a Terraform configuration for two AWS Chime Voice Connectors with encryption and group them together using an aws_chime_voice_connector_group resource.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # AWS Chime resources are only available in us-east-1
}

# First Voice Connector
resource ""aws_chime_voice_connector"" ""connector_1"" {
  name               = ""voice-connector-1""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Second Voice Connector
resource ""aws_chime_voice_connector"" ""connector_2"" {
  name               = ""voice-connector-2""
  aws_region         = ""us-east-1""
  require_encryption = true
}

# Voice Connector Group
resource ""aws_chime_voice_connector_group"" ""connector_group"" {
  name = ""voice-connector-group""

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_1.id
    priority          = 1
  }

  connector {
    voice_connector_id = aws_chime_voice_connector.connector_2.id
    priority          = 2
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two voice connectors with encryption enabled
3. Creates a voice connector group that includes both connectors with different priorities
4. Both voice connectors are set to require encryption (require_encryption = true)
5. The connectors are added to the group with different priorities (1 and 2)

The voice connector group can be used for routing and failover scenarios in your AWS Chime implementation.",,,,,
"aws_cloudfront_distribution, aws_cloudfront_origin_access_control, aws_route53_record, aws_route53_zone, aws_s3_bucket","The infrastructure for a video streaming site that distribute video content globally. Accomplish this using the following services: AWS CloudFront, AWS Route53, and AWS S3. Add any necessary services.","package netflix_middle

import data.set
import rego.v1

bucket_valid(bucket) := true

access_control_valid(access_control) if {
	access_control.expressions.name
	access_control.expressions.origin_access_control_origin_type.constant_value == ""s3""
	access_control.expressions.signing_behavior
	access_control.expressions.signing_protocol
}

distribution_valid(distribution, access_control, bucket) if {
	some origin in distribution.expressions.origin
	bucket.address in origin.domain_name.references
	access_control.address in origin.origin_access_control_id.references

	some cache_behavior in distribution.expressions.default_cache_behavior
	{method | method := cache_behavior.allowed_methods.constant_value[_]} == {""GET"", ""HEAD""}
	{method | method := cache_behavior.cached_methods.constant_value[_]} == {""GET"", ""HEAD""}
	cache_behavior.viewer_protocol_policy.constant_value == ""allow-all""

	origin.origin_id == cache_behavior.target_origin_id

	some restrictions in distribution.expressions.restrictions
	some restriction in restrictions.geo_restriction
	restriction.restriction_type
	restriction.locations
}

zone_valid(zone) if {
	zone.expressions.name
}

record_valid(record, zone, distribution) if {
	record.expressions.type
	record.expressions.name
	zone.address in record.expressions.zone_id.references

	some alias in record.expressions.alias
	distribution.address in alias.name.references
	distribution.address in alias.zone_id.references
	alias.evaluate_target_health
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	some access_control in resources
	access_control.type == ""aws_cloudfront_origin_access_control""

	some distribution in resources
	distribution.type == ""aws_cloudfront_distribution""

	some zone in resources
	zone.type == ""aws_route53_zone""

	some record in resources
	record.type == ""aws_route53_record""

	bucket_valid(bucket)
	access_control_valid(access_control)
	distribution_valid(distribution, access_control, bucket)
	zone_valid(zone)
	record_valid(record, zone, distribution)
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_s3_bucket"" ""video_content"" {
  bucket_prefix = ""video-content-""
}

resource ""aws_s3_object"" ""put_website"" {
  bucket = aws_s3_bucket.video_content.bucket
  key = ""index.html""
  source = ""./supplement/index.html""
  content_type = ""text/html""
}

resource ""aws_s3_object"" ""put_website_css"" {
  bucket = aws_s3_bucket.video_content.bucket
  key = ""css/style.css""
  source = ""./supplement/css/style.css""
  content_type = ""text/css""
}

resource ""aws_s3_bucket_policy"" ""bucket_policy"" {
  bucket = aws_s3_bucket.video_content.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""cloudfront.amazonaws.com""
        }
        Action = ""s3:GetObject""
        Resource = ""${aws_s3_bucket.video_content.arn}/*""
        Condition = {
          StringEquals = {
            ""AWS:SourceArn"" = aws_cloudfront_distribution.my_distribution.arn
          }
        }
      }
    ]
  })
}


locals {
  s3_origin_id = ""s3_video_content_origin""
}

resource ""aws_cloudfront_origin_access_control"" ""s3_oac"" {
  name                              = ""s3-oac""
  description                       = ""aws access control policy""
  origin_access_control_origin_type = ""s3""
  signing_behavior                  = ""always""
  signing_protocol                  = ""sigv4""
}

resource ""aws_cloudfront_distribution"" ""my_distribution"" {
  origin {
    domain_name              = aws_s3_bucket.video_content.bucket_regional_domain_name
    origin_access_control_id = aws_cloudfront_origin_access_control.s3_oac.id
    origin_id                = local.s3_origin_id
  }

  enabled             = true
  default_root_object = ""index.html""

  default_cache_behavior {
    allowed_methods  = [""GET"", ""HEAD""]
    cached_methods   = [""GET"", ""HEAD""]
    target_origin_id = local.s3_origin_id

    forwarded_values {
      query_string = false

      cookies {
        forward = ""none""
      }
    }

    viewer_protocol_policy = ""allow-all""
  }

  price_class = ""PriceClass_200""

  restrictions {
    geo_restriction {
      restriction_type = ""whitelist""
      locations        = [""US"", ""CA"", ""GB"", ""DE""]
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }
}

# Route53
resource ""aws_route53_zone"" ""my_zone"" {
  name = ""my-test-cloudfront.com""
}

resource ""aws_route53_record"" ""cdn_ipv4"" {
  type    = ""A""
  name    = ""cdn""
  zone_id = aws_route53_zone.my_zone.zone_id

  alias {
    name                   = aws_cloudfront_distribution.my_distribution.domain_name
    zone_id                = aws_cloudfront_distribution.my_distribution.hosted_zone_id
    evaluate_target_health = true
  }
}","have one ""aws_cloudfront_origin_access_control"" resource
    with ""name"" argument
    with ""origin_access_control_origin_type"" with value ""s3""
    with ""signing_behavior"" argument
    with ""signing_protocol"" argument
have one ""aws_cloudfront_distribution"" resource
    with ""origin.domain_name"" argument linking to ""aws_s3_bucket"" resource
    with ""origin.origin_access_control_id"" argument linking to ""aws_cloudfront_origin_access_control"" resource
    with ""origin.origin_id"" argument
    with ""default_cache_behavior.allowed_methods"" argument with value [""GET"", ""HEAD""]
    with ""default_cache_behavior.cached_methods"" argument with value [""GET"", ""HEAD""]
    with ""default_cache_behavior.target_origin_id"" argument having the same value as ""origin.origin_id"" argument
    with ""default_cache_behavior.viewer_protocol_policy"" argument with value ""allow-all""
    with ""restrictions.geo_restriction.restriction_type"" argument
    with ""restrictions.geo_restriction.locations"" argument
    with ""viewer_certificate.cloudfront_default_certificate"" argument
have one ""aws_s3_bucket"" resource
have one ""aws_route53_zone"" resource
    with ""name"" argument
have one ""aws_route53_record"" resource
    with ""type"" argument
    with ""name"" argument
    with ""zone_id"" argument linking to ""aws_route53_zone"" resource
    with ""alias.name"" argument linking to ""aws_cloudfront_distribution"" resource
    with ""alias.zone_id"" argument linking to ""aws_cloudfront_distribution"" resource
    with ""alias.evaluate_target_health"" argument","I'll help you create a Terraform configuration for a global video streaming infrastructure using AWS CloudFront, Route53, and S3. This solution will provide a scalable and efficient way to distribute video content worldwide.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Primary region for CloudFront and Route53
}

# S3 bucket for video content
resource ""aws_s3_bucket"" ""video_bucket"" {
  bucket = ""my-video-streaming-bucket""
}

# Bucket policy to allow CloudFront access
resource ""aws_s3_bucket_policy"" ""video_bucket_policy"" {
  bucket = aws_s3_bucket.video_bucket.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowCloudFrontAccess""
        Effect    = ""Allow""
        Principal = {
          Service = ""cloudfront.amazonaws.com""
        }
        Action   = ""s3:GetObject""
        Resource = ""${aws_s3_bucket.video_bucket.arn}/*""
        Condition = {
          StringEquals = {
            ""AWS:SourceArn"" = aws_cloudfront_distribution.video_distribution.arn
          }
        }
      }
    ]
  })
}

# CloudFront Origin Access Control
resource ""aws_cloudfront_origin_access_control"" ""oac"" {
  name                              = ""video-streaming-oac""
  description                       = ""Origin Access Control for video streaming""
  origin_access_control_origin_type = ""s3""
  signing_behavior                  = ""always""
  signing_protocol                  = ""sigv4""
}

# CloudFront distribution
resource ""aws_cloudfront_distribution"" ""video_distribution"" {
  enabled             = true
  is_ipv6_enabled    = true
  default_root_object = ""index.html""
  price_class        = ""PriceClass_All""  # Global distribution

  origin {
    domain_name              = aws_s3_bucket.video_bucket.bucket_regional_domain_name
    origin_id                = ""S3Origin""
    origin_access_control_id = aws_cloudfront_origin_access_control.oac.id
  }

  default_cache_behavior {
    allowed_methods        = [""GET"", ""HEAD"", ""OPTIONS""]
    cached_methods         = [""GET"", ""HEAD""]
    target_origin_id       = ""S3Origin""
    viewer_protocol_policy = ""redirect-to-https""
    compress              = true

    forwarded_values {
      query_string = false
      cookies {
        forward = ""none""
      }
    }

    min_ttl     = 0
    default_ttl = 3600
    max_ttl     = 86400
  }

  restrictions {
    geo_restriction {
      restriction_type = ""none""
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }
}

# Route53 hosted zone (assuming it exists)
data ""aws_route53_zone"" ""domain"" {
  name = ""example.com""  # Replace with your domain
}

# Route53 record for the CloudFront distribution
resource ""aws_route53_record"" ""video_domain"" {
  zone_id = data.aws_route53_zone.domain.zone_id
  name    = ""video.example.com""  # Replace with your subdomain
  type    = ""A""

  alias {
    name                   = aws_cloudfront_distribution.video_distribution.domain_name
    zone_id                = aws_cloudfront_distribution.video_distribution.hosted_zone_id
    evaluate_target_health = false
  }
}

# Optional: S3 bucket for logs
resource ""aws_s3_bucket"" ""logs_bucket"" {
  bucket = ""my-video-streaming-logs""
}

# Optional: Bucket versioning for video content
resource",,,,,,"I'll help you create a Terraform configuration for a global video streaming infrastructure using AWS CloudFront, Route53, and S3. Here's a solution that includes all necessary components:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket for video content
resource ""aws_s3_bucket"" ""video_bucket"" {
  bucket = ""video-streaming-content-bucket""
}

# Bucket policy to allow CloudFront access
resource ""aws_s3_bucket_policy"" ""video_bucket_policy"" {
  bucket = aws_s3_bucket.video_bucket.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowCloudFrontAccess""
        Effect    = ""Allow""
        Principal = {
          AWS = aws_cloudfront_origin_access_identity.oai.iam_arn
        }
        Action   = ""s3:GetObject""
        Resource = ""${aws_s3_bucket.video_bucket.arn}/*""
      }
    ]
  })
}

# CloudFront Origin Access Identity
resource ""aws_cloudfront_origin_access_identity"" ""oai"" {
  comment = ""OAI for video streaming""
}

# CloudFront Distribution
resource ""aws_cloudfront_distribution"" ""video_distribution"" {
  enabled             = true
  is_ipv6_enabled    = true
  default_root_object = ""index.html""
  price_class        = ""PriceClass_All""

  origin {
    domain_name = aws_s3_bucket.video_bucket.bucket_regional_domain_name
    origin_id   = ""S3Origin""

    s3_origin_config {
      origin_access_identity = aws_cloudfront_origin_access_identity.oai.cloudfront_access_identity_path
    }
  }

  default_cache_behavior {
    allowed_methods  = [""GET"", ""HEAD"", ""OPTIONS""]
    cached_methods   = [""GET"", ""HEAD""]
    target_origin_id = ""S3Origin""

    forwarded_values {
      query_string = false
      cookies {
        forward = ""none""
      }
    }

    viewer_protocol_policy = ""redirect-to-https""
    min_ttl                = 0
    default_ttl            = 3600
    max_ttl                = 86400
  }

  restrictions {
    geo_restriction {
      restriction_type = ""none""
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }
}

# Route53 zone (assuming it exists)
data ""aws_route53_zone"" ""domain"" {
  name = ""example.com""
}

# Route53 record for the CloudFront distribution
resource ""aws_route53_record"" ""video_dns"" {
  zone_id = data.aws_route53_zone.domain.zone_id
  name    = ""video.example.com""
  type    = ""A""

  alias {
    name                   = aws_cloudfront_distribution.video_distribution.domain_name
    zone_id                = aws_cloudfront_distribution.video_distribution.hosted_zone_id
    evaluate_target_health = false
  }
}

# S3 bucket versioning
resource ""aws_s3_bucket_versioning"" ""video_bucket_versioning"" {
  bucket = aws_s3_bucket.video_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# S3 bucket encryption
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""bucket_encryption"" {
  bucket = aws_s3_bucket.video_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = ""AES256""
    }
  }
}
```

This configuration creates:

1. An S3 bucket",,,,,,"I'll help you create a Terraform configuration for a global video streaming infrastructure using AWS CloudFront, Route53, and S3. I'll include all necessary components and follow AWS best practices.

```hcl
# Configure AWS Provider
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1"" # Primary region for CloudFront and Route53
}

# S3 Bucket for video content
resource ""aws_s3_bucket"" ""video_bucket"" {
  bucket = ""my-video-streaming-bucket""
}

# Bucket policy to allow CloudFront access
resource ""aws_s3_bucket_policy"" ""video_bucket_policy"" {
  bucket = aws_s3_bucket.video_bucket.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowCloudFrontAccess""
        Effect    = ""Allow""
        Principal = {
          Service = ""cloudfront.amazonaws.com""
        }
        Action   = ""s3:GetObject""
        Resource = ""${aws_s3_bucket.video_bucket.arn}/*""
        Condition = {
          StringEquals = {
            ""AWS:SourceArn"" = aws_cloudfront_distribution.video_distribution.arn
          }
        }
      }
    ]
  })
}

# CloudFront Origin Access Control
resource ""aws_cloudfront_origin_access_control"" ""oac"" {
  name                              = ""video-streaming-oac""
  description                       = ""Origin Access Control for video streaming""
  origin_access_control_origin_type = ""s3""
  signing_behavior                  = ""always""
  signing_protocol                  = ""sigv4""
}

# CloudFront Distribution
resource ""aws_cloudfront_distribution"" ""video_distribution"" {
  enabled             = true
  is_ipv6_enabled    = true
  default_root_object = ""index.html""
  price_class        = ""PriceClass_All"" # Global distribution

  origin {
    domain_name              = aws_s3_bucket.video_bucket.bucket_regional_domain_name
    origin_id                = ""S3Origin""
    origin_access_control_id = aws_cloudfront_origin_access_control.oac.id
  }

  default_cache_behavior {
    allowed_methods        = [""GET"", ""HEAD"", ""OPTIONS""]
    cached_methods         = [""GET"", ""HEAD""]
    target_origin_id       = ""S3Origin""
    viewer_protocol_policy = ""redirect-to-https""
    
    forwarded_values {
      query_string = false
      cookies {
        forward = ""none""
      }
    }

    min_ttl                = 0
    default_ttl            = 3600
    max_ttl                = 86400
  }

  restrictions {
    geo_restriction {
      restriction_type = ""none""
    }
  }

  viewer_certificate {
    cloudfront_default_certificate = true
  }
}

# Route 53 Zone (assuming domain is already registered)
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Route 53 Record for CloudFront
resource ""aws_route53_record"" ""video_streaming"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""video.example.com""
  type    = ""A""

  alias {
    name                   = aws_cloudfront_distribution.video_distribution.domain_name
    zone_id                = aws_cloudfront_distribution.video_distribution.hosted_zone_id
    evaluate_target_health = false
  }
}

# Optional: S3 bucket versioning
resource ""aws_s3_bucket_versioning"" ""video_bucket_versioning"" {
  bucket = aws_s3_bucket.video_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Optional",,,,,
aws_db_instance,create a basic AWS SQL database,"package terraform.validation

default is_valid_db_instance = false

# Rule to check if a valid aws_db_instance exists
is_valid_db_instance {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        has_required_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_db_arguments {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_db_instance""
        requirement1(resource.expressions)
        requirement2(resource.expressions)
    requirement3(resource.expressions)
}

# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
        expressions.allocated_storage
        expressions.engine.constant_value == ""mysql""
    expressions.username
        is_valid_engine(expressions.engine.constant_value)
}

requirement2(expressions) {
        expressions.instance_class
        is_valid_instance_class(expressions.instance_class.constant_value)
}

requirement3(expressions) {
        expressions.password
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

requirement3(expressions) {
        expressions.snapshot_identifier
}

requirement3(expressions) {
        expressions.replicate_source_db
}

requirement3(expressions) {
        expressions.manage_master_user_password
}

# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}

",1,"resource ""aws_db_instance"" ""default"" {
  allocated_storage    = 10
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""foo""
  password             = ""foobarbaz""
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""","I'll help you create a basic AWS RDS (Relational Database Service) SQL database using Terraform. Here's a basic configuration that creates an RDS instance with MySQL:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_db_instance"" ""default"" {
  identifier           = ""my-sql-database""
  engine              = ""mysql""
  engine_version      = ""8.0.28""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  # Database authentication and access
  username            = ""admin""
  password            = ""your-password-here""  # Replace with a secure password
  
  # Network configuration
  publicly_accessible = false
  
  # Backup configuration
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  
  # Maintenance window
  maintenance_window     = ""Mon:04:00-Mon:05:00""
  
  # Skip final snapshot when destroying the database
  skip_final_snapshot    = true
  
  # Enable deletion protection
  deletion_protection    = false
  
  tags = {
    Name        = ""MyDatabase""
    Environment = ""Production""
  }
}
```

This configuration creates:
1. A MySQL RDS instance
2. Uses a t3.micro instance type (suitable for development/testing)
3. Allocates 20GB of GP2 storage
4. Sets up basic backup and maintenance windows
5. Configures basic security settings

Important notes:
- Replace `your-password-here` with a secure password
- Consider adding a security group to control access
- For production use, you might want to:
  - Use a larger instance type
  - Enable Multi-AZ deployment
  - Configure parameter groups
  - Set up enhanced monitoring
  - Configure VPC settings

You can customize the configuration based on your specific requirements by adjusting the parameters accordingly.",,,,,,"I'll help you create a basic AWS RDS (Relational Database Service) instance using Terraform. This example will create a basic MySQL database with some common configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create a security group for the RDS instance
resource ""aws_security_group"" ""rds_sg"" {
  name        = ""rds_security_group""
  description = ""Security group for RDS instance""

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]  # Note: In production, restrict this to specific IPs
  }
}

# Create a subnet group for RDS
resource ""aws_db_subnet_group"" ""default"" {
  name       = ""main""
  subnet_ids = [""subnet-xxxxxxxx"", ""subnet-yyyyyyyy""]  # Replace with your subnet IDs

  tags = {
    Name = ""My DB subnet group""
  }
}

# Create the RDS instance
resource ""aws_db_instance"" ""default"" {
  identifier           = ""my-database""
  allocated_storage    = 20
  storage_type        = ""gp2""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  db_name             = ""mydb""
  username            = ""admin""
  password            = ""your-password-here""  # Use AWS Secrets Manager in production
  skip_final_snapshot = true

  vpc_security_group_ids = [aws_security_group.rds_sg.id]
  db_subnet_group_name   = aws_db_subnet_group.default.name

  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  tags = {
    Environment = ""production""
  }
}

output ""db_endpoint"" {
  value = aws_db_instance.default.endpoint
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a security group allowing MySQL access (port 3306)
3. Creates a DB subnet group (you'll need to specify your subnet IDs)
4. Creates an RDS instance with:
   - MySQL 8.0
   - 20GB GP2 storage
   - t3.micro instance type
   - Basic backup and maintenance windows
   - Outputs the database endpoint

Remember to:
- Replace the subnet IDs with your actual subnet IDs
- Use a strong password
- In production, use AWS Secrets Manager for credentials
- Consider restricting the security group ingress to specific IP ranges
- Adjust the instance size and storage based on your needs",,,,,,"I'll help you create a basic AWS RDS (Relational Database Service) SQL database using Terraform. This example will create a basic MySQL database instance with standard configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create a security group for the RDS instance
resource ""aws_security_group"" ""rds_sg"" {
  name        = ""rds_security_group""
  description = ""Security group for RDS instance""

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]  # Warning: In production, restrict this to specific IP ranges
  }
}

# Create a subnet group for RDS
resource ""aws_db_subnet_group"" ""default"" {
  name       = ""main""
  subnet_ids = [""subnet-xxxxxx"", ""subnet-yyyyyy""]  # Replace with your subnet IDs

  tags = {
    Name = ""My DB subnet group""
  }
}

# Create the RDS instance
resource ""aws_db_instance"" ""default"" {
  identifier           = ""my-sql-db""
  allocated_storage    = 20
  storage_type        = ""gp2""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  db_name             = ""mydb""
  username            = ""admin""
  password            = ""your-password-here""  # In production, use secrets management
  parameter_group_name = ""default.mysql8.0""
  skip_final_snapshot = true

  vpc_security_group_ids = [aws_security_group.rds_sg.id]
  db_subnet_group_name   = aws_db_subnet_group.default.name

  tags = {
    Name = ""MyRDSInstance""
  }
}

output ""db_endpoint"" {
  value = aws_db_instance.default.endpoint
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a security group for the database
3. Creates a DB subnet group (you'll need to specify your subnet IDs)
4. Creates an RDS instance with MySQL
5. Outputs the database endpoint

Important notes:
- Replace the subnet IDs with your actual subnet IDs
- Change the password to a secure value
- In production, use AWS Secrets Manager or similar for sensitive data
- The security group allows access from anywhere (0.0.0.0/0) - restrict this in production
- Adjust the instance class, storage size, and other parameters according to your needs
- The example uses MySQL, but you can change the engine to other supported databases like PostgreSQL

Remember to properly secure your database in a production environment by:
- Using private subnets
- Implementing more restrictive security group rules
- Using encrypted storage
- Implementing backup strategies
- Using stronger instance types as needed",,,,,
"aws_internet_gateway, aws_route_table, aws_vpc","sets up a dedicated-tenancy AWS VPC with DNS support and hostnames enabled, an internet gateway, and a route table for outbound internet access, all tagged with a variable name.","package terraform.validation

default is_valid_vpc = false
default is_valid_internet_gateway = false
default is_valid_route_table = false

# Validate aws_vpc resource
is_valid_vpc {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_vpc""
    resource.expressions.cidr_block != null
    resource.expressions.enable_dns_support.constant_value == true
    resource.expressions.enable_dns_hostnames.constant_value == true
}

# Validate aws_internet_gateway resource
is_valid_internet_gateway {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_internet_gateway""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.dgraph.id""
    resource.expressions.tags.constant_value[""Name""] != null
}

# Validate aws_route_table resource
is_valid_route_table {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route_table""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.dgraph.id""
    resource.expressions.route.references[0] == ""aws_internet_gateway.dgraph_gw.id""
    resource.expressions.tags.constant_value[""Name""] != null
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""dgraph"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_support   = true
  instance_tenancy     = ""dedicated""

  # For enabling assignment of private dns addresses within AWS.
  enable_dns_hostnames = true

  tags = {
    Name = ""var.name""
  }
}

resource ""aws_internet_gateway"" ""dgraph_gw"" {
  vpc_id = aws_vpc.dgraph.id

  tags = {
    Name = ""var.name""
  }
}

resource ""aws_route_table"" ""dgraph_igw"" {
  vpc_id = aws_vpc.dgraph.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.dgraph_gw.id
  }

  tags = {
    Name = ""var.name""
  }
}","Has one ""aws_vpc"" resource
with a specified ""cidr_block""
with ""enable_dns_support"" and ""enable_dns_hostnames"" set to true

Has one ""aws_internet_gateway"" resource
with ""vpc_id"" referencing the ""aws_vpc"" resource
with a ""tags"" map including a ""Name"" key referencing a variable

Has one ""aws_route_table"" resource
with ""vpc_id"" referencing the ""aws_vpc"" resource
containing a default route for internet access through the ""aws_internet_gateway""
with a ""tags"" map including a ""Name"" key referencing a variable","I'll help you create a Terraform configuration for a dedicated-tenancy VPC with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # You can change this to your desired region
}

variable ""vpc_name"" {
  description = ""Name tag for the VPC and related resources""
  type        = string
  default     = ""dedicated-vpc""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block                       = ""10.0.0.0/16""
  instance_tenancy                 = ""dedicated""
  enable_dns_support               = true
  enable_dns_hostnames            = true

  tags = {
    Name = var.vpc_name
  }
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""${var.vpc_name}-igw""
  }
}

# Create Route Table
resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""${var.vpc_name}-rt""
  }
}

# Associate Route Table with VPC Main Route Table
resource ""aws_main_route_table_association"" ""main"" {
  vpc_id         = aws_vpc.main.id
  route_table_id = aws_route_table.main.id
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a variable for naming resources
3. Creates a VPC with dedicated tenancy and DNS features enabled
4. Creates an Internet Gateway attached to the VPC
5. Creates a route table with a route for outbound internet access (0.0.0.0/0)
6. Associates the route table as the main route table for the VPC
7. Tags all resources with the provided name variable

All resources are properly tagged and connected. The VPC is configured with dedicated tenancy as requested, and both DNS support and DNS hostnames are enabled.",,,,,,"I'll help you create a Terraform configuration for a dedicated-tenancy VPC with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

variable ""vpc_name"" {
  description = ""Name tag for the VPC""
  type        = string
}

resource ""aws_vpc"" ""dedicated_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  instance_tenancy     = ""dedicated""
  enable_dns_support   = true
  enable_dns_hostnames = true

  tags = {
    Name = var.vpc_name
  }
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.dedicated_vpc.id

  tags = {
    Name = ""${var.vpc_name}-igw""
  }
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.dedicated_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""${var.vpc_name}-rt""
  }
}
```

This configuration:
1. Sets up the AWS provider requirement
2. Defines a variable for the VPC name that will be used in tags
3. Creates a VPC with dedicated tenancy, DNS support, and DNS hostnames enabled
4. Creates an Internet Gateway attached to the VPC
5. Creates a route table with a route for outbound internet access (0.0.0.0/0)
6. Tags all resources with the provided name variable

You can use this configuration by providing a value for the vpc_name variable in your tfvars file or when running terraform apply.",,,,,,"I'll help you create a Terraform configuration for a dedicated-tenancy VPC with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # You can change this to your desired region
}

variable ""vpc_name"" {
  description = ""Name tag for the VPC and related resources""
  type        = string
  default     = ""dedicated-vpc""
}

# Create VPC with dedicated tenancy
resource ""aws_vpc"" ""main"" {
  cidr_block                       = ""10.0.0.0/16""
  instance_tenancy                 = ""dedicated""
  enable_dns_support               = true
  enable_dns_hostnames            = true

  tags = {
    Name = var.vpc_name
  }
}

# Create Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name = ""${var.vpc_name}-igw""
  }
}

# Create Route Table
resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""${var.vpc_name}-rt""
  }
}

# Associate Route Table with VPC (Main Route Table)
resource ""aws_main_route_table_association"" ""main"" {
  vpc_id         = aws_vpc.main.id
  route_table_id = aws_route_table.main.id
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a variable for naming resources
3. Creates a VPC with dedicated tenancy, DNS support, and DNS hostnames enabled
4. Creates an Internet Gateway and attaches it to the VPC
5. Creates a route table with a route for outbound internet access (0.0.0.0/0)
6. Associates the route table as the main route table for the VPC
7. Tags all resources with the provided name variable

All resources are properly tagged and connected. The VPC will have dedicated tenancy, which means all instances launched in this VPC will run on dedicated hardware.",,,,,
"aws_elastic_beanstalk_application, aws_elastic_beanstalk_application_version, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment, aws_route53_record, aws_route53_zone, aws_s3_bucket, aws_s3_object","Configure Route 53 resources to manage a domain and directs traffic to an Elastic Beanstalk environment. 
Name the zone ""main"", the environment ""eb-env"", the iam role ""eb_ec2_role"", the instance profile ""eb_ec2_profile"", the bucket (with unique name), the object ""examplebucket_object"", the application ""myapp"" and the application version ""version""","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_s3_bucket = false

default is_valid_s3_object = false

default is_valid_eb_app = false

default is_valid_eb_env = false

default is_valid_r53_zone = false

default is_valid_r53_record = false



# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.name
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records.references[0] == ""aws_elastic_beanstalk_environment.eb-env.cname""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.main.zone_id""

}

is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0] == ""aws_iam_role.eb_ec2_role.name""
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0] == ""aws_iam_role.eb_ec2_role.name""
}

is_valid_s3_bucket {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    resource.name
    # resource.expressions.bucket.constant_value == ""sampleapril26426""
}

is_valid_s3_object {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_s3_object""
    # resource.expressions.bucket.references[0] == ""aws_s3_bucket.sampleapril26426.id""
    resource.expressions.key
    resource.expressions.source
    
}

# Validate aws_eb_app
is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}

is_valid_eb_app_version {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application_version""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.myapp.name""
    resource.expressions.bucket.references[0] == ""aws_s3_object.examplebucket_object.bucket""
    resource.expressions.key.references[0] == ""aws_s3_object.examplebucket_object.key""
}
# Validate aws_eb_env
is_valid_eb_env {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.myapp.name""
    resource.expressions.solution_stack_name
    resource.expressions.version_label.references[0] == ""aws_elastic_beanstalk_application_version.version.name""
        some a
    resource.expressions.setting[a].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
}


# Combine all checks into a final rule
is_configuration_valid {
        is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
        is_valid_r53_zone
        is_valid_r53_record
    is_valid_s3_bucket
    is_valid_s3_object 
    is_valid_eb_app_version
    is_valid_eb_app
    is_valid_eb_env
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}


# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Assuming you have a hosted zone for your domain in Route 53.
# Route 53 Hosted Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example56.com""
}

# DNS Record to point to the Elastic Beanstalk environment
resource ""aws_route53_record"" ""www"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""app.example56.com""
  type    = ""CNAME""
  ttl     = 60
  records = [aws_elastic_beanstalk_environment.eb-env.cname]
}

resource ""aws_s3_bucket"" ""sample_bucket"" {
  bucket_prefix = ""sample-""
}

resource ""aws_s3_object"" ""examplebucket_object"" {
  key    = ""randofile""
  bucket = aws_s3_bucket.sample_bucket.id
  source = ""./supplement/app.zip""
}

# Define the Elastic Beanstalk application
resource ""aws_elastic_beanstalk_application"" ""myapp"" {
  name        = ""something-application""
}

# Define the Elastic Beanstalk application version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""app-version""
  application = aws_elastic_beanstalk_application.myapp.name
  bucket = aws_s3_object.examplebucket_object.bucket
  key    = aws_s3_object.examplebucket_object.key
}


# Elastic Beanstalk environment
resource ""aws_elastic_beanstalk_environment"" ""eb-env"" {
  name                = ""awesomeenv""
  application         = aws_elastic_beanstalk_application.myapp.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""
  version_label = aws_elastic_beanstalk_application_version.version.name

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""

Has one ""aws_s3_bucket"" resource
    with ""bucket""

Has one ""aws_s3_object"" resource
    with ""key""
    with ""source""
    with ""bucket"" referencing the ""aws_s3_bucket"" resource

Has one ""aws_elastic_beanstalk_application_version"" resource
    with ""name
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""bucket"" referencing the ""bucket"" from the ""aws_s3_object"" resource
    with ""key"" referencing the ""key"" from the ""aws_s3_object"" resource

Has one ""aws_elastic_beanstalk_environment"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with ""version_label"" referencing the ""aws_elastic_beanstalk_application_version"" resource
    with one ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource

Has one ""aws_route53_zone"" resource
    with ""name""

Has one ""aws_route53_record"" resources
    with ""name""
    with ""type""
    with ""ttl""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
    with ""records"" referencing one of the ""aws_elastic_beanstalk_environment"" resources
","I'll help you create a Terraform configuration for Route 53 with Elastic Beanstalk integration. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""  # Replace with your domain name
}

# IAM Role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Instance Profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# S3 Bucket for application versions
resource ""aws_s3_bucket"" ""eb_bucket"" {
  bucket = ""my-unique-eb-bucket-name-123456""  # Replace with unique bucket name
}

# S3 Bucket Object
resource ""aws_s3_object"" ""examplebucket_object"" {
  bucket = aws_s3_bucket.eb_bucket.id
  key    = ""beanstalk/application.zip""
  source = ""path/to/your/application.zip""  # Replace with your application source
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""myapp"" {
  name        = ""myapp""
  description = ""My Application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""version""
  application = aws_elastic_beanstalk_application.myapp.name
  description = ""application version created by terraform""
  bucket      = aws_s3_bucket.eb_bucket.id
  key         = aws_s3_object.examplebucket_object.key
}

# Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""eb-env"" {
  name                = ""eb-env""
  application         = aws_elastic_beanstalk_application.myapp.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.4 running Node.js 18""  # Update with your desired platform
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Route 53 Record
resource ""aws_route53_record"" ""eb_record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""myapp.example.com""  # Replace with your desired subdomain
  type    = ""A""

  alias {
    name                   = aws_elastic_beanstalk_environment.eb-env.cname
    zone_id                = aws_elastic_beanstalk_environment.eb-env.zone_id
    evaluate_target_health = true
  }
}
```

This configuration:
1. Creates a Route 53 zone
2. Sets up IAM role and instance profile for Elastic Beanstalk
3. Creates an S3 bucket and object for the application code
4. Creates an Elastic Beanstalk application and version
5. Sets up an Elastic Beanstalk environment
6. Creates a Route 53 record pointing to the Elastic Beanstalk environment

Remember",,,,,,"I'll help you create a Terraform configuration for Route 53 resources that manage a domain and direct traffic to an Elastic Beanstalk environment. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# IAM Role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# Instance Profile for the IAM role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# S3 Bucket for application versions
resource ""aws_s3_bucket"" ""eb_bucket"" {
  bucket = ""my-unique-eb-bucket-name-12345""
}

# S3 Bucket Object
resource ""aws_s3_object"" ""examplebucket_object"" {
  bucket = aws_s3_bucket.eb_bucket.id
  key    = ""beanstalk/application.zip""
  source = ""application.zip""  # Make sure this file exists
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""myapp"" {
  name        = ""myapp""
  description = ""My Application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""version""
  application = aws_elastic_beanstalk_application.myapp.name
  description = ""application version created by terraform""
  bucket      = aws_s3_bucket.eb_bucket.id
  key         = aws_s3_object.examplebucket_object.key
}

# Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""eb-env"" {
  name                = ""eb-env""
  application         = aws_elastic_beanstalk_application.myapp.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.0 running Node.js 18""
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Route 53 Record
resource ""aws_route53_record"" ""www"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""www.example.com""
  type    = ""A""

  alias {
    name                   = aws_elastic_beanstalk_environment.eb-env.cname
    zone_id               = aws_elastic_beanstalk_environment.eb-env.id
    evaluate_target_health = true
  }
}
```

This configuration:
1. Creates a Route 53 zone
2. Sets up the necessary IAM role and instance profile for Elastic Beanstalk
3. Creates an S3 bucket and object for the application code
4. Configures the Elastic Beanstalk application, version, and environment
5. Creates a Route 53 record that points to the Elastic Beanstalk environment

Note: You'll need to:
- Replace ""example.com"" with your actual domain name
- Ensure the S3 bucket name is globally unique
- Adjust the solution_stack_name",,,,,,"I'll help you create a Terraform configuration for Route 53 resources that manage a domain and direct traffic to an Elastic Beanstalk environment. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust region as needed
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""  # Replace with your domain name
}

# IAM Role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# Instance Profile for EC2 instances
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# S3 Bucket for application versions
resource ""aws_s3_bucket"" ""eb_bucket"" {
  bucket = ""my-unique-eb-bucket-name-12345""  # Replace with unique bucket name
}

# S3 Bucket Object for application code
resource ""aws_s3_object"" ""examplebucket_object"" {
  bucket = aws_s3_bucket.eb_bucket.id
  key    = ""beanstalk/application.zip""
  source = ""path/to/your/application.zip""  # Replace with actual path
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""myapp"" {
  name        = ""myapp""
  description = ""My Application""
}

# Elastic Beanstalk Application Version
resource ""aws_elastic_beanstalk_application_version"" ""version"" {
  name        = ""version""
  application = aws_elastic_beanstalk_application.myapp.name
  description = ""application version created by terraform""
  bucket      = aws_s3_bucket.eb_bucket.id
  key         = aws_s3_object.examplebucket_object.key
}

# Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""eb-env"" {
  name                = ""eb-env""
  application         = aws_elastic_beanstalk_application.myapp.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.1 running Node.js 18""  # Adjust as needed
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Route 53 Record for Elastic Beanstalk Environment
resource ""aws_route53_record"" ""eb_env_record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""myapp.example.com""  # Replace with your subdomain
  type    = ""CNAME""
  ttl     = ""300""
  records = [aws_elastic_beanstalk_environment.eb-env.cname]
}
```

This configuration:
1. Creates a Route 53 zone
2. Sets up IAM role and instance profile for Elastic Beanstalk
3. Creates an S3 bucket and object for the application code
4. Configures the Elastic Beanstalk application, version, and environment
5. Creates a Route 53 record pointing to the Elastic Beanstalk environment

Remember to:
- Replace ""example.com"" with your actual domain name
-",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer with IP Target Group,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.values.subnet_mapping != null
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.ipv4_ipam_pool_id.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.ipv6_cider_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value != ""forward""
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    resources.values.target_type == ""ip""
    
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group""
        resource.expressions.port != null
        resource.expressions.protocol != null
        resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
        is_valid_lb
        is_valid_lb_listener
        is_valid_lb_target_group_attachment
        is_valid_lb_target_group
        is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.public_subnets[0]
  security_groups = [ aws_security_group.instance-sg.id ]
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
  target_type = ""ip""
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.private_ip
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id
    with target_type = ""ip""

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you create a Terraform configuration for an AWS Load Balancer with an IP Target Group. This configuration will include the necessary resources for creating an Application Load Balancer (ALB) and its associated IP-based target group.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create VPC (for the ALB)
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  tags = {
    Name = ""main-vpc""
  }
}

# Create public subnets
resource ""aws_subnet"" ""public_1"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""Public Subnet 1""
  }
}

resource ""aws_subnet"" ""public_2"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""Public Subnet 2""
  }
}

# Create Application Load Balancer
resource ""aws_lb"" ""example"" {
  name               = ""example-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb_sg.id]
  subnets            = [aws_subnet.public_1.id, aws_subnet.public_2.id]

  tags = {
    Environment = ""production""
  }
}

# Create Target Group
resource ""aws_lb_target_group"" ""ip_target"" {
  name        = ""ip-target-group""
  port        = 80
  protocol    = ""HTTP""
  target_type = ""ip""
  vpc_id      = aws_vpc.main.id

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 3
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.ip_target.arn
  }
}

# Security Group for the Load Balancer
resource ""aws_security_group"" ""lb_sg"" {
  name        = ""lb-security-group""
  description = ""Security group for load balancer""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""lb-security-group""
  }
}

# Example of adding IP targets (you would replace these with your actual IP addresses)
resource ""aws_lb_target_group_attachment"" ""ip_target"" {
  target_group_arn = aws_lb_target_group.ip_target.arn
  target_id        = ""10.0.1.10""  # Replace with your IP address
  port             = 80
}
```

This configuration creates:
1. A VPC and two public subnets (required for the AL",,,,,,"I'll help you create a Terraform configuration for an AWS Load Balancer with an IP Target Group. This is commonly used when you need to route traffic to IP addresses instead of EC2 instances, such as when using private IPs or resources outside of AWS.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create VPC (Application Load Balancer requires a VPC)
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  tags = {
    Name = ""main-vpc""
  }
}

# Create public subnet
resource ""aws_subnet"" ""public"" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.${count.index + 1}.0/24""
  availability_zone = data.aws_availability_zones.available.names[count.index]

  tags = {
    Name = ""Public Subnet ${count.index + 1}""
  }
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Create Application Load Balancer
resource ""aws_lb"" ""app_lb"" {
  name               = ""ip-target-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb_sg.id]
  subnets            = aws_subnet.public[*].id

  tags = {
    Environment = ""production""
  }
}

# Create Target Group
resource ""aws_lb_target_group"" ""ip_target"" {
  name        = ""ip-target-group""
  port        = 80
  protocol    = ""HTTP""
  target_type = ""ip""
  vpc_id      = aws_vpc.main.id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Create Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.app_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.ip_target.arn
  }
}

# Create Security Group for the Load Balancer
resource ""aws_security_group"" ""lb_sg"" {
  name        = ""lb-security-group""
  description = ""Security group for load balancer""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""lb-security-group""
  }
}

# Example of how to register an IP target (optional)
resource ""aws_lb_target_group_attachment"" ""ip_target"" {
  target_group_arn = aws_lb_target_group.ip_target.arn
  target_id        = ""10.0.1.100""  # Replace with your target IP
  port             = 80
}
```

This configuration creates:
1. An Application Load Balancer
2. An IP-based Target Group
3. A listener on port 80
4. Necessary networking components (VPC, subnets)
5. Security group",,,,,,"I'll help you create a Terraform configuration for an AWS Load Balancer with an IP Target Group. This configuration will include the required provider and the necessary resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create VPC (Network Load Balancer needs a VPC)
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  tags = {
    Name = ""main-vpc""
  }
}

# Create public subnet
resource ""aws_subnet"" ""public"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""public-subnet""
  }
}

# Create Network Load Balancer
resource ""aws_lb"" ""nlb"" {
  name               = ""ip-target-nlb""
  internal           = false
  load_balancer_type = ""network""
  subnets            = [aws_subnet.public.id]

  tags = {
    Environment = ""production""
  }
}

# Create Target Group
resource ""aws_lb_target_group"" ""ip_target"" {
  name        = ""ip-target-group""
  port        = 80
  protocol    = ""TCP""
  target_type = ""ip""
  vpc_id      = aws_vpc.main.id

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    port               = ""traffic-port""
    protocol           = ""TCP""
    timeout            = 10
    unhealthy_threshold = 3
  }

  tags = {
    Name = ""ip-target-group""
  }
}

# Create Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.nlb.arn
  port              = 80
  protocol          = ""TCP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.ip_target.arn
  }
}

# Example of adding IP targets (uncomment and modify as needed)
/*
resource ""aws_lb_target_group_attachment"" ""ip_target"" {
  target_group_arn = aws_lb_target_group.ip_target.arn
  target_id        = ""10.0.1.10""  # Replace with your IP address
  port             = 80
}
*/
```

This configuration creates:
1. A Network Load Balancer (NLB)
2. An IP-based target group
3. A listener on port 80
4. The necessary VPC and subnet infrastructure
5. An example (commented out) of how to attach IP targets

Key features:
- The target group is configured with `target_type = ""ip""` which allows you to specify IP addresses as targets
- Health checks are configured for the target group
- The load balancer is set up as external (public-facing)
- The listener forwards traffic to the target group

You can uncomment and modify the target group attachment resource to add your specific IP addresses as targets. Remember to replace the IP address with your actual target IP addresses.

Note: This is a basic configuration. You might want to add more security groups, multiple availability zones, and other production-ready features depending on your specific requirements.",,,,,
"aws_db_instance, aws_db_instance, aws_elastic_beanstalk_application, aws_elastic_beanstalk_environment, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment","Configure production and staging Elastic Beanstalk environments with a respective db_instance from RDS. Name the instance profile ""eb_ec2_profile"", the production database ""prod_db"", and the staging database ""staging_db""
","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_eb_app = false

default is_valid_eb_env = false

default is_valid_db_instance = false


is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0]
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0]
}

# Validate aws_eb_app
is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}

# Validate aws_eb_env
is_valid_eb_env {
        some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.expressions.name
    resource.expressions.application.references[0]
    resource.expressions.solution_stack_name
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_elastic_beanstalk_environment""
    resource2.expressions.name
    resource2.expressions.application.references[0]
    resource2.expressions.solution_stack_name
    some a, b, c, d, e, f, g, h
    resource.expressions.setting[a].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource.expressions.setting[b].value.references[0] == ""aws_db_instance.prod_db.username""
    resource.expressions.setting[c].value.references[0] == ""aws_db_instance.prod_db.password""
    resource.expressions.setting[d].value.references[0] == ""aws_db_instance.prod_db.address""
    resource2.expressions.setting[e].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource2.expressions.setting[f].value.references[0] == ""aws_db_instance.staging_db.username""
    resource2.expressions.setting[g].value.references[0] == ""aws_db_instance.staging_db.password""
    resource2.expressions.setting[h].value.references[0] == ""aws_db_instance.staging_db.address""

}

is_valid_db_instance {
                some i, j
        resource := input.configuration.root_module.resources[i]
    resource2 := input.configuration.root_module.resources[j]
    resource.type == ""aws_db_instance""
    resource2.type == ""aws_db_instance""
    has_required_main_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_main_db_arguments {
                some i, j
    resource := input.configuration.root_module.resources[i]
    resource2 := input.configuration.root_module.resources[j]
    resource.type == ""aws_db_instance""
    resource2.type == ""aws_db_instance""
    # Check for allocated_storage, engine, instance_class, username, password, and skip_final_snapshot
    requirement1(resource.expressions)
    requirement1(resource2.expressions)
    # Check for instance_class validity
    requirement2(resource.expressions)
    requirement2(resource2.expressions)
}



# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
    expressions.allocated_storage
    expressions.engine
    expressions.username
    expressions.password
    is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
    expressions.snapshot_identifier
}

# Check for instance_class validity
requirement2(expressions) {
    expressions.instance_class
    is_valid_instance_class(expressions.instance_class.constant_value)
}


# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}


# Combine all checks into a final rule
is_configuration_valid {
        is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
    is_valid_eb_app
    is_valid_eb_env
    is_valid_db_instance
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}


# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}


# RDS Database for Production
resource ""aws_db_instance"" ""prod_db"" {
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  username             = ""dbuser""
  password             = ""prodsecret""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
  identifier = ""production""
}

# Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""my_app"" {
  name        = ""my-app""
  description = ""My awesome application""
}

# Elastic Beanstalk Environment for Production
resource ""aws_elastic_beanstalk_environment"" ""prod_env"" {
  name                = ""my-app-prod""
  application         = aws_elastic_beanstalk_application.my_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.prod_db.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.prod_db.username
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.prod_db.password
  }

  setting {
      namespace = ""aws:autoscaling:launchconfiguration""
      name      = ""IamInstanceProfile""
      value     = aws_iam_instance_profile.eb_ec2_profile.name
    }
}

# RDS Database for Staging
resource ""aws_db_instance"" ""staging_db"" {
  allocated_storage    = 10
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  username             = ""dbuser""
  password             = ""stagingsecret""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
  identifier = ""staging""
}

# Elastic Beanstalk Environment for Staging
resource ""aws_elastic_beanstalk_environment"" ""staging_env"" {
  name                = ""my-app-staging""
  application         = aws_elastic_beanstalk_application.my_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.staging_db.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.staging_db.username
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.staging_db.password
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""

Has a ""aws_elastic_beanstalk_environment"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with one ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource
    with a ""setting"" block that has a value that references the ""address"" from the production ""db_instance"" resource
    with a ""setting"" block that has a value that references the ""username"" from the production ""db_instance"" resource 
    with a ""setting"" block that has a value that references the ""password"" from the production ""db_instance"" resource

Has a ""aws_elastic_beanstalk_environment"" resource
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with one ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource
    with a ""setting"" block that has a ""value""  that references the ""address"" from the staging ""db_instance"" resource
    with a ""setting"" block that has a ""value""  that references the ""username"" from the staging ""db_instance"" resource 
    with a ""setting"" block that has a ""value""  that references the ""password"" from the staging ""db_instance"" resource  

Has two ""aws_db_instance"" resources
    with the ""identifier"" for one set to ""production"" and ""identifier"" for the other set to ""staging""
    with ""instance_class""
    with ""allocated_storage""
    with ""engine""
    with ""username""
    with ""password""
    with ""backup_retention_period"" set to a non-zero number
    with ""skip_final_snapshot""

","I'll help you create a Terraform configuration for Production and Staging Elastic Beanstalk environments with their respective RDS instances. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role and Instance Profile for Elastic Beanstalk
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb-ec2-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Production RDS Instance
resource ""aws_db_instance"" ""prod_db"" {
  identifier           = ""prod-db""
  allocated_storage    = 20
  storage_type        = ""gp2""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  username            = ""admin""
  password            = ""YOUR_PASSWORD_HERE""
  skip_final_snapshot = true
  
  tags = {
    Environment = ""Production""
  }
}

# Staging RDS Instance
resource ""aws_db_instance"" ""staging_db"" {
  identifier           = ""staging-db""
  allocated_storage    = 10
  storage_type        = ""gp2""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  username            = ""admin""
  password            = ""YOUR_PASSWORD_HERE""
  skip_final_snapshot = true
  
  tags = {
    Environment = ""Staging""
  }
}

# Production Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""app"" {
  name        = ""my-app""
  description = ""My Application""
}

# Production Environment
resource ""aws_elastic_beanstalk_environment"" ""prod_env"" {
  name                = ""production-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.1 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.prod_db.endpoint
  }
}

# Staging Environment
resource ""aws_elastic_beanstalk_environment"" ""staging_env"" {
  name                = ""staging-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.1 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.staging_db.endpoint
  }
}
```

This configuration creates:
1. An IAM role and instance profile named ""eb_ec2_profile""",,,,,,"I'll help you create a Terraform configuration for Production and Staging Elastic Beanstalk environments with their respective RDS instances. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM role and instance profile for Elastic Beanstalk
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Production RDS Instance
resource ""aws_db_instance"" ""prod_db"" {
  identifier           = ""prod-db""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  username            = ""admin""
  password            = ""password123""  # Use secrets management in production
  skip_final_snapshot = true
  
  tags = {
    Environment = ""Production""
  }
}

# Staging RDS Instance
resource ""aws_db_instance"" ""staging_db"" {
  identifier           = ""staging-db""
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  username            = ""admin""
  password            = ""password123""  # Use secrets management in production
  skip_final_snapshot = true
  
  tags = {
    Environment = ""Staging""
  }
}

# Production Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""app"" {
  name        = ""my-app""
  description = ""My Application""
}

# Production Environment
resource ""aws_elastic_beanstalk_environment"" ""prod_env"" {
  name                = ""production-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.1 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.prod_db.endpoint
  }
}

# Staging Environment
resource ""aws_elastic_beanstalk_environment"" ""staging_env"" {
  name                = ""staging-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.1 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.staging_db.endpoint
  }
}
```

This configuration:
1. Creates an IAM instance profile named ""eb_ec2_profile""
2. Sets up two RDS instances named ""prod_db"" and ""staging_",,,,,,"I'll help you create a Terraform configuration for Elastic Beanstalk environments with RDS databases for both production and staging environments. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM role for EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# Instance profile for EC2 instances
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Production RDS instance
resource ""aws_db_instance"" ""prod_db"" {
  identifier           = ""prod-db""
  allocated_storage    = 20
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  username            = ""admin""
  password            = ""your-password-here""
  skip_final_snapshot = true
  tags = {
    Environment = ""production""
  }
}

# Staging RDS instance
resource ""aws_db_instance"" ""staging_db"" {
  identifier           = ""staging-db""
  allocated_storage    = 20
  engine              = ""mysql""
  engine_version      = ""8.0""
  instance_class      = ""db.t3.micro""
  username            = ""admin""
  password            = ""your-password-here""
  skip_final_snapshot = true
  tags = {
    Environment = ""staging""
  }
}

# Production Elastic Beanstalk application
resource ""aws_elastic_beanstalk_application"" ""app"" {
  name        = ""my-app""
  description = ""My Application""
}

# Production environment
resource ""aws_elastic_beanstalk_environment"" ""production"" {
  name                = ""production-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.1 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.prod_db.endpoint
  }
}

# Staging environment
resource ""aws_elastic_beanstalk_environment"" ""staging"" {
  name                = ""staging-environment""
  application         = aws_elastic_beanstalk_application.app.name
  solution_stack_name = ""64bit Amazon Linux 2 v5.6.1 running Node.js 18""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.staging_db.endpoint
  }
}
```

This configuration:
1. Creates an IAM role and instance profile named ""eb_ec2_profile""
2. Sets up two RDS instances: ""prod_db"" and ""staging_db""
3. Creates an Elastic Bean",,,,,
"aws_vpc, aws_vpc_dhcp_options, aws_vpc_dhcp_options_association","sets up a basic AWS network infrastructure consisting of a Virtual Private Cloud (VPC) with a custom DHCP options set. The VPC is configured with a CIDR block of 192.168.0.0/16. The DHCP options include a domain name (windomain.local), a combination of custom and Google's DNS servers (192.168.56.102 and 8.8.8.8), and a NetBIOS name server (192.168.56.102). These DHCP options are then associated with the created VPC, customizing its DHCP settings.






","package terraform.validation

# Set default validation states
default is_valid_vpc = false

default is_valid_dhcp_options = false

default is_valid_dhcp_options_association = false

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value == ""192.168.0.0/16""
}

# Validate aws_vpc_dhcp_options resource
is_valid_dhcp_options {
        some i
        resource := input.planned_values.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options""
        resource.values.domain_name == ""windomain.local""
    resource.values.domain_name_servers == [""192.168.56.102"", ""8.8.8.8""]
        resource.values.netbios_name_servers == [""192.168.56.102""]
}

# Validate aws_vpc_dhcp_options_association resource
is_valid_dhcp_options_association {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options_association""
        resource.expressions.dhcp_options_id.references[0] == ""aws_vpc_dhcp_options.default.id""
        resource.expressions.vpc_id.references[0] == ""aws_vpc.default.id""
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""default"" {
  cidr_block = ""192.168.0.0/16""
}

resource ""aws_vpc_dhcp_options"" ""default"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = concat([""192.168.56.102""], [""8.8.8.8""])
  netbios_name_servers = [""192.168.56.102""]
}

resource ""aws_vpc_dhcp_options_association"" ""default"" {
  vpc_id          = aws_vpc.default.id
  dhcp_options_id = aws_vpc_dhcp_options.default.id
}","Has one ""aws_vpc"" resource:
Configured with a ""cidr_block"" of ""192.168.0.0/16""
Includes associated custom DHCP options

Has one ""aws_vpc_dhcp_options"" resource:
Specifies a ""domain_name"" of ""windomain.local""
Includes DNS servers with a combination of a custom DNS server (""192.168.56.102"") and Google's DNS server (""8.8.8.8"")
Configures a ""NetBIOS"" name server at ""192.168.56.102""

Has one ""aws_vpc_dhcp_options_association"" resource:
Associates the ""aws_vpc_dhcp_options"" with the created ""aws_vpc""
Customizes DHCP settings specifically for the created VPC environment","I'll help you create a Terraform configuration for a VPC with custom DHCP options. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""dhcp_assoc"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a custom DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The configuration enables DNS hostnames and DNS support in the VPC for proper name resolution. All resources are properly tagged for better resource management.",,,,,,"I'll help you create a Terraform configuration for setting up a VPC with custom DHCP options based on your requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""dns_resolver"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a custom DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The configuration enables both DNS hostnames and DNS support in the VPC for proper name resolution. All resources are properly tagged for better resource management.",,,,,,"I'll help you create a Terraform configuration for setting up a VPC with custom DHCP options. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Modify this to your desired region
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""192.168.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""custom_dhcp_options"" {
  domain_name          = ""windomain.local""
  domain_name_servers  = [""192.168.56.102"", ""8.8.8.8""]
  netbios_name_servers = [""192.168.56.102""]

  tags = {
    Name = ""custom-dhcp-options""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""dhcp_association"" {
  vpc_id          = aws_vpc.main.id
  dhcp_options_id = aws_vpc_dhcp_options.custom_dhcp_options.id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC with the specified CIDR block (192.168.0.0/16)
3. Creates a custom DHCP options set with:
   - Domain name: windomain.local
   - DNS servers: 192.168.56.102 and 8.8.8.8
   - NetBIOS name server: 192.168.56.102
4. Associates the DHCP options set with the VPC

The VPC has DNS hostnames and DNS support enabled for better network functionality. All resources are properly tagged for better resource management.",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer with Lambda Target Group,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.values.subnet_mapping != null
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.ipv4_ipam_pool_id.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.ipv6_cider_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value != ""forward""
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    resources.values.target_type == ""lambda""
    
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group""
        resource.expressions.port != null
        resource.expressions.protocol != null
        resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
        is_valid_lb
        is_valid_lb_listener
        is_valid_lb_target_group_attachment
        is_valid_lb_target_group
        is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_iam_role"" ""lambda_role"" {
  name = ""example-lambda-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy_attachment"" ""lambda_policy_attachment"" {
  name       = ""example-lambda-policy-attachment""
  roles      = [aws_iam_role.lambda_role.name]
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}

data ""archive_file"" ""lambda-func"" {
  type        = ""zip""
  source_file = ""./supplement/index.js""
  output_path = ""./supplement/lambda_func.zip""
}

resource ""aws_lambda_function"" ""example"" {
  function_name = ""example-lambda""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs18.x""

  filename         = data.archive_file.lambda-func.output_path
  source_code_hash = data.archive_file.lambda-func.output_base64sha256
}

resource ""aws_lambda_permission"" ""elb_invocation"" {
  statement_id  = ""AllowExecutionFromALB""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.example.function_name
  principal     = ""elasticloadbalancing.amazonaws.com""
  source_arn    = aws_lb_target_group.target_group.arn
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  target_type = ""lambda""
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_lambda_function.example.arn
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id
    with target_type = ""lambda""

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Lambda target group. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create the Lambda function
resource ""aws_lambda_function"" ""example"" {
  filename         = ""lambda_function.zip""
  function_name    = ""example-lambda""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs16.x""

  environment {
    variables = {
      foo = ""bar""
    }
  }
}

# Create ALB
resource ""aws_lb"" ""example"" {
  name               = ""example-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb_sg.id]
  subnets            = [""subnet-12345678"", ""subnet-87654321""] # Replace with your subnet IDs

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
  }
}

# Create target group
resource ""aws_lb_target_group"" ""lambda"" {
  name        = ""example-lambda-target-group""
  target_type = ""lambda""

  health_check {
    enabled = true
    path    = ""/""
  }
}

# Create listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.lambda.arn
  }
}

# Attach Lambda to target group
resource ""aws_lb_target_group_attachment"" ""lambda"" {
  target_group_arn = aws_lb_target_group.lambda.arn
  target_id        = aws_lambda_function.example.arn
  depends_on       = [aws_lambda_permission.with_lb]
}

# Lambda permission for ALB
resource ""aws_lambda_permission"" ""with_lb"" {
  statement_id  = ""AllowExecutionFromlb""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.example.function_name
  principal     = ""elasticloadbalancing.amazonaws.com""
  source_arn    = aws_lb_target_group.lambda.arn
}

# Security group for the load balancer
resource ""aws_security_group"" ""lb_sg"" {
  name        = ""allow_http""
  description = ""Allow HTTP inbound traffic""
  vpc_id      = ""vpc-12345678"" # Replace with your VPC ID

  ingress {
    description = ""HTTP from anywhere""
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""allow_http""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_execution_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Basic execution policy for Lambda
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = """,,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Lambda target group. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create the ALB
resource ""aws_lb"" ""example"" {
  name               = ""example-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb.id]
  subnets            = [aws_subnet.example1.id, aws_subnet.example2.id]

  tags = {
    Environment = ""production""
  }
}

# Create ALB listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.lambda.arn
  }
}

# Create Lambda target group
resource ""aws_lb_target_group"" ""lambda"" {
  name        = ""lambda-target-group""
  target_type = ""lambda""

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    timeout             = 5
    matcher            = ""200""
    path               = ""/""
  }
}

# Create Lambda permission for ALB
resource ""aws_lambda_permission"" ""with_lb"" {
  statement_id  = ""AllowExecutionFromlb""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.example.function_name
  principal     = ""elasticloadbalancing.amazonaws.com""
  source_arn    = aws_lb_target_group.lambda.arn
}

# Attach Lambda to target group
resource ""aws_lb_target_group_attachment"" ""lambda"" {
  target_group_arn = aws_lb_target_group.lambda.arn
  target_id        = aws_lambda_function.example.arn
  depends_on       = [aws_lambda_permission.with_lb]
}

# Security group for ALB
resource ""aws_security_group"" ""alb"" {
  name        = ""allow_http""
  description = ""Allow HTTP inbound traffic""
  vpc_id      = aws_vpc.main.id

  ingress {
    description = ""HTTP from anywhere""
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""allow_http""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. A Lambda target group
3. An ALB listener that forwards traffic to the Lambda target group
4. The necessary Lambda permissions for the ALB to invoke the function
5. A target group attachment connecting the Lambda function to the target group
6. A security group for the ALB allowing HTTP traffic

Note: This configuration assumes you have already:
- Created a VPC with subnets (referenced as aws_subnet.example1 and aws_subnet.example2)
- Created a Lambda function (referenced as aws_lambda_function.example)
- Created a VPC (referenced as aws_vpc.main)

You'll need to replace these references with your actual resources or add the corresponding resource definitions to the configuration.",,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a Lambda target group. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create the ALB
resource ""aws_lb"" ""example"" {
  name               = ""example-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb_sg.id]
  subnets            = [aws_subnet.example1.id, aws_subnet.example2.id]

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
  }
}

# Create the target group
resource ""aws_lb_target_group"" ""lambda"" {
  name        = ""example-lambda-tg""
  target_type = ""lambda""

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval           = 30
    timeout            = 5
    matcher            = ""200""
    path               = ""/health""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    unhealthy_threshold = 3
  }
}

# Create a listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.example.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.lambda.arn
  }
}

# Create Lambda permission for ALB
resource ""aws_lambda_permission"" ""with_lb"" {
  statement_id  = ""AllowExecutionFromlb""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.example.function_name
  principal     = ""elasticloadbalancing.amazonaws.com""
  source_arn    = aws_lb_target_group.lambda.arn
}

# Attach the Lambda function to the target group
resource ""aws_lb_target_group_attachment"" ""lambda"" {
  target_group_arn = aws_lb_target_group.lambda.arn
  target_id        = aws_lambda_function.example.arn
  depends_on       = [aws_lambda_permission.with_lb]
}

# Example Security Group for the ALB
resource ""aws_security_group"" ""lb_sg"" {
  name        = ""allow_http""
  description = ""Allow HTTP inbound traffic""
  vpc_id      = aws_vpc.example.id

  ingress {
    description = ""HTTP from anywhere""
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""allow_http""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. A Lambda target group
3. A listener on port 80
4. Required permissions for the ALB to invoke the Lambda function
5. The target group attachment
6. A security group for the ALB

Note: This configuration assumes you have already defined:
- VPC resources (vpc, subnets)
- A Lambda function (aws_lambda_function.example)

You'll need to adjust the subnet IDs, VPC ID, and other specific values according to your environment. Also, consider adding HTTPS support and proper security measures for production use.",,,,,
"aws_cloudwatch_log_group, aws_cloudwatch_log_resource_policy, aws_route53_query_log, aws_route53_zone, aws_iam_policy_document","Configure a query log that can create a log stream and put log events using Route 53 resources. Name the zone ""primary"", the cloudwatch log group ""aws_route53_example_com"", and the cloudwatch log resource policy ""route53-query-logging-policy""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_cloudwatch_log_group = false

default is_valid_cloudwatch_log_resource_policy = false

default is_valid_route53_query_log = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name
}

is_valid_cloudwatch_log_group {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_cloudwatch_log_group""
}

is_valid_cloudwatch_log_resource_policy {
    some i
    resource := input.resource_changes[i]
    resource.type == ""aws_cloudwatch_log_resource_policy""
    contains(resource.change.after.policy_document, ""logs:PutLogEvents"")
    contains(resource.change.after.policy_document, ""logs:CreateLogStream"")
    resource.change.after.policy_name
}

is_valid_route53_query_log {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_query_log""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.primary.zone_id""
    resource.expressions.cloudwatch_log_group_arn.references[0] == ""aws_cloudwatch_log_group.aws_route53_example_com.arn""
    resource.depends_on[0] == ""aws_cloudwatch_log_resource_policy.route53-query-logging-policy""
}

# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_cloudwatch_log_group
    is_valid_cloudwatch_log_resource_policy
    is_valid_route53_query_log
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}


resource ""aws_route53_zone"" ""primary"" {
  name = ""example53.com""
}

resource ""aws_cloudwatch_log_group"" ""aws_route53_example_com"" {
  name              = ""/aws/route53/${aws_route53_zone.primary.name}""
  retention_in_days = 30
}

# Example CloudWatch log resource policy to allow Route53 to write logs
# to any log group under /aws/route53/*

data ""aws_iam_policy_document"" ""route53-query-logging-policy"" {
  statement {
    actions = [
      ""logs:CreateLogStream"",
      ""logs:PutLogEvents"",
    ]

    resources = [""arn:aws:logs:*:*:log-group:/aws/route53/*""]

    principals {
      identifiers = [""route53.amazonaws.com""]
      type        = ""Service""
    }
  }
}

resource ""aws_cloudwatch_log_resource_policy"" ""route53-query-logging-policy"" {
  policy_document = data.aws_iam_policy_document.route53-query-logging-policy.json
  policy_name     = ""route53-query-logging-policy""
}

resource ""aws_route53_query_log"" ""example_com"" {
  depends_on = [aws_cloudwatch_log_resource_policy.route53-query-logging-policy]

  cloudwatch_log_group_arn = aws_cloudwatch_log_group.aws_route53_example_com.arn
  zone_id                  = aws_route53_zone.primary.zone_id
}","Has one ""aws_route53_zone"" resource
    with ""name""

Has one ""aws_cloudwatch_log_group"" resource

Has one ""aws_cloudwatch_log_resource_policy"" resource
    with ""policy_document"" enabling ""logs:CreateLogStream"" and ""logs:PutLogEvents""
    with ""policy_name""

Has one ""aws_route53_query_log"" resource
    with ""cloudwatch_log_group_arn"" referencing the ""aws_cloudwatch_log_group"" resource
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
    and depends on the ""aws_cloudwatch_log_resource_policy"" resource","I'll help you create a Terraform configuration for Route 53 query logging with CloudWatch Log Group integration. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust the region as needed
}

# Create a Route 53 Zone
resource ""aws_route53_zone"" ""primary"" {
  name = ""example.com""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""aws_route53_example_com"" {
  name              = ""/aws/route53/example.com""
  retention_in_days = 30
}

# Create CloudWatch Log Resource Policy
resource ""aws_cloudwatch_log_resource_policy"" ""route53-query-logging-policy"" {
  policy_name = ""route53-query-logging-policy""

  policy_document = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""route53.amazonaws.com""
        }
        Action = [
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = ""${aws_cloudwatch_log_group.aws_route53_example_com.arn}:*""
      }
    ]
  })
}

# Create Route 53 Query Log
resource ""aws_route53_query_log"" ""query_log"" {
  depends_on = [aws_cloudwatch_log_resource_policy.route53-query-logging-policy]

  cloudwatch_log_group_arn = aws_cloudwatch_log_group.aws_route53_example_com.arn
  zone_id                  = aws_route53_zone.primary.zone_id
}
```

This configuration:
1. Creates a Route 53 hosted zone named ""primary""
2. Creates a CloudWatch Log Group named ""aws_route53_example_com""
3. Creates a CloudWatch Log Resource Policy named ""route53-query-logging-policy""
4. Sets up Route 53 query logging to send logs to the CloudWatch Log Group

The policy allows Route 53 to create log streams and put log events in the specified CloudWatch Log Group. The `depends_on` block ensures that the log resource policy is created before attempting to set up the query log.",,,,,,"I'll help you create a Terraform configuration for Route 53 query logging with CloudWatch Log Group integration. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Replace with your desired region
}

# Create Route 53 Zone
resource ""aws_route53_zone"" ""primary"" {
  name = ""example.com""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""aws_route53_example_com"" {
  name              = ""/aws/route53/example.com""
  retention_in_days = 30
}

# Create CloudWatch Log Resource Policy
resource ""aws_cloudwatch_log_resource_policy"" ""route53-query-logging-policy"" {
  policy_name = ""route53-query-logging-policy""

  policy_document = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""route53.amazonaws.com""
        }
        Action = [
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = ""${aws_cloudwatch_log_group.aws_route53_example_com.arn}:*""
      }
    ]
  })
}

# Create Route 53 Query Log
resource ""aws_route53_query_log"" ""query_log"" {
  depends_on = [aws_cloudwatch_log_resource_policy.route53-query-logging-policy]

  cloudwatch_log_group_arn = aws_cloudwatch_log_group.aws_route53_example_com.arn
  zone_id                  = aws_route53_zone.primary.zone_id
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a Route 53 zone named ""primary""
3. Creates a CloudWatch Log Group named ""aws_route53_example_com""
4. Creates a CloudWatch Log Resource Policy named ""route53-query-logging-policy""
5. Sets up Route 53 query logging with the specified log group and zone

The configuration includes the necessary permissions for Route 53 to create log streams and put log events in the CloudWatch Log Group. The `depends_on` block ensures that the log resource policy is created before the query log is configured.

Remember to replace ""example.com"" with your actual domain name if different. Also, adjust the region in the provider block according to your needs.",,,,,,"I'll help you create a Terraform configuration for Route 53 query logging with CloudWatch Log Group integration. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create a Route 53 zone
resource ""aws_route53_zone"" ""primary"" {
  name = ""example.com""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""aws_route53_example_com"" {
  name              = ""/aws/route53/example.com""
  retention_in_days = 30
}

# Create CloudWatch Log Resource Policy
resource ""aws_cloudwatch_log_resource_policy"" ""route53-query-logging-policy"" {
  policy_name = ""route53-query-logging-policy""

  policy_document = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""route53.amazonaws.com""
        }
        Action = [
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = ""${aws_cloudwatch_log_group.aws_route53_example_com.arn}:*""
      }
    ]
  })
}

# Create Route 53 Query Log
resource ""aws_route53_query_log"" ""query_log"" {
  depends_on = [aws_cloudwatch_log_resource_policy.route53-query-logging-policy]

  cloudwatch_log_group_arn = aws_cloudwatch_log_group.aws_route53_example_com.arn
  zone_id                  = aws_route53_zone.primary.zone_id
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Route 53 zone named ""primary""
3. Creates a CloudWatch Log Group named ""aws_route53_example_com""
4. Creates a CloudWatch Log Resource Policy named ""route53-query-logging-policy""
5. Sets up Route 53 query logging to send logs to the CloudWatch Log Group

The policy allows Route 53 to create log streams and put log events in the specified CloudWatch Log Group. The `depends_on` ensures that the policy is created before attempting to set up query logging.",,,,,
"aws_db_instance, aws_db_subnet_group, aws_elastic_beanstalk_application, aws_elastic_beanstalk_environment, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment, aws_internet_gateway, aws_route_table, aws_route_table_association, aws_route_table_association, aws_security_group, aws_subnet, aws_subnet, aws_vpc","Configure an AWS VPC with internet gateway, two public subnets, a security group, and a route table associated with the public subnets. Then connect these resources to two Elastic Beanstalk environments. These environments are also connected to a shared RDS database that is linked to the subnets. Name the database ""shared_rds"", the security group ""eb_env_sg"", the vpc ""eb_vpc"", and the subnets ""eb_subnet_public_1"" and ""eb_subnet_public_2""","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_eb_app = false

default is_valid_eb_env = false

default is_valid_db_instance = false

default is_valid_vpc = false

default is_valid_internet_gateway = false

default is_valid_subnet = false

default is_valid_subnet_group = false

default is_valid_security_group = false

default is_valid_route_table = false

default is_valid_route_table_association = false


is_valid_vpc {
                some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_vpc""
    resource.expressions.cidr_block
}

is_valid_internet_gateway {
                        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_internet_gateway""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.eb_vpc.id""
}

is_valid_subnet {
                some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_subnet""
    resource.expressions.cidr_block
    resource.expressions.availability_zone
    
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_subnet""
    resource2.expressions.cidr_block
    resource2.expressions.availability_zone
    resource2.expressions.vpc_id.references[0] == resource.expressions.vpc_id.references[0]

}

is_valid_subnet_group {
                some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_db_subnet_group""
    resource.expressions.subnet_ids.references[0]

}

is_valid_security_group {
                some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_security_group""
    resource.expressions.vpc_id.references[0]
    resource.expressions.egress
    resource.expressions.ingress
}

is_valid_route_table {
                some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route_table""
    resource.expressions.route.references[0]
    resource.expressions.vpc_id.references
}

is_valid_route_table_association {
                some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route_table_association""
    resource.expressions.subnet_id.references[0]
    resource.expressions.route_table_id.references[0]
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_route_table_association""
    resource2.expressions.subnet_id.references[0]
    resource2.expressions.route_table_id.references[0]
    resource2.expressions.route_table_id.references[0] == resource.expressions.route_table_id.references[0]
    resource2.expressions.subnet_id.references[0] != resource.expressions.subnet_id.references[0]

}

is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0]
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0]
}

# Validate aws_eb_app
is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}

# Validate aws_eb_env
is_valid_eb_env {
        some a, b
    resource := input.configuration.root_module.resources[a]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.expressions.name
    resource.expressions.application.references[0]
    resource.expressions.solution_stack_name
    
    resource2 := input.configuration.root_module.resources[b]
    resource2.type == ""aws_elastic_beanstalk_environment""
    resource2.expressions.name
    resource2.expressions.application.references[0]
    resource2.expressions.solution_stack_name
    
    some c, d, e, f, g, h, i, j, k, l, m, n, o, p
    resource.expressions.setting[c].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource.expressions.setting[d].value.references[0] == ""aws_db_instance.shared_rds.username""
    resource.expressions.setting[e].value.references[0] == ""aws_db_instance.shared_rds.password""
    resource.expressions.setting[f].value.references[0] == ""aws_db_instance.shared_rds.address""
    resource.expressions.setting[g].value.references[0] == ""aws_security_group.eb_env_sg.id""
    resource.expressions.setting[h].value.references[0] == ""aws_vpc.eb_vpc.id""
    resource.expressions.setting[i].value.references[0] == ""aws_subnet.eb_subnet_public_1.id""
    resource.expressions.setting[i].value.references[2] == ""aws_subnet.eb_subnet_public_2.id""
    
    resource2.expressions.setting[j].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile.name""
    resource2.expressions.setting[k].value.references[0] == ""aws_db_instance.shared_rds.username""
    resource2.expressions.setting[l].value.references[0] == ""aws_db_instance.shared_rds.password""
    resource2.expressions.setting[m].value.references[0] == ""aws_db_instance.shared_rds.address""
    resource2.expressions.setting[n].value.references[0] == ""aws_security_group.eb_env_sg.id""
    resource2.expressions.setting[o].value.references[0] == ""aws_vpc.eb_vpc.id""
    resource2.expressions.setting[p].value.references[0] == ""aws_subnet.eb_subnet_public_1.id""
    resource2.expressions.setting[p].value.references[2] == ""aws_subnet.eb_subnet_public_2.id""
}

is_valid_db_instance {
                some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_db_instance""
    has_required_main_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_main_db_arguments {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_db_instance""
    resource.expressions.db_subnet_group_name.references[0]
    resource.expressions.vpc_security_group_ids.references[0]
    # Check for allocated_storage, engine, instance_class, username, password, and skip_final_snapshot
    requirement1(resource.expressions)
    # Check for instance_class validity
    requirement2(resource.expressions)
}



# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
    expressions.allocated_storage
    expressions.engine
    expressions.username
    expressions.password
    is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
    expressions.snapshot_identifier
}

# Check for instance_class validity
requirement2(expressions) {
    expressions.instance_class
    is_valid_instance_class(expressions.instance_class.constant_value)
}


# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}


# Combine all checks into a final rule
is_configuration_valid {
        is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
    is_valid_eb_app
    is_valid_eb_env
    is_valid_db_instance
    is_valid_vpc
    is_valid_internet_gateway
    is_valid_subnet
    is_valid_subnet_group
    is_valid_security_group
    is_valid_route_table
    is_valid_route_table_association
    
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# VPC
resource ""aws_vpc"" ""eb_vpc"" {
  cidr_block = ""10.0.0.0/16""
  enable_dns_support   = true
  enable_dns_hostnames = true
}

# Internet Gateway
resource ""aws_internet_gateway"" ""eb_igw"" {
  vpc_id = aws_vpc.eb_vpc.id
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Subnets
resource ""aws_subnet"" ""eb_subnet_public_1"" {
  vpc_id     = aws_vpc.eb_vpc.id
  cidr_block = ""10.0.1.0/24""

  map_public_ip_on_launch = true
  availability_zone       = data.aws_availability_zones.available.names[0]
}

resource ""aws_subnet"" ""eb_subnet_public_2"" {
  vpc_id     = aws_vpc.eb_vpc.id
  cidr_block = ""10.0.2.0/24""

  map_public_ip_on_launch = true
  availability_zone       = data.aws_availability_zones.available.names[1]
}

# Security groups for Elastic Beanstalk environments
resource ""aws_security_group"" ""eb_env_sg"" {
  name        = ""eb-env-sg""
  description = ""Security group for Elastic Beanstalk environments""
  vpc_id      = aws_vpc.eb_vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.eb_env_sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  from_port         = 80
  ip_protocol       = ""tcp""
  to_port           = 80
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress2"" {
  security_group_id = aws_security_group.eb_env_sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  from_port         = 443
  ip_protocol       = ""tcp""
  to_port           = 443
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.eb_env_sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  ip_protocol       = ""-1""
}

# DB subnet group for RDS instance
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name       = ""rds-subnet-group""
  subnet_ids = [aws_subnet.eb_subnet_public_1.id, aws_subnet.eb_subnet_public_2.id]
}

resource ""aws_route_table"" ""eb_route_table"" {
  vpc_id = aws_vpc.eb_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.eb_igw.id
  }
}

resource ""aws_route_table_association"" ""eb_route_table_association_1"" {
  subnet_id      = aws_subnet.eb_subnet_public_1.id
  route_table_id = aws_route_table.eb_route_table.id
}

resource ""aws_route_table_association"" ""eb_route_table_association_2"" {
  subnet_id      = aws_subnet.eb_subnet_public_2.id
  route_table_id = aws_route_table.eb_route_table.id
}

# RDS instance
resource ""aws_db_instance"" ""shared_rds"" {
  allocated_storage    = 20
  engine               = ""mysql""
  instance_class       = ""db.t3.micro""
  username             = ""dbadmin""
  password             = ""securepassword"" 
  backup_retention_period = 0
  skip_final_snapshot  = true
  db_subnet_group_name  = aws_db_subnet_group.rds_subnet_group.name

  vpc_security_group_ids = [aws_security_group.eb_env_sg.id]
}

# Elastic Beanstalk application
resource ""aws_elastic_beanstalk_application"" ""microservice_app"" {
  name        = ""MicroserviceApplication""
  description = ""An application for microservices""
}

resource ""aws_elastic_beanstalk_environment"" ""microservice_env1"" {
  name                = ""microservice-env1""
  application         = aws_elastic_beanstalk_application.microservice_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  # Elastic Beanstalk environment variables for RDS connection
  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOSTNAME""
    value     = aws_db_instance.shared_rds.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.shared_rds.username
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.shared_rds.password
  }

  setting {
    namespace = ""aws:ec2:vpc""
    name      = ""VPCId""
    value     = aws_vpc.eb_vpc.id
  }

  setting {
    namespace = ""aws:ec2:vpc""
    name      = ""Subnets""
    value     = ""${aws_subnet.eb_subnet_public_1.id},${aws_subnet.eb_subnet_public_2.id}""
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""SecurityGroups""
    value     = aws_security_group.eb_env_sg.id
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

resource ""aws_elastic_beanstalk_environment"" ""microservice_env2"" {
  name                = ""microservice-env2""
  application         = aws_elastic_beanstalk_application.microservice_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  # Elastic Beanstalk environment variables for RDS connection
  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOSTNAME""
    value     = aws_db_instance.shared_rds.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.shared_rds.username
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.shared_rds.password
  }

  setting {
    namespace = ""aws:ec2:vpc""
    name      = ""VPCId""
    value     = aws_vpc.eb_vpc.id
  }

  setting {
    namespace = ""aws:ec2:vpc""
    name      = ""Subnets""
    value     = ""${aws_subnet.eb_subnet_public_1.id},${aws_subnet.eb_subnet_public_2.id}""
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""SecurityGroups""
    value     = aws_security_group.eb_env_sg.id
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has one ""aws_vpc"" resource
    with ""cidr_block"" set to a valid value

Has one ""aws_internet_gateway"" resource
    with ""vpc_id"" referencing the ""aws_vpc"" resource

Has two ""aws_subnet"" resources
    with ""vpc_id"" referencing the ""aws_vpc"" resource
    with ""cidr_block"" set to a valid value
    with ""availability_zone"" set to a zone within the region

Has one ""aws_security_group"" resource
     with ""vpc_id"" referencing the ""aws_vpc"" resource
     with ""ingress"" blocks
     with ""egress"" block

Has one ""aws_db_subnet_group"" resource
     with ""subnet_ids"" referencing the ""aws_subnet"" resources

Has one ""aws_route_table"" resources
     with ""vpc_id"" referencing the ""aws_vpc"" resource
     with the ""route"" block containing ""gateway_id"" referencing the ""aws_internet_gateway"" resource

Has two ""aws_route_table_association"" resources
     with ""subnet_id"" referencing one of the ""aws_subnet"" resources
     with ""route_table_id"" referencing the ""aws_route_table"" resource

Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""


Has two ""aws_elastic_beanstalk_environment"" resources
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with a ""setting"" block that has a ""value"" that references the ""aws_iam_instance_profile"" resource
    with a ""setting"" block that has a ""value"" that references the ""id"" from all the ""aws_subnet"" resources
    with a ""setting"" block that has a ""value"" that references the ""id"" from the ""aws_vpc"" resource
    with a ""setting"" block that has a ""value"" that references the ""id"" from the ""aws_security_group"" resource
    with a ""setting"" block that has a ""value"" that references the ""address"" from the ""db_instance"" resource
    with a ""setting"" block that has a ""value"" that references the ""username"" from the ""db_instance"" resource 
    with a ""setting"" block that has a ""value"" that references the ""password"" from the ""db_instance"" resource 

Has a ""aws_db_instance"" resource
    with ""instance_class""
    with ""allocated_storage""
    with ""engine""
    with ""username""
    with ""password""
    with ""backup_retention_period"" set to a non-zero number
    with ""skip_final_snapshot""
    with ""db_subnet_group_name"" referencing the ""aws_db_subnet_group"" resource
    with ""vpc_security_group_ids"" referencing the ""aws_security_group"" resource","I'll help you create a Terraform configuration for the described AWS infrastructure. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC
resource ""aws_vpc"" ""eb_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""eb_vpc""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.eb_vpc.id

  tags = {
    Name = ""eb_igw""
  }
}

# Public Subnets
resource ""aws_subnet"" ""eb_subnet_public_1"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.1.0/24""
  availability_zone       = ""us-east-1a""
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_1""
  }
}

resource ""aws_subnet"" ""eb_subnet_public_2"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.2.0/24""
  availability_zone       = ""us-east-1b""
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_2""
  }
}

# Route Table
resource ""aws_route_table"" ""public"" {
  vpc_id = aws_vpc.eb_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""eb_public_route_table""
  }
}

# Route Table Associations
resource ""aws_route_table_association"" ""public1"" {
  subnet_id      = aws_subnet.eb_subnet_public_1.id
  route_table_id = aws_route_table.public.id
}

resource ""aws_route_table_association"" ""public2"" {
  subnet_id      = aws_subnet.eb_subnet_public_2.id
  route_table_id = aws_route_table.public.id
}

# Security Group
resource ""aws_security_group"" ""eb_env_sg"" {
  name        = ""eb_env_sg""
  description = ""Security group for Elastic Beanstalk environments""
  vpc_id      = aws_vpc.eb_vpc.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# RDS Subnet Group
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name       = ""eb-rds-subnet-group""
  subnet_ids = [aws_subnet.eb_subnet_public_1.id, aws_subnet.eb_subnet_public_2.id]

  tags = {
    Name = ""EB RDS subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""shared_rds"" {
  identifier           = ""shared-rds""
  allocated_storage    = 20
  storage_type        = ""gp2""
  engine              =",,,,,,"I'll help you create a Terraform configuration for AWS VPC with Elastic Beanstalk environments and RDS. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""eb_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""eb_vpc""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.eb_vpc.id

  tags = {
    Name = ""eb_igw""
  }
}

# Public Subnets
resource ""aws_subnet"" ""eb_subnet_public_1"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.1.0/24""
  availability_zone       = ""us-west-2a""
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_1""
  }
}

resource ""aws_subnet"" ""eb_subnet_public_2"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.2.0/24""
  availability_zone       = ""us-west-2b""
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_2""
  }
}

# Route Table
resource ""aws_route_table"" ""public"" {
  vpc_id = aws_vpc.eb_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""eb_public_rt""
  }
}

# Route Table Associations
resource ""aws_route_table_association"" ""public1"" {
  subnet_id      = aws_subnet.eb_subnet_public_1.id
  route_table_id = aws_route_table.public.id
}

resource ""aws_route_table_association"" ""public2"" {
  subnet_id      = aws_subnet.eb_subnet_public_2.id
  route_table_id = aws_route_table.public.id
}

# Security Group
resource ""aws_security_group"" ""eb_env_sg"" {
  name        = ""eb_env_sg""
  description = ""Security group for Elastic Beanstalk environments""
  vpc_id      = aws_vpc.eb_vpc.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# RDS Subnet Group
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name       = ""eb-rds-subnet-group""
  subnet_ids = [aws_subnet.eb_subnet_public_1.id, aws_subnet.eb_subnet_public_2.id]

  tags = {
    Name = ""EB RDS subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""shared_rds"" {
  identifier           = ""shared-rds""
  allocated_storage    = 20
  storage_type        = """,,,,,,"I'll help you create a Terraform configuration for this AWS infrastructure setup. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""eb_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""eb_vpc""
  }
}

# Internet Gateway
resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.eb_vpc.id

  tags = {
    Name = ""eb_igw""
  }
}

# Public Subnets
resource ""aws_subnet"" ""eb_subnet_public_1"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.1.0/24""
  availability_zone       = ""us-west-2a""
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_1""
  }
}

resource ""aws_subnet"" ""eb_subnet_public_2"" {
  vpc_id                  = aws_vpc.eb_vpc.id
  cidr_block              = ""10.0.2.0/24""
  availability_zone       = ""us-west-2b""
  map_public_ip_on_launch = true

  tags = {
    Name = ""eb_subnet_public_2""
  }
}

# Route Table
resource ""aws_route_table"" ""public"" {
  vpc_id = aws_vpc.eb_vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name = ""eb_public_rt""
  }
}

# Route Table Associations
resource ""aws_route_table_association"" ""public_1"" {
  subnet_id      = aws_subnet.eb_subnet_public_1.id
  route_table_id = aws_route_table.public.id
}

resource ""aws_route_table_association"" ""public_2"" {
  subnet_id      = aws_subnet.eb_subnet_public_2.id
  route_table_id = aws_route_table.public.id
}

# Security Group
resource ""aws_security_group"" ""eb_env_sg"" {
  name        = ""eb_env_sg""
  description = ""Security group for Elastic Beanstalk environments""
  vpc_id      = aws_vpc.eb_vpc.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# RDS Subnet Group
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name       = ""rds-subnet-group""
  subnet_ids = [aws_subnet.eb_subnet_public_1.id, aws_subnet.eb_subnet_public_2.id]

  tags = {
    Name = ""RDS subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""shared_rds"" {
  identifier           = ""shared-rds""
  allocated_storage    = 20
  storage_type        = ""gp2""
  engine              = ""mysql""",,,,,
"aws_codebuild_project, aws_iam_policy, aws_iam_role, aws_iam_role_policy_attachment, aws_s3_bucket, aws_iam_policy_document, aws_iam_policy_document",Infrastructure for an autograder for a CS class. The autograder needs to run students' code stored on GitHub in a specific environment and store the result. Accomplish this using AWS CodeBuild.,"package autograder_high

import rego.v1

codebuild_project_valid(codebuild_project) if {
        some artifact in codebuild_project.expressions.artifacts
        artifact.location
        artifact.name
        artifact.type
        some environment in codebuild_project.expressions.environment
        environment.compute_type
        environment.image
        environment.type

        some source in codebuild_project.expressions.source
        source.type.constant_value == ""GITHUB""
        source.location
}

default valid := false

valid if {
        resources := input.configuration.root_module.resources
        some codebuild_project in resources
        codebuild_project.type == ""aws_codebuild_project""
        some security_group in resources
        codebuild_project_valid(codebuild_project)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""artifact-bucket-""
}

resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder_build""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip"" # include this
  }

  environment { # arguments required, exact value not specified
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1 # good to have, not required
    location        = ""github.com/source-location""
  }
}

resource ""aws_iam_role"" ""autograder_build_role"" {
  assume_role_policy = data.aws_iam_policy_document.autograder_build_policy_assume.json
}

data ""aws_iam_policy_document"" ""autograder_build_policy_assume"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }
  }

}

data ""aws_iam_policy_document"" ""autograder_build_policy"" {
  statement {
    effect = ""Allow""
    actions = [
      ""ec2:DescribeSubnets"",
      ""ec2:DescribeSecurityGroups"",
      ""ec2:DescribeVpcs""
    ]
    resources = [
      ""*""
    ]
  }
}

resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name        = ""lambda_policy""
  description = ""Grants permissions to Lambda to describe vpc, subnet, security group""

  policy = data.aws_iam_policy_document.autograder_build_policy.json
}

resource ""aws_iam_role_policy_attachment"" ""lambda_policy_attach"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}","have one ""aws_codebuild_project"" resource
  with ""artifacts"" argument block
    with ""type"" argument
    with ""location"" argument
    with ""name"" argument
  with ""environment"" argument block
    with ""computer_type"" argument
    with ""image"" argument
    with ""type"" argument
  with ""source"" argument block
    with ""type"" argument with value ""GITHUB""
    with ""location"" argument","I'll help you create a Terraform configuration for an AWS CodeBuild-based autograder system. This will include a CodeBuild project that can pull from GitHub, execute code in a controlled environment, and store results.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust region as needed
}

# S3 bucket to store build results
resource ""aws_s3_bucket"" ""autograder_results"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""autograder_results_versioning"" {
  bucket = aws_s3_bucket.autograder_results.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          aws_s3_bucket.autograder_results.arn,
          ""${aws_s3_bucket.autograder_results.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class assignments""
  build_timeout = ""30""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.autograder_results.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""GITHUB_REPO""
      value = ""https://github.com/your-org/assignment-repo""  # Replace with your repo
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/assignment-repo.git""  # Replace with your repo
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""autograder-log-group""
      stream_name = ""autograder-log-stream""
    }
  }

  cache {
    type = ""NO_CACHE""
  }
}

# GitHub webhook for automated builds
resource ""aws_codebuild_webhook"" ""autograder_webhook"" {
  project_name = aws_codebuild_project.autograder",,,,,,"I'll help you create a Terraform configuration for an AWS CodeBuild-based autograder. This solution will include a CodeBuild project that can pull code from GitHub, run it in a specified environment, and store the results.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# S3 bucket to store build results
resource ""aws_s3_bucket"" ""autograder_results"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""autograder_results_versioning"" {
  bucket = aws_s3_bucket.autograder_results.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.autograder_results.arn}"",
          ""${aws_s3_bucket.autograder_results.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""cs-class-autograder""
  description   = ""Autograder for CS class assignments""
  build_timeout = ""15""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.autograder_results.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""GITHUB_TOKEN""
      value = ""placeholder""  # Replace with actual GitHub token using variables
      type  = ""PARAMETER_STORE""
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""  # Replace with actual repo
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      status = ""ENABLED""
    }

    s3_logs {
      status   = ""ENABLED""
      location = ""${aws_s3_bucket.autograder_results.id}/build-logs""
    }
  }

  tags = {
    Environment = ""Education""
    Purpose     = ""Autograding""
  }
}

# CloudWatch Log Group",,,,,,"I'll help you create a Terraform configuration for an AWS CodeBuild-based autograder. This solution will include:
1. CodeBuild project to run the tests
2. IAM roles and policies
3. S3 bucket for storing results
4. GitHub integration

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# S3 bucket for storing autograder results
resource ""aws_s3_bucket"" ""autograder_results"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""autograder_results_versioning"" {
  bucket = aws_s3_bucket.autograder_results.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for the CodeBuild role
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          aws_s3_bucket.autograder_results.arn,
          ""${aws_s3_bucket.autograder_results.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:ListBucket""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""student-code-autograder""
  description   = ""Autograder for student code submissions""
  build_timeout = ""10""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.autograder_results.id
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""RESULTS_BUCKET""
      value = aws_s3_bucket.autograder_results.id
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""autograder-logs""
      stream_name = ""build-logs""
      status      = ""ENABLED""
    }
  }

  cache {
    type  = ""LOCAL""
    modes = [""LOCAL_DOCKER_LAYER_CACHE""]
  }
}

# GitHub webhook for automated builds
resource ""aws_codebuild_webhook"" ""autograder_webhook"" {
  ",,,,,
"aws_lightsail_disk, aws_lightsail_disk, aws_lightsail_disk_attachment, aws_lightsail_disk_attachment, aws_lightsail_instance, aws_availability_zones",Create a Lightsail instance with multiple attached disks,"package terraform.validation

default has_valid_resources = false

# Rule for multiple aws_lightsail_disk resources
has_valid_lightsail_disks {
    count([disk | disk := input.planned_values.root_module.resources[_]; disk.type == ""aws_lightsail_disk""; disk.values.name; disk.values.size_in_gb; disk.values.availability_zone]) > 0
}

# Rule for multiple aws_lightsail_disk_attachment resources
has_valid_lightsail_disk_attachments {
    count([attachment | attachment := input.planned_values.root_module.resources[_]; attachment.type == ""aws_lightsail_disk_attachment""; attachment.values.disk_name; attachment.values.instance_name; attachment.values.disk_path]) > 0
}

# Rule for aws_lightsail_instance resource with specific arguments
has_valid_lightsail_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id
    resource.values.bundle_id
}

# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lightsail_disks
    has_valid_lightsail_disk_attachments
    has_valid_lightsail_instance
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""

  filter {
    name   = ""opt-in-status""
    values = [""opt-in-not-required""]
  }
}

resource ""aws_lightsail_disk"" ""test1"" {
  name              = ""test-disk-1""
  size_in_gb        = 8
  availability_zone = data.aws_availability_zones.available.names[0]
}

resource ""aws_lightsail_disk"" ""test2"" {
  name              = ""test-disk-2""
  size_in_gb        = 8
  availability_zone = data.aws_availability_zones.available.names[0]
}

resource ""aws_lightsail_instance"" ""test"" {
  name              = ""test-instance""
  availability_zone = data.aws_availability_zones.available.names[0]
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_1_0""
}

resource ""aws_lightsail_disk_attachment"" ""test1"" {
  disk_name     = aws_lightsail_disk.test1.name
  instance_name = aws_lightsail_instance.test.name
  disk_path     = ""/dev/xvdf""
}

resource ""aws_lightsail_disk_attachment"" ""test2"" {
  disk_name     = aws_lightsail_disk.test2.name
  instance_name = aws_lightsail_instance.test.name
  disk_path     = ""/dev/xvdg""
}","have multiple aws_lightsail_disk resource
    with name
    with size_in_gb
    with availability_zone

have multiple aws_lightsail_disk_attachment
    with disk_name
    with instance_name
    with disk_path

have one ""aws_lightsail_instance"" resource
    with ""name"" argument
    with ""availability_zone"" argument
    with ""blueprint_id"" argument
    with ""bundle_id"" argument","I'll help you create a Terraform configuration for an AWS Lightsail instance with multiple attached disks. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Lightsail Instance
resource ""aws_lightsail_instance"" ""example_instance"" {
  name              = ""example-lightsail-instance""
  availability_zone = ""us-west-2a""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""  # Smallest instance size
  
  tags = {
    Environment = ""Development""
  }
}

# Create first attached disk
resource ""aws_lightsail_disk"" ""disk1"" {
  name              = ""example-disk-1""
  size_in_gb       = 32
  availability_zone = ""us-west-2a""
}

# Create second attached disk
resource ""aws_lightsail_disk"" ""disk2"" {
  name              = ""example-disk-2""
  size_in_gb       = 64
  availability_zone = ""us-west-2a""
}

# Attach first disk to instance
resource ""aws_lightsail_disk_attachment"" ""disk1_attachment"" {
  disk_name     = aws_lightsail_disk.disk1.name
  instance_name = aws_lightsail_instance.example_instance.name
  disk_path     = ""/dev/xvdf""
}

# Attach second disk to instance
resource ""aws_lightsail_disk_attachment"" ""disk2_attachment"" {
  disk_name     = aws_lightsail_disk.disk2.name
  instance_name = aws_lightsail_instance.example_instance.name
  disk_path     = ""/dev/xvdg""
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail instance using Amazon Linux 2
3. Creates two separate Lightsail disks (32GB and 64GB)
4. Attaches both disks to the Lightsail instance with different disk paths

You can modify the following parameters according to your needs:
- Region and availability zone
- Instance blueprint_id and bundle_id
- Disk sizes
- Names and tags
- Disk paths

Make sure to initialize Terraform and apply the configuration to create these resources in your AWS account.",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail instance with multiple attached disks. Here's a complete solution that includes the required provider and creates a Lightsail instance with multiple attached disks:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

# Create Lightsail Instance
resource ""aws_lightsail_instance"" ""example_instance"" {
  name              = ""example-lightsail-instance""
  availability_zone = ""us-east-1a""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""  # Smallest instance size

  tags = {
    Environment = ""Development""
  }
}

# Create first attached disk
resource ""aws_lightsail_disk"" ""disk1"" {
  name              = ""example-disk-1""
  size_in_gb       = 32
  availability_zone = ""us-east-1a""
}

# Create second attached disk
resource ""aws_lightsail_disk"" ""disk2"" {
  name              = ""example-disk-2""
  size_in_gb       = 64
  availability_zone = ""us-east-1a""
}

# Attach first disk to instance
resource ""aws_lightsail_disk_attachment"" ""disk1_attachment"" {
  disk_name     = aws_lightsail_disk.disk1.name
  instance_name = aws_lightsail_instance.example_instance.name
  disk_path     = ""/dev/xvdf""
}

# Attach second disk to instance
resource ""aws_lightsail_disk_attachment"" ""disk2_attachment"" {
  disk_name     = aws_lightsail_disk.disk2.name
  instance_name = aws_lightsail_instance.example_instance.name
  disk_path     = ""/dev/xvdg""
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail instance using Amazon Linux 2
3. Creates two separate Lightsail disks (32GB and 64GB)
4. Attaches both disks to the Lightsail instance with different device paths

You can modify the following parameters according to your needs:
- Region and availability zone
- Instance blueprint_id and bundle_id
- Disk sizes
- Disk paths
- Instance and disk names
- Tags

Make sure to initialize Terraform and apply the configuration to create these resources in your AWS account.",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail instance with multiple attached disks. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create the main Lightsail instance
resource ""aws_lightsail_instance"" ""example_instance"" {
  name              = ""example-instance""
  availability_zone = ""us-west-2a""
  blueprint_id      = ""amazon_linux_2""
  bundle_id         = ""nano_2_0""  # Smallest instance size

  tags = {
    Environment = ""Development""
  }
}

# Create the first attached disk
resource ""aws_lightsail_disk"" ""disk1"" {
  name              = ""example-disk-1""
  size_in_gb       = 8
  availability_zone = aws_lightsail_instance.example_instance.availability_zone
}

# Create the second attached disk
resource ""aws_lightsail_disk"" ""disk2"" {
  name              = ""example-disk-2""
  size_in_gb       = 16
  availability_zone = aws_lightsail_instance.example_instance.availability_zone
}

# Attach the first disk to the instance
resource ""aws_lightsail_disk_attachment"" ""disk1_attachment"" {
  disk_name     = aws_lightsail_disk.disk1.name
  instance_name = aws_lightsail_instance.example_instance.name
  disk_path     = ""/dev/xvdf""
}

# Attach the second disk to the instance
resource ""aws_lightsail_disk_attachment"" ""disk2_attachment"" {
  disk_name     = aws_lightsail_disk.disk2.name
  instance_name = aws_lightsail_instance.example_instance.name
  disk_path     = ""/dev/xvdg""
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail instance using Amazon Linux 2
3. Creates two separate Lightsail disks (8GB and 16GB)
4. Attaches both disks to the instance with different device paths

You can modify the following parameters according to your needs:
- Region and availability zone
- Instance blueprint_id and bundle_id
- Disk sizes
- Device paths for the disk attachments
- Instance and disk names
- Tags

Remember to initialize Terraform and apply the configuration after saving it.",,,,,
aws_kinesis_stream,Generate a basic Amazon Kinesis stream,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_kinesis_stream {
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_stream""
    resource.values.name
}

has_valid_resources {
	has_valid_kinesis_stream
}",2,"provider ""aws"" {
  region = ""us-west-2""
}

resource ""aws_kinesis_stream"" ""test_stream"" {
  name             = ""drow1""
  shard_count      = 1
  retention_period = 48

  shard_level_metrics = [
    ""IncomingBytes"",
    ""OutgoingBytes"",
  ]

  stream_mode_details {
    stream_mode = ""PROVISIONED""
  }

  tags = {
    Environment = ""test""
  }
}","Has one ""aws_kinesis_stream"" resource","I'll help you create a basic Amazon Kinesis stream using Terraform. Here's a configuration that sets up a Kinesis stream with basic settings:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_kinesis_stream"" ""example_stream"" {
  name             = ""example-kinesis-stream""
  shard_count      = 1  # Number of shards, adjust based on your throughput needs
  retention_period = 24 # Data retention in hours (default is 24)

  stream_mode_details {
    stream_mode = ""PROVISIONED""  # Can be PROVISIONED or ON_DEMAND
  }

  tags = {
    Environment = ""production""
    Project     = ""example""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Kinesis stream with:
   - A single shard (can be increased based on throughput needs)
   - 24-hour retention period
   - PROVISIONED stream mode
   - Basic tags for resource management

You can modify the configuration by adjusting:
- The `shard_count` based on your throughput requirements
- The `retention_period` (between 24 to 168 hours)
- The stream mode between PROVISIONED and ON_DEMAND
- Tags according to your organization's needs",,,,,,"I'll help you create a basic Amazon Kinesis stream using Terraform. Here's a solution that includes the required AWS provider and a basic Kinesis stream configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired AWS region
}

resource ""aws_kinesis_stream"" ""example_stream"" {
  name             = ""example-kinesis-stream""
  shard_count      = 1
  retention_period = 24  # Retention period in hours

  stream_mode_details {
    stream_mode = ""PROVISIONED""
  }

  tags = {
    Environment = ""development""
    Name        = ""example-kinesis-stream""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Kinesis stream with:
   - A single shard
   - 24-hour retention period
   - Provisioned capacity mode
   - Basic tagging for resource management

You can modify the shard count, retention period, and tags according to your specific needs. The stream will be created in the specified AWS region with the default encryption settings (Server-Side Encryption using AWS-managed KMS keys).",,,,,,"I'll help you create a basic Amazon Kinesis stream using Terraform. Here's a configuration that sets up a Kinesis stream with basic settings:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_kinesis_stream"" ""example_stream"" {
  name             = ""example-kinesis-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""development""
    Project     = ""example""
  }

  stream_mode_details {
    stream_mode = ""PROVISIONED""
  }

  encryption_type = ""KMS""
  kms_key_id     = ""alias/aws/kinesis""  # Using default AWS managed key for Kinesis
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Kinesis stream with:
   - 1 shard (can be increased based on your throughput needs)
   - 24-hour retention period
   - Server-side encryption using AWS KMS
   - PROVISIONED stream mode
   - Basic tagging for resource management
   
You can customize the configuration by:
- Adjusting the `shard_count` based on your throughput requirements
- Modifying the `retention_period` (between 24 to 168 hours)
- Changing the `stream_mode` to ""ON_DEMAND"" if you prefer dynamic scaling
- Adding more tags or modifying the encryption settings",,,,,
"aws_instance, aws_placement_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc",Create 3 EC2 instances within an EC2 placement group usign clustering.,"package main

import future.keywords.in

default allow = false

# Check for VPC with public and private subnets
vpc_exists {
    some i
    input.resource_changes[i].type == ""aws_vpc""
}

public_subnet_exists(resources) {
    some resource in resources
    resource.type == ""aws_subnet""
    resource.change.after.map_public_ip_on_launch == true
}

private_subnet_exists(resources) {
    some resource in resources
    resource.type == ""aws_subnet""
    resource.change.after.map_public_ip_on_launch == false
}

aws_instances[resource] {
    resource := input.resource_changes[_]
    resource.type == ""aws_instance""
}

# Aggregate all checks
allow {
    vpc_exists
    public_subnet_exists(input.resource_changes)
    private_subnet_exists(input.resource_changes)
    count(aws_instances) == 3
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

# Create a Virtual Private Cloud (VPC)
resource ""aws_vpc"" ""my_vpc"" {
  cidr_block = ""10.0.0.0/16""
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

# Create Public Subnet
resource ""aws_subnet"" ""public_subnet"" {
  vpc_id                  = aws_vpc.my_vpc.id
  cidr_block              = ""10.0.1.0/24""
  availability_zone       = data.aws_availability_zones.azs.names[0]
  map_public_ip_on_launch = true
}

# Create Private Subnet
resource ""aws_subnet"" ""private_subnet"" {
  vpc_id            = aws_vpc.my_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = data.aws_availability_zones.azs.names[1]
}

# Create EC2 Placement Group
resource ""aws_placement_group"" ""my_placement_group"" {
  name     = ""my-placement-group""
  strategy = ""cluster""
}

# Create Security Group for EC2 instances
resource ""aws_security_group"" ""ec2_sg"" {
  vpc_id = aws_vpc.my_vpc.id
  name   = ""ec2_sg""
}

data ""aws_ami"" ""amzn2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

# Create EC2 instances in the Placement Group in the Private Subnet
resource ""aws_instance"" ""ec2_instance"" {
  count           = 3
  ami             = data.aws_ami.amzn2.id
  instance_type   = ""m5.large""
  subnet_id       = aws_subnet.private_subnet.id
  security_groups = [aws_security_group.ec2_sg.id]
  placement_group = aws_placement_group.my_placement_group.name
}",,"I'll help you create a Terraform configuration for 3 EC2 instances within a clustered placement group. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create the placement group
resource ""aws_placement_group"" ""cluster"" {
  name     = ""cluster-placement-group""
  strategy = ""cluster""
}

# Create EC2 instances in the placement group
resource ""aws_instance"" ""cluster_instances"" {
  count = 3

  ami           = ""ami-0c55b159cbfafe1f0""  # Replace with your desired AMI ID
  instance_type = ""c5.large""  # Use instance types that support placement groups

  # Place the instances in the placement group
  placement_group = aws_placement_group.cluster.id

  tags = {
    Name = ""cluster-instance-${count.index + 1}""
  }

  # Add any additional configuration as needed
  root_block_device {
    volume_size = 20
    volume_type = ""gp3""
  }

  vpc_security_group_ids = []  # Add your security group IDs
  subnet_id              = """"  # Add your subnet ID

  lifecycle {
    create_before_destroy = true
  }
}

# Output the instance IDs
output ""instance_ids"" {
  value = aws_instance.cluster_instances[*].id
}

# Output the placement group name
output ""placement_group_name"" {
  value = aws_placement_group.cluster.name
}
```

This configuration:
1. Creates a placement group with the ""cluster"" strategy
2. Launches 3 EC2 instances within the placement group using count
3. Tags each instance with a unique name
4. Includes basic instance configuration with root volume
5. Provides outputs for instance IDs and placement group name

Note: 
- Remember to replace the AMI ID with a valid one for your region
- Add appropriate security group IDs and subnet ID
- Choose an appropriate instance type that supports placement groups
- The cluster placement strategy requires specific instance types and is best for applications that need low latency and high network throughput

You can customize the instance configuration (instance type, storage, etc.) based on your specific requirements.",,,,,,"I'll help you create a Terraform configuration for 3 EC2 instances within a placement group using clustering strategy. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create the placement group
resource ""aws_placement_group"" ""cluster"" {
  name     = ""cluster-placement-group""
  strategy = ""cluster""
}

# Create EC2 instances
resource ""aws_instance"" ""cluster_instances"" {
  count = 3

  ami           = ""ami-0c55b159cbfafe1f0""  # Replace with your desired AMI ID
  instance_type = ""c5.large""  # Use instance type that supports placement groups

  # Place the instances in the placement group
  placement_group = aws_placement_group.cluster.id

  tags = {
    Name = ""cluster-instance-${count.index + 1}""
  }

  # Add other necessary configurations
  vpc_security_group_ids = []  # Add your security group IDs
  subnet_id             = """"   # Add your subnet ID

  root_block_device {
    volume_size = 20
    volume_type = ""gp3""
  }
}

# Output the instance IDs
output ""instance_ids"" {
  value = aws_instance.cluster_instances[*].id
}

# Output the placement group name
output ""placement_group_name"" {
  value = aws_placement_group.cluster.name
}
```

This configuration:
1. Creates a placement group with clustering strategy
2. Launches 3 EC2 instances using count
3. Places all instances in the same placement group
4. Includes basic configurations for the instances
5. Provides outputs for instance IDs and placement group name

Note: 
- Remember to replace the AMI ID with a valid one for your region
- Add appropriate security group IDs and subnet ID
- Clustering placement strategy requires specific instance types that support it
- The instances should be launched in the same AZ for optimal performance

Make sure to adjust the configuration according to your specific needs and network setup.",,,,,,"I'll help you create a Terraform configuration for 3 EC2 instances within a clustered placement group. Here's a solution that includes the required AWS provider and creates the placement group and EC2 instances:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create the placement group
resource ""aws_placement_group"" ""cluster"" {
  name     = ""cluster-placement-group""
  strategy = ""cluster""
}

# Create 3 EC2 instances in the placement group
resource ""aws_instance"" ""clustered_instances"" {
  count = 3

  ami           = ""ami-0c55b159cbfafe1f0""  # Replace with your desired AMI ID
  instance_type = ""c5.large""  # Use an instance type that supports placement groups

  placement_group = aws_placement_group.cluster.id

  tags = {
    Name = ""clustered-instance-${count.index + 1}""
  }

  # You might want to add other configurations like:
  # vpc_security_group_ids = [aws_security_group.example.id]
  # subnet_id             = aws_subnet.example.id
  
  root_block_device {
    volume_size = 20
    volume_type = ""gp3""
  }
}

output ""instance_ids"" {
  value = aws_instance.clustered_instances[*].id
}

output ""instance_private_ips"" {
  value = aws_instance.clustered_instances[*].private_ip
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a placement group with clustering strategy
3. Launches 3 EC2 instances within the placement group
4. Uses count to create multiple instances
5. Tags each instance with a unique name
6. Includes outputs for instance IDs and private IPs

Note: When using cluster placement groups:
- Not all instance types support placement groups
- Instances should be launched in a single launch request for optimal performance
- It's recommended to use enhanced networking instance types
- All instances should be of the same or similar type for best results

Remember to replace the AMI ID with one valid in your region and adjust the instance type according to your needs.",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer that Specifying private IP addresses for an internal-facing load balancer,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource

is_valid_lb {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.expressions.load_balancer_type.constant_value == ""network""
    resource.expressions.subnet_mapping[0].private_ipv4_address != null
    resource.expressions.subnet_mapping[0].subnet_id != null
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    valid_type := {""instance"", ""ip"", ""alb""}
    type := resources.values.target_type
    valid_type[type]

    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_target_group""
    resource.expressions.port != null
    resource.expressions.protocol != null
    resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
    is_valid_lb
    is_valid_lb_listener
    is_valid_lb_target_group_attachment
    is_valid_lb_target_group
    is_valid_instance
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.azs.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""     # Replace with your desired instance type
  subnet_id     = module.vpc.public_subnets[0]
}

resource ""aws_lb"" ""test"" {
  load_balancer_type = ""network""
  
  subnet_mapping {
    subnet_id            = module.vpc.public_subnet_arns[0]
    private_ipv4_address = ""10.0.4.30""
  }

  subnet_mapping {
    subnet_id            = module.vpc.public_subnet_arns[1]
    private_ipv4_address = ""10.0.5.15""
  }
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.id
}","has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument

has one ""aws_lb"" resource
    with ""subnet_mapping""
        with private_ipv4_address
    with load_balancer_type = ""network""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument
    with one protocol argument
        with one of following values: TCP, UDP, TLS, TCP_UDP

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument

has at least one ""aws_eip""","I'll help you create a Terraform configuration for an internal-facing Application Load Balancer (ALB) with specified private IP addresses. This is commonly used when you need to maintain specific IP addresses for your internal load balancer.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# VPC (assuming you have an existing VPC)
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  tags = {
    Name = ""main-vpc""
  }
}

# Private subnets
resource ""aws_subnet"" ""private_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""private-subnet-1""
  }
}

resource ""aws_subnet"" ""private_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""private-subnet-2""
  }
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""internal-alb-sg""
  description = ""Security group for internal ALB""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Internal Application Load Balancer
resource ""aws_lb"" ""internal"" {
  name               = ""internal-alb""
  internal           = true
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [aws_subnet.private_1.id, aws_subnet.private_2.id]

  subnet_mapping {
    subnet_id            = aws_subnet.private_1.id
    private_ipv4_address = ""10.0.1.10""  # Specify your desired private IP
  }

  subnet_mapping {
    subnet_id            = aws_subnet.private_2.id
    private_ipv4_address = ""10.0.2.10""  # Specify your desired private IP
  }

  tags = {
    Environment = ""production""
  }
}

# ALB Listener
resource ""aws_lb_listener"" ""internal"" {
  load_balancer_arn = aws_lb.internal.arn
  port              = 80
  protocol          = ""HTTP""

  default_action {
    type = ""fixed-response""

    fixed_response {
      content_type = ""text/plain""
      message_body = ""Fixed response content""
      status_code  = ""200""
    }
  }
}
```

This configuration creates:
1. An internal-facing Application Load Balancer
2. Two private subnets in different availability zones
3. A security group for the ALB
4. Subnet mappings with specific private IP addresses
5. A basic listener for the ALB

Key features:
- The ALB is set as internal (`internal = true`)
- Private IP addresses are specified using `subnet_mapping` blocks
- The ALB is placed in private subnets
- Security group allows internal traffic on port 80

Remember to:
1. Adjust the CIDR blocks and IP addresses according to your network design
2. Modify the region to match your requirements
3. Ensure the private IP addresses you specify are within the CIDR range",,,,,,"I'll help you create a Terraform configuration for an internal-facing Application Load Balancer (ALB) with specified private IP addresses. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC (assuming you have an existing VPC)
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  tags = {
    Name = ""main-vpc""
  }
}

# Create private subnets
resource ""aws_subnet"" ""private_subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""Private Subnet 1""
  }
}

resource ""aws_subnet"" ""private_subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""Private Subnet 2""
  }
}

# Create security group for the ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""internal-alb-sg""
  description = ""Security group for internal ALB""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Create the internal Application Load Balancer
resource ""aws_lb"" ""internal"" {
  name               = ""internal-alb""
  internal           = true
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [aws_subnet.private_subnet_1.id, aws_subnet.private_subnet_2.id]

  subnet_mapping {
    subnet_id            = aws_subnet.private_subnet_1.id
    private_ipv4_address = ""10.0.1.10""
  }

  subnet_mapping {
    subnet_id            = aws_subnet.private_subnet_2.id
    private_ipv4_address = ""10.0.2.10""
  }

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
  }
}

# Create ALB listener
resource ""aws_lb_listener"" ""internal"" {
  load_balancer_arn = aws_lb.internal.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type = ""fixed-response""

    fixed_response {
      content_type = ""text/plain""
      message_body = ""No target groups configured""
      status_code  = ""200""
    }
  }
}
```

This configuration creates:
1. An internal Application Load Balancer
2. The necessary VPC and private subnets (you can modify or remove if you already have existing ones)
3. A security group for the ALB
4. Subnet mappings with specific private IP addresses (10.0.1.10 and 10.0.2.10)
5. A basic listener configuration

Key features:
- The ALB is set as internal (internal = true)
- Specific private IP addresses are assigned using subnet_mapping blocks
- The ALB is spread across two availability zones for high availability
- Basic security group rules are included
- HTTP listener is configured on port 80

Remember to:
1. Adjust the CIDR blocks and IP addresses according to your network design",,,,,,"I'll help you create a Terraform configuration for an internal-facing Application Load Balancer (ALB) with specified private IP addresses. This is commonly used in VPC environments where you need internal load balancing with specific IP addresses.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC (assuming you have an existing VPC)
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  enable_dns_hostnames = true
  enable_dns_support   = true
  
  tags = {
    Name = ""main-vpc""
  }
}

# Private subnets
resource ""aws_subnet"" ""private_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""private-subnet-1""
  }
}

resource ""aws_subnet"" ""private_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""private-subnet-2""
  }
}

# Security Group for ALB
resource ""aws_security_group"" ""alb"" {
  name        = ""internal-alb-sg""
  description = ""Security group for internal ALB""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Internal Application Load Balancer
resource ""aws_lb"" ""internal"" {
  name               = ""internal-alb""
  internal           = true
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb.id]
  subnets            = [aws_subnet.private_1.id, aws_subnet.private_2.id]

  subnet_mapping {
    subnet_id            = aws_subnet.private_1.id
    private_ipv4_address = ""10.0.1.10""
  }

  subnet_mapping {
    subnet_id            = aws_subnet.private_2.id
    private_ipv4_address = ""10.0.2.10""
  }

  tags = {
    Environment = ""production""
  }
}

# ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.internal.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type = ""fixed-response""

    fixed_response {
      content_type = ""text/plain""
      message_body = ""No targets are responding to this request""
      status_code  = ""200""
    }
  }
}
```

This configuration creates:
1. An internal Application Load Balancer
2. Two private subnets in different availability zones
3. A security group for the ALB
4. Subnet mappings with specific private IP addresses (10.0.1.10 and 10.0.2.10)
5. A basic listener configuration

Key points about this configuration:
- The ALB is set as internal (internal = true)
- Specific private IP addresses are assigned using subnet_mapping blocks
- The ALB is deployed across two availability zones for high availability
- The security group allows inbound traffic on port 80 from within the VPC
- A basic HTTP listener is configured on port 80

You can customize the CIDR blocks",,,,,
"aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_s3_bucket",An AWS service that holds a web server which allows you to upload cat pictures and provides random cat pictures on demand. Accomplishes this with API Gateway and S3.,"package caas_high

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
	api.address in cat.expressions.parent_id.references
	api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
	method.expressions.http_method.constant_value == method_string
	resource.address in method.expressions.resource_id.references
	api.address in method.expressions.rest_api_id.references
	method.expressions.authorization
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some api in resources
	api.type == ""aws_api_gateway_rest_api""

	some cat in resources
	cat.type == ""aws_api_gateway_resource""

	some method_get in resources
	method_get.type == ""aws_api_gateway_method""

	some method_put in resources
	method_put.type == ""aws_api_gateway_method""

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	api_valid(api)
	cat_valid(cat, api)
	method_valid(method_get, ""GET"", cat, api)
	method_valid(method_put, ""PUT"", cat, api)
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource
    with ""http_method"" argument with values ""GET"" and ""PUT""
    with ""authorization"" argument
have one ""aws_s3_bucket"" resource","I'll help you create a Terraform configuration for a solution that uses API Gateway and S3 to handle cat pictures. This will include an S3 bucket for storage and API Gateway endpoints for uploading and retrieving random cat pictures.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-bucket""
}

# S3 bucket configuration
resource ""aws_s3_bucket_public_access_block"" ""cat_pictures"" {
  bucket = aws_s3_bucket.cat_pictures.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name        = ""cat-pictures-api""
  description = ""API for cat pictures service""
}

# API Gateway resource for /upload endpoint
resource ""aws_api_gateway_resource"" ""upload"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""upload""
}

# API Gateway resource for /random endpoint
resource ""aws_api_gateway_resource"" ""random"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""random""
}

# IAM role for API Gateway
resource ""aws_iam_role"" ""api_gateway_role"" {
  name = ""api_gateway_s3_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""apigateway.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for S3 access
resource ""aws_iam_role_policy"" ""api_gateway_s3_policy"" {
  name = ""api_gateway_s3_policy""
  role = aws_iam_role.api_gateway_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*""
        ]
      }
    ]
  })
}

# POST method for upload
resource ""aws_api_gateway_method"" ""upload_post"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.upload.id
  http_method   = ""POST""
  authorization = ""NONE""
}

# GET method for random
resource ""aws_api_gateway_method"" ""random_get"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.random.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# API Gateway deployment
resource ""aws_api_gateway_deployment"" ""cat_api"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id

  depends_on = [
    aws_api_gateway_method.upload_post,
    aws_api_gateway_",,,,,,"I'll help you create a Terraform configuration for an API Gateway and S3 setup to handle cat pictures. Here's a solution that includes:
- S3 bucket for storing cat pictures
- API Gateway with methods for uploading and getting random cat pictures
- Required IAM roles and policies

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-storage""
}

resource ""aws_s3_bucket_public_access_block"" ""cat_pictures"" {
  bucket = aws_s3_bucket.cat_pictures.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name        = ""cat-pictures-api""
  description = ""API for cat pictures service""
}

# API Gateway resource for /pictures
resource ""aws_api_gateway_resource"" ""pictures"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""pictures""
}

# GET method for random pictures
resource ""aws_api_gateway_method"" ""get_picture"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.pictures.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# POST method for uploading pictures
resource ""aws_api_gateway_method"" ""upload_picture"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.pictures.id
  http_method   = ""POST""
  authorization = ""NONE""
}

# IAM role for API Gateway
resource ""aws_iam_role"" ""api_gateway_role"" {
  name = ""api_gateway_s3_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""apigateway.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for S3 access
resource ""aws_iam_role_policy"" ""api_gateway_s3_policy"" {
  name = ""api_gateway_s3_policy""
  role = aws_iam_role.api_gateway_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*""
        ]
      }
    ]
  })
}

# API Gateway deployment
resource ""aws_api_gateway_deployment"" ""cat_api"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  depends_on = [
    aws_api_gateway_method.get_picture,
    aws_api_gateway_method.upload_picture
  ]
}

# API Gateway stage
resource ""aws_api_gateway_stage"" ""cat_api"" {
  deployment_id = aws_api_gateway_deployment.cat_api.id
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  stage",,,,,,"I'll help you create a Terraform configuration for an API Gateway and S3 setup that handles cat pictures. This solution will include:
- S3 bucket for storing cat pictures
- API Gateway with endpoints for uploading and getting random cat pictures
- Necessary IAM roles and policies

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-bucket""
}

# S3 bucket policy
resource ""aws_s3_bucket_policy"" ""allow_access"" {
  bucket = aws_s3_bucket.cat_pictures.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowAPIGatewayAccess""
        Effect    = ""Allow""
        Principal = {
          Service = ""apigateway.amazonaws.com""
        }
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Resource = ""${aws_s3_bucket.cat_pictures.arn}/*""
      }
    ]
  })
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name        = ""cat-pictures-api""
  description = ""API for cat pictures service""
}

# API Gateway resources
resource ""aws_api_gateway_resource"" ""cats"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# GET method for random cat picture
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cats.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# POST method for uploading cat picture
resource ""aws_api_gateway_method"" ""upload_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cats.id
  http_method   = ""POST""
  authorization = ""NONE""
}

# Integration with S3 for GET method
resource ""aws_api_gateway_integration"" ""s3_get"" {
  rest_api_id             = aws_api_gateway_rest_api.cat_api.id
  resource_id             = aws_api_gateway_resource.cats.id
  http_method             = aws_api_gateway_method.get_cat.http_method
  integration_http_method = ""GET""
  type                    = ""AWS""
  uri                     = ""arn:aws:apigateway:${var.aws_region}:s3:path/${aws_s3_bucket.cat_pictures.bucket}/random""
  credentials             = aws_iam_role.api_gateway_s3_role.arn
}

# Integration with S3 for POST method
resource ""aws_api_gateway_integration"" ""s3_post"" {
  rest_api_id             = aws_api_gateway_rest_api.cat_api.id
  resource_id             = aws_api_gateway_resource.cats.id
  http_method             = aws_api_gateway_method.upload_cat.http_method
  integration_http_method = ""POST""
  type                    = ""AWS""
  uri                     = ""arn:aws:apigateway:${var.aws_region}:s3:path/${aws_s3_bucket.cat_pictures.bucket}""
  credentials             = aws_iam_role.api_gateway_s3_role.arn
}

# IAM role for API",,,,,
"aws_msk_serverless_cluster, aws_security_group, aws_subnet, aws_subnet, aws_subnet, aws_vpc, aws_availability_zones",Create a serverless MSK with 3 broker nodes.,"package main

import future.keywords.in

default allow = false

# Check if MSK cluster is serverless
is_serverless(resource) {
resource.type == ""aws_msk_serverless_cluster""
}


# Check if cluster spans three AZs
has_three_azs(resource) {
resource.type == ""aws_msk_serverless_cluster""
count(resource.expressions.vpc_config[_].subnet_ids.references) == 6
}

# Main rule combining all checks
allow {
some resource in input.configuration.root_module.resources
is_serverless(resource)
has_three_azs(resource)
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""vpc"" {
    cidr_block = ""192.168.0.0/22""
    enable_dns_support = true
    enable_dns_hostnames = true
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

resource ""aws_subnet"" ""subnet_az1"" {
    availability_zone = data.aws_availability_zones.azs.names[0]
    cidr_block = ""192.168.0.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az2"" {
    availability_zone = data.aws_availability_zones.azs.names[1]
    cidr_block = ""192.168.1.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az3"" {
    availability_zone = data.aws_availability_zones.azs.names[2]
    cidr_block = ""192.168.2.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.vpc.id
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }
}

resource ""aws_route_table_association"" ""first"" {
  subnet_id      = aws_subnet.subnet_az1.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""second"" {
  subnet_id      = aws_subnet.subnet_az2.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""third"" {
  subnet_id      = aws_subnet.subnet_az3.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_security_group"" ""sg"" {
    vpc_id = aws_vpc.vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4 = ""0.0.0.0/0""
  ip_protocol = ""-1""
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  ip_protocol       = ""-1""
}

resource ""aws_msk_serverless_cluster"" ""example"" {
    cluster_name = ""Example""

    vpc_config {
        subnet_ids = [aws_subnet.subnet_az1.id, aws_subnet.subnet_az2.id, aws_subnet.subnet_az3.id]
        security_group_ids = [aws_security_group.sg.id]
    }

    client_authentication {
        sasl {
            iam {
                enabled = true
            }
        }
    }
}","Has one ""aws_msk_serverless_cluster"" resource
with one ""vpc_config""
with one ""subnet_ids"" with 6 references (3 subnets)","I'll help you create a Terraform configuration for a serverless Amazon MSK (Managed Streaming for Apache Kafka) cluster with 3 broker nodes. Note that MSK Serverless manages the broker nodes automatically, but we'll set it up according to best practices.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_msk_serverless_cluster"" ""msk_serverless"" {
  cluster_name = ""msk-serverless-cluster""

  vpc_config {
    subnet_ids = [
      ""subnet-12345678"",  # Replace with your subnet IDs
      ""subnet-87654321"",
      ""subnet-11223344""
    ]
    security_group_ids = [aws_security_group.msk_sg.id]
  }

  client_authentication {
    sasl {
      iam {
        enabled = true
      }
    }
  }

  tags = {
    Environment = ""Production""
    Name        = ""msk-serverless""
  }
}

resource ""aws_security_group"" ""msk_sg"" {
  name        = ""msk-serverless-sg""
  description = ""Security group for MSK Serverless cluster""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID

  ingress {
    from_port   = 9098
    to_port     = 9098
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]  # Replace with your CIDR block
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""msk-serverless-sg""
  }
}
```

This configuration creates:

1. A serverless MSK cluster with:
   - IAM authentication enabled
   - Placement in 3 subnets for high availability
   - Associated security group for access control

2. A security group that:
   - Allows inbound traffic on port 9098 (MSK standard port)
   - Allows all outbound traffic
   - Is associated with your VPC

Important notes:
- Replace the subnet IDs and VPC ID with your actual values
- Adjust the CIDR blocks in the security group to match your network architecture
- The serverless cluster automatically manages the broker nodes, so you don't need to specify the number of brokers
- The configuration uses IAM authentication for better security

Make sure you have the necessary IAM permissions to create MSK resources before applying this configuration.",,,,,,"I'll help you create a Terraform configuration for a serverless Amazon MSK (Managed Streaming for Apache Kafka) cluster with 3 broker nodes. Here's the code:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_msk_serverless_cluster"" ""msk_serverless"" {
  cluster_name = ""msk-serverless-cluster""

  vpc_config {
    subnet_ids = [
      ""subnet-xxxxxxxx"",  # Replace with your subnet IDs
      ""subnet-yyyyyyyy"",
      ""subnet-zzzzzzzz""
    ]
    security_group_ids = [
      ""sg-xxxxxxxx""  # Replace with your security group ID
    ]
  }

  client_authentication {
    sasl {
      iam {
        enabled = true
      }
    }
  }

  number_of_broker_nodes = 3

  tags = {
    Environment = ""Production""
    Name        = ""msk-serverless""
  }
}

# IAM role for MSK cluster
resource ""aws_iam_role"" ""msk_role"" {
  name = ""msk-serverless-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kafka.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for MSK cluster
resource ""aws_iam_role_policy"" ""msk_policy"" {
  name = ""msk-serverless-policy""
  role = aws_iam_role.msk_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kafka:*""
        ]
        Resource = ""*""
      }
    ]
  })
}
```

This configuration creates:
1. A serverless MSK cluster with 3 broker nodes
2. IAM authentication enabled (using SASL/IAM)
3. Required IAM role and policy for the MSK cluster
4. VPC configuration (you'll need to replace the subnet IDs and security group ID with your own)

Key points to note:
- Make sure to replace the subnet IDs and security group ID with your actual VPC resources
- The configuration uses IAM authentication for better security
- The cluster is tagged for better resource management
- The region is set to us-west-2, but you can change it to your preferred region

Remember to configure your VPC with proper routing and security group rules to allow Kafka traffic (default ports 9092 for plaintext, 9094 for TLS, and 9098 for SASL/IAM).",,,,,,"I'll help you create a Terraform configuration for a serverless Amazon MSK (Managed Streaming for Apache Kafka) cluster with 3 broker nodes. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_msk_serverless_cluster"" ""kafka_cluster"" {
  cluster_name = ""my-serverless-msk-cluster""

  vpc_config {
    subnet_ids = [
      ""subnet-12345678"",  # Replace with your subnet IDs
      ""subnet-87654321"",
      ""subnet-11223344""
    ]
    security_group_ids = [
      ""sg-12345678""  # Replace with your security group ID
    ]
  }

  client_authentication {
    sasl {
      iam {
        enabled = true
      }
    }
  }

  tags = {
    Environment = ""production""
    Purpose     = ""kafka-streaming""
  }
}

# IAM Role for MSK access
resource ""aws_iam_role"" ""msk_access_role"" {
  name = ""msk-access-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kafka.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Policy for MSK access
resource ""aws_iam_role_policy"" ""msk_access_policy"" {
  name = ""msk-access-policy""
  role = aws_iam_role.msk_access_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kafka:Connect"",
          ""kafka:DescribeCluster"",
          ""kafka:GetBootstrapBrokers""
        ]
        Resource = aws_msk_serverless_cluster.kafka_cluster.arn
      }
    ]
  })
}
```

This configuration creates:
1. A serverless MSK cluster with 3 broker nodes (managed automatically by AWS)
2. IAM authentication enabled for security
3. Necessary IAM roles and policies for cluster access
4. VPC configuration for network isolation

Note that you'll need to:
1. Replace the subnet IDs with your actual subnet IDs
2. Replace the security group ID with your actual security group ID
3. Adjust the region as needed
4. Modify the tags according to your requirements

The serverless MSK cluster will automatically scale based on your workload, and AWS manages the broker nodes for you. IAM authentication is enabled for secure access to the cluster.",,,,,
"aws_api_gateway_integration, aws_api_gateway_integration, aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_dynamodb_table, aws_iam_role, aws_iam_role, aws_lambda_function, aws_lambda_function, aws_lambda_permission, aws_lambda_permission, aws_s3_bucket, archive_file, archive_file","An API Gateway Rest API named ""caas"" that has a resource endpoint named ""caas_cat"" with a path_part ""cat"" linking to two methods ""caas_get"" and ""caas_put"" with http_method ""GET"" and ""PUT."" Each of the methods should have a corresponding Lambda function that the API Gateway will have permission to access. A S3 bucket that stores cat pictures which will be accessed by a Lambda function, and a DynamoDB table with one keyed attribute, strings ""name"" of the cat.","package caas

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
        api.address in cat.expressions.parent_id.references
        api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
        method.expressions.http_method.constant_value == method_string
        resource.address in method.expressions.resource_id.references
        api.address in method.expressions.rest_api_id.references
}

table_valid(table) if {
        some attribute in table.expressions.attribute
        attribute.name.constant_value == ""name""
        attribute.type.constant_value == ""S""

        table.expressions.hash_key.constant_value == ""name""
}

lambda_valid(lambda, bucket) if {
        some env in lambda.expressions.environment
        bucket.address in env.variables.references
}

permission_valid(permission, lambda, api) if {
        permission.expressions.action.constant_value == ""lambda:InvokeFunction""
        lambda.address in permission.expressions.function_name.references
        permission.expressions.principal.constant_value == ""apigateway.amazonaws.com""
        api.address in permission.expressions.source_arn.references
}

integration_valid(integration, lambda, method, resource, api, integration_method) if {
        method.address in integration.expressions.http_method.references
        resource.address in integration.expressions.resource_id.references
        api.address in integration.expressions.rest_api_id.references
        integration.expressions.integration_http_method.constant_value == integration_method
        integration.expressions.type.constant_value == ""AWS_PROXY""
        lambda.address in integration.expressions.uri.references
}

default valid := false

valid if {
        resources := input.configuration.root_module.resources

        some api in resources
        api.type == ""aws_api_gateway_rest_api""

        some cat in resources
        cat.type == ""aws_api_gateway_resource""

        some method_get in resources
        method_get.type == ""aws_api_gateway_method""

        some method_put in resources
        method_put.type == ""aws_api_gateway_method""

        some table in resources
        table.type == ""aws_dynamodb_table""

        some lambda_get in resources
        lambda_get.type == ""aws_lambda_function""

        some lambda_put in resources
        lambda_put.type == ""aws_lambda_function""

        some bucket in resources
        bucket.type == ""aws_s3_bucket""

        some permission_get in resources
        permission_get.type == ""aws_lambda_permission""

        some permission_put in resources
        permission_put.type == ""aws_lambda_permission""

        some integration_get in resources
        integration_get.type == ""aws_api_gateway_integration""

        some integration_put in resources
        integration_put.type == ""aws_api_gateway_integration""

        api_valid(api)
        cat_valid(cat, api)
        method_valid(method_get, ""GET"", cat, api)
        method_valid(method_put, ""PUT"", cat, api)
        lambda_valid(lambda_get, bucket)
        lambda_valid(lambda_put, bucket)
        permission_valid(permission_get, lambda_get, api)
        permission_valid(permission_put, lambda_put, api)
        integration_valid(integration_get, lambda_get, method_get, cat, api, ""GET"")
        integration_valid(integration_put, lambda_put, method_put, cat, api, ""PUT"")
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument name
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument with value ""cat""
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_api resource id
    with ""resource_id"" linking to aws_api_gateway_resource resource
    with ""authorization"" argument with value ""NONE""
    with ""http_method"" argument with values ""GET"" and ""PUT""
have one ""aws_s3_bucket"" resource
have one ""aws_dynamodb_table"" resource
    with ""name"" argument
    with ""hash_key"" argument with value ""name""
    with ""attribute"" argument block
        with ""name"" argument with value ""name""
        with ""type"" argument with value ""S""
with two ""aws_lambda_function"" resources
    with ""function_name"" argument
    with ""role"" argument linking to aws_iam_role resources
    with ""environment"" argument block
        with ""variables"" argument block linking to the aws_s3_bucket
with two ""aws_lambda_permission"" resources
    with ""action"" argument with value ""lambda:InvokeFunction""
    with ""principal"" argument with value ""apigateway.amazonaws.com""
    with ""function_name"" argument linking to aws_lambda_function resource
    with ""source_arn"" argument linking to aws_api_gateway_rest_api (two different end points)
with two ""aws_api_gateway_integration"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource resource
    with ""http_method"" argument linking to aws_api_gateway_method resource
    with ""type"" argument with value ""AWS_PROXY""
    with ""integration_http_method"" argument with values ""GET"" and ""PUT""","I'll help you create a Terraform configuration for your API Gateway setup with Lambda functions, S3 bucket, and DynamoDB table. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

# API Gateway Resource
resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

# GET Method
resource ""aws_api_gateway_method"" ""caas_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# PUT Method
resource ""aws_api_gateway_method"" ""caas_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

# Lambda Functions
resource ""aws_lambda_function"" ""get_function"" {
  filename      = ""get_function.zip""
  function_name = ""caas_get_function""
  role         = aws_iam_role.lambda_role.arn
  handler      = ""index.handler""
  runtime      = ""nodejs14.x""
}

resource ""aws_lambda_function"" ""put_function"" {
  filename      = ""put_function.zip""
  function_name = ""caas_put_function""
  role         = aws_iam_role.lambda_role.arn
  handler      = ""index.handler""
  runtime      = ""nodejs14.x""
}

# Lambda Permissions for API Gateway
resource ""aws_lambda_permission"" ""get_lambda_permission"" {
  statement_id  = ""AllowAPIGatewayInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.get_function.function_name
  principal     = ""apigateway.amazonaws.com""
  source_arn    = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/${aws_api_gateway_method.caas_get.http_method}${aws_api_gateway_resource.caas_cat.path}""
}

resource ""aws_lambda_permission"" ""put_lambda_permission"" {
  statement_id  = ""AllowAPIGatewayInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.put_function.function_name
  principal     = ""apigateway.amazonaws.com""
  source_arn    = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/${aws_api_gateway_method.caas_put.http_method}${aws_api_gateway_resource.caas_cat.path}""
}

# S3 Bucket for Cat Pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""caas-cat-pictures""
}

resource ""aws_s3_bucket_versioning"" ""cat_pictures"" {
  bucket = aws_s3_bucket.cat_pictures.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# DynamoDB Table
resource ""aws_dynamodb_table"" ""cats"" {
  name           = ""cats""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""name""

  attribute {
    name = ""name""
    type =",,,,,,"I'll help create a Terraform configuration for your API Gateway, Lambda functions, S3 bucket, and DynamoDB table setup. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""caas"" {
  name        = ""caas""
  description = ""Cats as a Service API""
}

# API Gateway Resource
resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

# GET Method
resource ""aws_api_gateway_method"" ""caas_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# PUT Method
resource ""aws_api_gateway_method"" ""caas_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

# Lambda Functions
resource ""aws_lambda_function"" ""get_function"" {
  filename      = ""get_function.zip""
  function_name = ""caas_get_function""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""
}

resource ""aws_lambda_function"" ""put_function"" {
  filename      = ""put_function.zip""
  function_name = ""caas_put_function""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""
}

# Lambda Permissions for API Gateway
resource ""aws_lambda_permission"" ""get_lambda_permission"" {
  statement_id  = ""AllowAPIGatewayInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.get_function.function_name
  principal     = ""apigateway.amazonaws.com""
  source_arn    = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/${aws_api_gateway_method.caas_get.http_method}${aws_api_gateway_resource.caas_cat.path}""
}

resource ""aws_lambda_permission"" ""put_lambda_permission"" {
  statement_id  = ""AllowAPIGatewayInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.put_function.function_name
  principal     = ""apigateway.amazonaws.com""
  source_arn    = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/${aws_api_gateway_method.caas_put.http_method}${aws_api_gateway_resource.caas_cat.path}""
}

# S3 Bucket for Cat Pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-bucket""
}

resource ""aws_s3_bucket_versioning"" ""cat_pictures_versioning"" {
  bucket = aws_s3_bucket.cat_pictures.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# DynamoDB Table
resource ""aws_dynamodb_table"" ""cats"" {
  name           = ""cats""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""name""",,,,,,"I'll help you create a Terraform configuration for your API Gateway, Lambda functions, S3 bucket, and DynamoDB table setup. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

# API Gateway Resource
resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

# GET Method
resource ""aws_api_gateway_method"" ""caas_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# PUT Method
resource ""aws_api_gateway_method"" ""caas_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

# Lambda Functions
resource ""aws_lambda_function"" ""get_function"" {
  filename      = ""get_function.zip""
  function_name = ""caas_get_function""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""
}

resource ""aws_lambda_function"" ""put_function"" {
  filename      = ""put_function.zip""
  function_name = ""caas_put_function""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""
}

# Lambda permissions for API Gateway
resource ""aws_lambda_permission"" ""get_permission"" {
  statement_id  = ""AllowAPIGatewayInvokeGet""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.get_function.function_name
  principal     = ""apigateway.amazonaws.com""
  source_arn    = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/${aws_api_gateway_method.caas_get.http_method}${aws_api_gateway_resource.caas_cat.path}""
}

resource ""aws_lambda_permission"" ""put_permission"" {
  statement_id  = ""AllowAPIGatewayInvokePut""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.put_function.function_name
  principal     = ""apigateway.amazonaws.com""
  source_arn    = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/${aws_api_gateway_method.caas_put.http_method}${aws_api_gateway_resource.caas_cat.path}""
}

# S3 Bucket for cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-bucket""
}

resource ""aws_s3_bucket_versioning"" ""cat_pictures"" {
  bucket = aws_s3_bucket.cat_pictures.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# DynamoDB Table
resource ""aws_dynamodb_table"" ""cats"" {
  name           = ""cats""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""name""

  attribute {
    name = ""name""
    type = ""S""",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate an Application Load Balancer,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# has valid subnet
is_valid_subnet {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_subnet""
    resource.expressions.vpc_id != null
    have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_listener""
    resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    valid_type := {""instance"", ""ip"", ""alb""}
    type := resources.values.target_type
    valid_type[type]

    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_lb_target_group""
    resource.expressions.port != null
    resource.expressions.protocol != null
    resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
    is_valid_lb
    is_valid_lb_listener
    is_valid_lb_target_group_attachment
    is_valid_lb_target_group
    is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.public_subnets[0]
  security_groups = [ aws_security_group.instance-sg.id ]
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.id
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you create a Terraform configuration for an AWS Application Load Balancer (ALB). This will include the ALB itself, a target group, and the necessary security group.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""alb-security-group""
  }
}

# Application Load Balancer
resource ""aws_lb"" ""app_lb"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = data.aws_subnet_ids.default.ids

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
  }
}

# Target Group
resource ""aws_lb_target_group"" ""app_tg"" {
  name     = ""my-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = data.aws_vpc.default.id

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.app_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.app_tg.arn
  }
}

# Data sources for VPC and Subnets
data ""aws_vpc"" ""default"" {
  default = true
}

data ""aws_subnet_ids"" ""default"" {
  vpc_id = data.aws_vpc.default.id
}
```

This configuration creates:
1. A security group allowing HTTP (80) and HTTPS (443) traffic
2. An Application Load Balancer
3. A target group for the ALB
4. A listener on port 80
5. Uses the default VPC and its subnets

Key features:
- The ALB is internet-facing (internal = false)
- Health checks are configured for the target group
- Basic security group rules are in place
- Uses the default VPC and subnets (you may want to customize this for your specific VPC)

Remember to:
- Adjust the security group rules based on your needs
- Modify the health check settings according to your application
- Consider adding HTTPS listener with SSL certificate if needed
- Customize the VPC and subnet selection based on your infrastructure",,,,,,"I'll help you create a Terraform configuration for an AWS Application Load Balancer (ALB). This will include the ALB itself, a target group, and the necessary security group.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# ALB Security Group
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = ""vpc-XXXXX"" # Replace with your VPC ID

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""alb-security-group""
  }
}

# Application Load Balancer
resource ""aws_lb"" ""application_lb"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [""subnet-XXXXX"", ""subnet-YYYYY""] # Replace with your subnet IDs

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
  }
}

# Target Group
resource ""aws_lb_target_group"" ""target_group"" {
  name     = ""my-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = ""vpc-XXXXX"" # Replace with your VPC ID

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.application_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}
```

This configuration creates:
1. A security group for the ALB allowing HTTP (80) and HTTPS (443) traffic
2. An Application Load Balancer
3. A target group for the ALB
4. A listener that forwards HTTP traffic to the target group

Note: You'll need to replace the following placeholder values with your actual values:
- vpc-XXXXX: Your VPC ID
- subnet-XXXXX and subnet-YYYYY: Your subnet IDs (need at least two subnets in different Availability Zones)

You can also modify the tags, names, and health check settings according to your requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS Application Load Balancer (ALB). This will include the ALB itself, a target group, and the necessary provider configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create the Application Load Balancer
resource ""aws_lb"" ""application_lb"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = [""subnet-12345678"", ""subnet-87654321""]  # Replace with your subnet IDs

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
    Name        = ""my-application-lb""
  }
}

# Create a target group
resource ""aws_lb_target_group"" ""target_group"" {
  name     = ""my-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = ""vpc-12345678""  # Replace with your VPC ID

  health_check {
    enabled             = true
    healthy_threshold   = 3
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Create a listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.application_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

# Create security group for the ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = ""vpc-12345678""  # Replace with your VPC ID

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""alb-security-group""
  }
}
```

This configuration creates:
1. An Application Load Balancer
2. A target group for the ALB
3. A listener on port 80
4. A security group for the ALB

Note: You'll need to replace the following placeholder values with your actual values:
- subnet IDs in the `subnets` list
- VPC ID
- Region (if different from us-west-2)

The ALB is configured as internet-facing (internal = false) and has deletion protection disabled. The security group allows inbound HTTP traffic (port 80) and all outbound traffic. The target group is configured with standard health check settings for HTTP traffic.",,,,,
"aws_db_instance, aws_db_instance, aws_elastic_beanstalk_application, aws_elastic_beanstalk_application, aws_elastic_beanstalk_environment, aws_elastic_beanstalk_environment, aws_iam_instance_profile, aws_iam_role, aws_iam_role_policy_attachment, aws_route53_record, aws_route53_record, aws_route53_zone","Configure multi-region application deployment with Route 53 resources to manage a domain and direct traffic to Elastic Beanstalk environments that have a connected RDS database. Provision the respective resources in either ""us-east-1"" or ""eu-west-1"" regions and name the aliases ""us-east-1"" and ""eu-west-1"" respectively. Name the environments ""myenv_us_east"" and ""myenv_eu_west"", the applications ""myapp_us_east"" and ""myapp_eu_west"", the instance profile ""eb_ec2_profile3"", and the databases ""main_db_us_east"" and ""main_db_eu_west""","package terraform.validation

default is_configuration_valid = false

default is_valid_iam_instance_profile = false

default is_valid_iam_role = false

default is_valid_iam_role_policy_attachment = false

default is_valid_eb_app = false

default is_valid_eb_env = false

default is_valid_db_instance = false

default is_valid_r53_zone = false

default is_valid_r53_record = false



# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.name
}

# Validate aws_route53_record
is_valid_r53_record {
        some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.provider_config_key == ""aws.us_east_1""
    resource.expressions.name
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.set_identifier
    resource.expressions.latency_routing_policy[0].region.constant_value == ""us-east-1""
    resource.expressions.records.references[0] == ""aws_elastic_beanstalk_environment.myenv_us_east.cname""
    resource.expressions.zone_id.references[0]
    
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_route53_record""    
    resource2.provider_config_key == ""aws.eu_west_1""
    resource2.expressions.name
    resource2.expressions.type
    resource2.expressions.ttl
    resource2.expressions.set_identifier
    resource2.expressions.latency_routing_policy[0].region.constant_value == ""eu-west-1""
    resource2.expressions.records.references[0] == ""aws_elastic_beanstalk_environment.myenv_eu_west.cname""
    resource2.expressions.zone_id.references[0]

}


is_valid_iam_role {
        some i
    resource := input.resource_changes[i]
    resource.type == ""aws_iam_role""
    contains(resource.change.after.assume_role_policy,""ec2.amazonaws.com"")
}

is_valid_iam_role_policy_attachment {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_role_policy_attachment""
    resource.expressions.role.references[0]
    resource.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Validate aws_iam_instance_profile resource
is_valid_iam_instance_profile {
                 some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_iam_instance_profile""
    resource.expressions.role.references[0]
}

# Validate aws_eb_app
is_valid_eb_app {
        some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
    resource.provider_config_key == ""aws.us_east_1""
        
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_elastic_beanstalk_application""
    resource2.expressions.name
    resource2.provider_config_key == ""aws.eu_west_1""
}

# Validate aws_eb_env
is_valid_eb_env {
        some i, j
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_environment""
    resource.provider_config_key == ""aws.us_east_1""
    resource.expressions.name
    resource.expressions.application.references[0] == ""aws_elastic_beanstalk_application.myapp_us_east.name""
    resource.expressions.solution_stack_name
    
    resource2 := input.configuration.root_module.resources[j]
    resource2.type == ""aws_elastic_beanstalk_environment""
    resource2.provider_config_key == ""aws.eu_west_1""
    resource2.expressions.name
    resource2.expressions.application.references[0] == ""aws_elastic_beanstalk_application.myapp_eu_west.name""
    resource2.expressions.solution_stack_name
    
            some a, b, c, d, e, f, g, h
    resource.expressions.setting[a].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile3.name""
    resource.expressions.setting[b].value.references[0] == ""aws_db_instance.main_db_us_east.username""
    resource.expressions.setting[c].value.references[0] == ""aws_db_instance.main_db_us_east.password""
    resource.expressions.setting[d].value.references[0] == ""aws_db_instance.main_db_us_east.address""
    
    resource2.expressions.setting[e].value.references[0] == ""aws_iam_instance_profile.eb_ec2_profile3.name""
    resource2.expressions.setting[f].value.references[0] == ""aws_db_instance.main_db_eu_west.username""
    resource2.expressions.setting[g].value.references[0] == ""aws_db_instance.main_db_eu_west.password""
    resource2.expressions.setting[h].value.references[0] == ""aws_db_instance.main_db_eu_west.address""

}

is_valid_db_instance {
    resource := input.configuration.root_module.resources[_]
    resource.type == ""aws_db_instance""
    has_required_main_db_arguments
}

# Helper rule to check if all required arguments are present and valid
has_required_main_db_arguments {
                some i, j
    resource := input.configuration.root_module.resources[i]
    resource2 := input.configuration.root_module.resources[j]
    resource.type == ""aws_db_instance""
    resource.provider_config_key == ""aws.us_east_1""
    resource2.type == ""aws_db_instance""
    resource2.provider_config_key == ""aws.eu_west_1""
    # Check for allocated_storage, engine, instance_class, username, password, and skip_final_snapshot
    requirement1(resource.expressions)
    requirement1(resource2.expressions)
    # Check for instance_class validity
    requirement2(resource.expressions)
    requirement2(resource2.expressions)
}



# 1, allocated_storage and engine or snapshot_identifier or replace_source_db
requirement1(expressions) {
    expressions.allocated_storage
    expressions.engine
    expressions.username
    expressions.password
    is_valid_engine(expressions.engine.constant_value)
}

requirement1(expressions) {
    expressions.snapshot_identifier
}

# Check for instance_class validity
requirement2(expressions) {
    expressions.instance_class
    is_valid_instance_class(expressions.instance_class.constant_value)
}


# Helper rule to validate engine value
is_valid_engine(engine) {
        engine_set := {
        ""mysql"",
        ""postgres"",
        ""mariadb"",
        ""oracle-se"",
        ""oracle-se1"",
        ""oracle-se2"",
        ""oracle-ee"",
        ""sqlserver-ee"",
        ""sqlserver-se"",
        ""sqlserver-ex"",
        ""sqlserver-web""
    }
        engine_set[engine]
}

# Helper rule to validate instance class type
is_valid_instance_class(instance_class) {
        instance_class_starts_with(instance_class, ""db."")
}

# Helper rule to check prefix of instance class
instance_class_starts_with(instance_class, prefix) {
        startswith(instance_class, prefix)
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_iam_role
    is_valid_iam_role_policy_attachment
    is_valid_iam_instance_profile
    is_valid_eb_app
    is_valid_eb_env
    is_valid_db_instance
    is_valid_r53_zone
    is_valid_r53_record
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""

      configuration_aliases = [ aws.us_east_1, aws.eu_west_1 ]
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  alias  = ""us_east_1""
  region = ""us-east-1""

  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

provider ""aws"" {
  alias  = ""eu_west_1""
  region = ""eu-west-1""

  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}


resource ""aws_iam_role"" ""eb_ec2_role"" {
  provider = aws.us_east_1
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk to the role
resource ""aws_iam_role_policy_attachment"" ""eb_managed_policy"" {
  provider = aws.us_east_1
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}


# Create an instance profile tied to the role
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  provider = aws.us_east_1
  name = ""elastic_beanstalk_ec2_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# RDS in US East (N. Virginia)
resource ""aws_db_instance"" ""main_db_us_east"" {
  provider             = aws.us_east_1
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  username             = ""dbadmin""
  password             = ""mysecurepassword""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
  multi_az             = true
}

# Elastic Beanstalk Application in US East
resource ""aws_elastic_beanstalk_application"" ""myapp_us_east"" {
  provider = aws.us_east_1
  name     = ""my-application-us-east""
}

# Elastic Beanstalk Environment in US East
resource ""aws_elastic_beanstalk_environment"" ""myenv_us_east"" {
  provider            = aws.us_east_1
  name                = ""my-application-env-us-east""
  application         = aws_elastic_beanstalk_application.myapp_us_east.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.main_db_us_east.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.main_db_us_east.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.main_db_us_east.address
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# RDS in EU West (Ireland)
resource ""aws_db_instance"" ""main_db_eu_west"" {
  provider             = aws.eu_west_1
  allocated_storage    = 20
  storage_type         = ""gp2""
  engine               = ""mysql""
  engine_version       = ""5.7""
  instance_class       = ""db.t3.micro""
  multi_az             = true
  username             = ""dbadmin""
  password             = ""mysecurepassword""
  parameter_group_name = ""default.mysql5.7""
  skip_final_snapshot  = true
}

# Elastic Beanstalk Application in EU West
resource ""aws_elastic_beanstalk_application"" ""myapp_eu_west"" {
  provider = aws.eu_west_1
  name     = ""my-application-eu-west""
}

# Elastic Beanstalk Environment in EU West
resource ""aws_elastic_beanstalk_environment"" ""myenv_eu_west"" {
  provider            = aws.eu_west_1
  name                = ""my-application-env-eu-west""
  application         = aws_elastic_beanstalk_application.myapp_eu_west.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_HOST""
    value     = aws_db_instance.main_db_eu_west.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_USERNAME""
    value     = aws_db_instance.main_db_eu_west.address
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""DB_PASSWORD""
    value     = aws_db_instance.main_db_eu_west.address
  }

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Main Route 53 Hosted Zone
resource ""aws_route53_zone"" ""main"" {
  provider = aws.us_east_1
  name = ""myapp.com""
}

# Latency Based Routing in Route 53 for US East Elastic Beanstalk Environment
resource ""aws_route53_record"" ""eb_env_us_east"" {
  provider = aws.us_east_1
  zone_id = aws_route53_zone.main.zone_id
  name    = ""us-east.myapp.com""
  type    = ""CNAME""
  ttl     = ""60""
  records = [aws_elastic_beanstalk_environment.myenv_us_east.cname]
  set_identifier = ""EastEnv""
  latency_routing_policy {
    region = ""us-east-1""
  }
}

resource ""aws_route53_record"" ""eb_env_eu_west"" {
  provider = aws.eu_west_1
  zone_id = aws_route53_zone.main.zone_id
  name    = ""eu-west.myapp.com""
  type    = ""CNAME""
  ttl     = ""60""
  records = [aws_elastic_beanstalk_environment.myenv_eu_west.cname]
  set_identifier = ""WestEnv""
  latency_routing_policy {
    region = ""eu-west-1""
  }
}
","Has one ""aws_iam_role"" resource
    with ""name""
    with ""assume_role_policy"" and ""Service"" set to ""ec2.amazonaws.com""

Has one ""aws_iam_role_policy_attachment"" resource
    with ""role"" referencing the ""aws_iam_role""
    with ""policy_arn"" set to ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""

Has one ""aws_iam_instance_profile"" resource
    with ""name""
    with ""role"" referencing the ""aws_iam_role"" resource

Has two ""aws_elastic_beanstalk_application"" resource provisioned in different regions
    with ""name""

Has two ""aws_elastic_beanstalk_environment"" resources provisioned in different regions
    with ""name""
    with ""application"" referencing the ""aws_elastic_beanstalk_application"" resource
    with ""solution_stack_name""
    with a ""setting"" block which has a ""value"" that references the ""aws_iam_instance_profile"" resource
    with a ""setting"" block that has a ""value""  that references the ""address"" from one of the ""db_instance"" resource
    with a ""setting"" block that has a ""value""  that references the ""username"" from one of the ""db_instance"" resource 
    with a ""setting"" block that has a ""value""  that references the ""password"" from one of the ""db_instance"" resource 

Has two ""aws_db_instance"" resources provisioned in different regions
    with ""instance_class""
    with ""allocated_storage""
    with ""engine""
    with ""username""
    with ""password""
    with ""backup_retention_period"" set to a non-zero number
    with ""skip_final_snapshot""


Has one ""aws_route53_zone"" resource
    with ""name""

Has two ""aws_route53_record"" resources provisioned in different regions
    with ""name""
    with ""type""
    with ""ttl""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
    with ""set_identifier""
    with ""records"" referencing one of the ""aws_elastic_beanstalk_environment"" resource
    with ""latency_routing_policy"" containing ""region"" 


","I'll help you create a Terraform configuration for a multi-region application deployment with Route 53, Elastic Beanstalk, and RDS. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# US East Provider
provider ""aws"" {
  region = ""us-east-1""
  alias  = ""us_east_1""
}

# EU West Provider
provider ""aws"" {
  region = ""eu-west-1""
  alias  = ""eu_west_1""
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  provider = aws.us_east_1
  name     = ""example.com""
}

# IAM Role for EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# Instance Profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile3"" {
  name = ""eb_ec2_profile3""
  role = aws_iam_role.eb_ec2_role.name
}

# US East Resources
resource ""aws_elastic_beanstalk_application"" ""myapp_us_east"" {
  provider = aws.us_east_1
  name     = ""myapp_us_east""
}

resource ""aws_db_instance"" ""main_db_us_east"" {
  provider          = aws.us_east_1
  identifier        = ""main-db-us-east""
  engine           = ""mysql""
  instance_class   = ""db.t3.micro""
  allocated_storage = 20
  username         = ""admin""
  password         = ""password123""  # Use secrets management in production
}

resource ""aws_elastic_beanstalk_environment"" ""myenv_us_east"" {
  provider    = aws.us_east_1
  name        = ""myenv_us_east""
  application = aws_elastic_beanstalk_application.myapp_us_east.name

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile3.name
  }
}

# EU West Resources
resource ""aws_elastic_beanstalk_application"" ""myapp_eu_west"" {
  provider = aws.eu_west_1
  name     = ""myapp_eu_west""
}

resource ""aws_db_instance"" ""main_db_eu_west"" {
  provider          = aws.eu_west_1
  identifier        = ""main-db-eu-west""
  engine           = ""mysql""
  instance_class   = ""db.t3.micro""
  allocated_storage = 20
  username         = ""admin""
  password         = ""password123""  # Use secrets management in production
}

resource ""aws_elastic_beanstalk_environment"" ""myenv_eu_west"" {
  provider    = aws.eu_west_1
  name        = ""myenv_eu_west""
  application = aws_elastic_beanstalk_application.myapp_eu_west.name

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile3.name
  }
}

# Route 53 Records
resource ""aws_route53_record"" ""us_east"" {
  provider = aws.us_east_1
  zone_id  = aws_route53_zone.main.zone_id
  name     = ""us-",,,,,,"I'll help you create a Terraform configuration for a multi-region application deployment with Route 53, Elastic Beanstalk, and RDS. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Provider configurations for both regions
provider ""aws"" {
  alias  = ""us_east""
  region = ""us-east-1""
}

provider ""aws"" {
  alias  = ""eu_west""
  region = ""eu-west-1""
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  provider = aws.us_east
  name     = ""example.com""
}

# IAM Role for Elastic Beanstalk
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  name = ""eb_ec2_profile3""
  role = aws_iam_role.eb_ec2_role.name
}

# US East Resources
resource ""aws_elastic_beanstalk_application"" ""us_east"" {
  provider = aws.us_east
  name     = ""myapp_us_east""
}

resource ""aws_db_instance"" ""us_east"" {
  provider          = aws.us_east
  identifier        = ""main-db-us-east""
  engine           = ""mysql""
  engine_version   = ""8.0""
  instance_class   = ""db.t3.micro""
  allocated_storage = 20
  username         = ""admin""
  password         = ""password123""  # Change this in production
  skip_final_snapshot = true
}

resource ""aws_elastic_beanstalk_environment"" ""us_east"" {
  provider    = aws.us_east
  name        = ""myenv_us_east""
  application = aws_elastic_beanstalk_application.us_east.name
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# EU West Resources
resource ""aws_elastic_beanstalk_application"" ""eu_west"" {
  provider = aws.eu_west
  name     = ""myapp_eu_west""
}

resource ""aws_db_instance"" ""eu_west"" {
  provider          = aws.eu_west
  identifier        = ""main-db-eu-west""
  engine           = ""mysql""
  engine_version   = ""8.0""
  instance_class   = ""db.t3.micro""
  allocated_storage = 20
  username         = ""admin""
  password         = ""password123""  # Change this in production
  skip_final_snapshot = true
}

resource ""aws_elastic_beanstalk_environment"" ""eu_west"" {
  provider    = aws.eu_west
  name        = ""myenv_eu_west""
  application = aws_elastic_beanstalk_application.eu_west.name
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

# Route 53 Records
resource ""aws_route53_record"" ""us_east"" {
  provider = aws.us_east
  zone_id  = aws_route53_zone.main.zone_id
  name     = ""us-east-1.example.com""",,,,,,"I'll help you create a Terraform configuration for a multi-region application deployment with Route 53, Elastic Beanstalk, and RDS. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Provider for us-east-1
provider ""aws"" {
  region = ""us-east-1""
  alias  = ""us_east_1""
}

# Provider for eu-west-1
provider ""aws"" {
  region = ""eu-west-1""
  alias  = ""eu_west_1""
}

# Route 53 Zone
resource ""aws_route53_zone"" ""main"" {
  provider = aws.us_east_1
  name     = ""example.com""
}

# IAM Instance Profile
resource ""aws_iam_instance_profile"" ""eb_ec2_profile"" {
  provider = aws.us_east_1
  name     = ""eb_ec2_profile3""
  role     = aws_iam_role.eb_ec2_role.name
}

resource ""aws_iam_role"" ""eb_ec2_role"" {
  provider = aws.us_east_1
  name     = ""eb_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# US East Resources
resource ""aws_elastic_beanstalk_application"" ""us_east"" {
  provider = aws.us_east_1
  name     = ""myapp_us_east""
}

resource ""aws_elastic_beanstalk_environment"" ""us_east"" {
  provider    = aws.us_east_1
  name        = ""myenv_us_east""
  application = aws_elastic_beanstalk_application.us_east.name
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

resource ""aws_db_instance"" ""us_east"" {
  provider          = aws.us_east_1
  identifier        = ""main-db-us-east""
  engine            = ""mysql""
  engine_version    = ""8.0""
  instance_class    = ""db.t3.micro""
  allocated_storage = 20
  username          = ""admin""
  password          = ""password123""  # Use secrets management in production
}

# EU West Resources
resource ""aws_elastic_beanstalk_application"" ""eu_west"" {
  provider = aws.eu_west_1
  name     = ""myapp_eu_west""
}

resource ""aws_elastic_beanstalk_environment"" ""eu_west"" {
  provider    = aws.eu_west_1
  name        = ""myenv_eu_west""
  application = aws_elastic_beanstalk_application.eu_west.name
  
  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_ec2_profile.name
  }
}

resource ""aws_db_instance"" ""eu_west"" {
  provider          = aws.eu_west_1
  identifier        = ""main-db-eu-west""
  engine            = ""mysql""
  engine_version    = ""8.0""
  instance_class    = ""db.t3.micro""
  allocated_storage = 20
  username          = ""admin""
  password          = ""password123""  # Use secrets management in production
}

# Route 53 Records
resource ""aws_route53_record"" ""us_east"" {
  provider = aws.us_east_1
  zone",,,,,
aws_lightsail_database,"create AWS Lightsail that creates a mysql database. It should allow daily backups to take place between 16:00 and 16:30 each day and  requires any maintiance tasks (anything that would cause an outage, including changing some attributes) to take place on Tuesdays between 17:00 and 17:30","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.bundle_id
    resource.values.preferred_backup_window == ""16:00-16:30""
    resource.values.preferred_maintenance_window == ""Tue:17:00-Tue:17:30""
    
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name     = ""test""
  availability_zone            = ""us-east-1a""
  master_database_name         = ""testdatabasename""
  master_password              = ""testdatabasepassword""
  master_username              = ""test""
  blueprint_id                 = ""mysql_8_0""
  bundle_id                    = ""micro_1_0""
  preferred_backup_window      = ""16:00-16:30""
  preferred_maintenance_window = ""Tue:17:00-Tue:17:30""
}","have one ""aws_lightsail_database"" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id
    with bundle_id
    with  preferred_backup_window    = ""16:00-16:30""
  preferred_maintenance_window = ""Tue:17:00-Tue:17:30""

    ","I'll help you create a Terraform configuration for an AWS Lightsail MySQL database with the specified backup and maintenance windows. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

resource ""aws_lightsail_database"" ""mysql_db"" {
  relational_database_name = ""my-mysql-database""
  availability_zone       = ""us-east-1a""  # Change to match your region
  master_database_name   = ""main_db""
  master_username       = ""admin""
  master_password       = ""YourSecurePassword123!""  # Change this to a secure password
  blueprint_id          = ""mysql_8_0""
  bundle_id            = ""micro_1_0""  # Smallest instance size, adjust as needed
  
  # Backup window (16:00-16:30 UTC daily)
  preferred_backup_window = ""16:00-16:30""
  
  # Maintenance window (Tuesday 17:00-17:30 UTC)
  preferred_maintenance_window = ""tue:17:00-tue:17:30""
  
  # Enable automatic backups
  backup_retention_enabled = true
  
  # Additional optional configurations
  apply_immediately = false  # Set to true if you want changes to apply immediately
  
  tags = {
    Name = ""mysql-lightsail-db""
    Environment = ""production""
  }
}

# Optional: Create a static IP for the database
resource ""aws_lightsail_database_static_ip_attachment"" ""mysql_static_ip"" {
  static_ip_name          = aws_lightsail_static_ip.mysql_static_ip.id
  database_name           = aws_lightsail_database.mysql_db.id
}

resource ""aws_lightsail_static_ip"" ""mysql_static_ip"" {
  name = ""mysql-db-static-ip""
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a MySQL 8.0 database instance in Lightsail
3. Configures daily backups between 16:00-16:30 UTC
4. Sets maintenance window for Tuesdays between 17:00-17:30 UTC
5. Enables automatic backups
6. Optionally creates and attaches a static IP to the database

Important notes:
- Remember to replace the `master_password` with a secure password
- Adjust the `availability_zone` to match your desired region
- The `bundle_id` can be changed to a larger instance size if needed
- All times are in UTC
- The configuration includes tags for better resource management
- The static IP attachment is optional but recommended for production use

Make sure to adjust the values according to your specific requirements, especially the region, availability zone, and password.",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail MySQL database with your specified backup and maintenance windows. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

resource ""aws_lightsail_database"" ""mysql_db"" {
  relational_database_name = ""my-mysql-database""
  availability_zone       = ""us-east-1a""  # Change to match your region
  master_database_name   = ""main_db""
  master_username        = ""admin""
  master_password        = ""your-secure-password""  # Replace with a secure password
  blueprint_id           = ""mysql_8_0""
  bundle_id             = ""micro_1_0""  # Smallest instance size, adjust as needed
  
  preferred_backup_window      = ""16:00-16:30""
  preferred_maintenance_window = ""tue:17:00-tue:17:30""
  
  backup_retention_enabled = true
  
  # Configure other optional parameters
  skip_final_snapshot     = true
  apply_immediately       = false

  tags = {
    Name = ""mysql-lightsail-db""
    Environment = ""production""
  }
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a Lightsail MySQL database with:
   - Daily backup window between 16:00-16:30 (specified in preferred_backup_window)
   - Maintenance window on Tuesdays between 17:00-17:30 (specified in preferred_maintenance_window)
   - Uses MySQL 8.0 as the database engine
   - Enables backup retention
   - Configures basic security and identification parameters

Make sure to:
1. Replace the region with your desired AWS region
2. Update the availability_zone to match your region
3. Set a secure master_password
4. Adjust the bundle_id based on your performance needs
5. Modify tags according to your naming conventions

The maintenance window format is ""ddd:hh24:mi-ddd:hh24:mi"" where ddd is the day of week (tue in this case).",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail MySQL database with your specified backup and maintenance windows. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

resource ""aws_lightsail_database"" ""mysql_db"" {
  relational_database_name = ""my-mysql-database""
  availability_zone       = ""us-east-1a""  # Change to match your region
  master_database_name   = ""main_db""
  master_username        = ""admin""
  master_password        = ""your-secure-password""  # Replace with a secure password
  blueprint_id           = ""mysql_8_0""
  bundle_id             = ""micro_1_0""  # Smallest instance size, adjust as needed
  
  preferred_backup_window      = ""16:00-16:30""  # Daily backup window
  preferred_maintenance_window = ""tue:17:00-tue:17:30""  # Tuesday maintenance window
  
  backup_retention_enabled = true
  
  tags = {
    Name = ""mysql-lightsail-db""
  }
}

# Configure public accessibility if needed
resource ""aws_lightsail_database_public_port"" ""mysql_public_port"" {
  relational_database_name = aws_lightsail_database.mysql_db.id
  port_info {
    port     = 3306
    protocol = ""tcp""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a MySQL Lightsail database with:
   - Daily backup window between 16:00-16:30
   - Maintenance window on Tuesdays between 17:00-17:30
   - Uses MySQL 8.0 as the database engine
   - Enables backup retention
3. Optionally configures public accessibility for the database

Make sure to:
- Replace the region with your desired AWS region
- Change the availability zone to match your region
- Set a secure master password
- Adjust the bundle_id based on your performance needs
- Remove the public port resource if you don't need public access

The database will automatically perform backups during the specified backup window and schedule any maintenance operations during the specified maintenance window on Tuesdays.",,,,,
"aws_cloudwatch_log_group, aws_msk_cluster, aws_security_group, aws_subnet, aws_subnet, aws_subnet, aws_vpc, aws_availability_zones",Create a managed MSK cluster in us-east1 logging to a CloudWatch group.,"package main

import future.keywords.in

default allow = false

# Check if any MSK cluster is being created
msk_cluster_created(resources) {
some resource in resources
resource.type == ""aws_msk_cluster""
resource.change.actions[_] == ""create""
}

# Check if CloudWatch logging is enabled for broker logs
cloudwatch_logging_enabled(resource) {
resource.type == ""aws_msk_cluster""
resource.change.after.logging_info[_].broker_logs[_].cloudwatch_logs[_].enabled == true
}

# Aggregate all checks
allow {
msk_cluster_created(input.resource_changes)
some resource in input.resource_changes
cloudwatch_logging_enabled(resource)
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""vpc"" {
    cidr_block = ""192.168.0.0/22""
}

data ""aws_availability_zones"" ""azs"" {
    state = ""available""
}

resource ""aws_subnet"" ""subnet_az1"" {
    availability_zone = data.aws_availability_zones.azs.names[0]
    cidr_block = ""192.168.0.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az2"" {
    availability_zone = data.aws_availability_zones.azs.names[1]
    cidr_block = ""192.168.1.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_subnet"" ""subnet_az3"" {
    availability_zone = data.aws_availability_zones.azs.names[2]
    cidr_block = ""192.168.2.0/24""
    vpc_id = aws_vpc.vpc.id
    map_public_ip_on_launch = true
}

resource ""aws_internet_gateway"" ""main"" {
  vpc_id = aws_vpc.vpc.id
}

resource ""aws_route_table"" ""main"" {
  vpc_id = aws_vpc.vpc.id

  route {
    cidr_block = ""0.0.0.0/0""
    gateway_id = aws_internet_gateway.main.id
  }
}

resource ""aws_route_table_association"" ""first"" {
  subnet_id      = aws_subnet.subnet_az1.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""second"" {
  subnet_id      = aws_subnet.subnet_az2.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_route_table_association"" ""third"" {
  subnet_id      = aws_subnet.subnet_az3.id
  route_table_id = aws_route_table.main.id
}

resource ""aws_security_group"" ""sg"" {
    vpc_id = aws_vpc.vpc.id
}

resource ""aws_vpc_security_group_ingress_rule"" ""ingress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4 = ""0.0.0.0/0""
  ip_protocol = ""-1""
}

resource ""aws_vpc_security_group_egress_rule"" ""egress1"" {
  security_group_id = aws_security_group.sg.id
  cidr_ipv4         = ""0.0.0.0/0""
  ip_protocol       = ""-1""
}

resource ""aws_cloudwatch_log_group"" ""test"" {
    name = ""msk_broker_logs""
}

resource ""aws_msk_cluster"" ""example"" {
    cluster_name = ""example""
    kafka_version = ""3.2.0""
    number_of_broker_nodes = 3

    broker_node_group_info {
        instance_type = ""kafka.t3.small""
        client_subnets = [
            aws_subnet.subnet_az1.id,
            aws_subnet.subnet_az2.id,
            aws_subnet.subnet_az3.id,
        ]

        storage_info {
            ebs_storage_info {
                volume_size = 1000
            }
        }

        security_groups = [aws_security_group.sg.id]
    }

    open_monitoring {
        prometheus {
            jmx_exporter {
                enabled_in_broker = true
            }
            node_exporter {
                enabled_in_broker = true
            }
        }
    }

    logging_info {
        broker_logs {
            cloudwatch_logs {
                enabled = true
                log_group = aws_cloudwatch_log_group.test.name
            }
            firehose {
                enabled = false
            }
            s3 {
                enabled = false
            }
        }
    }
}

output ""zookeeper_connect_string"" {
    value = aws_msk_cluster.example.zookeeper_connect_string
}

output ""bootstrap_brokers_tls"" {
    description = ""TLS connection host:port pairs""
    value = aws_msk_cluster.example.bootstrap_brokers_tls
}","Has one ""aws_msk_cluster"" resource
with one ""logging_info""
with one ""broker_logs""
with one ""cloudwatch_logs""
with one ""enabled"" = true","I'll help you create a Terraform configuration for an AWS MSK (Managed Streaming for Apache Kafka) cluster with CloudWatch logging. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""msk_log_group"" {
  name              = ""/aws/msk/cluster/msk-cluster-logs""
  retention_in_days = 30
}

# Create MSK Configuration
resource ""aws_msk_configuration"" ""msk_config"" {
  kafka_versions = [""2.8.1""]
  name          = ""msk-config""

  server_properties = <<PROPERTIES
auto.create.topics.enable=true
delete.topic.enable=true
PROPERTIES
}

# Create MSK Cluster
resource ""aws_msk_cluster"" ""msk_cluster"" {
  cluster_name           = ""msk-cluster""
  kafka_version         = ""2.8.1""
  number_of_broker_nodes = 3

  broker_node_group_info {
    instance_type   = ""kafka.t3.small""
    client_subnets  = [aws_subnet.subnet1.id, aws_subnet.subnet2.id, aws_subnet.subnet3.id]
    security_groups = [aws_security_group.msk_sg.id]
    storage_info {
      ebs_storage_info {
        volume_size = 100
      }
    }
  }

  encryption_info {
    encryption_at_rest_kms_key_arn = aws_kms_key.msk_key.arn
  }

  logging_info {
    broker_logs {
      cloudwatch_logs {
        enabled   = true
        log_group = aws_cloudwatch_log_group.msk_log_group.name
      }
    }
  }

  tags = {
    Environment = ""production""
  }
}

# Create KMS key for encryption
resource ""aws_kms_key"" ""msk_key"" {
  description = ""KMS key for MSK cluster encryption""
}

# Create Security Group for MSK
resource ""aws_security_group"" ""msk_sg"" {
  name        = ""msk-security-group""
  description = ""Security group for MSK cluster""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 9092
    to_port     = 9092
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Create VPC and Subnets
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""msk-vpc""
  }
}

resource ""aws_subnet"" ""subnet1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-east-1a""

  tags = {
    Name = ""msk-subnet-1""
  }
}

resource ""aws_subnet"" ""subnet2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-east-1b""

  tags = {
    Name = ""msk-subnet-2""
  }
}

resource ""aws_subnet"" ""subnet3"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.3.0/24""
  availability_zone = ""us-east-1c""

  tags = {
    Name = ""msk-subnet-3""
  }
}
```

This configuration:",,,,,,"I'll help you create a Terraform configuration for an AWS MSK (Managed Streaming for Apache Kafka) cluster with CloudWatch logging. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create CloudWatch Log Group
resource ""aws_cloudwatch_log_group"" ""msk_log_group"" {
  name              = ""/aws/msk/cluster/msk-cluster-logs""
  retention_in_days = 30
}

# Create MSK Configuration
resource ""aws_msk_configuration"" ""msk_config"" {
  kafka_versions = [""2.8.1""]
  name          = ""msk-config""

  server_properties = <<PROPERTIES
auto.create.topics.enable=true
delete.topic.enable=true
PROPERTIES
}

# Create MSK Cluster
resource ""aws_msk_cluster"" ""msk_cluster"" {
  cluster_name           = ""msk-cluster""
  kafka_version         = ""2.8.1""
  number_of_broker_nodes = 3

  broker_node_group_info {
    instance_type   = ""kafka.t3.small""
    client_subnets  = [aws_subnet.subnet1.id, aws_subnet.subnet2.id, aws_subnet.subnet3.id]
    security_groups = [aws_security_group.msk_sg.id]
    storage_info {
      ebs_storage_info {
        volume_size = 100
      }
    }
  }

  encryption_info {
    encryption_at_rest_kms_key_arn = aws_kms_key.msk_key.arn
  }

  logging_info {
    broker_logs {
      cloudwatch_logs {
        enabled   = true
        log_group = aws_cloudwatch_log_group.msk_log_group.name
      }
    }
  }

  tags = {
    Environment = ""production""
  }
}

# Supporting resources (you'll need to configure these according to your VPC setup)
resource ""aws_kms_key"" ""msk_key"" {
  description = ""KMS key for MSK cluster encryption""
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  tags = {
    Name = ""msk-vpc""
  }
}

resource ""aws_subnet"" ""subnet1"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
  availability_zone = ""us-east-1a""
}

resource ""aws_subnet"" ""subnet2"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
  availability_zone = ""us-east-1b""
}

resource ""aws_subnet"" ""subnet3"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.3.0/24""
  availability_zone = ""us-east-1c""
}

resource ""aws_security_group"" ""msk_sg"" {
  name        = ""msk-security-group""
  description = ""Security group for MSK cluster""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 9092
    to_port     = 9092
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}
```

This configuration:
1. Creates a CloudWatch log group for MSK logs
2. Sets up an MSK configuration with basic Kafka properties
3. Creates an MSK cluster with:
   - 3 broker nodes
   - CloudWatch logging enabled
   - Encryption at rest using KMS
   - Necessary networking components (VPC, subnets, security group)
4. Uses t3.small instances for",,,,,,"I'll help you create a Terraform configuration for an AWS MSK (Managed Streaming for Apache Kafka) cluster with CloudWatch logging. Here's a solution that includes the required provider and necessary resources:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_msk_cluster"" ""kafka_cluster"" {
  cluster_name           = ""msk-cluster-example""
  kafka_version         = ""2.8.1""
  number_of_broker_nodes = 3

  broker_node_group_info {
    instance_type   = ""kafka.t3.small""
    client_subnets  = [aws_subnet.subnet1.id, aws_subnet.subnet2.id, aws_subnet.subnet3.id]
    security_groups = [aws_security_group.sg.id]
    storage_info {
      ebs_storage_info {
        volume_size = 100
      }
    }
  }

  logging_info {
    broker_logs {
      cloudwatch_logs {
        enabled   = true
        log_group = aws_cloudwatch_log_group.msk_log_group.name
      }
    }
  }

  encryption_info {
    encryption_at_rest_kms_key_arn = aws_kms_key.msk_key.arn
  }

  tags = {
    Environment = ""Production""
  }
}

resource ""aws_cloudwatch_log_group"" ""msk_log_group"" {
  name              = ""/aws/msk/cluster/msk-cluster-logs""
  retention_in_days = 30
}

resource ""aws_kms_key"" ""msk_key"" {
  description = ""KMS key for MSK cluster encryption""
  enable_key_rotation = true
}

# Note: You'll need to define the following resources as well:
# - VPC
# - Subnets
# - Security Group
# These are required for the MSK cluster but not shown here for brevity
```

This configuration:
1. Creates an MSK cluster with 3 broker nodes
2. Sets up CloudWatch logging for the broker logs
3. Configures encryption at rest using KMS
4. Creates the necessary CloudWatch log group
5. Uses t3.small instances for the brokers
6. Allocates 100GB EBS volumes for storage

Note that you'll need to have the appropriate VPC, subnets, and security groups configured. The actual subnet IDs and security group IDs should be replaced with your existing resources or you should define these resources in your Terraform configuration.

You can customize the following parameters based on your needs:
- `kafka_version`
- `number_of_broker_nodes`
- `instance_type`
- `volume_size`
- `retention_in_days` for the CloudWatch log group

Make sure you have the necessary IAM permissions to create these resources in your AWS account.",,,,,
"aws_backup_plan, aws_backup_selection, aws_backup_vault, aws_iam_role, aws_iam_role_policy_attachment, aws_instance, aws_iam_policy_document","An AWS service that provisions an EC2 instance and backs it up every day at midnight. Accomplish this using the following services: AWS EC2, AWS Backup. Add any necessary services.","package cloud_desktop_high

import rego.v1

default valid := false

instance_valid(instance, ami) if {
        instance.expressions.instance_type
        ami.address in instance.expressions.ami.references
}

backup_plan_valid(backup_plan, backup_vault) if {
        # advanced_backup_setting
        some backup_setting in backup_plan.expressions.advanced_backup_setting
        backup_setting.backup_options

        backup_setting.resource_type.constant_value == ""EC2""

        # rule
        some rule in backup_plan.expressions.rule
        backup_vault.address in rule.target_vault_name.references
        rule.schedule.constant_value == ""cron(0 0 * * ? *)""
}

backup_selection_valid(backup_selection, backup_plan, instance) if {
        backup_plan.address in backup_selection.expressions.plan_id.references
        instance.address in backup_selection.expressions.resources.references
}

valid if {
        resources := input.configuration.root_module.resources
        some backup_plan in resources
        backup_plan.type == ""aws_backup_plan""
        some backup_vault in resources
        backup_vault.type == ""aws_backup_vault""
        some backup_selection in resources
        backup_selection.type == ""aws_backup_selection""
        some instance in resources
        instance.type == ""aws_instance""
        some ami in resources
        ami.type == ""aws_ami""

        instance_valid(instance, ami)
        backup_plan_valid(backup_plan, backup_vault)
        backup_selection_valid(backup_selection, backup_plan, instance)
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_ami"" ""amzn2"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}


resource ""aws_instance"" ""cloud_desktop"" {
  instance_type = ""t2.micro""
  ami = data.aws_ami.amzn2.id
}

resource ""aws_backup_plan"" ""cloud_desktop_backup"" {
  name = ""cloud_desktop_backup_plan""
  rule {
    rule_name         = ""cloud_desktop_backup_rule"" # required, not specified
    target_vault_name = aws_backup_vault.valut.name # required, not specified
    schedule          = ""cron(0 0 * * ? *)""
  }

  advanced_backup_setting {
    backup_options = {
      WindowsVSS = ""enabled""
    }
    resource_type = ""EC2""
  }
}

resource ""aws_backup_vault"" ""valut"" {
  name = ""cloud_desktop_backup_valut""
}

resource ""aws_backup_selection"" ""backup_ec2"" {
  iam_role_arn = aws_iam_role.backup.arn

  name    = ""backup_ec2""
  plan_id = aws_backup_plan.cloud_desktop_backup.id

  resources = [
    aws_instance.cloud_desktop.arn,
  ]
}

data ""aws_iam_policy_document"" ""backup_assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""backup.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""backup"" {
  name               = ""backup""
  assume_role_policy = data.aws_iam_policy_document.backup_assume_role.json
}

resource ""aws_iam_role_policy_attachment"" ""backup"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup""
  role       = aws_iam_role.backup.name
}","have one ""aws_ami"" data resource
have one ""aws_instance"" resource
  with ""ami"" argument linking to the aws_ami data resource
  with ""instance_type"" argument
have one ""aws_backup_plan"" resource
  with ""name"" argument
  with ""rule"" argument block
    with ""rule_name"" argument
    with ""target_vault_name"" argument linking to backup_vault
    with ""schedule"" argument with value ""cron(0 0 * * ? *)""
  with ""advanced_backup_setting"" argument block
    with ""backup_options"" argument block
      with ""WindowsVSS"" argument
    with ""resource_type"" argument with value ""EC2""
have one ""aws_backup_vault"" resource
  with ""name"" argument
have one ""aws_backup_selection"" resource
  with ""iam_role_arn"" argument
  with ""name"" argument
  with ""plan_id"" argument linking to aws_backup_plan resource
  with ""resources"" argument including the aws_instance resource","I'll help you create a Terraform configuration that provisions an EC2 instance and sets up AWS Backup to create daily backups at midnight. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create a VPC for the EC2 instance
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main""
  }
}

# Create a subnet
resource ""aws_subnet"" ""main"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""main""
  }
}

# EC2 Instance
resource ""aws_instance"" ""example"" {
  ami           = ""ami-0c55b159cbfafe1f0"" # Replace with your desired AMI
  instance_type = ""t2.micro""
  subnet_id     = aws_subnet.main.id

  tags = {
    Name = ""backup-example-instance""
  }
}

# AWS Backup vault
resource ""aws_backup_vault"" ""example"" {
  name = ""example-backup-vault""
}

# AWS Backup plan
resource ""aws_backup_plan"" ""example"" {
  name = ""example-backup-plan""

  rule {
    rule_name         = ""daily_backup""
    target_vault_name = aws_backup_vault.example.name
    schedule          = ""cron(0 0 ? * * *)"" # Midnight every day

    lifecycle {
      delete_after = 14 # Keep backups for 14 days
    }
  }
}

# IAM role for AWS Backup
resource ""aws_iam_role"" ""backup_role"" {
  name = ""backup-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""backup.amazonaws.com""
        }
      }
    ]
  })
}

# Attach AWS Backup service role policy
resource ""aws_iam_role_policy_attachment"" ""backup_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup""
  role       = aws_iam_role.backup_role.name
}

# AWS Backup selection
resource ""aws_backup_selection"" ""example"" {
  name         = ""example-backup-selection""
  iam_role_arn = aws_iam_role.backup_role.arn
  plan_id      = aws_backup_plan.example.id

  resources = [
    aws_instance.example.arn
  ]
}
```

This configuration:
1. Creates a VPC and subnet for the EC2 instance
2. Provisions an EC2 instance
3. Creates an AWS Backup vault to store the backups
4. Sets up a backup plan that runs daily at midnight
5. Creates necessary IAM roles and policies for AWS Backup
6. Configures a backup selection that targets the EC2 instance

Remember to:
- Replace the AMI ID with one that's appropriate for your region and needs
- Adjust the backup retention period (currently set to 14 days) as needed
- Modify the VPC and subnet configurations according to your network requirements
- Consider adding security groups and other network configurations as needed

The backups will automatically run every day at midnight (UTC) and be stored in the AWS Backup vault.",,,,,,"I'll help you create a Terraform configuration that provisions an EC2 instance and sets up AWS Backup to create daily backups at midnight. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create VPC and networking components for EC2
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main""
  }
}

resource ""aws_subnet"" ""main"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""Main""
  }
}

# EC2 Instance
resource ""aws_instance"" ""example"" {
  ami           = ""ami-0c55b159cbfafe1f0""  # Replace with desired AMI
  instance_type = ""t2.micro""
  subnet_id     = aws_subnet.main.id

  tags = {
    Name = ""BackupExample""
  }
}

# AWS Backup vault
resource ""aws_backup_vault"" ""example"" {
  name = ""example_backup_vault""
}

# AWS Backup plan
resource ""aws_backup_plan"" ""example"" {
  name = ""daily_backup_plan""

  rule {
    rule_name         = ""daily_backup_rule""
    target_vault_name = aws_backup_vault.example.name
    schedule          = ""cron(0 0 * * ? *)""  # Midnight every day

    lifecycle {
      delete_after = 14  # Keep backups for 14 days
    }
  }
}

# IAM role for AWS Backup
resource ""aws_iam_role"" ""backup_role"" {
  name = ""backup_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""backup.amazonaws.com""
        }
      }
    ]
  })
}

# Attach AWS Backup service role policy
resource ""aws_iam_role_policy_attachment"" ""backup_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup""
  role       = aws_iam_role.backup_role.name
}

# AWS Backup selection
resource ""aws_backup_selection"" ""example"" {
  name         = ""ec2_backup_selection""
  iam_role_arn = aws_iam_role.backup_role.arn
  plan_id      = aws_backup_plan.example.id

  resources = [
    aws_instance.example.arn
  ]
}
```

This configuration:
1. Creates an EC2 instance in a VPC
2. Sets up an AWS Backup vault to store the backups
3. Creates a backup plan that runs daily at midnight
4. Configures the necessary IAM roles and permissions
5. Creates a backup selection that targets the EC2 instance

The backup will occur automatically every day at midnight (UTC) and retain backups for 14 days. You can modify the schedule and retention period by adjusting the `schedule` and `delete_after` values in the backup plan.

Make sure to:
1. Replace the AMI ID with a valid one for your region
2. Adjust the instance type if needed
3. Modify the backup retention period as per your requirements
4. Add any additional networking components (internet gateway, route tables) if the EC2 instance needs internet access",,,,,,"I'll help you create a Terraform configuration that provisions an EC2 instance and sets up AWS Backup to create daily backups at midnight. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create VPC and networking components for EC2
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main""
  }
}

resource ""aws_subnet"" ""main"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""Main""
  }
}

# EC2 Instance
resource ""aws_instance"" ""example"" {
  ami           = ""ami-0c55b159cbfafe1f0"" # Replace with desired AMI
  instance_type = ""t2.micro""
  subnet_id     = aws_subnet.main.id

  tags = {
    Name = ""BackupExample""
  }
}

# AWS Backup vault
resource ""aws_backup_vault"" ""example"" {
  name = ""example_backup_vault""
}

# AWS Backup plan
resource ""aws_backup_plan"" ""example"" {
  name = ""daily_backup_plan""

  rule {
    rule_name         = ""daily_backup_rule""
    target_vault_name = aws_backup_vault.example.name
    schedule          = ""cron(0 0 * * ? *)"" # Midnight every day

    lifecycle {
      delete_after = 14 # Keep backups for 14 days
    }
  }
}

# IAM role for AWS Backup
resource ""aws_iam_role"" ""backup_role"" {
  name = ""backup_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""backup.amazonaws.com""
        }
      }
    ]
  })
}

# Attach AWS Backup service role policy
resource ""aws_iam_role_policy_attachment"" ""backup_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSBackupServiceRolePolicyForBackup""
  role       = aws_iam_role.backup_role.name
}

# AWS Backup selection
resource ""aws_backup_selection"" ""example"" {
  name         = ""ec2_backup_selection""
  iam_role_arn = aws_iam_role.backup_role.arn
  plan_id      = aws_backup_plan.example.id

  resources = [
    aws_instance.example.arn
  ]
}
```

This configuration:
1. Creates an EC2 instance in a VPC
2. Sets up an AWS Backup vault to store the backups
3. Creates a backup plan that runs daily at midnight
4. Configures the necessary IAM roles and permissions
5. Creates a backup selection to target the EC2 instance

The backup will run automatically every day at midnight (UTC) and keep the backups for 14 days. You can modify the schedule using the cron expression and adjust the retention period in the lifecycle block according to your needs.

Remember to:
- Replace the AMI ID with one that's available in your region
- Adjust the instance type if needed
- Modify the VPC/subnet CIDR blocks if they conflict with existing resources
- Update the backup retention period as required",,,,,
"aws_lex_bot, aws_lex_intent",create an aws_lex_bot,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for multiple aws_lex_intent resources
has_valid_lex_intents {
    count([intent | intent := input.planned_values.root_module.resources[_]; intent.type == ""aws_lex_intent""; intent.values.name; intent.values.fulfillment_activity])
}

# Rule for aws_lex_bot resource with specific arguments
has_valid_lex_bot_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lex_bot""
    is_boolean(resource.values.child_directed)
    resource.values.clarification_prompt
    resource.values.abort_statement
    resource.values.name
    # is_boolean(resource.values.create_version)
    # resource.values.description
    # is_boolean(resource.values.detect_sentiment)
    # resource.values.idle_session_ttl_in_seconds
    # resource.values.process_behavior
    # resource.values.voice_id
    # resource.values.locale
    # is_boolean(resource.values.enable_model_improvements)
    # resource.values.nlu_intent_confidence_threshold
}

# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lex_intents
    has_valid_lex_bot_instance
}
",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_lex_intent"" ""order_flowers"" {
  name = ""OrderFlowers""
  description = ""Intent to order a bouquet of flowers for pick up""

  sample_utterances = [
    ""I would like to pick up flowers"",
    ""I would like to order some flowers"",
  ]

  slot {
    name                     = ""FlowerType""
    description              = ""Type of flower to order""
    slot_constraint          = ""Required"" 
    slot_type                = ""AMAZON.AlphaNumeric""
    priority                 = 1

    sample_utterances = [""I would like to get {FlowerType} flower.""]

    value_elicitation_prompt {
      message {
        content             = ""What type of flower would you like?""
        content_type        = ""PlainText""
      }
      max_attempts         = 2
    }
  }

  slot {
    name                     = ""PickupDate""
    description              = ""Date of flower pickup""
    slot_constraint          = ""Required"" 
    slot_type                = ""AMAZON.DATE""
    priority                 = 2

    value_elicitation_prompt {
      message {
        content             = ""When would you like to pickup your flower?""
        content_type        = ""PlainText""
      }
      max_attempts         = 2
    }
  }

  confirmation_prompt {
    max_attempts = 2

    message {
      content      = ""Okay, your {FlowerType} will be ready for pickup on {PickupDate}.  Does this sound okay?""
      content_type = ""PlainText""
    }
  }

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  rejection_statement {
    message {
      content      = ""Okay, I will not place your order.""
      content_type = ""PlainText""
    }
  }
}

resource ""aws_lex_bot"" ""order_flowers"" {
  name     = ""OrderFlowers""
  description = ""Bot to order flowers on the behalf of a user""
  locale   = ""en-US""
  process_behavior = ""BUILD""
  voice_id = ""Salli""
  child_directed = false
  create_version = false
  detect_sentiment            = false
  idle_session_ttl_in_seconds = 600

  abort_statement {
    message {
      content_type = ""PlainText""
      content      = ""Sorry, I am not able to assist at this time""
    }
  }

  clarification_prompt {
    max_attempts = 2

    message {
      content_type = ""PlainText""
      content      = ""I didn't understand you, what would you like to do?""
    }
  }
  
  intent {
    intent_name    = aws_lex_intent.order_flowers.name
    intent_version = aws_lex_intent.order_flowers.version
  }
}","Has one resource ""aws_lex_bot"" and one resource ""aws_lex_intent""","I'll help you create a Terraform configuration for an AWS Lex bot. Here's a code example that includes the required AWS provider and creates a basic Lex bot:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust the region as needed
}

resource ""aws_lex_bot"" ""example_bot"" {
  name = ""ExampleBot""
  description = ""An example Lex bot created with Terraform""
  
  abort_statement {
    message {
      content      = ""Sorry, I am not able to assist at this time""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, could you please rephrase that?""
      content_type = ""PlainText""
    }
  }

  idle_session_ttl_in_seconds = 300
  
  child_directed = false
  
  process_behavior = ""BUILD""
  
  voice_id = ""Salli""

  locale = ""en-US""

  create_version = false
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Lex bot with basic configuration including:
   - Name and description
   - Abort statement for when the bot needs to end the conversation
   - Clarification prompt for when the bot doesn't understand the user
   - Session timeout of 300 seconds (5 minutes)
   - Sets the bot as not child-directed
   - Uses ""Salli"" as the voice ID
   - Sets English (US) as the locale

You can customize the values according to your needs. Additional configurations like intents and slots can be added using separate resources if needed.

Remember to authenticate your AWS provider either through environment variables, shared credentials file, or other AWS authentication methods before applying this configuration.",,,,,,"I'll help you create a Terraform configuration for an AWS Lex bot. I'll include the required AWS provider and create a basic Lex bot configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

resource ""aws_lex_bot"" ""example_bot"" {
  name = ""ExampleBot""
  description = ""An example Lex bot created with Terraform""
  
  abort_statement {
    message {
      content      = ""Sorry, I'm not able to assist at this time""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, could you please repeat that?""
      content_type = ""PlainText""
    }
  }

  idle_session_ttl_in_seconds = 300
  
  process_behavior = ""BUILD""
  voice_id        = ""Salli""
  
  child_directed = false

  locale = ""en-US""

  create_version = false
}
```

This configuration creates a basic AWS Lex bot with the following features:

1. Required AWS provider configuration
2. A basic Lex bot with:
   - Name and description
   - Abort statement for error handling
   - Clarification prompt for when the bot doesn't understand the input
   - Session timeout of 300 seconds (5 minutes)
   - English (US) locale
   - Salli voice for responses
   - Child-directed flag set to false (important for COPPA compliance)

You can customize this further by:
- Adding intents
- Modifying the timeout
- Changing the voice
- Adding additional prompts and responses
- Configuring more advanced features

Remember to:
1. Change the region to match your requirements
2. Modify the bot name and description
3. Adjust the messages and prompts to match your use case
4. Add any additional configurations specific to your needs

This is a basic starting point that you can build upon based on your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS Lex bot. Here's a complete example that includes the required provider and creates a basic Lex bot:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Replace with your desired region
}

resource ""aws_lex_bot"" ""example_bot"" {
  name = ""ExampleBot""
  description = ""An example Lex bot created with Terraform""
  
  child_directed = false  # Indicates whether the bot is directed to children under 13
  
  abort_statement {
    message {
      content      = ""Sorry, I'm having trouble understanding. Could you try again?""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, could you please rephrase that?""
      content_type = ""PlainText""
    }
  }

  idle_session_ttl_in_seconds = 300

  process_behavior = ""BUILD""
  voice_id        = ""Joanna""

  locale = ""en-US""

  intent {
    intent_name    = ""HelloWorld""
    intent_version = ""1""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lex bot with basic configuration including:
   - Name and description
   - Abort statement for when the conversation fails
   - Clarification prompt for when the bot needs clarification
   - Session timeout settings
   - Voice settings
   - Basic intent configuration

You can customize this further by:
- Adding more intents
- Modifying the messages
- Adjusting the session timeout
- Adding additional prompts or statements
- Changing the voice ID
- Modifying the locale

Remember to have the corresponding intent already created before referencing it in the bot configuration.",,,,,
"aws_iam_role, aws_sagemaker_code_repository, aws_sagemaker_notebook_instance, aws_iam_policy_document, aws_sagemaker_prebuilt_ecr_image","Create an aws_sagemaker_notebook instance from ""https://github.com/hashicorp/terraform-provider-aws.git""","package main

import future.keywords.in

default allow = false

aws_iam_role_valid(resources) {
    some resource in resources
    resource.type == ""aws_iam_role""
}

aws_sagemaker_code_repository_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_code_repository""
    resource.change.after.git_config[0].repository_url ==  ""https://github.com/hashicorp/terraform-provider-aws.git""
}

aws_sagemaker_notebook_instance_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_notebook_instance""
    resource.change.after.default_code_repository != null
}

# Aggregate all checks
allow {
    aws_iam_role_valid(input.resource_changes)
    aws_sagemaker_code_repository_valid(input.resource_changes)
    aws_sagemaker_notebook_instance_valid(input.resource_changes)
}",3,"resource ""aws_sagemaker_code_repository"" ""example"" {
  code_repository_name = ""my-notebook-instance-code-repo""

  git_config {
    repository_url = ""https://github.com/hashicorp/terraform-provider-aws.git""
  }
}

resource ""aws_sagemaker_notebook_instance"" ""ni"" {
  name                    = ""my-notebook-instance""
  role_arn                = aws_iam_role.example.arn
  instance_type           = ""ml.t2.medium""
  default_code_repository = aws_sagemaker_code_repository.example.code_repository_name
}

resource ""aws_iam_role"" ""example"" {
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""sagemaker.amazonaws.com""]
    }
  }
}

data ""aws_sagemaker_prebuilt_ecr_image"" ""test"" {
  repository_name = ""kmeans""
}",,"I'll help you create a Terraform configuration for an AWS SageMaker notebook instance. Here's a complete example with the required provider and necessary configurations:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_sagemaker_notebook_instance"" ""example"" {
  name                    = ""my-notebook-instance""
  role_arn               = aws_iam_role.sagemaker_role.arn
  instance_type          = ""ml.t2.medium""  # Choose appropriate instance type
  volume_size            = 5
  subnet_id              = aws_subnet.example.id  # Optional: Specify if you want VPC connectivity
  security_groups        = [aws_security_group.example.id]  # Optional: Specify if you want VPC connectivity

  tags = {
    Name = ""sagemaker-notebook""
    Environment = ""dev""
  }
}

# IAM role for SageMaker
resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-notebook-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""sagemaker_policy"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

# Example VPC resources (if needed)
resource ""aws_vpc"" ""example"" {
  cidr_block = ""10.0.0.0/16""
  
  tags = {
    Name = ""sagemaker-vpc""
  }
}

resource ""aws_subnet"" ""example"" {
  vpc_id     = aws_vpc.example.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-subnet""
  }
}

resource ""aws_security_group"" ""example"" {
  name        = ""sagemaker-sg""
  description = ""Security group for SageMaker notebook instance""
  vpc_id      = aws_vpc.example.id

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""sagemaker-security-group""
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a SageMaker notebook instance
3. Creates necessary IAM roles and policies
4. Sets up VPC resources (subnet and security group) for the notebook instance
5. Includes basic security group rules for HTTPS access

You can customize the configuration by:
- Changing the instance type
- Adjusting the volume size
- Modifying the VPC and subnet configurations
- Adding additional security group rules
- Customizing the tags
- Adding additional IAM policies as needed

Remember to adjust the region and other parameters according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS SageMaker notebook instance. I'll include the required provider block and create a basic SageMaker notebook instance configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust the region as needed
}

resource ""aws_sagemaker_notebook_instance"" ""example"" {
  name                    = ""example-notebook-instance""
  role_arn               = aws_iam_role.sagemaker_role.arn
  instance_type          = ""ml.t2.medium""
  platform_identifier    = ""notebook-al2-v1""
  
  tags = {
    Name = ""example-sagemaker-notebook""
  }
}

# IAM role for SageMaker
resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-notebook-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Attach the necessary policy for SageMaker
resource ""aws_iam_role_policy_attachment"" ""sagemaker_policy"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a SageMaker notebook instance with basic configuration
3. Creates the necessary IAM role and policy attachment for the notebook instance
4. Uses the ml.t2.medium instance type which is suitable for development and testing
5. Uses the Amazon Linux 2 platform identifier

You can customize this further by adding additional configuration options such as:
- volume_size
- subnet_id
- security_groups
- kms_key_id
- direct_internet_access
- root_access

Make sure to adjust the region, instance type, and other parameters according to your specific requirements.",,,,,,"I'll help you create a Terraform configuration for an AWS SageMaker notebook instance. Here's a complete example with the required configurations:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-notebook-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""sagemaker_policy"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

resource ""aws_sagemaker_notebook_instance"" ""example"" {
  name                    = ""my-notebook-instance""
  role_arn               = aws_iam_role.sagemaker_role.arn
  instance_type          = ""ml.t2.medium""
  platform_identifier    = ""notebook-al2-v1""

  tags = {
    Name = ""sagemaker-notebook""
    Environment = ""development""
  }

  lifecycle_config_name  = aws_sagemaker_notebook_instance_lifecycle_configuration.example.name
}

resource ""aws_sagemaker_notebook_instance_lifecycle_configuration"" ""example"" {
  name = ""notebook-lifecycle-config""
  on_create = base64encode(<<EOF
#!/bin/bash
set -e
git clone https://github.com/hashicorp/terraform-provider-aws.git
EOF
  )
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role for SageMaker with necessary permissions
3. Creates a SageMaker notebook instance with:
   - A specified instance type (ml.t2.medium)
   - Platform identifier for Amazon Linux 2
   - Tags for better resource management
   - A lifecycle configuration that clones the specified GitHub repository
4. Includes a lifecycle configuration that will run when the notebook instance is created

You can customize this configuration by:
- Changing the instance type
- Modifying the tags
- Adjusting the region
- Adding additional lifecycle configuration scripts
- Modifying the IAM role permissions as needed

Remember to replace values like the region and instance name according to your requirements.",,,,,
aws_nat_gateway,"creates a NAT Gateway associated with a specified subnet and Elastic IP allocation ID. The NAT Gateway is configured for public connectivity and tagged with a key-value pair (pike = ""permissions""), indicating its purpose or ownership.","package terraform.validation

# Set default validation state
default is_valid_nat_gateway = false

# Validate aws_nat_gateway resource
is_valid_nat_gateway {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_nat_gateway""

        # Ensure it is associated with a specified subnet
        resource.expressions.subnet_id != null

        # Ensure it uses a specific Elastic IP allocation ID
        resource.expressions.allocation_id != null

        # Check for the specific tag indicating its purpose or ownership
        resource.expressions.tags.constant_value.pike == ""permissions""
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_nat_gateway"" ""pike"" {
  subnet_id         = ""subnet-0562ef1d304b968f4""
  allocation_id     = ""eipalloc-0047fa56c40637c3b""
  connectivity_type = ""public""
  tags = {
    pike = ""permissions""
  }
}","Has one ""aws_nat_gateway"" resource:
Associated with a specified ""aws_subnet"" resource for hosting the NAT Gateway.
Utilizes an ""Elastic IP allocation ID"" to provide the NAT Gateway with a public IP address.
Configured for public connectivity, allowing resources within the private subnet to access the internet securely.
Tagged with a key-value pair: ""pike"" set to ""permissions"", which categorizes its role, purpose, or ownership within the infrastructure.","I'll help you create a Terraform configuration for an AWS NAT Gateway with the specified requirements. Here's the solution in HCL:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create Elastic IP for NAT Gateway
resource ""aws_eip"" ""nat_eip"" {
  domain = ""vpc""
  tags = {
    pike = ""permissions""
  }
}

# Create NAT Gateway
resource ""aws_nat_gateway"" ""nat_gateway"" {
  allocation_id = aws_eip.nat_eip.id
  subnet_id     = var.subnet_id  # Reference to the subnet ID where NAT Gateway will be placed
  
  connectivity_type = ""public""   # Ensures public connectivity
  
  tags = {
    pike = ""permissions""
  }

  # Ensure the NAT Gateway is created after the EIP
  depends_on = [aws_eip.nat_eip]
}

# Variable declaration for subnet ID
variable ""subnet_id"" {
  description = ""The ID of the subnet where the NAT Gateway will be created""
  type        = string
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an Elastic IP (EIP) for the NAT Gateway
3. Creates a NAT Gateway with public connectivity
4. Associates the NAT Gateway with the specified subnet
5. Tags both resources with the key-value pair pike = ""permissions""
6. Includes a variable for the subnet ID input

You'll need to provide the subnet ID when applying this configuration. The NAT Gateway will be created in the specified subnet and will use the automatically created Elastic IP for outbound internet connectivity.",,,,,,"I'll help you create a Terraform configuration for an AWS NAT Gateway with the specified requirements. The configuration will include the required AWS provider, NAT Gateway resource, and the necessary tag.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create NAT Gateway
resource ""aws_nat_gateway"" ""main"" {
  subnet_id         = var.subnet_id
  allocation_id     = var.eip_allocation_id
  connectivity_type = ""public""

  tags = {
    pike = ""permissions""
  }
}

# Variables
variable ""subnet_id"" {
  description = ""The ID of the subnet in which to create the NAT Gateway""
  type        = string
}

variable ""eip_allocation_id"" {
  description = ""The Allocation ID of the Elastic IP address for the NAT Gateway""
  type        = string
}
```

This configuration:
1. Declares the required AWS provider
2. Creates a NAT Gateway resource with:
   - Association to a specified subnet
   - Association with an Elastic IP via allocation_id
   - Public connectivity type
   - The required tag (pike = ""permissions"")
3. Defines the necessary variables for subnet_id and eip_allocation_id

Note: Make sure you have already created the subnet and allocated an Elastic IP before applying this configuration. You'll need to provide values for the variables when applying the configuration.",,,,,,"I'll help you create a Terraform configuration for an AWS NAT Gateway that meets your requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create Elastic IP for NAT Gateway
resource ""aws_eip"" ""nat_eip"" {
  domain = ""vpc""
  tags = {
    pike = ""permissions""
  }
}

# Create NAT Gateway
resource ""aws_nat_gateway"" ""main"" {
  allocation_id = aws_eip.nat_eip.id
  subnet_id     = var.subnet_id  # Reference to the subnet where NAT Gateway will be placed
  
  connectivity_type = ""public""
  
  tags = {
    pike = ""permissions""
  }

  # Wait for the NAT Gateway to be created
  depends_on = [aws_eip.nat_eip]
}

# Variable for subnet ID
variable ""subnet_id"" {
  description = ""The ID of the subnet where the NAT Gateway will be created""
  type        = string
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an Elastic IP (EIP) for the NAT Gateway
3. Creates a NAT Gateway with public connectivity
4. Associates the NAT Gateway with the specified subnet and Elastic IP
5. Adds the requested tag (pike = ""permissions"")
6. Includes a variable for the subnet ID input

To use this configuration, you'll need to provide the subnet ID when applying the Terraform configuration. The NAT Gateway will be created in the specified subnet with public connectivity and the requested tagging.",,,,,
"aws_s3_bucket, aws_s3_bucket_acl, aws_s3_bucket_policy","Create a Terraform configuration that defines an AWS S3 bucket, an ACL for the bucket, and a bucket policy allowing s3:GetObject actions with a condition based on the AWS Referer header. The bucket should have a unique name like ""my_unique_bucket_name,"" and the policy should grant access to all principals. ","package terraform.validation

default has_aws_s3_bucket = false
default has_aws_s3_bucket_acl = false
default has_aws_s3_bucket_policy = false
has_aws_s3_bucket {
    bucket := input.planned_values.root_module.resources[_]
    bucket.type == ""aws_s3_bucket""
    bucket.name == ""my_bucket""
    bucket.values.bucket == ""my_unique_bucket_name""
}

has_aws_s3_bucket_acl {
    acl := input.planned_values.root_module.resources[_]
    acl.type == ""aws_s3_bucket_acl""
    acl.name == ""my_bucket_acl""
    acl.values.bucket == input.planned_values.root_module.resources[_].values.id  # Ensure correct bucket reference
    acl.values.acl == ""private""
}

has_aws_s3_bucket_policy {
    policy := input.planned_values.root_module.resources[_]
    policy.type == ""aws_s3_bucket_policy""
    policy.name == ""my_bucket_policy""
    policy.values.bucket == input.planned_values.root_module.resources[_].values.id  # Ensure correct bucket reference
}

valid_configuration {
    has_aws_s3_bucket
    has_aws_s3_bucket_acl
    has_aws_s3_bucket_policy
}
",3,"provider ""aws"" {
    region = ""us-west-1""
}

resource ""aws_s3_bucket"" ""my_bucket"" {
  bucket = ""my_unique_bucket_name""
}

resource ""aws_s3_bucket_acl"" ""my_bucket_acl"" {
  bucket = aws_s3_bucket.my_bucket.id
  acl    = ""private""
}

resource ""aws_s3_bucket_policy"" ""my_bucket_policy"" {
  bucket = aws_s3_bucket.my_bucket.id
  policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Sid       = ""ExampleStatement1"",
        Effect    = ""Allow"",
        Principal = ""*"",
        Action    = [
          ""s3:GetObject""
        ],
        Resource = [
          ""${aws_s3_bucket.my_bucket.arn}/*""
        ],
        Condition = {
          StringEquals = {
            ""aws:Referer"" = ""http://www.example.com""
          }
        }
      }
    ]
  })
}","Resource ""aws_s3_bucket""
has one ""bucket""

Resource ""aws_s3_bucket_acl""
has one ""bucket""
has one ""acl""

Resource ""aws_s3_bucket_policy""
has one ""bucket""
has one ""policy""","I'll help you create a Terraform configuration for an S3 bucket with the specified ACL and bucket policy. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""my-unique-bucket-name-2023""
}

resource ""aws_s3_bucket_acl"" ""bucket_acl"" {
  bucket = aws_s3_bucket.example_bucket.id
  acl    = ""private""
}

resource ""aws_s3_bucket_public_access_block"" ""public_access"" {
  bucket = aws_s3_bucket.example_bucket.id

  block_public_acls       = false
  block_public_policy     = false
  ignore_public_acls      = false
  restrict_public_buckets = false
}

resource ""aws_s3_bucket_policy"" ""bucket_policy"" {
  bucket = aws_s3_bucket.example_bucket.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowGetObjectWithReferer""
        Effect    = ""Allow""
        Principal = ""*""
        Action    = ""s3:GetObject""
        Resource  = ""${aws_s3_bucket.example_bucket.arn}/*""
        Condition = {
          StringLike = {
            ""aws:Referer"" = [""http://example.com/*""]
          }
        }
      }
    ]
  })

  depends_on = [aws_s3_bucket_public_access_block.public_access]
}
```

This configuration:

1. Sets up the AWS provider
2. Creates an S3 bucket with a unique name
3. Configures the bucket ACL as private
4. Disables the public access block settings to allow the bucket policy to work
5. Creates a bucket policy that:
   - Allows s3:GetObject actions
   - Applies to all principals (*)
   - Includes a condition based on the Referer header
   - Applies to all objects in the bucket

Note that the bucket name uses hyphens instead of underscores as S3 bucket names must follow DNS naming conventions. The policy will only allow access to objects when the request includes a Referer header matching ""http://example.com/*"". You can modify the Referer condition according to your specific needs.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with ACL and a bucket policy that includes a referer-based condition. Here's a secure and well-structured solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

# S3 Bucket
resource ""aws_s3_bucket"" ""example_bucket"" {
  bucket = ""my-unique-bucket-name-2023""  # Replace with your desired unique bucket name
}

# Bucket ACL
resource ""aws_s3_bucket_acl"" ""bucket_acl"" {
  bucket = aws_s3_bucket.example_bucket.id
  acl    = ""private""  # Setting private ACL as a security best practice
}

# Bucket Policy
resource ""aws_s3_bucket_policy"" ""bucket_policy"" {
  bucket = aws_s3_bucket.example_bucket.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""PublicReadGetObject""
        Effect    = ""Allow""
        Principal = ""*""
        Action    = ""s3:GetObject""
        Resource  = ""${aws_s3_bucket.example_bucket.arn}/*""
        Condition = {
          StringLike = {
            ""aws:Referer"" = [
              ""http://www.example.com/*"",
              ""https://www.example.com/*""
            ]
          }
        }
      }
    ]
  })
}

# Enable versioning for the bucket
resource ""aws_s3_bucket_versioning"" ""versioning"" {
  bucket = aws_s3_bucket.example_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Server-side encryption configuration
resource ""aws_s3_bucket_server_side_encryption_configuration"" ""encryption"" {
  bucket = aws_s3_bucket.example_bucket.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = ""AES256""
    }
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates an S3 bucket with a unique name
3. Configures a private ACL for the bucket
4. Adds a bucket policy that:
   - Allows s3:GetObject actions
   - Applies to all principals (*)
   - Includes a condition based on the AWS Referer header
5. Enables versioning for added data protection
6. Configures server-side encryption using AES256

The bucket policy specifically allows access only when the request's referer header matches the specified domains. Remember to replace ""www.example.com"" with your actual domain in the referer condition.

Best practices included:
- Private ACL by default
- Enabled versioning
- Server-side encryption
- Specific condition to restrict access based on referer
- Well-structured resource naming and organization",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with ACL and a bucket policy that includes a Referer condition. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# Create S3 bucket
resource ""aws_s3_bucket"" ""website_bucket"" {
  bucket = ""my-unique-bucket-name-2023""
}

# Create bucket ACL
resource ""aws_s3_bucket_acl"" ""bucket_acl"" {
  bucket = aws_s3_bucket.website_bucket.id
  acl    = ""private""
}

# Create bucket policy
resource ""aws_s3_bucket_policy"" ""allow_access_with_referer"" {
  bucket = aws_s3_bucket.website_bucket.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Sid       = ""AllowGetObjectWithReferer""
        Effect    = ""Allow""
        Principal = ""*""
        Action    = ""s3:GetObject""
        Resource  = ""${aws_s3_bucket.website_bucket.arn}/*""
        Condition = {
          StringLike = {
            ""aws:Referer"" = [
              ""http://example.com/*"",
              ""https://example.com/*""
            ]
          }
        }
      }
    ]
  })
}

# Enable bucket versioning (optional but recommended)
resource ""aws_s3_bucket_versioning"" ""versioning"" {
  bucket = aws_s3_bucket.website_bucket.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# Block public access (optional security measure)
resource ""aws_s3_bucket_public_access_block"" ""public_access"" {
  bucket = aws_s3_bucket.website_bucket.id

  block_public_acls       = true
  block_public_policy     = false
  ignore_public_acls      = true
  restrict_public_buckets = false
}
```

This configuration:

1. Sets up the AWS provider
2. Creates an S3 bucket with a unique name
3. Applies a private ACL to the bucket
4. Creates a bucket policy that:
   - Allows s3:GetObject actions
   - Applies to all principals (*)
   - Includes a condition based on the Referer header
5. Enables versioning for the bucket
6. Configures public access block settings

The bucket policy allows access only when the request comes from specified referrers (example.com in this case). You can modify the referrer URLs in the StringLike condition to match your requirements.

Remember to replace ""my-unique-bucket-name-2023"" with your desired bucket name, ensuring it's globally unique across all AWS accounts.",,,,,
aws_lightsail_instance,create AWS Lightsail with WordPress blueprint,"package terraform.validation

default has_valid_lightsail_instance = false

# Main rule to check for a valid aws_lightsail_instance
has_valid_lightsail_instance {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lightsail_instance""
    resource.values.name
    resource.values.availability_zone
    resource.values.blueprint_id == ""wordpress""
    resource.values.bundle_id
}",1,"provider ""aws"" {
  region     = ""us-east-1""
}

resource ""aws_lightsail_instance"" ""custom"" {
  name              = ""custom""
  availability_zone = ""us-east-1b""
  blueprint_id      = ""wordpress""
  bundle_id         = ""nano_2_0""
 }","have one ""aws_lightsail_instance"" resource
with ""name"" argument
with ""availability_zone"" argument
with ""blueprint_id"" = wordpress
with ""bundle_id"" argument","I'll help you create an AWS Lightsail instance with WordPress blueprint using Terraform. Here's a configuration that sets up a Lightsail instance with WordPress:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change this to your desired region
}

resource ""aws_lightsail_instance"" ""wordpress_instance"" {
  name              = ""wordpress-instance""
  availability_zone = ""us-east-1a""  # Change this to match your region
  blueprint_id      = ""wordpress""
  bundle_id         = ""nano_2_0""    # Smallest instance size, change if needed

  tags = {
    Name = ""WordPress-Site""
    Environment = ""Production""
  }
}

# Optional: Create a static IP for the instance
resource ""aws_lightsail_static_ip"" ""wordpress_static_ip"" {
  name = ""wordpress-static-ip""
}

# Attach static IP to the instance
resource ""aws_lightsail_static_ip_attachment"" ""wordpress_static_ip_attachment"" {
  static_ip_name = aws_lightsail_static_ip.wordpress_static_ip.name
  instance_name  = aws_lightsail_instance.wordpress_instance.name
}

# Optional: Open required ports
resource ""aws_lightsail_instance_public_ports"" ""wordpress_ports"" {
  instance_name = aws_lightsail_instance.wordpress_instance.name

  port_info {
    protocol  = ""tcp""
    from_port = 80
    to_port   = 80
  }

  port_info {
    protocol  = ""tcp""
    from_port = 443
    to_port   = 443
  }
}
```

This configuration:

1. Sets up the AWS provider
2. Creates a Lightsail instance using the WordPress blueprint
3. Uses the 'nano_2_0' bundle (smallest instance size)
4. Creates and attaches a static IP
5. Opens HTTP (80) and HTTPS (443) ports

You can customize this further by:
- Changing the region and availability zone
- Selecting a different bundle_id for more resources
- Adding more port configurations
- Adjusting tags

The instance will be provisioned with WordPress pre-installed and ready to configure. After deployment, you can access the WordPress installation through the Lightsail console or via the static IP address.",,,,,,"I'll help you create an AWS Lightsail instance with WordPress blueprint using Terraform. Here's a complete configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

resource ""aws_lightsail_instance"" ""wordpress_instance"" {
  name              = ""wordpress-instance""
  availability_zone = ""us-east-1a""  # Change to match your region
  blueprint_id      = ""wordpress""   # WordPress blueprint
  bundle_id         = ""nano_2_0""    # Smallest instance size, adjust as needed

  tags = {
    Name = ""WordPress-Site""
    Environment = ""Production""
  }
}

# Optional: Create a static IP for the instance
resource ""aws_lightsail_static_ip"" ""wordpress_static_ip"" {
  name = ""wordpress-static-ip""
}

# Attach static IP to the instance
resource ""aws_lightsail_static_ip_attachment"" ""wordpress_static_ip_attachment"" {
  static_ip_name = aws_lightsail_static_ip.wordpress_static_ip.name
  instance_name  = aws_lightsail_instance.wordpress_instance.name
}

# Open ports for WordPress
resource ""aws_lightsail_instance_public_ports"" ""wordpress_ports"" {
  instance_name = aws_lightsail_instance.wordpress_instance.name

  port_info {
    protocol  = ""tcp""
    from_port = 80
    to_port   = 80
  }

  port_info {
    protocol  = ""tcp""
    from_port = 443
    to_port   = 443
  }
}
```

This configuration:
1. Sets up the AWS provider
2. Creates a Lightsail instance using the WordPress blueprint
3. Assigns a static IP to the instance
4. Configures the necessary ports (80 and 443) for web traffic

You can customize the following parameters:
- `region` and `availability_zone` to match your desired location
- `bundle_id` to choose a different instance size (e.g., ""micro_2_0"", ""small_2_0"", etc.)
- Add or modify tags as needed
- Adjust the port configurations based on your requirements

Remember to have your AWS credentials configured properly before running this configuration.",,,,,,"I'll help you create an AWS Lightsail instance with WordPress blueprint using Terraform. Here's a solution that creates a Lightsail instance with WordPress pre-installed:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change this to your desired region
}

# Create a Lightsail Instance with WordPress
resource ""aws_lightsail_instance"" ""wordpress_instance"" {
  name              = ""wordpress-instance""
  availability_zone = ""us-east-1a""  # Change this to match your region
  blueprint_id      = ""wordpress""
  bundle_id         = ""micro_2_0""   # 2GB RAM, 1 vCPU, 40 GB SSD

  tags = {
    Name = ""WordPress-Site""
    Environment = ""Production""
  }
}

# Create a static IP for the Lightsail instance
resource ""aws_lightsail_static_ip"" ""wordpress_static_ip"" {
  name = ""wordpress-static-ip""
}

# Attach static IP to the instance
resource ""aws_lightsail_static_ip_attachment"" ""wordpress_static_ip_attachment"" {
  static_ip_name = aws_lightsail_static_ip.wordpress_static_ip.name
  instance_name  = aws_lightsail_instance.wordpress_instance.name
}

# Optional: Create a Lightsail Instance Public Ports
resource ""aws_lightsail_instance_public_ports"" ""wordpress_ports"" {
  instance_name = aws_lightsail_instance.wordpress_instance.name

  port_info {
    protocol  = ""tcp""
    from_port = 80
    to_port   = 80
  }

  port_info {
    protocol  = ""tcp""
    from_port = 443
    to_port   = 443
  }
}
```

This configuration:

1. Sets up the AWS provider
2. Creates a Lightsail instance using the WordPress blueprint
3. Uses the ""micro_2_0"" bundle (2GB RAM, 1 vCPU)
4. Creates and attaches a static IP to the instance
5. Configures public ports for HTTP (80) and HTTPS (443)

You can modify the following parameters according to your needs:
- `region` and `availability_zone`
- `name` for both instance and static IP
- `bundle_id` (available options: nano_2_0, micro_2_0, small_2_0, medium_2_0, etc.)
- Add or modify tags as needed
- Adjust the public ports configuration

After applying this configuration, you can access your WordPress site using the static IP address assigned to the instance.",,,,,
"aws_db_instance, aws_db_parameter_group, aws_db_subnet_group, aws_kms_key, aws_security_group, aws_subnet, aws_subnet","Provision a secure PostgreSQL database (version is 15 and storage size is 50GB) instance within a specified AWS VPC, leveraging AWS services like RDS, subnets, and KMS for encryption. It sets up a custom parameter group for PostgreSQL settings, and a security group to manage access. The database instance is configured with specifications like storage size, engine version, multi-AZ deployment for high availability, and encryption using a KMS key.","package terraform.validation

default is_valid_db_instance = false

default is_valid_security_group = false

default is_valid_db_subnet_group = false

default is_valid_db_parameter_group = false

default is_valid_kms_key = false

# Validate aws_db_instance resource
is_valid_db_instance {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_instance""
        resource.expressions.allocated_storage.constant_value == 20
        resource.expressions.engine.constant_value == ""postgres""
    resource.expressions.engine_version.constant_value == ""15""
        resource.expressions.instance_class != null
        resource.expressions.username != null
        resource.expressions.password != null
        resource.expressions.multi_az.constant_value == true
        resource.expressions.storage_encrypted.constant_value == true
        resource.expressions.kms_key_id != null
}

# Validate aws_security_group resource
is_valid_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""
        count(resource.expressions.ingress) > 0
        count(resource.expressions.egress) > 0
}

# Validate aws_db_subnet_group resource
is_valid_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}

# Validate aws_db_parameter_group resource
is_valid_db_parameter_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_parameter_group""
        # Additional checks for specific parameter values can be added here if needed
}

# Validate aws_kms_key resource
is_valid_kms_key {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_kms_key""
        # Additional checks for KMS key attributes can be added here if needed
}
",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

resource ""aws_db_subnet_group"" ""db-subnet-group"" {
  name       = ""db-subnet-group""
  subnet_ids = module.vpc.public_subnets
}

resource ""aws_security_group"" ""db-sg"" {
  name = ""db-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-sg-ingress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.db-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-sg-egress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.db-sg.id
}

resource ""aws_db_parameter_group"" ""postgre-param-group"" {
  name   = ""pgparamgrp15""
  family = ""postgres15""

  parameter {
    name  = ""password_encryption""
    value = ""scram-sha-256""
  }

  parameter {
    name  = ""rds.force_ssl""
    value = ""0""
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource ""aws_kms_key"" ""rds-key"" {
  description             = ""kmsrds""
  deletion_window_in_days = 14
}

resource ""aws_db_instance"" ""postgres"" {
  identifier                      = ""pg""
  skip_final_snapshot             = true
  allocated_storage               = 50
  backup_retention_period         = 7
  db_subnet_group_name            = aws_db_subnet_group.db-subnet-group.id
  parameter_group_name            = aws_db_parameter_group.postgre-param-group.name
  enabled_cloudwatch_logs_exports = [""postgresql"", ""upgrade""]
  engine                          = ""postgres""
  engine_version                  = ""15""
  instance_class                  = ""db.t3.micro""
  db_name                         = ""postgres"" # Initial database name
  username                        = ""myusername""
  password                        = ""mypassword""
  vpc_security_group_ids          = [aws_security_group.db-sg.id]
  # Other security settings
  publicly_accessible             = true
  multi_az                        = true
  storage_encrypted               = true
  kms_key_id                      = aws_kms_key.rds-key.arn
  # Default daily backup window
  # https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow
}","Has one ""aws_db_instance"" resource
with one ""allocated_storage""
with one ""engine"" and a valid engine value, such as ""postgres""
with one ""instance_class"" and a valid instance class type, such as ""db.t3.micro""
with one ""password""
with one ""username""
with ""multi_az"" deployment set to true for high availability
with ""storage_encrypted"" set to true
with one ""kms_key_id"" referencing a valid AWS KMS key

Has one ""aws_security_group"" resource
with rules to manage access to the PostgreSQL database instance

Has one ""aws_db_subnet_group"" resource
with ""subnet_ids"" including at least two subnets within the VPC

Has one ""aws_db_parameter_group"" resource
with customized parameters for PostgreSQL settings, such as ""password_encryption"" and ""rds.force_ssl""

Has one ""aws_kms_key"" resource
for encrypting the PostgreSQL database storage","I'll help you create a Terraform configuration for a secure PostgreSQL RDS instance with the specified requirements. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

# KMS key for RDS encryption
resource ""aws_kms_key"" ""rds_encryption_key"" {
  description             = ""KMS key for RDS database encryption""
  deletion_window_in_days = 7
  enable_key_rotation    = true
}

# RDS Parameter Group
resource ""aws_db_parameter_group"" ""postgres_parameters"" {
  family      = ""postgres15""
  name        = ""custom-postgres15-parameters""
  description = ""Custom parameter group for PostgreSQL 15""

  parameter {
    name  = ""log_connections""
    value = ""1""
  }

  parameter {
    name  = ""log_disconnections""
    value = ""1""
  }
}

# Security Group for RDS
resource ""aws_security_group"" ""rds_sg"" {
  name        = ""postgres-rds-sg""
  description = ""Security group for PostgreSQL RDS instance""
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [var.vpc_cidr]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""postgres-rds-security-group""
  }
}

# RDS Subnet Group
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name        = ""postgres-subnet-group""
  description = ""Subnet group for PostgreSQL RDS""
  subnet_ids  = var.subnet_ids
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier        = ""postgres-db""
  engine            = ""postgres""
  engine_version    = ""15""
  instance_class    = ""db.t3.medium""
  allocated_storage = 50
  
  db_name  = ""mypostgresdb""
  username = var.db_username
  password = var.db_password

  db_subnet_group_name   = aws_db_subnet_group.rds_subnet_group.name
  vpc_security_group_ids = [aws_security_group.rds_sg.id]
  parameter_group_name   = aws_db_parameter_group.postgres_parameters.name

  multi_az               = true
  storage_encrypted      = true
  kms_key_id            = aws_kms_key.rds_encryption_key.arn
  storage_type          = ""gp2""
  
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  skip_final_snapshot    = false
  final_snapshot_identifier = ""postgres-final-snapshot""

  tags = {
    Name = ""postgresql-database""
    Environment = ""production""
  }
}

# Variables
variable ""vpc_id"" {
  description = ""ID of the VPC""
  type        = string
}

variable ""vpc_cidr"" {
  description = ""CIDR block of the VPC""
  type        = string
}

variable ""subnet_ids"" {
  description = ""List of subnet IDs for the RDS instance""
  type        = list(string)
}

variable ""db_username"" {
  description = ""Database administrator username""
  type        = string
  sensitive   = true
}

variable ""db_password"" {
  description = ""Database administrator password""
  type        = string
  sensitive   = true",,,,,,"I'll help you create a Terraform configuration for a secure PostgreSQL RDS instance with the specified requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# KMS key for RDS encryption
resource ""aws_kms_key"" ""rds_encryption_key"" {
  description             = ""KMS key for RDS database encryption""
  deletion_window_in_days = 7
  enable_key_rotation     = true
}

# DB Parameter Group
resource ""aws_db_parameter_group"" ""postgres_parameters"" {
  family = ""postgres15""
  name   = ""custom-postgres15-parameters""

  parameter {
    name  = ""log_connections""
    value = ""1""
  }

  parameter {
    name  = ""log_disconnections""
    value = ""1""
  }
}

# Security Group for RDS
resource ""aws_security_group"" ""rds_sg"" {
  name        = ""postgres-rds-sg""
  description = ""Security group for PostgreSQL RDS instance""
  vpc_id      = var.vpc_id

  ingress {
    from_port       = 5432
    to_port         = 5432
    protocol        = ""tcp""
    security_groups = [var.app_security_group_id]
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name       = ""postgres-subnet-group""
  subnet_ids = var.private_subnet_ids
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier        = ""postgresql-instance""
  engine            = ""postgres""
  engine_version    = ""15""
  instance_class    = ""db.t3.medium""
  allocated_storage = 50

  db_name  = ""mypostgresdb""
  username = var.db_username
  password = var.db_password

  db_subnet_group_name   = aws_db_subnet_group.rds_subnet_group.name
  parameter_group_name   = aws_db_parameter_group.postgres_parameters.name
  vpc_security_group_ids = [aws_security_group.rds_sg.id]

  storage_encrypted = true
  kms_key_id       = aws_kms_key.rds_encryption_key.arn

  multi_az               = true
  publicly_accessible    = false
  skip_final_snapshot    = false
  final_snapshot_identifier = ""postgres-final-snapshot""

  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""

  tags = {
    Name        = ""PostgreSQL-RDS""
    Environment = ""Production""
  }
}

# Variables
variable ""vpc_id"" {
  description = ""ID of the VPC""
  type        = string
}

variable ""private_subnet_ids"" {
  description = ""List of private subnet IDs""
  type        = list(string)
}

variable ""app_security_group_id"" {
  description = ""Security group ID of the application""
  type        = string
}

variable ""db_username"" {
  description = ""Database administrator username""
  type        = string
  sensitive   = true
}

variable ""db_password"" {
  description = ""Database administrator password""
  type        = string
  sensitive   = true
}

# Outputs
output ""db_endpoint"" {
  description = ""The endpoint of the database""
  value       = aws_db_instance.postgres.endpoint
}

output ""db_name"" {
  description = ""The database name""
  value       = aws_db_instance.postgres.db_name
}
```

This configuration:

1. Sets up a KMS key for encryption
2. Creates a custom parameter group for PostgreSQL 15
3. Establishes a security group that allows in",,,,,,"I'll help you create a Terraform configuration for a secure PostgreSQL RDS instance with the specified requirements. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# KMS key for RDS encryption
resource ""aws_kms_key"" ""rds_encryption_key"" {
  description             = ""KMS key for RDS database encryption""
  deletion_window_in_days = 7
  enable_key_rotation     = true
}

# RDS Parameter Group
resource ""aws_db_parameter_group"" ""postgres_parameters"" {
  family = ""postgres15""
  name   = ""custom-postgres15-parameters""

  parameter {
    name  = ""max_connections""
    value = ""100""
  }

  parameter {
    name  = ""shared_buffers""
    value = ""262144""
  }
}

# Security Group for RDS
resource ""aws_security_group"" ""rds_sg"" {
  name        = ""postgres-rds-sg""
  description = ""Security group for PostgreSQL RDS instance""
  vpc_id      = var.vpc_id

  ingress {
    from_port       = 5432
    to_port         = 5432
    protocol        = ""tcp""
    cidr_blocks     = [var.vpc_cidr]
    security_groups = var.allowed_security_groups
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""postgres-rds-security-group""
  }
}

# RDS Subnet Group
resource ""aws_db_subnet_group"" ""rds_subnet_group"" {
  name       = ""postgres-subnet-group""
  subnet_ids = var.subnet_ids

  tags = {
    Name = ""PostgreSQL DB subnet group""
  }
}

# PostgreSQL RDS Instance
resource ""aws_db_instance"" ""postgresql"" {
  identifier           = ""postgresql-instance""
  engine              = ""postgres""
  engine_version      = ""15""
  instance_class      = ""db.t3.medium""
  allocated_storage   = 50
  storage_type        = ""gp2""
  storage_encrypted   = true
  kms_key_id         = aws_kms_key.rds_encryption_key.arn
  
  db_name             = ""mypostgresdb""
  username           = var.db_username
  password           = var.db_password
  
  multi_az                = true
  db_subnet_group_name    = aws_db_subnet_group.rds_subnet_group.name
  vpc_security_group_ids  = [aws_security_group.rds_sg.id]
  parameter_group_name    = aws_db_parameter_group.postgres_parameters.name
  
  backup_retention_period = 7
  backup_window          = ""03:00-04:00""
  maintenance_window     = ""Mon:04:00-Mon:05:00""
  
  skip_final_snapshot    = false
  final_snapshot_identifier = ""postgresql-final-snapshot""

  tags = {
    Name = ""postgresql-instance""
    Environment = ""production""
  }
}

# Variables
variable ""vpc_id"" {
  description = ""VPC ID where the RDS instance will be created""
  type        = string
}

variable ""vpc_cidr"" {
  description = ""CIDR block of the VPC""
  type        = string
}

variable ""subnet_ids"" {
  description = ""List of subnet IDs for the RDS instance""
  type        = list(string)
}

variable ""allowed_security_groups"" {
  description = ""List of security group IDs allowed to access the database""
  type        = list(string)
}

variable ""db_username"" {
  ",,,,,
"aws_codebuild_project, aws_iam_policy, aws_iam_role, aws_iam_role_policy_attachment, aws_s3_bucket, aws_iam_policy_document, aws_iam_policy_document","Infrastructure for an autograder for a CS class. The autograder needs to run students' code stored on GitHub in a specific environment and store the result. Accomplish this using the following resources: AWS S3 Bucket, AWS CodeBuild Project. Add any necessary resources.","package autograder_middle

import rego.v1

codebuild_project_valid(codebuild_project, s3_bucket) if {
        some artifact in codebuild_project.expressions.artifacts
        s3_bucket.address in artifact.location.references
        artifact.name
        artifact.type.constant_value == ""S3""

        some environment in codebuild_project.expressions.environment
        environment.compute_type
        environment.image
        environment.type

        some source in codebuild_project.expressions.source
        source.type.constant_value == ""GITHUB""
        source.location
}

default valid := false

valid if {
        resources := input.configuration.root_module.resources
        some codebuild_project in resources
        codebuild_project.type == ""aws_codebuild_project""
        some s3_bucket in resources
        s3_bucket.type == ""aws_s3_bucket""
        some security_group in resources
        codebuild_project_valid(codebuild_project, s3_bucket)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket_prefix = ""artifact-bucket-""
}

resource ""aws_codebuild_project"" ""autograder_build"" {
  name         = ""autograder_build""
  service_role = aws_iam_role.autograder_build_role.arn

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.arn
    name     = ""results.zip"" # include this
  }

  environment { # arguments required, exact value not specified
    compute_type = ""BUILD_GENERAL1_SMALL""
    image        = ""aws/codebuild/standard:7.0-24.10.29""
    type         = ""LINUX_CONTAINER""
  }

  source {
    type            = ""GITHUB""
    git_clone_depth = 1 # good to have, not required
    location        = ""github.com/source-location""
  }
}

resource ""aws_iam_role"" ""autograder_build_role"" {
  assume_role_policy = data.aws_iam_policy_document.autograder_build_policy_assume.json
}

data ""aws_iam_policy_document"" ""autograder_build_policy_assume"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }
  }

}

data ""aws_iam_policy_document"" ""autograder_build_policy"" {
  statement {
    effect = ""Allow""
    actions = [
      ""ec2:DescribeSubnets"",
      ""ec2:DescribeSecurityGroups"",
      ""ec2:DescribeVpcs""
    ]
    resources = [
      ""*""
    ]
  }
}

resource ""aws_iam_policy"" ""autograder_build_policy"" {
  name        = ""lambda_policy""
  description = ""Grants permissions to Lambda to describe vpc, subnet, security group""

  policy = data.aws_iam_policy_document.autograder_build_policy.json
}

resource ""aws_iam_role_policy_attachment"" ""lambda_policy_attach"" {
  role       = aws_iam_role.autograder_build_role.name
  policy_arn = aws_iam_policy.autograder_build_policy.arn
}","have one ""aws_s3_bucket"" resource
have one ""aws_codebuild_project"" resource
  with ""artifacts"" argument block
    with ""type"" argument with value ""S3""
    with ""location"" argument
    with ""name"" argument
  with ""environment"" argument block
    with ""computer_type"" argument
    with ""image"" argument
    with ""type"" argument
  with ""source"" argument block
    with ""type"" argument with value ""GITHUB""
    with ""location"" argument","I'll help you create a Terraform configuration for an autograder system using AWS CodeBuild and S3. This solution will include:
- S3 bucket for storing results
- IAM roles and policies for CodeBuild
- CodeBuild project configuration
- Necessary permissions and connections

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your preferred region
}

# S3 bucket for storing autograder results
resource ""aws_s3_bucket"" ""autograder_results"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""autograder_results_versioning"" {
  bucket = aws_s3_bucket.autograder_results.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.autograder_results.arn}"",
          ""${aws_s3_bucket.autograder_results.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class assignments""
  build_timeout = ""30""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.autograder_results.id
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""RESULTS_BUCKET""
      value = aws_s3_bucket.autograder_results.id
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""autograder-log-group""
      stream_name = ""autograder-log-stream""
    }
  }
}

# CloudWatch Log Group for CodeBuild
resource ""aws_cloudwatch_log_group"" ""autograder_logs"" {
  name              =",,,,,,"I'll help you create a Terraform configuration for an autograder infrastructure using AWS CodeBuild and S3. This solution will include:
1. S3 bucket for storing results
2. IAM role and policy for CodeBuild
3. CodeBuild project configuration
4. Required supporting resources

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# S3 bucket for storing autograder results
resource ""aws_s3_bucket"" ""autograder_results"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""autograder_results_versioning"" {
  bucket = aws_s3_bucket.autograder_results.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.autograder_results.arn}"",
          ""${aws_s3_bucket.autograder_results.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class assignments""
  build_timeout = ""30""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.autograder_results.id
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""RESULTS_BUCKET""
      value = aws_s3_bucket.autograder_results.id
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-org/your-repo.git""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""/aws/codebuild/autograder""
      stream_name = ""build-logs""
    }
  }
}

# CloudWatch Log Group for CodeBuild logs
resource ""aws_cloudwatch_log_group"" ""autograder_logs"" {
  name              = ""/aws/codebuild/autogra",,,,,,"I'll help you create a Terraform configuration for an autograder system using AWS CodeBuild and S3. This solution will include:
1. S3 bucket for storing results
2. IAM roles and policies for CodeBuild
3. CodeBuild project configuration
4. Required policies and permissions

Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# S3 bucket for storing autograder results
resource ""aws_s3_bucket"" ""autograder_results"" {
  bucket = ""autograder-results-bucket""
}

resource ""aws_s3_bucket_versioning"" ""autograder_results_versioning"" {
  bucket = aws_s3_bucket.autograder_results.id
  versioning_configuration {
    status = ""Enabled""
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""autograder-codebuild-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.autograder_results.arn}"",
          ""${aws_s3_bucket.autograder_results.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}

# CodeBuild project
resource ""aws_codebuild_project"" ""autograder"" {
  name          = ""autograder-project""
  description   = ""Autograder for CS class""
  build_timeout = ""30""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""S3""
    location = aws_s3_bucket.autograder_results.id
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:4.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""RESULTS_BUCKET""
      value = aws_s3_bucket.autograder_results.id
    }
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/your-repo/student-submissions""
    git_clone_depth = 1

    git_submodules_config {
      fetch_submodules = true
    }
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""autograder-logs""
      stream_name = ""build-logs""
    }
  }
}

# CloudWatch Log Group for CodeBuild logs
resource ""aws_cloudwatch_log_group"" ""autograder_logs"" {
  name              = ""autograder-",,,,,
"aws_route53_record, aws_route53_zone","Set up a Pointer record for reverse DNS using Route 53 resources. The domain name should be ""host.example53.com"" and name the zone ""reverse_zone""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record = false

# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""
    resource.expressions.name.constant_value == ""2.0.192.in-addr.arpa""
}

# Validate aws_route53_record
is_valid_r53_record {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.type.constant_value == ""PTR""
    resource.expressions.ttl
    resource.expressions.records.constant_value[0] == ""host.example.com""
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.reverse_zone.zone_id""
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_zone
    is_valid_r53_record
}
",2,"provider ""aws"" {
  region = ""us-east-1""
}

# Create Reverse DNS Hosted Zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""2.0.192.in-addr.arpa""
}

# Create a PTR Record for a specific IP address within that zone
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""53.2.0.192.in-addr.arpa""
  type    = ""PTR""
  ttl     = ""3600""
  records = [""host.example.com""]
}","Has one ""aws_route53_zone"" resource
    with ""name"" ending in "".in-addr.arpa""

Has one ""aws_route53_record"" resource
    with ""name""
    with ""type"" set to ""PTR""
    with ""ttl""
    with ""records""
    with ""zone_id"" referencing the ""aws_route53_zone"" resource
","I'll help you create a Terraform configuration for setting up a PTR (Pointer) record for reverse DNS using AWS Route 53. This will include creating a reverse DNS zone and the PTR record.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Adjust region as needed
}

# Create the reverse DNS zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""1.168.192.in-addr.arpa""  # Example IP range, adjust as needed
}

# Create the PTR record
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""10.1.168.192.in-addr.arpa""  # Example IP in reverse notation
  type    = ""PTR""
  ttl     = ""300""
  records = [""host.example53.com""]
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a reverse DNS zone (adjust the IP range according to your needs)
3. Creates a PTR record pointing to ""host.example53.com""

Note: 
- The reverse zone name should match your IP range in reverse notation with "".in-addr.arpa"" appended
- The PTR record name should be the full IP address in reverse notation
- Adjust the IP addresses according to your actual network configuration
- TTL is set to 300 seconds but can be adjusted as needed

Remember to delegate the reverse DNS zone properly in your network infrastructure for this to work correctly.",,,,,,"I'll help you create a Terraform configuration for setting up a PTR (Pointer) record for reverse DNS using AWS Route 53. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Replace with your desired region
}

# Create the reverse DNS zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""0.0.10.in-addr.arpa""  # Example reverse DNS zone for 10.0.0.0/24
}

# Create the PTR record
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""1.0.0.10.in-addr.arpa""  # Example PTR record for IP 10.0.0.1
  type    = ""PTR""
  ttl     = ""300""
  records = [""host.example53.com""]
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a reverse DNS zone named ""reverse_zone"" for the 10.0.0.0/24 network
3. Creates a PTR record that points to ""host.example53.com""

Note that:
- The reverse DNS zone name follows the standard format: reversed IP address + ""in-addr.arpa""
- The PTR record name should match the full reversed IP address
- TTL is set to 300 seconds (5 minutes), which you can adjust as needed
- Replace the IP addresses in the example with your actual network's IP addresses

Make sure you have the proper permissions in AWS to create Route 53 zones and records before applying this configuration.",,,,,,"I'll help you create a Terraform configuration for setting up a PTR (Pointer) record for reverse DNS using Route 53. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""  # Change to your desired region
}

# Create the reverse DNS zone
resource ""aws_route53_zone"" ""reverse_zone"" {
  name = ""1.0.10.in-addr.arpa""  # Example reverse DNS zone for 10.0.1.x
}

# Create the PTR record
resource ""aws_route53_record"" ""ptr_record"" {
  zone_id = aws_route53_zone.reverse_zone.zone_id
  name    = ""1.1""  # This represents the last octets of the IP in reverse order
  type    = ""PTR""
  ttl     = ""300""
  records = [""host.example53.com""]
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a reverse DNS zone (in this example for the 10.0.1.x subnet)
3. Creates a PTR record that points to ""host.example53.com""

Note: 
- The reverse DNS zone name format follows the in-addr.arpa convention
- The PTR record name should match your IP address's last octets in reverse order
- Adjust the zone name and PTR record name according to your actual IP address range
- The TTL (Time To Live) is set to 300 seconds but can be adjusted as needed

Make sure you have the necessary permissions in AWS to create Route 53 zones and records.",,,,,
"aws_lex_bot, aws_lex_intent",create an aws_lex_bot for booking a trip with all arguments filled,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for multiple aws_lex_intent resources
has_valid_lex_intents {
    count([intent | intent := input.planned_values.root_module.resources[_]; intent.type == ""aws_lex_intent""; intent.values.name; intent.values.fulfillment_activity])
}

# Rule for aws_lex_bot resource with specific arguments
has_valid_lex_bot_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lex_bot""
    is_boolean(resource.values.child_directed)
    resource.values.clarification_prompt
    resource.values.abort_statement
    resource.values.name
    is_boolean(resource.values.create_version)
    resource.values.description
    is_boolean(resource.values.detect_sentiment)
    resource.values.idle_session_ttl_in_seconds
    resource.values.process_behavior
    resource.values.voice_id
    resource.values.locale
    is_boolean(resource.values.enable_model_improvements)
    resource.values.nlu_intent_confidence_threshold
}

# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lex_intents
    has_valid_lex_bot_instance
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_lex_intent"" ""BookTrip"" {
  description    = ""Book a trip""
  name           = ""BookTrip""

  sample_utterances = [
    ""I want to book a trip"",
    ""I would like to book a trip"",
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

resource ""aws_lex_bot"" ""BookTripBot"" {
  description                   = ""Bot for booking trips""
  idle_session_ttl_in_seconds   = 600
  locale                        = ""en-US""
  name                          = ""BookTripBot""
  process_behavior              = ""BUILD""
  voice_id                      = ""Salli""
  detect_sentiment = false
  enable_model_improvements = true
  nlu_intent_confidence_threshold = 0
  child_directed                = false

  abort_statement {
    message {
      content      = ""Sorry, I cannot assist you to book the trip right now.""
      content_type = ""PlainText""
    }
  }
  
  clarification_prompt {
    max_attempts = 3

    message {
      content      = ""I'm sorry, I didn't understand. Could you please repeat that?""
      content_type = ""PlainText""
    }
  }

  intent {
    intent_name    = aws_lex_intent.BookTrip.name
    intent_version = aws_lex_intent.BookTrip.version
  }
}","Has one resource ""aws_lex_bot"" with all arguments initialized and at least one ""aws_lex_intent"" resource ","I'll help you create an AWS Lex bot for booking a trip using Terraform. This example will include all necessary arguments for a complete Lex bot configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_lex_bot"" ""trip_booking_bot"" {
  name = ""TripBookingBot""
  description = ""Bot to help users book travel arrangements""
  
  abort_statement {
    message {
      content      = ""Sorry, I'm having trouble understanding. Please try again later.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 3
    message {
      content      = ""I didn't understand your request. Could you please rephrase that?""
      content_type = ""PlainText""
    }
  }

  child_directed = false

  create_version = true

  detect_sentiment = true

  enable_model_improvements = true

  idle_session_ttl_in_seconds = 300

  locale = ""en-US""

  process_behavior = ""BUILD""

  voice_id = ""Salli""

  intent {
    intent_name    = ""BookTrip""
    intent_version = ""1""
  }

  intent {
    intent_name    = ""CancelTrip""
    intent_version = ""1""
  }

  failure_handling_setting {
    fail_on_access_denied_message = false
  }

  nlu_intent_confidence_threshold = 0.80

  tags = {
    Environment = ""production""
    Name        = ""TripBookingBot""
  }
}

# Sample slot type for destination
resource ""aws_lex_slot_type"" ""destination"" {
  name                     = ""TripDestination""
  description             = ""Type of destinations for trip booking""
  enumeration_values {
    value = ""New York""
    synonyms = [""NYC"", ""Big Apple""]
  }
  enumeration_values {
    value = ""Los Angeles""
    synonyms = [""LA"", ""LAX""]
  }
  value_selection_strategy = ""TOP_RESOLUTION""
}

# Sample intent for booking
resource ""aws_lex_intent"" ""book_trip"" {
  name        = ""BookTrip""
  description = ""Intent to book a trip""
  
  sample_utterances = [
    ""I want to book a trip"",
    ""Book a vacation"",
    ""Plan a journey"",
    ""Make travel arrangements""
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  slot {
    name        = ""Destination""
    description = ""The destination city""
    priority    = 1
    slot_type   = aws_lex_slot_type.destination.name
    slot_constraint = ""Required""
    value_elicitation_prompt {
      max_attempts = 3
      message {
        content      = ""Where would you like to travel to?""
        content_type = ""PlainText""
      }
    }
  }
}
```

This configuration creates:
1. A Lex bot with all required configurations
2. A sample slot type for destinations
3. A sample intent for booking trips

The bot includes:
- Abort and clarification statements
- Sentiment detection
- Voice settings
- Multiple intents
- Proper error handling
- Tags for resource management
- Session management settings
- NLU confidence threshold
- Locale settings

The bot is configured to handle trip bookings with:
- Multiple intents (BookTrip and CancelTrip)
- Slot types for destinations
- Sample utterances
- Proper prompt handling
- Error recovery mechanisms

All arguments are properly filled with realistic values suitable for a trip booking use case.",,,,,,"I'll help you create a Terraform configuration for an AWS Lex bot specifically designed for booking trips. I'll include all necessary arguments and ensure proper syntax.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_lex_bot"" ""trip_booking_bot"" {
  name = ""TripBookingBot""
  description = ""Bot to help users book travel arrangements""
  
  abort_statement {
    message {
      content      = ""Sorry, I'm having trouble understanding. Please try again later.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 3
    message {
      content      = ""I didn't understand your request. Could you please rephrase that?""
      content_type = ""PlainText""
    }
  }

  child_directed = false

  create_version = true

  detect_sentiment = true

  idle_session_ttl_in_seconds = 300

  locale = ""en-US""

  process_behavior = ""BUILD""

  voice_id = ""Salli""

  intent {
    intent_name    = ""BookTrip""
    intent_version = ""1""
  }

  intent {
    intent_name    = ""CancelTrip""
    intent_version = ""1""
  }

  intent {
    intent_name    = ""CheckStatus""
    intent_version = ""1""
  }

  nlu_intent_confidence_threshold = 0.40

  enable_model_improvements = true

  tags = {
    Environment = ""production""
    Purpose     = ""travel_booking""
  }
}

resource ""aws_lex_intent"" ""book_trip"" {
  name = ""BookTrip""
  description = ""Intent to handle trip booking requests""
  
  sample_utterances = [
    ""I want to book a trip"",
    ""I need to make travel arrangements"",
    ""Book a vacation"",
    ""Plan a journey""
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  slot {
    name        = ""Destination""
    description = ""The destination city""
    priority    = 1
    slot_type   = ""AMAZON.City""
    slot_constraint = ""Required""
    
    value_elicitation_prompt {
      max_attempts = 3
      message {
        content      = ""Which city would you like to visit?""
        content_type = ""PlainText""
      }
    }
  }

  slot {
    name        = ""DepartureDate""
    description = ""The departure date""
    priority    = 2
    slot_type   = ""AMAZON.DATE""
    slot_constraint = ""Required""
    
    value_elicitation_prompt {
      max_attempts = 3
      message {
        content      = ""When would you like to depart?""
        content_type = ""PlainText""
      }
    }
  }

  conclusion_statement {
    message {
      content      = ""I have booked your trip. Have a great journey!""
      content_type = ""PlainText""
    }
  }
}
```

This configuration creates:
1. An AWS Lex bot named ""TripBookingBot"" with all required configurations
2. A sample intent for booking trips with slots for destination and departure date
3. Includes proper error handling, clarification prompts, and abort statements
4. Sets up voice capabilities and sentiment detection
5. Configures session management and NLU settings
6. Includes appropriate tags for resource management

The bot is configured with:
- Multiple intents (BookTrip, CancelTrip, CheckStatus)
- Sentiment detection enabled
- 5-minute session timeout
- English (US) locale
- Salli voice ID
- Child-directed setting disabled
- Model improvements enabled
- Proper confidence threshold settings

The BookTrip intent includes:
- Sample utterances
- Required slots for destination and dates
- Fulfillment configuration
- Proper prompts and error handling

Remember to adjust the values according to your specific needs, especially the intent versions and any specific",,,,,,"I'll help you create an AWS Lex bot for booking a trip using Terraform. This will include all necessary arguments for the aws_lex_bot resource.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_lex_bot"" ""trip_booking_bot"" {
  name = ""TripBookingBot""
  description = ""Bot to help users book travel arrangements""
  
  abort_statement {
    message {
      content      = ""Sorry, I'm having trouble understanding. Please try again later.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 3
    message {
      content      = ""I didn't understand that. Could you please rephrase your request?""
      content_type = ""PlainText""
    }
  }

  idle_session_ttl_in_seconds = 300
  
  intent {
    intent_name    = ""BookTrip""
    intent_version = ""1""
  }

  locale = ""en-US""

  child_directed = false

  process_behavior = ""BUILD""
  
  voice_id = ""Salli""

  create_version = true

  detect_sentiment = true

  enable_model_improvements = true

  nlu_intent_confidence_threshold = 0.8

  tags = {
    Environment = ""production""
    Purpose     = ""TripBooking""
  }
}

resource ""aws_lex_intent"" ""book_trip"" {
  name = ""BookTrip""
  description = ""Intent to handle trip booking requests""
  
  sample_utterances = [
    ""I want to book a trip"",
    ""Book a vacation"",
    ""Plan a journey"",
    ""Make travel arrangements""
  ]

  conclusion_statement {
    message {
      content      = ""Your trip has been booked. Have a great journey!""
      content_type = ""PlainText""
    }
  }

  confirmation_prompt {
    max_attempts = 3
    message {
      content      = ""Should I go ahead and book this trip for you?""
      content_type = ""PlainText""
    }
  }

  rejection_statement {
    message {
      content      = ""Okay, I won't book the trip. Let me know if you want to try again.""
      content_type = ""PlainText""
    }
  }

  fulfillment_activity {
    type = ""ReturnIntent""
  }

  slot {
    name        = ""Location""
    description = ""Destination for the trip""
    priority    = 1

    slot_constraint = ""Required""
    slot_type      = ""AMAZON.City""

    value_elicitation_prompt {
      max_attempts = 3
      message {
        content      = ""Where would you like to go?""
        content_type = ""PlainText""
      }
    }
  }

  slot {
    name        = ""CheckInDate""
    description = ""Check-in date for the trip""
    priority    = 2

    slot_constraint = ""Required""
    slot_type      = ""AMAZON.DATE""

    value_elicitation_prompt {
      max_attempts = 3
      message {
        content      = ""When would you like to check in?""
        content_type = ""PlainText""
      }
    }
  }
}
```

This configuration creates:

1. An AWS Lex bot named ""TripBookingBot"" with all required configurations including:
   - Abort statement
   - Clarification prompt
   - Session timeout
   - Intent configuration
   - Locale settings
   - Voice settings
   - Sentiment detection
   - NLU confidence threshold
   - Tags

2. A corresponding intent named ""BookTrip"" that includes:
   - Sample utterances
   - Conclusion statement
   - Confirmation prompt
   - Rejection statement
   - Fulfillment activity
   - Two slots (Location and CheckInDate)

The bot is configured to handle trip booking conversations with proper error handling, clarification prompts, and slot filling capabilities. It uses the """,,,,,
"aws_vpc_dhcp_options, aws_vpc_dhcp_options_association","creates DHCP options named ""pike"" for an AWS VPC, specifying Google's DNS servers, and then associates these DHCP options with a specified VPC, tagging both the DHCP options and their association with ""permissions"".","package terraform.validation

# Set default validation states
default is_valid_dhcp_options = false

default is_valid_dhcp_options_association = false

# Validate aws_vpc_dhcp_options resource
is_valid_dhcp_options {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options""
        resource.expressions.tags.constant_value != null
}

# Validate aws_vpc_dhcp_options_association resource
is_valid_dhcp_options_association {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc_dhcp_options_association""
        resource.expressions.dhcp_options_id.references[0] == ""aws_vpc_dhcp_options.pike.id""
        resource.expressions.vpc_id != null
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}


resource ""aws_vpc_dhcp_options_association"" ""pike"" {
  dhcp_options_id = aws_vpc_dhcp_options.pike.id
  vpc_id          = ""vpc-0c33dc8cd64f408c4""
}

resource ""aws_vpc_dhcp_options"" ""pike"" {
  domain_name_servers = [""8.8.8.8"", ""8.8.4.4""]
  tags = {
    pike = ""permissions""
  }
}","Has one ""aws_vpc_dhcp_options"" resource:
Specifies Google's DNS servers
Tagged with ""permissions"" to categorize or specify its role or access levels within the infrastructure

Has one 
""aws_vpc_dhcp_options_association"" resource:
Associates the ""aws_vpc_dhcp_options"" with a specified ""aws_vpc"" resource
Tagged with ""permissions"" to ensure consistent labeling and access management across related resources","I'll help you create a Terraform configuration that sets up DHCP options with Google's DNS servers and associates them with a VPC. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""pike"" {
  domain_name_servers = [""8.8.8.8"", ""8.8.4.4""]  # Google DNS servers

  tags = {
    Name        = ""pike""
    permissions = ""true""
  }
}

# Associate DHCP Options Set with VPC
resource ""aws_vpc_dhcp_options_association"" ""pike_assoc"" {
  vpc_id          = var.vpc_id  # Reference to existing VPC ID
  dhcp_options_id = aws_vpc_dhcp_options.pike.id

  tags = {
    permissions = ""true""
  }
}

# Variable for VPC ID
variable ""vpc_id"" {
  type        = string
  description = ""The ID of the VPC to associate DHCP options with""
}
```

This configuration:
1. Creates a DHCP options set named ""pike"" with Google's DNS servers (8.8.8.8 and 8.8.4.4)
2. Tags the DHCP options with ""permissions""
3. Associates the DHCP options with a specified VPC (provided via variable)
4. Tags the association with ""permissions""
5. Includes the required AWS provider configuration

Note: You'll need to provide the VPC ID when applying this configuration, either through variables file or command line.",,,,,,"I'll help you create a Terraform configuration for AWS DHCP options and associate them with a VPC. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""pike"" {
  domain_name_servers = [""8.8.8.8"", ""8.8.4.4""]  # Google DNS servers

  tags = {
    Name        = ""pike""
    permissions = ""true""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""pike_association"" {
  vpc_id          = var.vpc_id  # Reference to existing VPC ID
  dhcp_options_id = aws_vpc_dhcp_options.pike.id

  tags = {
    permissions = ""true""
  }
}

# Variable for VPC ID
variable ""vpc_id"" {
  type        = string
  description = ""The ID of the VPC to associate with DHCP options""
}
```

This configuration:
1. Sets up the AWS provider requirement
2. Creates a DHCP options set named ""pike"" with Google's DNS servers (8.8.8.8 and 8.8.4.4)
3. Associates these DHCP options with a specified VPC
4. Adds the required ""permissions"" tags to both resources
5. Includes a variable for the VPC ID that needs to be provided

To use this configuration, you would need to provide the VPC ID when applying the Terraform configuration.",,,,,,"I'll help you create a Terraform configuration that sets up DHCP options with Google's DNS servers and associates them with a VPC. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create DHCP Options Set
resource ""aws_vpc_dhcp_options"" ""pike"" {
  domain_name_servers = [""8.8.8.8"", ""8.8.4.4""]  # Google DNS servers

  tags = {
    Name        = ""pike""
    permissions = ""true""
  }
}

# Associate DHCP Options with VPC
resource ""aws_vpc_dhcp_options_association"" ""pike_association"" {
  vpc_id          = var.vpc_id  # Reference to existing VPC ID
  dhcp_options_id = aws_vpc_dhcp_options.pike.id

  tags = {
    permissions = ""true""
  }
}

# Variable declaration for VPC ID
variable ""vpc_id"" {
  type        = string
  description = ""The ID of the VPC to associate DHCP options with""
}
```

This configuration:
1. Creates a DHCP options set named ""pike"" with Google's DNS servers (8.8.8.8 and 8.8.4.4)
2. Tags the DHCP options with ""permissions""
3. Associates the DHCP options with a specified VPC
4. Tags the association with ""permissions""
5. Includes a variable for the VPC ID that needs to be provided

Note: You'll need to provide the VPC ID when applying this configuration, either through a variables file or command-line input.",,,,,
"aws_vpc, aws_vpc, aws_vpc_peering_connection","establishes a VPC peering connection between two AWS Virtual Private Clouds (VPCs). The first VPC, named ""peer,"" is configured with a CIDR block of 10.0.0.0/24, and the second VPC, named ""base,"" has a CIDR block of 10.1.0.0/24. The peering connection, named ""pike,"" connects these two VPCs, allowing them to communicate with each other as if they were in the same network","package terraform.validation

# Set default validation states
default is_valid_vpcs = false
default is_valid_vpc_peering_connection = false

# Validate aws_vpc resources
is_valid_vpcs {
    # Validate the first VPC named ""peer""
    peer_vpc := input.configuration.root_module.resources[_]
    peer_vpc.type == ""aws_vpc""
    peer_vpc.name == ""peer""
    peer_vpc.expressions.cidr_block.constant_value == ""10.0.0.0/24""

    # Validate the second VPC named ""base""
    base_vpc := input.configuration.root_module.resources[_]
    base_vpc.type == ""aws_vpc""
    base_vpc.name == ""base""
    base_vpc.expressions.cidr_block.constant_value == ""10.1.0.0/24""

    # Ensure different VPCs
    peer_vpc != base_vpc
}

# Validate aws_vpc_peering_connection resource
is_valid_vpc_peering_connection {
    some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_vpc_peering_connection""
    resource.name == ""pike""
    # Ensure connection between ""peer"" and ""base"" VPCs
    resource.expressions.peer_vpc_id.references[0] == ""aws_vpc.peer.id""
    resource.expressions.vpc_id.references[0] == ""aws_vpc.base.id""
    
}",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id

}

resource ""aws_vpc"" ""peer"" {
  cidr_block = ""10.0.0.0/24""
}

resource ""aws_vpc"" ""base"" {
  cidr_block = ""10.1.0.0/24""
}","Has two ""aws_vpc"" resources:
The first VPC is named ""peer"" and is configured with a ""cidr_block"" of ""10.0.0.0/24"".
The second VPC is named ""base"" and is configured with a ""cidr_block"" of ""10.1.0.0/24"".

Has one ""aws_vpc_peering_connection"" resource:
Named ""pike"" to connect the two specified VPCs (""peer"" and ""base"").
Configured to allow the VPCs to communicate with each other as if they were in the same network.","I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create the peer VPC
resource ""aws_vpc"" ""peer"" {
  cidr_block           = ""10.0.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""peer-vpc""
  }
}

# Create the base VPC
resource ""aws_vpc"" ""base"" {
  cidr_block           = ""10.1.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""base-vpc""
  }
}

# Create VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    Name = ""pike-peering-connection""
  }
}

# Update route table for peer VPC
resource ""aws_route"" ""peer_to_base"" {
  route_table_id            = aws_vpc.peer.main_route_table_id
  destination_cidr_block    = aws_vpc.base.cidr_block
  vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
}

# Update route table for base VPC
resource ""aws_route"" ""base_to_peer"" {
  route_table_id            = aws_vpc.base.main_route_table_id
  destination_cidr_block    = aws_vpc.peer.cidr_block
  vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
}
```

This configuration:
1. Sets up the AWS provider
2. Creates two VPCs (""peer"" and ""base"") with specified CIDR blocks
3. Establishes a VPC peering connection named ""pike"" between them
4. Updates the route tables of both VPCs to allow traffic between them

The `auto_accept = true` parameter automatically accepts the peering connection request since both VPCs are in the same AWS account. If the VPCs were in different accounts, you would need to handle the acceptance separately.

The route table updates ensure that traffic can flow between the VPCs through the peering connection.

Remember to adjust the region in the provider block according to your needs.",,,,,,"I'll help you create a Terraform configuration for establishing a VPC peering connection between two VPCs. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # You can change this to your desired region
}

# Create the peer VPC
resource ""aws_vpc"" ""peer_vpc"" {
  cidr_block           = ""10.0.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""peer-vpc""
  }
}

# Create the base VPC
resource ""aws_vpc"" ""base_vpc"" {
  cidr_block           = ""10.1.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""base-vpc""
  }
}

# Create VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer_vpc.id
  vpc_id      = aws_vpc.base_vpc.id
  auto_accept = true

  tags = {
    Name = ""pike-peering-connection""
  }
}

# Add route table entry for peer VPC
resource ""aws_route_table"" ""peer_route_table"" {
  vpc_id = aws_vpc.peer_vpc.id

  route {
    cidr_block                = aws_vpc.base_vpc.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""peer-route-table""
  }
}

# Add route table entry for base VPC
resource ""aws_route_table"" ""base_route_table"" {
  vpc_id = aws_vpc.base_vpc.id

  route {
    cidr_block                = aws_vpc.peer_vpc.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""base-route-table""
  }
}

# Associate route tables with their respective VPCs
resource ""aws_main_route_table_association"" ""peer_route_association"" {
  vpc_id         = aws_vpc.peer_vpc.id
  route_table_id = aws_route_table.peer_route_table.id
}

resource ""aws_main_route_table_association"" ""base_route_association"" {
  vpc_id         = aws_vpc.base_vpc.id
  route_table_id = aws_route_table.base_route_table.id
}
```

This configuration:
1. Creates two VPCs with the specified CIDR blocks
2. Establishes a VPC peering connection named ""pike"" between them
3. Creates and configures route tables for both VPCs to enable traffic flow
4. Associates the route tables with their respective VPCs

Key features:
- The VPCs are created with DNS support enabled
- The peering connection auto-accepts (assuming both VPCs are in the same account)
- Route tables are configured to allow traffic between the VPCs
- All resources are properly tagged for identification

Make sure to update the region in the provider block according to your needs. The configuration will create all necessary resources to enable communication between the two VPCs through the peering connection.",,,,,,"I'll help you create a Terraform configuration for VPC peering between two VPCs. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change this to your desired region
}

# Create the peer VPC
resource ""aws_vpc"" ""peer"" {
  cidr_block           = ""10.0.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""peer-vpc""
  }
}

# Create the base VPC
resource ""aws_vpc"" ""base"" {
  cidr_block           = ""10.1.0.0/24""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""base-vpc""
  }
}

# Create the VPC peering connection
resource ""aws_vpc_peering_connection"" ""pike"" {
  peer_vpc_id = aws_vpc.peer.id
  vpc_id      = aws_vpc.base.id
  auto_accept = true

  tags = {
    Name = ""pike-peering-connection""
  }
}

# Add route table entry for peer VPC
resource ""aws_route_table"" ""peer_route_table"" {
  vpc_id = aws_vpc.peer.id

  route {
    cidr_block                = aws_vpc.base.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""peer-route-table""
  }
}

# Add route table entry for base VPC
resource ""aws_route_table"" ""base_route_table"" {
  vpc_id = aws_vpc.base.id

  route {
    cidr_block                = aws_vpc.peer.cidr_block
    vpc_peering_connection_id = aws_vpc_peering_connection.pike.id
  }

  tags = {
    Name = ""base-route-table""
  }
}
```

This configuration:
1. Creates two VPCs (""peer"" and ""base"") with specified CIDR blocks
2. Establishes a VPC peering connection named ""pike"" between them
3. Creates route tables for both VPCs to enable traffic flow
4. Includes auto-accept for the peering connection
5. Sets up appropriate tags for resource identification

Make sure to:
- Adjust the region in the provider block according to your needs
- Review and modify tags as needed
- Ensure the CIDR blocks don't overlap with other existing VPCs in your account",,,,,
"aws_elasticache_user, aws_elasticache_user_group",Provide an ElastiCache user group resource.,"package main

import future.keywords.in

default allow = false

aws_elasticache_user_group_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_user_group""
}

# Aggregate all checks
allow {
    aws_elasticache_user_group_valid(input.resource_changes)
}",2,"resource ""aws_elasticache_user"" ""test"" {
  user_id       = ""testUserId""
  user_name     = ""default""
  access_string = ""on ~app::* -@all +@read +@hash +@bitmap +@geo -setbit -bitfield -hset -hsetnx -hmset -hincrby -hincrbyfloat -hdel -bitop -geoadd -georadius -georadiusbymember""
  engine        = ""REDIS""
  passwords     = [""password123456789""]
}

resource ""aws_elasticache_user_group"" ""test"" {
  engine        = ""REDIS""
  user_group_id = ""userGroupId""
  user_ids      = [aws_elasticache_user.test.user_id]
}","creates elasticache user resources(requires access string, engine = REDIS, user id and user name) and sets engine , creates group resource using the user ids","I'll help you create a Terraform configuration for an AWS ElastiCache user group resource. This configuration will include the required AWS provider and the user group resource.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_elasticache_user_group"" ""example"" {
  engine        = ""REDIS""
  user_group_id = ""example-user-group""
  user_ids      = [""example-user-1"", ""example-user-2""]

  tags = {
    Name = ""example-user-group""
    Environment = ""production""
  }
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an ElastiCache user group with:
   - Redis as the engine
   - A unique user group ID
   - A list of user IDs that belong to this group
   - Tags for better resource management

Note that you'll need to have the referenced users (""example-user-1"", ""example-user-2"") already created using `aws_elasticache_user` resources. You can adjust the user_ids list according to your specific needs.",,,,,,"I'll help you create a Terraform configuration for an AWS ElastiCache user group resource. This will include the required AWS provider and the ElastiCache user group resource configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_elasticache_user_group"" ""example"" {
  engine        = ""redis""
  user_group_id = ""example-user-group""
  user_ids      = [""example-user""]

  tags = {
    Name = ""example-user-group""
    Environment = ""production""
  }
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an ElastiCache user group with:
   - Redis as the engine
   - A unique user group ID
   - A list of user IDs that belong to this group
   - Tags for better resource management

Note: Make sure you have created the ElastiCache users (referenced in user_ids) before creating the user group, as they must exist for the user group creation to succeed.

You can customize the values for `user_group_id`, `user_ids`, and tags according to your specific needs.",,,,,,"I'll help you create a Terraform configuration for an AWS ElastiCache user group resource. This configuration will include the required AWS provider and create a user group for ElastiCache.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_elasticache_user_group"" ""example"" {
  engine        = ""redis""
  user_group_id = ""example-user-group""
  user_ids      = [""example-user""]

  tags = {
    Name = ""example-elasticache-user-group""
    Environment = ""production""
  }
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an ElastiCache user group with:
   - Redis as the engine
   - A unique user group ID
   - A list of user IDs that belong to this group
   - Tags for better resource management

You can customize the `user_group_id`, `user_ids`, and tags according to your specific needs. Note that the users referenced in `user_ids` should exist before creating this user group.",,,,,
"aws_eks_addon, aws_eks_cluster, aws_iam_role, aws_subnet, aws_vpc","Generate a Terraform script to add an addon with name ""example"", to an AWS EKS cluster. Define a resource that specifies the cluster's name and sets the addon name to 'vpc-cni'.","package terraform.validation

default has_aws_eks_addon_example = false

has_aws_eks_addon_example {
    addon := input.planned_values.root_module.resources[_]
    addon.type == ""aws_eks_addon""
    addon.name == ""example""
    addon.values.cluster_name == input.planned_values.root_module.resources[_].values.name  # Ensures it references aws_eks_cluster.example correctly
    addon.values.addon_name == ""vpc-cni""
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_iam_role"" ""example"" {
  name = ""eks-cluster-1""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""eks.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_eks_cluster"" ""example"" {
  name     = ""test""
  version  = ""test-version""
  role_arn = aws_iam_role.example.arn

  vpc_config {
    subnet_ids = [
      aws_subnet.private-us-east-1a.id,
    ]
  }
}

resource ""aws_eks_addon"" ""example"" {
  cluster_name = aws_eks_cluster.example.name
  addon_name   = ""vpc-cni""
}","has one ""aws_eks_addon"" resource
with one ""cluster_name""
with one ""addon_name""","I'll help you create a Terraform script to add an addon to an AWS EKS cluster. Here's the solution using HCL:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Replace with your desired region
}

resource ""aws_eks_addon"" ""example"" {
  cluster_name = ""your-eks-cluster-name""  # Replace with your cluster name
  addon_name   = ""vpc-cni""

  # Optional: Add addon version if needed
  # addon_version = ""v1.12.0-eksbuild.1""

  # Optional: Add resolve conflicts if needed
  # resolve_conflicts = ""OVERWRITE""
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an EKS addon resource named ""example""
3. Specifies the cluster name where the addon should be installed
4. Sets the addon name to 'vpc-cni' as requested

Note: Make sure to replace ""your-eks-cluster-name"" with the actual name of your EKS cluster. The vpc-cni addon is essential for networking in EKS clusters.

You can also uncomment and adjust the optional parameters (addon_version and resolve_conflicts) if you need to specify particular versions or conflict resolution behaviors.",,,,,,"I'll help you create a Terraform configuration to add a VPC CNI addon to an AWS EKS cluster. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

resource ""aws_eks_addon"" ""example"" {
  cluster_name = ""example-cluster""  # Replace with your cluster name
  addon_name   = ""vpc-cni""
  
  # Optional: Specify addon version if needed
  # addon_version = ""v1.12.0-eksbuild.1""
  
  # Optional: Configure resolution for version conflicts
  resolve_conflicts_on_create = ""OVERWRITE""
  resolve_conflicts_on_update = ""PRESERVE""
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an EKS addon resource named ""example""
3. Specifies the cluster name it should be attached to
4. Sets the addon_name to 'vpc-cni' as requested
5. Includes optional parameters (commented out) that you can uncomment and customize if needed

Make sure to replace ""example-cluster"" with your actual EKS cluster name. The vpc-cni addon is essential for pod networking in EKS clusters.",,,,,,"I'll help you create a Terraform configuration to add a VPC-CNI addon to an AWS EKS cluster. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Specify your desired region
}

resource ""aws_eks_addon"" ""example"" {
  cluster_name = ""example-cluster""  # Replace with your cluster name
  addon_name   = ""vpc-cni""
  
  # Optional: Add addon version if needed
  # addon_version = ""v1.12.0-eksbuild.1""
  
  # Optional: Add configuration settings
  # configuration_values = jsonencode({
  #   enableNetworkPolicy = ""true""
  # })
}
```

This configuration:
1. Defines the required AWS provider
2. Creates an EKS addon resource named ""example""
3. Specifies the cluster name and sets the addon to 'vpc-cni'
4. Includes commented optional parameters that you can uncomment and customize if needed

Make sure to replace ""example-cluster"" with your actual EKS cluster name. The vpc-cni addon is essential for networking in EKS clusters and handles IP address allocation and network interface configuration for pods.",,,,,
"aws_iam_policy, aws_iam_role, aws_iam_role_policy_attachment, aws_kinesis_analytics_application, aws_kinesis_stream, aws_iam_policy_document, aws_iam_policy_document",Generate a basic Amazon Kinesis Analytics Application with an input kinesis stream,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kinesisanalytics.amazonaws.com"") 
}

has_valid_kinesis_stream {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_stream""
    resource.values.name
}

has_valid_kinesis_analytics_application {
        some i
        resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesis_analytics_application""
    resource.values.name
    resource.values.inputs[_].kinesis_stream
}

has_valid_resources {
        has_valid_iam_role(input.resource_changes)
        has_valid_kinesis_stream
    has_valid_kinesis_analytics_application
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}


data ""aws_iam_policy_document"" ""StreamReaderWriter"" {
  statement {
    effect = ""Allow""
    actions = [""kinesis:DescribeStream"",
               ""kinesis:GetShardIterator"",
               ""kinesis:GetRecords""]
    resources = [aws_kinesis_stream.test_stream.arn]
  }
}

data ""aws_iam_policy_document"" ""KinesisAssume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}

resource ""aws_iam_policy"" ""StreamReaderWriter"" {
  name   = ""KinesisStreamReaderWriter""
  policy = data.aws_iam_policy_document.StreamReaderWriter.json
}

resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_role""
  assume_role_policy = data.aws_iam_policy_document.KinesisAssume.json
}

resource ""aws_iam_role_policy_attachment"" ""attach_kinesis_role"" {
  role       = aws_iam_role.kinesis_role.name
  policy_arn = aws_iam_policy.StreamReaderWriter.arn
}

resource ""aws_kinesis_stream"" ""test_stream"" {
  name             = ""events""
  shard_count      = 1
  retention_period = 168
}


resource ""aws_kinesis_analytics_application"" ""test_application"" {
  name = ""kinesis-analytics-application-test""

  inputs {
    name_prefix = ""test_prefix""

    kinesis_stream {
      resource_arn = aws_kinesis_stream.test_stream.arn
      role_arn     = aws_iam_role.kinesis_role.arn
    }

    parallelism {
      count = 1
    }

    schema {
      record_columns {
        mapping  = ""$.test""
        name     = ""test""
        sql_type = ""VARCHAR(8)""
      }

      record_encoding = ""UTF-8""

      record_format {
        mapping_parameters {
          json {
            record_row_path = ""$""
          }
        }
      }
    }
  }
}","Has one ""aws_kinesis_analytics_application"" resource with one ""aws_kinesis_stream"" resource as an input","I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with an input Kinesis stream. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create a Kinesis stream as input
resource ""aws_kinesis_stream"" ""input_stream"" {
  name             = ""input-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""dev""
    Purpose     = ""analytics-input""
  }
}

# Create an IAM role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-application-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Create IAM policy for Kinesis Analytics to read from input stream
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-application-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kinesis:GetShardIterator"",
          ""kinesis:GetRecords"",
          ""kinesis:DescribeStream"",
          ""kinesis:ListShards""
        ]
        Resource = aws_kinesis_stream.input_stream.arn
      }
    ]
  })
}

# Create Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""example-analytics-application""

  inputs {
    name_prefix = ""SOURCE_SQL_STREAM""

    kinesis_stream {
      resource_arn = aws_kinesis_stream.input_stream.arn
      role_arn    = aws_iam_role.kinesis_analytics_role.arn
    }

    schema_version = ""1""

    schema {
      record_columns {
        name     = ""example_column""
        sql_type = ""VARCHAR(8)""
        mapping  = ""$.example""
      }

      record_format {
        mapping_parameters {
          json {
            record_row_path = ""$""
          }
        }
      }
    }
  }
}
```

This configuration creates:

1. A Kinesis input stream with 1 shard and 24-hour retention
2. An IAM role for the Kinesis Analytics application
3. An IAM policy allowing the application to read from the input stream
4. A Kinesis Analytics application configured to read from the input stream

The application is set up with:
- A basic JSON input schema
- One example column
- JSON record format
- Proper IAM permissions to read from the input stream

You can customize this further by:
- Adjusting the input schema to match your data structure
- Adding more record columns
- Modifying the stream configuration (shard count, retention period)
- Adding application code and outputs
- Adjusting the region and tags as needed

Remember to initialize Terraform and apply the configuration in your desired AWS environment.",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with an input Kinesis stream. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create an IAM role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-application-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# Create an input Kinesis stream
resource ""aws_kinesis_stream"" ""input_stream"" {
  name             = ""input-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""dev""
  }
}

# Create the Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""example-analytics-application""

  inputs {
    name_prefix = ""SOURCE_SQL_STREAM""

    kinesis_stream {
      resource_arn = aws_kinesis_stream.input_stream.arn
      role_arn     = aws_iam_role.kinesis_analytics_role.arn
    }

    schema_version = ""1""

    schema {
      record_columns {
        name     = ""example_column""
        sql_type = ""VARCHAR(8)""
        mapping  = ""$.example""
      }

      record_format {
        mapping_parameters {
          json {
            record_row_path = ""$""
          }
        }
      }
    }
  }
}

# Add necessary IAM policy for Kinesis Analytics to read from input stream
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kinesis:GetShardIterator"",
          ""kinesis:GetRecords"",
          ""kinesis:DescribeStream"",
          ""kinesis:ListShards""
        ]
        Resource = aws_kinesis_stream.input_stream.arn
      }
    ]
  })
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role for the Kinesis Analytics application
3. Creates an input Kinesis stream
4. Creates a Kinesis Analytics application with the input stream configured
5. Adds the necessary IAM policy to allow the Analytics application to read from the input stream

The Kinesis Analytics application is configured with:
- A basic schema with one example column
- JSON record format
- Appropriate IAM permissions
- Connection to the input Kinesis stream

You can customize the configuration by:
- Adjusting the region
- Modifying the stream's shard count and retention period
- Adding more record columns to the schema
- Adjusting the IAM permissions as needed
- Changing the names of resources to match your requirements

Remember to replace placeholder values with your actual desired configurations before applying.",,,,,,"I'll help you create a Terraform configuration for an Amazon Kinesis Analytics Application with an input Kinesis stream. This will include the necessary resources for both the analytics application and the input stream.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""  # Change to your desired region
}

# Create Kinesis Stream as input
resource ""aws_kinesis_stream"" ""input_stream"" {
  name             = ""input-stream""
  shard_count      = 1
  retention_period = 24

  tags = {
    Environment = ""production""
    Purpose     = ""analytics-input""
  }
}

# Create IAM Role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name = ""kinesis-analytics-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kinesisanalytics.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Policy for Kinesis Analytics to read from input stream
resource ""aws_iam_role_policy"" ""kinesis_analytics_policy"" {
  name = ""kinesis-analytics-policy""
  role = aws_iam_role.kinesis_analytics_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kinesis:GetShardIterator"",
          ""kinesis:GetRecords"",
          ""kinesis:DescribeStream"",
          ""kinesis:ListShards""
        ]
        Resource = aws_kinesis_stream.input_stream.arn
      }
    ]
  })
}

# Create Kinesis Analytics Application
resource ""aws_kinesis_analytics_application"" ""example"" {
  name = ""example-analytics-application""

  inputs {
    name_prefix = ""SOURCE_SQL_STREAM""

    kinesis_stream {
      resource_arn = aws_kinesis_stream.input_stream.arn
      role_arn     = aws_iam_role.kinesis_analytics_role.arn
    }

    schema_version = ""1""

    schema {
      record_columns {
        name     = ""example_column""
        sql_type = ""VARCHAR(8)""
        mapping  = ""$.example""
      }

      record_format {
        mapping_parameters {
          json {
            record_row_path = ""$""
          }
        }
      }
    }
  }
}
```

This configuration creates:

1. A Kinesis Stream as the input source
2. An IAM role and policy for the Kinesis Analytics application to access the input stream
3. A Kinesis Analytics application with basic configuration that:
   - Connects to the input stream
   - Defines a basic schema for processing JSON data
   - Uses the IAM role for permissions

Key features:
- The input stream has 1 shard and 24-hour retention
- The application expects JSON input data
- Includes necessary IAM permissions for the analytics application to read from the input stream
- Defines a basic schema with one example column

You can customize this further by:
- Adjusting the shard count and retention period of the input stream
- Adding more columns to the schema
- Modifying the record format for different data structures
- Adding output configurations
- Adding application code for data processing

Remember to replace the region with your desired AWS region and adjust the names and tags according to your needs.",,,,,
aws_lambda_layer_version,"Create a Lambda Layer Version resource from ""lambda_layer_payload.zip""","package main

import future.keywords.in

default allow = false

# Check if the AWS Lambda Layer Version is valid
aws_lambda_layer_version_valid(resources) {
    some resource in resources
    resource.type == ""aws_lambda_layer_version""
    resource.change.after.filename == ""lambda_layer_payload.zip""
}

# Aggregate all checks
allow {
    aws_lambda_layer_version_valid(input.resource_changes)
}",1,"resource ""aws_lambda_layer_version"" ""lambda_layer"" {
  filename   = ""lambda_layer_payload.zip""
  layer_name = ""lambda_layer_name""

  compatible_runtimes = [""nodejs16.x""]
}","Create a aws_lambda_layer_version by using file_name = ""lambda_layer_payload.zip"". Layer name can be any.","I'll help you create a Terraform configuration for an AWS Lambda Layer Version resource using a ZIP file. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_lambda_layer_version"" ""example_layer"" {
  filename            = ""lambda_layer_payload.zip""
  layer_name         = ""example_lambda_layer""
  description        = ""Example Lambda Layer from ZIP file""
  compatible_runtimes = [""python3.8"", ""python3.9""] # Adjust runtimes as needed

  # Optional: Set compatible architectures if needed
  compatible_architectures = [""x86_64""]
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates a Lambda Layer Version using the specified ZIP file
3. Sets basic properties like layer name, description, and compatible runtimes
4. Includes compatible architectures (optional)

The layer will be created from the ""lambda_layer_payload.zip"" file in your working directory. You can adjust the compatible runtimes and architectures according to your specific needs.",,,,,,"I'll help you create a Terraform configuration for an AWS Lambda Layer Version resource using the specified ZIP file.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_lambda_layer_version"" ""example_layer"" {
  filename            = ""lambda_layer_payload.zip""
  layer_name         = ""example_lambda_layer""
  description        = ""Example Lambda Layer from local ZIP file""
  compatible_runtimes = [""python3.8"", ""python3.9""] # Adjust runtimes as needed

  # Optional: Specify compatible architectures
  compatible_architectures = [""x86_64""]
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates a Lambda Layer Version resource using the specified ZIP file
3. Sets basic configuration like layer name, description, and compatible runtimes
4. Includes compatible architectures (optional)

You can customize the `compatible_runtimes` and other parameters based on your specific requirements. The layer will be created using the contents of ""lambda_layer_payload.zip"" in your working directory.

Make sure the ""lambda_layer_payload.zip"" file exists in your Terraform working directory before applying this configuration.",,,,,,"I'll help you create a Terraform configuration for an AWS Lambda Layer Version resource using a zip file. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_lambda_layer_version"" ""example_layer"" {
  filename            = ""lambda_layer_payload.zip""
  layer_name         = ""example_lambda_layer""
  description        = ""Example Lambda Layer from zip file""
  compatible_runtimes = [""python3.8"", ""python3.9""] # Adjust runtimes as needed

  # Optional: Set compatible architectures
  compatible_architectures = [""x86_64""]
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates a Lambda Layer Version using the specified zip file
3. Sets basic properties like layer name, description, and compatible runtimes
4. Includes compatible architectures (optional)

The layer will be created from the ""lambda_layer_payload.zip"" file, which should be present in your working directory. You can modify the compatible runtimes and architectures according to your specific needs.",,,,,
