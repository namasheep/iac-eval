Resource,Prompt,Rego intent,Difficulty,Reference output,Intent,LLM Output #0,LLM Plannable? #0,LLM Correct? #0,LLM Plan Phase Error #0,LLM OPA match phase Error #0,LLM Notes #0,LLM Output #1,LLM Plannable? #1,LLM Correct? #1,LLM Plan Phase Error #1,LLM OPA match phase Error #1,LLM Notes #1,LLM Output #2,LLM Plannable? #2,LLM Correct? #2,LLM Plan Phase Error #2,LLM OPA match phase Error #2,LLM Notes #2
"aws_api_gateway_integration, aws_api_gateway_integration, aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_dynamodb_table, aws_iam_role, aws_iam_role, aws_lambda_function, aws_lambda_function, aws_lambda_permission, aws_lambda_permission, aws_s3_bucket","An AWS service that holds a web server which allows you to upload cat pictures and provides random cat pictures on demand. Accomplish this using the following resources: AWS DynamoDB table, AWS S3 bucket, AWS Lambda function, AWS Lambda permission, AWS API Gateway rest API, AWS API Gateway resource, AWS API Gateway method. Add any necessary resources.","package caas_middle

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
	api.address in cat.expressions.parent_id.references
	api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
	method.expressions.http_method.constant_value == method_string
	resource.address in method.expressions.resource_id.references
	api.address in method.expressions.rest_api_id.references
}

table_valid(table) if {
	some attribute in table.expressions.attribute
	attribute.name
	attribute.type

	table.expressions.hash_key
}

lambda_valid(lambda, bucket) if {
	some env in lambda.expressions.environment
	bucket.address in env.variables.references
}

permission_valid(permission, lambda, api) if {
	permission.expressions.action.constant_value == ""lambda:InvokeFunction""
	lambda.address in permission.expressions.function_name.references
	permission.expressions.principal.constant_value == ""apigateway.amazonaws.com""
	api.address in permission.expressions.source_arn.references
}

integration_valid(integration, lambda, method, resource, api, integration_method) if {
	method.address in integration.expressions.http_method.references
	resource.address in integration.expressions.resource_id.references
	api.address in integration.expressions.rest_api_id.references
	integration.expressions.integration_http_method.constant_value == integration_method
	integration.expressions.type.constant_value == ""AWS_PROXY""
	lambda.address in integration.expressions.uri.references
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some api in resources
	api.type == ""aws_api_gateway_rest_api""

	some cat in resources
	cat.type == ""aws_api_gateway_resource""

	some method_get in resources
	method_get.type == ""aws_api_gateway_method""

	some method_put in resources
	method_put.type == ""aws_api_gateway_method""

	some table in resources
	table.type == ""aws_dynamodb_table""

	some lambda_get in resources
	lambda_get.type == ""aws_lambda_function""

	some lambda_put in resources
	lambda_put.type == ""aws_lambda_function""

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	some permission_get in resources
	permission_get.type == ""aws_lambda_permission""

	some permission_put in resources
	permission_put.type == ""aws_lambda_permission""

	some integration_get in resources
	integration_get.type == ""aws_api_gateway_integration""

	some integration_put in resources
	integration_put.type == ""aws_api_gateway_integration""

	api_valid(api)
	cat_valid(cat, api)
	method_valid(method_get, ""GET"", cat, api)
	method_valid(method_put, ""PUT"", cat, api)
	lambda_valid(lambda_get, bucket, archive_get)
	lambda_valid(lambda_put, bucket, archive_put)
	permission_valid(permission_get, lambda_get, api)
	permission_valid(permission_put, lambda_put, api)
	integration_valid(integration_get, lambda_get, method_get, cat, api, ""GET"")
	integration_valid(integration_put, lambda_put, method_put, cat, api, ""PUT"")
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_api resource id
    with ""resource_id"" linking to aws_api_gateway_resource resource
    with ""authorization"" argument
    with ""http_method"" argument with values ""GET"" and ""PUT""
have one ""aws_s3_bucket"" resource
have one ""aws_dynamodb_table"" resource
    with ""name"" argument
    with ""hash_key"" argument
    with ""attribute"" argument block
        with ""name"" argument
        with ""type"" argument
with two ""aws_lambda_function"" resources
    with ""function_name"" argument
    with ""role"" argument
    with ""environment"" argument block
        with ""variables"" argument block linking to the aws_s3_bucket
with two ""aws_lambda_permission"" resources
    with ""action"" argument with value ""lambda:InvokeFunction""
    with ""principal"" argument with value ""apigateway.amazonaws.com""
    with ""function_name"" argument linking to aws_lambda_function resource
    with ""source_arn"" argument linking to aws_api_gateway_rest_api (two different end points)
with two ""aws_api_gateway_integration"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource resource
    with ""http_method"" argument linking to aws_api_gateway_method resource
    with ""type"" argument with value ""AWS_PROXY""
    with ""integration_http_method"" argument with values ""GET"" and ""PUT""","I'll help you create a Terraform configuration for a cat picture service using AWS services. This solution will be similar to Example 1 but I'll make sure it includes all the requested resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# DynamoDB table to store cat picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures"" {
  name           = ""cat_pictures""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# S3 bucket to store cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket_prefix = ""cat-pictures-storage""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsoncode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*""
        ]
      },
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = aws_dynamodb_table.cat_pictures.arn
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_pictures"" {
  filename         = ""lambda_function.zip""
  function_name    = ""cat_pictures_handler""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME = aws_s3_bucket.cat_pictures.id
      TABLE_NAME  = aws_dynamodb_table.cat_pictures.name
    }
  }
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat-pictures-api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway methods
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""upload_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = """,,,,,,"I'll help you create a Terraform configuration for your cat picture service using the specified AWS resources. This solution will be similar to Example 1 but tailored to your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# DynamoDB table to store cat picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures"" {
  name           = ""cat_pictures""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# S3 bucket to store cat pictures
resource ""aws_s3_bucket"" ""cat_storage"" {
  bucket_prefix = ""cat-pictures-storage""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_picture_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""lambda.amazonaws.com""
      }
    }]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_picture_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_storage.arn,
          ""${aws_s3_bucket.cat_storage.arn}/*""
        ]
      },
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = aws_dynamodb_table.cat_pictures.arn
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_service"" {
  filename         = ""lambda_function.zip""  # You'll need to provide this
  function_name    = ""cat_picture_service""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      S3_BUCKET = aws_s3_bucket.cat_storage.id
      DYNAMODB_TABLE = aws_dynamodb_table.cat_pictures.id
    }
  }
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat_picture_api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway GET method
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# API Gateway POST method
resource ""aws_api_gateway_method"" ""upload_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.",,,,,,"I'll help you create a Terraform configuration for a cat picture service using the specified AWS resources. This solution will be similar to Example 1 but I'll make sure to include all the required components.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# DynamoDB table to store cat picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures"" {
  name           = ""cat_pictures""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# S3 bucket to store cat pictures
resource ""aws_s3_bucket"" ""cat_storage"" {
  bucket_prefix = ""cat-pictures-storage""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_service_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""lambda.amazonaws.com""
      }
    }]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_service_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_storage.arn,
          ""${aws_s3_bucket.cat_storage.arn}/*""
        ]
      },
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = aws_dynamodb_table.cat_pictures.arn
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_service"" {
  filename         = ""lambda_function.zip""  # You'll need to create this
  function_name    = ""cat_picture_service""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME     = aws_s3_bucket.cat_storage.id
      DYNAMODB_TABLE  = aws_dynamodb_table.cat_pictures.name
    }
  }
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat_picture_api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway methods
resource ""aws_api_gateway_method"" ""upload_method"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""POST""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""get_method"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id",,,,,
"aws_codebuild_project, aws_iam_role, aws_s3_bucket, aws_s3_bucket",create a AWS codebuild project resource with example iam role and secondary artifiacts,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for aws_iam_role resource
has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""codebuild.amazonaws.com"") 
} 

has_valid_codebuild {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_codebuild_project""
        resource.values.artifacts
    resource.values.environment
    resource.values.name
    resource.values.source[_].type
    resource.values.secondary_artifacts[_].type
    role := input.configuration.root_module.resources[i]
    role.expressions.service_role
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    
}


has_valid_resources {
        has_valid_iam_role(input.resource_changes)
    has_valid_codebuild
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""test_role8"" {
  name = ""test_role8""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Sid    = """"
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      },
    ]
  })
}

resource ""aws_s3_bucket"" ""apriltwentyninth"" {
  bucket = ""apriltwentyninth""
}

resource ""aws_s3_bucket"" ""apriltwentyninth2"" {
  bucket = ""apriltwentyninth2""
}


resource ""aws_codebuild_project"" ""example7"" {
  name          = ""test-project8""
  service_role  = aws_iam_role.test_role8.arn

  artifacts {
    location  = aws_s3_bucket.apriltwentyninth.bucket
    type      = ""S3""
    name     = ""results.zip""
    path      = ""/""
    packaging = ""ZIP""
  }

  secondary_artifacts {
    artifact_identifier =  ""SecondaryArtifact""
    type = ""S3""
    location  = aws_s3_bucket.apriltwentyninth2.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:7.0-24.10.29""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  source_version = ""master""

  secondary_sources {
    source_identifier = ""source2""
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  secondary_source_version {
    source_identifier = ""source2""
    source_version = ""master""
  }
}","Has one ""aws_iam_role"" resource, at least one ""aws_s3_bucket"" resource, and one ""aws_codebuild_project"" resource with a ""secondary_artifacts"" block","I'll help you create an AWS CodeBuild project with secondary artifacts and an IAM role. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# IAM Role for CodeBuild
data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""codebuild_role"" {
  name               = ""codebuild-project-role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

# S3 Buckets for artifacts
resource ""aws_s3_bucket"" ""primary_artifacts"" {
  bucket = ""primary-artifacts-bucket""
}

resource ""aws_s3_bucket"" ""secondary_artifacts"" {
  bucket = ""secondary-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""project"" {
  name          = ""example-project""
  description   = ""Example project with secondary artifacts""
  service_role  = aws_iam_role.codebuild_role.arn
  build_timeout = ""60""

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.primary_artifacts.bucket
  }

  secondary_artifacts {
    type                = ""S3""
    location            = aws_s3_bucket.secondary_artifacts.bucket
    artifact_identifier = ""secondary_artifact""
    packaging          = ""ZIP""
    encryption_disabled = false
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:5.0""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      status = ""ENABLED""
    }
  }

  cache {
    type = ""NO_CACHE""
  }
}

# Basic IAM policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          aws_s3_bucket.primary_artifacts.arn,
          ""${aws_s3_bucket.primary_artifacts.arn}/*"",
          aws_s3_bucket.secondary_artifacts.arn,
          ""${aws_s3_bucket.secondary_artifacts.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}
```

This configuration includes:

1. Required AWS provider block
2. IAM role and assume role policy for CodeBuild
3. Two S3 buckets for primary and secondary artifacts
4. CodeBuild project with:
   - Primary artifacts configuration
   - Secondary artifacts configuration
   - Basic environment settings
   - GitHub source configuration
   - Logs configuration
5. IAM role policy with necessary permissions for S3 and CloudWatch Logs

The Code",,,,,,"I'll help you create an AWS CodeBuild project with secondary artifacts and an IAM role. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

# IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-secondary-artifacts-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# S3 bucket for artifacts
resource ""aws_s3_bucket"" ""artifacts_bucket"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild project
resource ""aws_codebuild_project"" ""project"" {
  name          = ""project-with-secondary-artifacts""
  description   = ""CodeBuild project with secondary artifacts""
  service_role  = aws_iam_role.codebuild_role.arn
  
  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
  }

  # Primary artifacts configuration
  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifacts_bucket.bucket
    name     = ""primary-artifact""
    packaging = ""ZIP""
  }

  # Secondary artifacts configuration
  secondary_artifacts {
    type                = ""S3""
    location            = aws_s3_bucket.artifacts_bucket.bucket
    artifact_identifier = ""secondary_artifact1""
    name               = ""secondary-artifact-1""
    packaging          = ""ZIP""
    namespace_type     = ""BUILD_ID""
  }

  secondary_artifacts {
    type                = ""S3""
    location            = aws_s3_bucket.artifacts_bucket.bucket
    artifact_identifier = ""secondary_artifact2""
    name               = ""secondary-artifact-2""
    packaging          = ""ZIP""
    namespace_type     = ""BUILD_ID""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:5.0""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  logs_config {
    cloudwatch_logs {
      status = ""ENABLED""
    }
  }
}

# Basic IAM role policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.artifacts_bucket.arn}/*"",
          aws_s3_bucket.artifacts_bucket.arn
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}
```

This configuration creates:
1. An IAM role for CodeBuild with necessary permissions
2. An S3 bucket to store artifacts
3. A CodeBuild project with:
   - Primary artifacts configuration
   - Two secondary artifacts configurations
   - Basic environment settings
   - CloudWatch logs enabled
4. IAM role policy with permissions for S3 and CloudWatch Logs",,,,,,"I'll help you create an AWS CodeBuild project with secondary artifacts and an IAM role. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# IAM Role for CodeBuild
data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""codebuild_role"" {
  name               = ""codebuild-project-role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

# S3 buckets for artifacts
resource ""aws_s3_bucket"" ""primary_artifacts"" {
  bucket = ""primary-artifacts-bucket""
}

resource ""aws_s3_bucket"" ""secondary_artifacts"" {
  bucket = ""secondary-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""project"" {
  name          = ""project-with-secondary-artifacts""
  service_role  = aws_iam_role.codebuild_role.arn
  
  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.primary_artifacts.bucket
  }

  secondary_artifacts {
    artifact_identifier = ""secondaryArtifact1""
    type               = ""S3""
    location           = aws_s3_bucket.secondary_artifacts.bucket
    path              = ""/secondary1""
    packaging         = ""ZIP""
    namespace_type    = ""BUILD_ID""
  }

  secondary_artifacts {
    artifact_identifier = ""secondaryArtifact2""
    type               = ""S3""
    location           = aws_s3_bucket.secondary_artifacts.bucket
    path              = ""/secondary2""
    packaging         = ""ZIP""
    namespace_type    = ""BUILD_ID""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:5.0""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
  }

  logs_config {
    cloudwatch_logs {
      status = ""ENABLED""
    }
  }
}
```

This configuration:
1. Creates the required AWS provider
2. Sets up an IAM role for CodeBuild with assume role policy
3. Creates two S3 buckets for primary and secondary artifacts
4. Creates a CodeBuild project with:
   - Primary artifacts configuration
   - Two secondary artifacts configurations
   - Basic environment settings
   - GitHub source configuration
   - Enabled CloudWatch logs

The secondary artifacts are configured to:
- Store in different paths within the secondary artifacts S3 bucket
- Use ZIP packaging
- Use BUILD_ID namespace type for artifact organization

Remember to:
- Replace the bucket names with unique names that follow your naming convention
- Update the GitHub repository URL to your actual repository
- Add any additional IAM policies needed for your specific use case
- Adjust the environment settings according to your build requirements",,,,,
"aws_db_instance, aws_db_subnet_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc","sets up an AWS environment with a main VPC, two subnets across different availability zones, and a security group to allow database access. It also configures a 20 GB 15.3 PostgresSQL RDS instance with public accessibility, linked to the defined subnets and security group, outputting the database endpoint for connectivity.","package terraform.validation

default is_valid_db_instance = false

default is_valid_vpc = false

default is_valid_subnet = false

default is_valid_security_group = false

default is_valid_db_subnet_group = false

# Validate aws_db_instance resource
is_valid_db_instance {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_instance""
        resource.expressions.allocated_storage.constant_value == 20
        resource.expressions.engine.constant_value == ""postgres""
        resource.expressions.engine_version.constant_value == ""15.3""
        resource.expressions.instance_class.constant_value != null
        resource.expressions.password.constant_value != null
        resource.expressions.username.constant_value != null
        resource.expressions.publicly_accessible.constant_value == true
        resource.expressions.db_subnet_group_name.references != null
        resource.expressions.vpc_security_group_ids.references != null
}

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# Validate aws_subnet resource (more than one)
is_valid_subnet {
        count(subnets) > 1
}

subnets[resource] {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
}

# Validate aws_security_group resource
is_valid_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""
        # Additional checks for specific ingress/egress rules can be added here
}

# Validate aws_db_subnet_group resource
is_valid_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}
",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type           = ""gp2""
  engine                 = ""postgres""
  engine_version         = ""17.2""
  instance_class         = ""db.t4g.micro""
  skip_final_snapshot    = true
  db_name                = ""mydbname""
  username               = ""username""
  password               = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name   = aws_db_subnet_group.main.id
  publicly_accessible    = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""
    with one publicly_accessible = true

has one aws_vpc

has more than one aws_subnet

has one security_group

has one db_subnet_group","I'll help you create a Terraform configuration for your PostgreSQL RDS instance setup. This will be similar to Example 1 but with your specific requirements for a 20 GB storage capacity and PostgreSQL 15.3.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type          = ""gp2""
  engine                = ""postgres""
  engine_version        = ""15.3""
  instance_class        = ""db.t3.micro""
  skip_final_snapshot   = true
  db_name               = ""mydbname""
  username              = ""username""
  password              = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name   = aws_db_subnet_group.main.id
  publicly_accessible    = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

output ""database_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a PostgreSQL RDS instance with 20 GB storage
2. Uses PostgreSQL version 15.3
3. Creates a VPC with two private and two public subnets
4. Configures security group rules for PostgreSQL (port 5432)
5. Makes the database publicly accessible
6. Outputs the database endpoint
7. Uses gp2 storage type
8. Uses db.t3.micro instance class

The database will be created in the VPC with the specified security group rules and subnet configuration. The endpoint will be available as an output after the infrastructure is created.",,,,,,"I'll help you create a Terraform configuration for your AWS environment with a PostgreSQL RDS instance based on your requirements. This solution will be similar to Example 1 but with your specific configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type          = ""gp2""
  engine                = ""postgres""
  engine_version        = ""15.3""
  instance_class        = ""db.t3.micro""
  skip_final_snapshot   = true
  db_name               = ""mydbname""
  username              = ""dbadmin""
  password              = ""dbpassword123""
  vpc_security_group_ids = [aws_security_group.rds-sg.id]
  db_subnet_group_name  = aws_db_subnet_group.main.id
  publicly_accessible   = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-sg"" {
  name        = ""rds-security-group""
  description = ""Security group for RDS PostgreSQL""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""postgres_ingress"" {
  security_group_id = aws_security_group.rds-sg.id
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
}

resource ""aws_vpc_security_group_egress_rule"" ""postgres_egress"" {
  security_group_id = aws_security_group.rds-sg.id
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

output ""database_endpoint"" {
  value = aws_db_instance.postgres.endpoint
  description = ""The connection endpoint for the PostgreSQL database""
}
```

This configuration:
1. Sets up a VPC with public and private subnets across multiple AZs
2. Creates a PostgreSQL RDS instance with 20GB storage and version 15.3
3. Configures security groups for database access
4. Makes the database publicly accessible
5. Outputs the database endpoint for connectivity
6. Uses private subnets for the database subnet group
7. Includes all necessary security group rules for PostgreSQL (port 5432)

The configuration uses the standard db.t3.micro instance class for cost-effectiveness while maintaining compatibility with PostgreSQL. Remember to replace the database username and password with secure values in a production environment, preferably using variables or secrets management.",,,,,,"I'll help you create a Terraform configuration for your PostgreSQL RDS setup. This will be similar to Example 1 but with your specific requirements for a 20 GB database instance.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type          = ""gp2""
  engine                = ""postgres""
  engine_version        = ""15.3""
  instance_class        = ""db.t3.micro""
  skip_final_snapshot   = true
  db_name              = ""mydbname""
  username             = ""username""
  password             = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name  = aws_db_subnet_group.main.id
  publicly_accessible   = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

output ""database_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with two private and two public subnets
2. Creates a security group for PostgreSQL access (port 5432)
3. Provisions a 20 GB PostgreSQL 15.3 RDS instance
4. Makes the database publicly accessible
5. Outputs the database endpoint
6. Uses gp2 storage type
7. Creates necessary subnet groups for the RDS instance

The main differences from the example are:
- Reduced storage to 20 GB
- Specific PostgreSQL version 15.3
- Added an output for the database endpoint
- Simplified provider configuration

Remember to replace the username and password with secure values in a production environment, preferably using variables or secrets management.",,,,,
"aws_rds_cluster, aws_vpc","Create an mySQL cluster with 100 GB allocated to us-east-1a, us-east-1b, us-east-1c.","package main

import future.keywords.in

default allow = false

# Required AZs
required_azs := [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]

# Check for RDS cluster creation
rds_cluster_created(resources) {
some resource in resources
resource.type == ""aws_rds_cluster""
resource.change.actions[_] == ""create""
}

# Check if engine is MySQL
rds_engine_mysql(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.engine == ""mysql""
}

# Check if AZs are correct and storage is 100GB
azs_and_storage_valid(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.availability_zones == required_azs
resource.change.after.allocated_storage == 100
}

# Aggregate checks for RDS cluster
allow {
rds_cluster_created(input.resource_changes)
some resource in input.resource_changes
rds_engine_mysql(resource)
azs_and_storage_valid(resource)
}",3,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

# Configure the AWS Provider
provider ""aws"" {
region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
cidr_block = ""192.168.0.0/22""
}

resource ""aws_rds_cluster"" ""example"" {
cluster_identifier = ""example""
availability_zones = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
engine = ""mysql""
db_cluster_instance_class = ""db.r6gd.large""
storage_type = ""io1""
allocated_storage = 100
iops = 1000
master_username = ""test""
master_password = ""mustbeeightcharaters""
skip_final_snapshot = true
}
","Has one ""aws_rds_cluster"" resource
with one ""allocated_storage"" = 100
with one ""availability_zones"" = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
with one ""engine"" = ""mysql""","I'll help you create a MySQL cluster with 100 GB allocated across three availability zones in us-east-1. Based on the examples provided, here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Configure the AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
  cidr_block = ""192.168.0.0/22""
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier      = ""mysql-cluster""
  engine                 = ""mysql""
  availability_zones     = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
  db_cluster_instance_class = ""db.r6gd.large""
  storage_type           = ""io1""
  allocated_storage      = 100
  iops                  = 1000
  master_username       = ""admin""
  master_password       = ""mysqlpassword123""
  skip_final_snapshot   = true
  
  tags = {
    Name = ""mysql-cluster""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC with CIDR block 192.168.0.0/22
3. Creates a MySQL cluster with:
   - 100 GB storage allocation
   - Deployment across three AZs (us-east-1a, us-east-1b, us-east-1c)
   - Uses io1 storage type for better performance
   - Includes basic security settings (username/password)
   - Sets skip_final_snapshot to true to allow easier cleanup

Note: Remember to replace the master_password with a secure password in a production environment and consider using secrets management.",,,,,,"I'll help you create a MySQL cluster with 100 GB allocated across three availability zones in us-east-1. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Configure the AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier      = ""mysql-cluster""
  engine                 = ""mysql""
  engine_version         = ""8.0""
  availability_zones     = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
  database_name          = ""mysqldb""
  master_username        = ""admin""
  master_password        = ""yourpassword123""
  storage_type           = ""io1""
  allocated_storage      = 100
  iops                   = 1000
  db_cluster_instance_class = ""db.r6gd.large""
  
  # Backup configuration
  backup_retention_period = 7
  preferred_backup_window = ""03:00-04:00""
  
  # Network configuration
  port = 3306
  
  # For development/testing purposes only. Set to false in production
  skip_final_snapshot    = true
  
  tags = {
    Name = ""mysql-cluster""
    Environment = ""production""
  }
}

# Security group for the RDS cluster
resource ""aws_security_group"" ""mysql_sg"" {
  name        = ""mysql-security-group""
  description = ""Security group for MySQL cluster""

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]  # Consider restricting this in production
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}
```

This configuration:
1. Creates a MySQL cluster across three availability zones
2. Allocates 100 GB of storage with Provisioned IOPS (io1)
3. Uses db.r6gd.large instance class for better performance
4. Sets up basic security group rules for MySQL access
5. Configures basic backup settings

Remember to:
- Replace ""yourpassword123"" with a secure password
- Adjust the security group rules according to your security requirements
- Consider enabling encryption and other security features for production use
- Review the backup retention period and backup window according to your needs",,,,,,"I'll help you create a MySQL cluster with 100 GB allocated across three availability zones in us-east-1. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Configure the AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier      = ""mysql-cluster""
  engine                 = ""mysql""
  engine_version         = ""8.0.28""
  availability_zones     = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
  database_name          = ""mydatabase""
  master_username        = ""admin""
  master_password        = ""your-secure-password""
  allocated_storage      = 100
  storage_type           = ""io1""
  iops                   = 1000
  db_cluster_instance_class = ""db.r6gd.large""
  
  # Backup settings
  backup_retention_period = 7
  preferred_backup_window = ""03:00-04:00""
  
  # For development/testing purposes only. Set to false in production
  skip_final_snapshot    = true
  
  # Enable encryption
  storage_encrypted      = true
}

resource ""aws_rds_cluster_instance"" ""cluster_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r6g.large""
  engine              = aws_rds_cluster.mysql_cluster.engine
  engine_version      = aws_rds_cluster.mysql_cluster.engine_version
  availability_zone   = element([""us-east-1a"", ""us-east-1b"", ""us-east-1c""], count.index)
}
```

This configuration:
1. Sets up a MySQL cluster with 100 GB storage allocation
2. Deploys across three availability zones (us-east-1a, us-east-1b, us-east-1c)
3. Uses io1 storage type with 1000 IOPS
4. Creates three cluster instances, one in each availability zone
5. Includes basic security features like storage encryption
6. Sets up basic backup configuration

Remember to:
- Replace ""your-secure-password"" with a strong password
- Adjust the backup window according to your needs
- Consider setting `skip_final_snapshot` to `false` in production
- Modify the instance class if you need different computing capacity",,,,,
