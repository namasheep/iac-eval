Resource,Prompt,Rego intent,Difficulty,Reference output,Intent,LLM Output #0,LLM Plannable? #0,LLM Correct? #0,LLM Plan Phase Error #0,LLM OPA match phase Error #0,LLM Notes #0,LLM Output #1,LLM Plannable? #1,LLM Correct? #1,LLM Plan Phase Error #1,LLM OPA match phase Error #1,LLM Notes #1,LLM Output #2,LLM Plannable? #2,LLM Correct? #2,LLM Plan Phase Error #2,LLM OPA match phase Error #2,LLM Notes #2
"aws_api_gateway_integration, aws_api_gateway_integration, aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_dynamodb_table, aws_iam_role, aws_iam_role, aws_lambda_function, aws_lambda_function, aws_lambda_permission, aws_lambda_permission, aws_s3_bucket","An AWS service that holds a web server which allows you to upload cat pictures and provides random cat pictures on demand. Accomplish this using the following resources: AWS DynamoDB table, AWS S3 bucket, AWS Lambda function, AWS Lambda permission, AWS API Gateway rest API, AWS API Gateway resource, AWS API Gateway method. Add any necessary resources.","package caas_middle

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
	api.address in cat.expressions.parent_id.references
	api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
	method.expressions.http_method.constant_value == method_string
	resource.address in method.expressions.resource_id.references
	api.address in method.expressions.rest_api_id.references
}

table_valid(table) if {
	some attribute in table.expressions.attribute
	attribute.name
	attribute.type

	table.expressions.hash_key
}

lambda_valid(lambda, bucket) if {
	some env in lambda.expressions.environment
	bucket.address in env.variables.references
}

permission_valid(permission, lambda, api) if {
	permission.expressions.action.constant_value == ""lambda:InvokeFunction""
	lambda.address in permission.expressions.function_name.references
	permission.expressions.principal.constant_value == ""apigateway.amazonaws.com""
	api.address in permission.expressions.source_arn.references
}

integration_valid(integration, lambda, method, resource, api, integration_method) if {
	method.address in integration.expressions.http_method.references
	resource.address in integration.expressions.resource_id.references
	api.address in integration.expressions.rest_api_id.references
	integration.expressions.integration_http_method.constant_value == integration_method
	integration.expressions.type.constant_value == ""AWS_PROXY""
	lambda.address in integration.expressions.uri.references
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some api in resources
	api.type == ""aws_api_gateway_rest_api""

	some cat in resources
	cat.type == ""aws_api_gateway_resource""

	some method_get in resources
	method_get.type == ""aws_api_gateway_method""

	some method_put in resources
	method_put.type == ""aws_api_gateway_method""

	some table in resources
	table.type == ""aws_dynamodb_table""

	some lambda_get in resources
	lambda_get.type == ""aws_lambda_function""

	some lambda_put in resources
	lambda_put.type == ""aws_lambda_function""

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	some permission_get in resources
	permission_get.type == ""aws_lambda_permission""

	some permission_put in resources
	permission_put.type == ""aws_lambda_permission""

	some integration_get in resources
	integration_get.type == ""aws_api_gateway_integration""

	some integration_put in resources
	integration_put.type == ""aws_api_gateway_integration""

	api_valid(api)
	cat_valid(cat, api)
	method_valid(method_get, ""GET"", cat, api)
	method_valid(method_put, ""PUT"", cat, api)
	lambda_valid(lambda_get, bucket, archive_get)
	lambda_valid(lambda_put, bucket, archive_put)
	permission_valid(permission_get, lambda_get, api)
	permission_valid(permission_put, lambda_put, api)
	integration_valid(integration_get, lambda_get, method_get, cat, api, ""GET"")
	integration_valid(integration_put, lambda_put, method_put, cat, api, ""PUT"")
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_api resource id
    with ""resource_id"" linking to aws_api_gateway_resource resource
    with ""authorization"" argument
    with ""http_method"" argument with values ""GET"" and ""PUT""
have one ""aws_s3_bucket"" resource
have one ""aws_dynamodb_table"" resource
    with ""name"" argument
    with ""hash_key"" argument
    with ""attribute"" argument block
        with ""name"" argument
        with ""type"" argument
with two ""aws_lambda_function"" resources
    with ""function_name"" argument
    with ""role"" argument
    with ""environment"" argument block
        with ""variables"" argument block linking to the aws_s3_bucket
with two ""aws_lambda_permission"" resources
    with ""action"" argument with value ""lambda:InvokeFunction""
    with ""principal"" argument with value ""apigateway.amazonaws.com""
    with ""function_name"" argument linking to aws_lambda_function resource
    with ""source_arn"" argument linking to aws_api_gateway_rest_api (two different end points)
with two ""aws_api_gateway_integration"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource resource
    with ""http_method"" argument linking to aws_api_gateway_method resource
    with ""type"" argument with value ""AWS_PROXY""
    with ""integration_http_method"" argument with values ""GET"" and ""PUT""","I'll help you create a Terraform configuration for a cat picture service using the specified AWS resources. This solution will be similar to Example 1 but I'll ensure it meets your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# DynamoDB table to store cat picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures"" {
  name           = ""cat_pictures""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# S3 bucket to store cat pictures
resource ""aws_s3_bucket"" ""cat_storage"" {
  bucket_prefix = ""cat-pictures-storage""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_service_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""lambda.amazonaws.com""
      }
    }]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_service_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_storage.arn,
          ""${aws_s3_bucket.cat_storage.arn}/*""
        ]
      },
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = aws_dynamodb_table.cat_pictures.arn
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_service"" {
  filename      = ""lambda_function.zip""  # You'll need to create this zip file
  function_name = ""cat_picture_service""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME    = aws_s3_bucket.cat_storage.id
      DYNAMODB_TABLE = aws_dynamodb_table.cat_pictures.name
    }
  }
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat_picture_api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway GET method
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# API Gateway POST method
resource ""aws_api_gateway_method"" ""post_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource",,,,,,"I'll help you create a Terraform configuration for a cat picture service using the specified AWS resources. This solution will be similar to Example 1 but customized for your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# DynamoDB table to store cat picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures"" {
  name           = ""cat_pictures""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# S3 bucket to store cat pictures
resource ""aws_s3_bucket"" ""cat_storage"" {
  bucket_prefix = ""cat-pictures-storage""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_picture_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""lambda.amazonaws.com""
      }
    }]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_picture_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_storage.arn,
          ""${aws_s3_bucket.cat_storage.arn}/*""
        ]
      },
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = aws_dynamodb_table.cat_pictures.arn
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_handler"" {
  filename         = ""lambda_function.zip""  # You'll need to create this
  function_name    = ""cat_picture_handler""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME    = aws_s3_bucket.cat_storage.id
      DYNAMODB_TABLE = aws_dynamodb_table.cat_pictures.name
    }
  }
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat_picture_api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway methods
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""upload_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   ",,,,,,"I'll help you create a Terraform configuration for a cat picture service using the specified AWS resources. This solution will be similar to Example 1 but customized for your specific requirements.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# DynamoDB table to store cat picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures"" {
  name           = ""cat_pictures""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# S3 bucket to store cat pictures
resource ""aws_s3_bucket"" ""cat_storage"" {
  bucket_prefix = ""cat-pictures-storage""
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_service_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""lambda.amazonaws.com""
      }
    }]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_service_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
        Resource = [
          aws_s3_bucket.cat_storage.arn,
          ""${aws_s3_bucket.cat_storage.arn}/*""
        ]
      },
      {
        Effect = ""Allow""
        Action = [
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = aws_dynamodb_table.cat_pictures.arn
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_service"" {
  filename      = ""lambda_function.zip""  # You'll need to create this zip file
  function_name = ""cat_picture_service""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME    = aws_s3_bucket.cat_storage.id
      DYNAMODB_TABLE = aws_dynamodb_table.cat_pictures.name
    }
  }
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat_picture_api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway methods
resource ""aws_api_gateway_method"" ""get_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""upload_cat"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cat_resource.id
  http_metho",,,,,
"aws_codebuild_project, aws_iam_role, aws_s3_bucket, aws_s3_bucket",create a AWS codebuild project resource with example iam role and secondary artifiacts,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for aws_iam_role resource
has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""codebuild.amazonaws.com"") 
} 

has_valid_codebuild {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_codebuild_project""
        resource.values.artifacts
    resource.values.environment
    resource.values.name
    resource.values.source[_].type
    resource.values.secondary_artifacts[_].type
    role := input.configuration.root_module.resources[i]
    role.expressions.service_role
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    
}


has_valid_resources {
        has_valid_iam_role(input.resource_changes)
    has_valid_codebuild
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""test_role8"" {
  name = ""test_role8""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Sid    = """"
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      },
    ]
  })
}

resource ""aws_s3_bucket"" ""apriltwentyninth"" {
  bucket = ""apriltwentyninth""
}

resource ""aws_s3_bucket"" ""apriltwentyninth2"" {
  bucket = ""apriltwentyninth2""
}


resource ""aws_codebuild_project"" ""example7"" {
  name          = ""test-project8""
  service_role  = aws_iam_role.test_role8.arn

  artifacts {
    location  = aws_s3_bucket.apriltwentyninth.bucket
    type      = ""S3""
    name     = ""results.zip""
    path      = ""/""
    packaging = ""ZIP""
  }

  secondary_artifacts {
    artifact_identifier =  ""SecondaryArtifact""
    type = ""S3""
    location  = aws_s3_bucket.apriltwentyninth2.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:7.0-24.10.29""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  source_version = ""master""

  secondary_sources {
    source_identifier = ""source2""
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  secondary_source_version {
    source_identifier = ""source2""
    source_version = ""master""
  }
}","Has one ""aws_iam_role"" resource, at least one ""aws_s3_bucket"" resource, and one ""aws_codebuild_project"" resource with a ""secondary_artifacts"" block","I'll help you create an AWS CodeBuild project with an IAM role and secondary artifacts. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-project-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# S3 bucket for artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""project"" {
  name          = ""my-codebuild-project""
  service_role  = aws_iam_role.codebuild_role.arn
  description   = ""CodeBuild project with primary and secondary artifacts""

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.artifact_bucket.bucket
  }

  secondary_artifacts {
    type                = ""S3""
    location            = aws_s3_bucket.artifact_bucket.bucket
    artifact_identifier = ""secondary_artifact""
    path               = ""secondary/""
    packaging          = ""ZIP""
    namespace_type     = ""BUILD_ID""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:5.0""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  cache {
    type = ""NO_CACHE""
  }

  logs_config {
    cloudwatch_logs {
      status = ""ENABLED""
    }
  }
}

# IAM policy for CodeBuild to access S3
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.artifact_bucket.arn}"",
          ""${aws_s3_bucket.artifact_bucket.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}
```

This configuration:
1. Creates an IAM role for CodeBuild with necessary permissions
2. Creates an S3 bucket to store artifacts
3. Creates a CodeBuild project with:
   - Primary artifacts configuration using S3
   - Secondary artifacts configuration also using S3
   - Basic environment setup
   - GitHub source configuration
   - Necessary IAM permissions to access S3 and CloudWatch Logs

The secondary artifacts configuration allows you to output additional artifacts to a different location in the S3 bucket. You can adjust the paths, packaging, and other settings according to your needs.

Make sure to:
1. Replace the GitHub repository URL with your actual repository",,,,,,"I'll help you create an AWS CodeBuild project with secondary artifacts and an IAM role. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create IAM role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-project-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# Create S3 bucket for artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild project
resource ""aws_codebuild_project"" ""project"" {
  name          = ""my-codebuild-project""
  description   = ""CodeBuild project with secondary artifacts""
  service_role  = aws_iam_role.codebuild_role.arn
  
  artifacts {
    type = ""S3""
    location = aws_s3_bucket.artifact_bucket.bucket
    name = ""primary-artifacts""
    packaging = ""ZIP""
  }

  secondary_artifacts {
    type = ""S3""
    location = aws_s3_bucket.artifact_bucket.bucket
    artifact_identifier = ""secondary_artifact1""
    name = ""secondary-artifacts-1""
    packaging = ""ZIP""
    path = ""/secondary1""
  }

  secondary_artifacts {
    type = ""S3""
    location = aws_s3_bucket.artifact_bucket.bucket
    artifact_identifier = ""secondary_artifact2""
    name = ""secondary-artifacts-2""
    packaging = ""ZIP""
    path = ""/secondary2""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:5.0""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  cache {
    type = ""NO_CACHE""
  }
}

# Add necessary IAM policy for CodeBuild role
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  role = aws_iam_role.codebuild_role.name

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [
          ""${aws_s3_bucket.artifact_bucket.arn}"",
          ""${aws_s3_bucket.artifact_bucket.arn}/*""
        ]
        Action = [
          ""s3:PutObject"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:GetBucketAcl"",
          ""s3:GetBucketLocation""
        ]
      },
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
      }
    ]
  })
}
```

This configuration:
1. Creates an IAM role for CodeBuild with necessary permissions
2. Creates an S3 bucket to store artifacts
3. Creates a CodeBuild project with:
   - Primary artifacts configuration
   - Two secondary artifacts configurations
   - Basic environment setup
   - GitHub source configuration
4. Adds an IAM policy to the role for S3 and CloudWatch Logs permissions",,,,,,"I'll help you create an AWS CodeBuild project with secondary artifacts and an IAM role. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# IAM Role for CodeBuild
data ""aws_iam_policy_document"" ""assume_role"" {
  statement {
    effect = ""Allow""

    principals {
      type        = ""Service""
      identifiers = [""codebuild.amazonaws.com""]
    }

    actions = [""sts:AssumeRole""]
  }
}

resource ""aws_iam_role"" ""codebuild_role"" {
  name               = ""codebuild-project-role""
  assume_role_policy = data.aws_iam_policy_document.assume_role.json
}

# S3 buckets for artifacts
resource ""aws_s3_bucket"" ""primary_artifacts"" {
  bucket = ""primary-artifacts-bucket""
}

resource ""aws_s3_bucket"" ""secondary_artifacts"" {
  bucket = ""secondary-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""project"" {
  name          = ""example-project""
  service_role  = aws_iam_role.codebuild_role.arn
  description   = ""CodeBuild project with secondary artifacts""

  artifacts {
    type     = ""S3""
    location = aws_s3_bucket.primary_artifacts.bucket
  }

  secondary_artifacts {
    artifact_identifier = ""secondaryArtifact1""
    type               = ""S3""
    location           = aws_s3_bucket.secondary_artifacts.bucket
    path              = ""/secondary1""
    packaging         = ""ZIP""
    namespace_type    = ""BUILD_ID""
  }

  secondary_artifacts {
    artifact_identifier = ""secondaryArtifact2""
    type               = ""S3""
    location           = aws_s3_bucket.secondary_artifacts.bucket
    path              = ""/secondary2""
    packaging         = ""ZIP""
    namespace_type    = ""BUILD_ID""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:5.0""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      status = ""ENABLED""
    }
  }
}
```

This configuration includes:

1. Required AWS provider declaration
2. IAM role creation with assume role policy for CodeBuild
3. Two S3 buckets for storing primary and secondary artifacts
4. CodeBuild project configuration with:
   - Primary artifacts configuration
   - Two secondary artifacts configurations
   - Basic environment settings
   - GitHub source configuration
   - Logs configuration

The secondary artifacts are configured to:
- Store in a separate S3 bucket
- Use different paths for each secondary artifact
- Use ZIP packaging
- Use BUILD_ID for namespace

Make sure to:
1. Replace the bucket names with unique names that don't exist yet
2. Update the GitHub repository URL to your actual repository
3. Adjust the IAM role permissions according to your needs
4. Modify the compute type and image if you need different specifications",,,,,
"aws_db_instance, aws_db_subnet_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc","sets up an AWS environment with a main VPC, two subnets across different availability zones, and a security group to allow database access. It also configures a 20 GB 15.3 PostgresSQL RDS instance with public accessibility, linked to the defined subnets and security group, outputting the database endpoint for connectivity.","package terraform.validation

default is_valid_db_instance = false

default is_valid_vpc = false

default is_valid_subnet = false

default is_valid_security_group = false

default is_valid_db_subnet_group = false

# Validate aws_db_instance resource
is_valid_db_instance {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_instance""
        resource.expressions.allocated_storage.constant_value == 20
        resource.expressions.engine.constant_value == ""postgres""
        resource.expressions.engine_version.constant_value == ""15.3""
        resource.expressions.instance_class.constant_value != null
        resource.expressions.password.constant_value != null
        resource.expressions.username.constant_value != null
        resource.expressions.publicly_accessible.constant_value == true
        resource.expressions.db_subnet_group_name.references != null
        resource.expressions.vpc_security_group_ids.references != null
}

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# Validate aws_subnet resource (more than one)
is_valid_subnet {
        count(subnets) > 1
}

subnets[resource] {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
}

# Validate aws_security_group resource
is_valid_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""
        # Additional checks for specific ingress/egress rules can be added here
}

# Validate aws_db_subnet_group resource
is_valid_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}
",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type           = ""gp2""
  engine                 = ""postgres""
  engine_version         = ""17.2""
  instance_class         = ""db.t4g.micro""
  skip_final_snapshot    = true
  db_name                = ""mydbname""
  username               = ""username""
  password               = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name   = aws_db_subnet_group.main.id
  publicly_accessible    = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""
    with one publicly_accessible = true

has one aws_vpc

has more than one aws_subnet

has one security_group

has one db_subnet_group","I'll help you create a Terraform configuration for your PostgreSQL RDS setup based on the examples provided. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type          = ""gp2""
  engine                = ""postgres""
  engine_version        = ""15.3""
  instance_class        = ""db.t3.micro""
  skip_final_snapshot   = true
  db_name              = ""mydbname""
  username             = ""username""
  password             = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name  = aws_db_subnet_group.main.id
  publicly_accessible   = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

output ""database_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with public and private subnets across multiple AZs
2. Creates a security group for PostgreSQL access (port 5432)
3. Configures a 20GB PostgreSQL 15.3 RDS instance
4. Makes the database publicly accessible
5. Sets up the necessary subnet group for the RDS instance
6. Outputs the database endpoint for connectivity

The main differences from the example are:
- Reduced storage to 20GB
- Changed PostgreSQL version to 15.3
- Added an output for the database endpoint
- Simplified the provider configuration

You can apply this configuration using `terraform init` followed by `terraform apply`.",,,,,,"I'll help you create a Terraform configuration for your PostgreSQL RDS setup. This will be similar to Example 1 but with your specific requirements for a 20 GB PostgreSQL 15.3 instance.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type          = ""gp2""
  engine                = ""postgres""
  engine_version        = ""15.3""
  instance_class        = ""db.t3.micro""
  skip_final_snapshot   = true
  db_name              = ""mydbname""
  username             = ""dbadmin""
  password             = ""yourpassword123""
  vpc_security_group_ids = [aws_security_group.rds-sg.id]
  db_subnet_group_name  = aws_db_subnet_group.main.id
  publicly_accessible   = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""
  version = ""~> 5.0""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-sg"" {
  name        = ""rds-security-group""
  description = ""Security group for RDS PostgreSQL""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""postgres_ingress"" {
  security_group_id = aws_security_group.rds-sg.id
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
}

resource ""aws_vpc_security_group_egress_rule"" ""postgres_egress"" {
  security_group_id = aws_security_group.rds-sg.id
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

output ""database_endpoint"" {
  description = ""The endpoint of the database""
  value       = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with public and private subnets across multiple AZs
2. Creates a security group allowing PostgreSQL traffic (port 5432)
3. Deploys a PostgreSQL 15.3 RDS instance with:
   - 20 GB storage
   - Public accessibility
   - GP2 storage type
   - db.t3.micro instance class
4. Creates a subnet group for the RDS instance
5. Outputs the database endpoint

Remember to replace ""yourpassword123"" with a secure password before deploying.",,,,,,"I'll help you create a Terraform configuration for your AWS environment with a PostgreSQL RDS instance based on your requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port         = 5432
  to_port           = 5432
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type          = ""gp2""
  engine                = ""postgres""
  engine_version        = ""15.3""
  instance_class        = ""db.t3.micro""
  skip_final_snapshot   = true
  db_name              = ""mydbname""
  username             = ""username""
  password             = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name  = aws_db_subnet_group.main.id
  publicly_accessible   = true
}

output ""db_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with private and public subnets across availability zones
2. Creates a security group for the RDS instance with necessary PostgreSQL port (5432) access
3. Creates a 20GB PostgreSQL 15.3 RDS instance with:
   - GP2 storage type
   - Public accessibility enabled
   - Connected to the created VPC and security group
4. Outputs the database endpoint for connectivity
5. Uses the db.t3.micro instance class for cost-effectiveness
6. Includes all necessary networking components and security group rules

The database will be accessible publicly and the endpoint will be provided as an output after the infrastructure is created.",,,,,
"aws_rds_cluster, aws_vpc","Create an mySQL cluster with 100 GB allocated to us-east-1a, us-east-1b, us-east-1c.","package main

import future.keywords.in

default allow = false

# Required AZs
required_azs := [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]

# Check for RDS cluster creation
rds_cluster_created(resources) {
some resource in resources
resource.type == ""aws_rds_cluster""
resource.change.actions[_] == ""create""
}

# Check if engine is MySQL
rds_engine_mysql(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.engine == ""mysql""
}

# Check if AZs are correct and storage is 100GB
azs_and_storage_valid(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.availability_zones == required_azs
resource.change.after.allocated_storage == 100
}

# Aggregate checks for RDS cluster
allow {
rds_cluster_created(input.resource_changes)
some resource in input.resource_changes
rds_engine_mysql(resource)
azs_and_storage_valid(resource)
}",3,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

# Configure the AWS Provider
provider ""aws"" {
region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
cidr_block = ""192.168.0.0/22""
}

resource ""aws_rds_cluster"" ""example"" {
cluster_identifier = ""example""
availability_zones = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
engine = ""mysql""
db_cluster_instance_class = ""db.r6gd.large""
storage_type = ""io1""
allocated_storage = 100
iops = 1000
master_username = ""test""
master_password = ""mustbeeightcharaters""
skip_final_snapshot = true
}
","Has one ""aws_rds_cluster"" resource
with one ""allocated_storage"" = 100
with one ""availability_zones"" = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
with one ""engine"" = ""mysql""","I'll help you create a MySQL cluster with 100GB allocated across three availability zones in us-east-1. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Configure the AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier      = ""mysql-cluster""
  engine                 = ""mysql""
  engine_version         = ""8.0.28""
  availability_zones     = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
  database_name          = ""mydb""
  master_username        = ""admin""
  master_password        = ""your-password-here""
  allocated_storage      = 100
  storage_type           = ""io1""
  iops                   = 1000
  skip_final_snapshot    = true
  db_cluster_instance_class = ""db.r6gd.large""

  backup_retention_period = 7
  preferred_backup_window = ""03:00-04:00""
}

resource ""aws_rds_cluster_instance"" ""cluster_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r6g.large""
  engine              = aws_rds_cluster.mysql_cluster.engine
  engine_version      = aws_rds_cluster.mysql_cluster.engine_version
  availability_zone   = element([""us-east-1a"", ""us-east-1b"", ""us-east-1c""], count.index)
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a MySQL RDS cluster with 100GB storage allocation
3. Deploys three cluster instances across the specified availability zones
4. Includes basic security and backup settings
5. Uses io1 storage type with 1000 IOPS for better performance
6. Uses r6g instance types for good performance

Remember to:
- Replace ""your-password-here"" with a secure password
- Adjust the backup window as needed
- Modify the instance class if you need different compute capacity
- Consider adding security groups and subnet groups if deploying in a VPC

The cluster will be distributed across all three availability zones for high availability.",,,,,,"I'll help you create a MySQL cluster with 100 GB allocated across three availability zones in us-east-1. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Configure the AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier      = ""mysql-cluster""
  engine                 = ""mysql""
  engine_version         = ""8.0""
  availability_zones     = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
  database_name          = ""mysqldb""
  master_username        = ""admin""
  master_password        = ""masterpassword123""
  storage_type           = ""io1""
  allocated_storage      = 100
  iops                   = 1000
  db_cluster_instance_class = ""db.r6gd.large""
  skip_final_snapshot    = true

  backup_retention_period = 7
  preferred_backup_window = ""03:00-04:00""
}

resource ""aws_rds_cluster_instance"" ""cluster_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r6g.large""
  engine              = aws_rds_cluster.mysql_cluster.engine
  engine_version      = aws_rds_cluster.mysql_cluster.engine_version
  availability_zone   = element([""us-east-1a"", ""us-east-1b"", ""us-east-1c""], count.index)
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a MySQL RDS cluster with 100 GB storage allocation
3. Deploys three cluster instances across the specified availability zones
4. Uses io1 storage type with 1000 IOPS
5. Includes basic backup configuration
6. Sets up master credentials (you should replace these with secure values)
7. Uses r6g instance class for good performance

Remember to:
- Change the master password to a secure value
- Adjust the backup window according to your needs
- Modify the instance class if you need different performance characteristics
- Consider enabling encryption and other security features for production use",,,,,,"Based on the examples provided, I'll help you create a MySQL cluster with 100 GB allocated across three availability zones in us-east-1. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Configure the AWS Provider
provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
  cidr_block = ""192.168.0.0/22""
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier      = ""mysql-cluster""
  engine                 = ""mysql""
  availability_zones     = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
  db_cluster_instance_class = ""db.r6gd.large""
  storage_type           = ""io1""
  allocated_storage      = 100
  iops                  = 1000
  master_username       = ""admin""
  master_password       = ""your-secure-password""
  skip_final_snapshot   = true
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC for the cluster
3. Creates a MySQL RDS cluster with:
   - 100 GB storage allocation
   - Distribution across three availability zones (us-east-1a, us-east-1b, us-east-1c)
   - IO1 storage type for better performance
   - 1000 IOPS
   - Basic security settings (you should adjust the username/password)

Note: Remember to replace ""your-secure-password"" with a secure password that meets RDS requirements (minimum 8 characters).",,,,,
"aws_cloudwatch_event_rule, aws_cloudwatch_event_target, aws_iam_role, aws_lambda_function, aws_lambda_permission, archive_file, aws_iam_policy_document",Create a Lambda function and invoke it every 15 minutes,"package main

import future.keywords.in

default allow = false

# Check if AWS Lambda function is being created
aws_lambda_function_created(resources) {
    some resource in resources
    resource.type == ""aws_lambda_function""
}

# Check if CloudWatch Event Rule is set to invoke Lambda every 15 minutes
cloudwatch_event_rule_for_lambda_valid(resources) {
    some resource in resources
    resource.type == ""aws_cloudwatch_event_rule""
    resource.change.after.schedule_expression == ""rate(15 minutes)""
}

# Aggregate all checks
allow {
    aws_lambda_function_created(input.resource_changes)
    cloudwatch_event_rule_for_lambda_valid(input.resource_changes)
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_cloudwatch_event_rule"" ""lambda_schedule"" {
  name        = ""lambda-schedule-rule""
  description = ""Invoke Lambda function every 15 minutes""
  schedule_expression = ""rate(15 minutes)""
  role_arn = aws_iam_role.cron.arn
}

# Create a CloudWatch Events Target to associate with the Lambda function
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule = aws_cloudwatch_event_rule.lambda_schedule.name
  arn = aws_lambda_function.cron.arn
}

data ""archive_file"" ""lambda-func"" {
  type        = ""zip""
  source_file = ""./supplement/lambda_func.py""
  output_path = ""./supplement/lambda_func.zip""
}

resource ""aws_lambda_function"" ""cron"" {
  function_name = ""cron-lambda-function""
  role          = aws_iam_role.cron.arn
  filename      = data.archive_file.lambda-func.output_path
  source_code_hash = data.archive_file.lambda-func.output_base64sha256
  handler       = ""lambda_func.handler""
  runtime       = ""python3.12""
}

resource ""aws_lambda_permission"" ""cron"" {
  function_name = aws_lambda_function.cron.function_name
  action        = ""lambda:InvokeFunction""
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.lambda_schedule.arn
}

data ""aws_iam_policy_document"" ""cron_assume_role"" {
  statement {
    actions = [""sts:AssumeRole""]
    principals {
      type = ""Service""
      identifiers = [
        ""lambda.amazonaws.com"",
        ""events.amazonaws.com""
      ]
    }
  }
}

resource ""aws_iam_role"" ""cron"" {
  name               = ""cron_assume_role""
  assume_role_policy = data.aws_iam_policy_document.cron_assume_role.json
}

resource ""aws_iam_role_policy_attachment"" ""cron"" {
  role       = aws_iam_role.cron.name
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
}
",Needs to have an aws_lambda_function defined. Create a aws_cloudwatch_event_rule to trigger every 15 minutes and aws_lambda_permission to have permission to run lambda function. Create a target to trigger the lambda function.,"Error: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}",,,,,,"I'll help you create a Terraform configuration for a Lambda function that gets invoked every 15 minutes using AWS EventBridge (CloudWatch Events). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create the Lambda function
resource ""aws_lambda_function"" ""scheduled_lambda"" {
  filename         = ""lambda_function.zip""  # Make sure this file exists
  function_name    = ""scheduled-lambda""
  role             = aws_iam_role.lambda_role.arn
  handler          = ""index.handler""
  runtime          = ""nodejs18.x""
  source_code_hash = filebase64sha256(""lambda_function.zip"")
}

# Create the EventBridge rule to trigger every 15 minutes
resource ""aws_cloudwatch_event_rule"" ""every_fifteen_minutes"" {
  name                = ""every-fifteen-minutes""
  description         = ""Triggers every fifteen minutes""
  schedule_expression = ""rate(15 minutes)""
}

# Set the Lambda function as the target for the EventBridge rule
resource ""aws_cloudwatch_event_target"" ""lambda_target"" {
  rule      = aws_cloudwatch_event_rule.every_fifteen_minutes.name
  target_id = ""SendToLambda""
  arn       = aws_lambda_function.scheduled_lambda.arn
}

# Grant EventBridge permission to invoke the Lambda function
resource ""aws_lambda_permission"" ""allow_eventbridge"" {
  statement_id  = ""AllowEventBridgeInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.scheduled_lambda.function_name
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.every_fifteen_minutes.arn
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""scheduled_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""lambda.amazonaws.com""
      }
    }]
  })
}

# Attach basic Lambda execution policy to the IAM role
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}
```

This configuration:

1. Creates a Lambda function with a basic NodeJS runtime (you'll need to provide the actual function code in a zip file)
2. Sets up an EventBridge (CloudWatch Events) rule that triggers every 15 minutes
3. Creates an event target that connects the EventBridge rule to the Lambda function
4. Sets up the necessary IAM roles and permissions for the Lambda function to execute and for EventBridge to invoke it

Key points to note:
- You need to provide the actual Lambda function code in a zip file named ""lambda_function.zip""
- The Lambda function is configured to use NodeJS 18.x runtime, but you can change this to any supported runtime
- The function will be triggered exactly every 15 minutes
- The IAM role includes the basic Lambda execution permissions

Make sure to:
1. Have your Lambda function code ready in a zip file
2. Adjust the region in the provider block if needed
3. Modify the runtime and handler if you're using a different programming language
4. Update the function name and other identifiers as needed for your use case",,,,,,"I'll help you create a Terraform configuration for a Lambda function that gets invoked every 15 minutes using EventBridge (CloudWatch Events). Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# EventBridge Rule
resource ""aws_cloudwatch_event_rule"" ""every_fifteen_minutes"" {
  name                = ""every-fifteen-minutes""
  description         = ""Fires every 15 minutes""
  schedule_expression = ""rate(15 minutes)""
}

# EventBridge Target
resource ""aws_cloudwatch_event_target"" ""lambda_trigger"" {
  rule      = aws_cloudwatch_event_rule.every_fifteen_minutes.name
  target_id = ""TriggerLambda""
  arn       = aws_lambda_function.scheduled_lambda.arn
}

# Lambda Function
resource ""aws_lambda_function"" ""scheduled_lambda"" {
  filename         = ""lambda_function.zip""
  function_name    = ""scheduled-lambda-function""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs18.x""
  source_code_hash = filebase64sha256(""lambda_function.zip"")
}

# Lambda Permission for EventBridge
resource ""aws_lambda_permission"" ""allow_eventbridge"" {
  statement_id  = ""AllowEventBridgeInvoke""
  action        = ""lambda:InvokeFunction""
  function_name = aws_lambda_function.scheduled_lambda.function_name
  principal     = ""events.amazonaws.com""
  source_arn    = aws_cloudwatch_event_rule.every_fifteen_minutes.arn
}

# IAM Role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""scheduled_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# Attach basic Lambda execution role policy
resource ""aws_iam_role_policy_attachment"" ""lambda_basic"" {
  policy_arn = ""arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole""
  role       = aws_iam_role.lambda_role.name
}
```

This configuration:
1. Creates an EventBridge rule that triggers every 15 minutes
2. Sets up a Lambda function as the target for the EventBridge rule
3. Creates necessary IAM roles and permissions for the Lambda function
4. Grants EventBridge permission to invoke the Lambda function

Note: You'll need to create a `lambda_function.zip` file containing your Lambda function code in the same directory as your Terraform configuration. The example assumes NodeJS runtime, but you can change the runtime to match your preferred programming language.

The Lambda function will be automatically triggered every 15 minutes using the EventBridge schedule. The basic execution role allows the Lambda function to create CloudWatch logs for monitoring and troubleshooting.",,,,,
"aws_dax_cluster, aws_iam_role",Create a cluster with dax.r4.large and replication factor = 1.,"package main

import future.keywords.in

default allow = false

# Check if the AWS DynamoDB table is being created in us-east-1
aws_dax_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_dax_cluster""
    resource.change.after.node_type == ""dax.r4.large""
    resource.change.after.replication_factor == 1
}

# Aggregate all checks
allow {
    aws_dax_cluster_valid(input.resource_changes)
}",3,"resource ""aws_iam_role"" ""example"" {
  name = ""redshift_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""redshift.amazonaws.com""
        },
      },
    ],
  })
}

resource ""aws_dax_cluster"" ""bar"" {
  cluster_name       = ""cluster-example""
  iam_role_arn       = aws_iam_role.example.arn
  node_type          = ""dax.r4.large""
  replication_factor = 1
}","require a cluster name, iam_role_arn, node_type, replication factor","I'll help you create a DAX cluster with the specified requirements. Based on Example 1, I'll create a configuration that includes both the necessary IAM role and the DAX cluster with dax.r4.large node type and replication factor of 1.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax_cluster_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
        Sid = """"
      }
    ]
  })
}

# Create DAX cluster
resource ""aws_dax_cluster"" ""dax_cluster"" {
  cluster_name       = ""my-dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an IAM role that DAX can assume
3. Creates a DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - Uses the created IAM role
   - Names the cluster ""my-dax-cluster""

The cluster will be created with the minimum required configuration while meeting your specified requirements for node type and replication factor.",,,,,,"I'll help you create a DAX cluster with the specified requirements. Based on Example 1, I'll create a configuration that includes both the necessary IAM role and the DAX cluster with a dax.r4.large node type and replication factor of 1.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
        Sid = """"
      }
    ]
  })
}

# Create DAX cluster
resource ""aws_dax_cluster"" ""dax_cluster"" {
  cluster_name       = ""my-dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role that DAX can assume
3. Creates a DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - Uses the created IAM role
   - Names the cluster ""my-dax-cluster""

Make sure you have the proper AWS credentials and permissions configured before applying this configuration.",,,,,,"I'll help you create a DAX cluster with the specified requirements. Based on Example 1, I'll create a solution that includes both the necessary IAM role and the DAX cluster configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax_cluster_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Create the DAX cluster
resource ""aws_dax_cluster"" ""example"" {
  cluster_name       = ""my-dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role that DAX can assume
3. Creates a DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - The IAM role created above

The cluster will be created with the minimum configuration needed while meeting your specified requirements for node type and replication factor.",,,,,
"aws_iam_role, aws_sagemaker_domain, aws_subnet, aws_vpc, aws_iam_policy_document",Create a SageMaker Domain resource.,"package main

import future.keywords.in

default allow = false


aws_sagemaker_domain_valid(resources) {
    some resource in resources
    resource.type == ""aws_sagemaker_domain""
}

aws_iam_role_valid(resources) {
    some resource in resources
    resource.type == ""aws_iam_role""
}

aws_vpc_valid(resources) {
    some resource in resources
    resource.type == ""aws_vpc""
}

aws_subnet_valid(resources) {
    some resource in resources
    resource.type == ""aws_subnet""
}

# Aggregate all checks
allow {
    aws_sagemaker_domain_valid(input.resource_changes)
    aws_iam_role_valid(input.resource_changes)
    aws_vpc_valid(input.resource_changes)
    aws_subnet_valid(input.resource_changes)
}
",3,"resource ""aws_vpc"" ""example"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""example"" {
  vpc_id     = aws_vpc.example.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_sagemaker_domain"" ""example"" {
  domain_name = ""example""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.example.id
  subnet_ids  = [aws_subnet.example.id]

  default_user_settings {
    execution_role = aws_iam_role.example.arn
  }
}

resource ""aws_iam_role"" ""example"" {
  name               = ""example""
  path               = ""/""
  assume_role_policy = data.aws_iam_policy_document.example.json
}

data ""aws_iam_policy_document"" ""example"" {
  statement {
    actions = [""sts:AssumeRole""]

    principals {
      type        = ""Service""
      identifiers = [""sagemaker.amazonaws.com""]
    }
  }
}",Has a sagemaker domain resources which requires a an iam role and a subnet,"I'll help you create a SageMaker Domain resource using Terraform. This will include the necessary provider configuration and required resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC and Subnet are required for SageMaker Domain
resource ""aws_vpc"" ""sagemaker_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""sagemaker-vpc""
  }
}

resource ""aws_subnet"" ""sagemaker_subnet"" {
  vpc_id     = aws_vpc.sagemaker_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-subnet""
  }
}

# IAM Role for SageMaker
resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-domain-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the role
resource ""aws_iam_role_policy_attachment"" ""sagemaker_full_access"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

# SageMaker Domain
resource ""aws_sagemaker_domain"" ""domain"" {
  domain_name = ""my-sagemaker-domain""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.sagemaker_vpc.id
  subnet_ids  = [aws_subnet.sagemaker_subnet.id]

  default_user_settings {
    execution_role = aws_iam_role.sagemaker_role.arn

    jupyter_server_app_settings {
      default_resource_spec {
        instance_type = ""ml.t3.medium""
      }
    }
  }

  tags = {
    Environment = ""development""
  }
}
```

This configuration creates:
1. A VPC and subnet for the SageMaker Domain
2. An IAM role with necessary permissions for SageMaker
3. A SageMaker Domain with basic settings

The domain is configured to:
- Use IAM authentication
- Run in the specified VPC and subnet
- Use the created IAM role for execution
- Default to ml.t3.medium instances for Jupyter notebooks
- Include basic tagging

Make sure to adjust the values (region, CIDR blocks, instance types, etc.) according to your specific requirements.",,,,,,"I'll help you create a SageMaker Domain resource using Terraform. This will include the necessary provider configuration, VPC resources, IAM role, and the SageMaker Domain resource itself.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC resources needed for the SageMaker Domain
resource ""aws_vpc"" ""sagemaker_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""sagemaker-vpc""
  }
}

resource ""aws_subnet"" ""sagemaker_subnet"" {
  vpc_id     = aws_vpc.sagemaker_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-subnet""
  }
}

# IAM role for SageMaker
resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-domain-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the role
resource ""aws_iam_role_policy_attachment"" ""sagemaker_full_access"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

# SageMaker Domain resource
resource ""aws_sagemaker_domain"" ""domain"" {
  domain_name = ""my-sagemaker-domain""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.sagemaker_vpc.id
  subnet_ids  = [aws_subnet.sagemaker_subnet.id]

  default_user_settings {
    execution_role = aws_iam_role.sagemaker_role.arn

    security_groups = []

    jupyter_server_app_settings {
      default_resource_spec {
        instance_type = ""ml.t3.medium""
      }
    }
  }

  retention_policy {
    home_efs_file_system = ""Delete""
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC and subnet for the SageMaker Domain
3. Creates an IAM role with the necessary permissions for SageMaker
4. Creates a SageMaker Domain with:
   - IAM authentication
   - VPC configuration
   - Default user settings including execution role and Jupyter server settings
   - Retention policy for the EFS file system
   - Basic tagging

The domain will use IAM authentication and will be deployed in the specified VPC and subnet. The default user settings include a ml.t3.medium instance type for Jupyter notebooks, and the domain will use the created IAM role for execution permissions.",,,,,,"I'll help you create a Terraform configuration for an AWS SageMaker Domain resource. Here's a complete example that includes the required provider, necessary IAM roles, VPC resources, and the SageMaker Domain configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC Configuration
resource ""aws_vpc"" ""sagemaker_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""sagemaker-vpc""
  }
}

resource ""aws_subnet"" ""sagemaker_subnet"" {
  vpc_id     = aws_vpc.sagemaker_vpc.id
  cidr_block = ""10.0.1.0/24""

  tags = {
    Name = ""sagemaker-subnet""
  }
}

# IAM Role for SageMaker
resource ""aws_iam_role"" ""sagemaker_role"" {
  name = ""sagemaker-domain-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""sagemaker.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the role
resource ""aws_iam_role_policy_attachment"" ""sagemaker_full_access"" {
  role       = aws_iam_role.sagemaker_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AmazonSageMakerFullAccess""
}

# SageMaker Domain
resource ""aws_sagemaker_domain"" ""domain"" {
  domain_name = ""my-sagemaker-domain""
  auth_mode   = ""IAM""
  vpc_id      = aws_vpc.sagemaker_vpc.id
  subnet_ids  = [aws_subnet.sagemaker_subnet.id]

  default_user_settings {
    execution_role = aws_iam_role.sagemaker_role.arn

    jupyter_server_app_settings {
      default_resource_spec {
        instance_type = ""ml.t3.medium""
      }
    }
  }

  retention_policy {
    home_efs_file_system = ""Delete""
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates a VPC and subnet for the SageMaker Domain
3. Creates an IAM role with the necessary permissions for SageMaker
4. Creates a SageMaker Domain with:
   - IAM authentication mode
   - VPC and subnet configuration
   - Default user settings including execution role and Jupyter server settings
   - Retention policy for the EFS file system
   - Basic tagging

The domain will be accessible via IAM authentication and will use the specified VPC and subnet for network isolation. The default instance type for Jupyter notebooks is set to ml.t3.medium, but you can adjust this based on your needs.

Remember to:
- Adjust the region in the provider block as needed
- Modify the CIDR blocks for VPC and subnet according to your network requirements
- Customize the domain name and tags
- Add any additional specific configurations required for your use case",,,,,
"aws_eks_cluster, aws_iam_role, aws_iam_role_policy_attachment, aws_subnet, aws_subnet, aws_vpc","Generate a Terraform configuration to create an AWS IAM role called ""demo"" with attribute name set to ""eks-cluster-demo"", for an Amazon EKS cluster. The role should allow assuming roles for EKS services and have the AmazonEKSClusterPolicy attached. Additionally, create an AWS EKS cluster with the name ""demo"" and configure its VPC with specified subnet IDs from in the us-east-1 region. Ensure dependency handling for the IAM role's policy attachment during EKS cluster management.","package terraform.analysis

default has_aws_iam_role = false
default has_aws_iam_role_policy_attachment = false
default has_aws_eks_cluster = false

find_resource(rtype, rname) = resource {
    resource = input.configuration.root_module.resources[_]
    resource.type == rtype
    resource.name == rname
}

has_aws_iam_role {
    r := find_resource(""aws_iam_role"", ""demo"")
    r.expressions.name.constant_value == ""eks-cluster-demo""
    r.expressions.assume_role_policy
}

has_aws_iam_role_policy_attachment {
    r := input.configuration.root_module.resources[_]
    r.type == ""aws_iam_role_policy_attachment""
    r.expressions.policy_arn.constant_value == ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
    r.expressions.role.references[_] == ""aws_iam_role.demo.name""
}

has_aws_eks_cluster {
    r := find_resource(""aws_eks_cluster"", ""demo"")
    r.expressions.role_arn.references[_] == ""aws_iam_role.demo.arn""
    count(r.expressions.vpc_config[_].subnet_ids.references) >= 4
    count(r.depends_on) == 1
}

valid_config {
    has_aws_iam_role
    has_aws_iam_role_policy_attachment
    has_aws_eks_cluster
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_subnet"" ""public-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
}

resource ""aws_iam_role"" ""example"" {
  name = ""eks-cluster-demo""

  assume_role_policy = <<POLICY
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""Service"": ""eks.amazonaws.com""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}
POLICY
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.example.name
}

resource ""aws_eks_cluster"" ""demo"" {
  name     = ""cluster-name""
  role_arn = aws_iam_role.example.arn

  vpc_config {
    subnet_ids = [
      aws_subnet.private-us-east-1a.id,
      aws_subnet.public-us-east-1a.id,
    ]
  }

  depends_on = [aws_iam_role_policy_attachment.eks_cluster_policy,
                aws_iam_role_policy_attachment.eks_service_policy]
}","has one ""aws_iam_role""
with one ""name""
with one ""assume_role_policy"" 

has one ""aws_iam_role_policy_attachment"" 
with one ""policy_arn""
with one ""role"" referencing the ""aws_iam_role.demo.name""

has one ""variable"" block for ""cluster_name""
with one ""default""
with one ""type""
with one ""description""
with one ""nullable""

has one ""aws_eks_cluster"" 
with one ""name""
with one ""role_arn"" referencing the ""aws_iam_role.demo.arn""
with one ""vpc_config"" containing ""subnet_ids""
with dependencies on one IAM role policy attachment","Based on the provided examples and the current question, I'll help you create a Terraform configuration that meets your requirements. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_subnet"" ""public"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
}

resource ""aws_iam_role"" ""demo"" {
  name = ""eks-cluster-demo""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""eks.amazonaws.com""
        }
        Action = ""sts:AssumeRole""
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.demo.name
}

resource ""aws_eks_cluster"" ""demo"" {
  name     = ""demo""
  version  = ""1.27""
  role_arn = aws_iam_role.demo.arn

  vpc_config {
    endpoint_private_access = false
    endpoint_public_access  = true
    subnet_ids = [
      aws_subnet.private.id,
      aws_subnet.public.id
    ]
  }

  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy
  ]
}
```

This configuration:
1. Sets up the AWS provider for us-east-1 region
2. Creates a VPC with public and private subnets
3. Creates an IAM role named ""eks-cluster-demo""
4. Attaches the AmazonEKSClusterPolicy to the role
5. Creates an EKS cluster named ""demo"" with:
   - The created IAM role
   - VPC configuration using the created subnets
   - Proper dependency handling through depends_on
6. Uses the latest common EKS version (1.27)

The configuration follows AWS best practices and ensures proper dependency handling between the IAM role, policy attachments, and the EKS cluster.",,,,,,"Based on the provided examples and requirements, I'll help you create a Terraform configuration for an EKS cluster with the specified IAM role settings. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_subnet"" ""public"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
}

resource ""aws_iam_role"" ""demo"" {
  name = ""eks-cluster-demo""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""eks.amazonaws.com""
        }
        Action = ""sts:AssumeRole""
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.demo.name
}

resource ""aws_eks_cluster"" ""demo"" {
  name     = ""demo""
  version  = ""1.27""
  role_arn = aws_iam_role.demo.arn

  vpc_config {
    endpoint_private_access = true
    endpoint_public_access  = true
    subnet_ids = [
      aws_subnet.private.id,
      aws_subnet.public.id
    ]
  }

  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy
  ]
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC and necessary subnets
3. Creates an IAM role named ""eks-cluster-demo"" with the specified assume role policy
4. Attaches the AmazonEKSClusterPolicy to the role
5. Creates an EKS cluster named ""demo"" with the specified role and VPC configuration
6. Ensures proper dependency handling using depends_on

The configuration follows AWS best practices and includes all the required components for a functional EKS cluster setup.",,,,,,"Based on the provided examples and requirements, I'll help you create a Terraform configuration for your EKS cluster setup. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
}

resource ""aws_subnet"" ""private-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.1.0/24""
}

resource ""aws_subnet"" ""public-us-east-1a"" {
  vpc_id     = aws_vpc.main.id
  cidr_block = ""10.0.2.0/24""
}

resource ""aws_iam_role"" ""demo"" {
  name = ""eks-cluster-demo""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Principal = {
          Service = ""eks.amazonaws.com""
        }
        Action = ""sts:AssumeRole""
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""eks_cluster_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSClusterPolicy""
  role       = aws_iam_role.demo.name
}

resource ""aws_iam_role_policy_attachment"" ""eks_service_policy"" {
  policy_arn = ""arn:aws:iam::aws:policy/AmazonEKSServicePolicy""
  role       = aws_iam_role.demo.name
}

resource ""aws_eks_cluster"" ""demo"" {
  name     = ""demo""
  version  = ""1.24""
  role_arn = aws_iam_role.demo.arn

  vpc_config {
    endpoint_private_access = false
    endpoint_public_access  = true

    subnet_ids = [
      aws_subnet.private-us-east-1a.id,
      aws_subnet.public-us-east-1a.id
    ]
  }

  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy,
    aws_iam_role_policy_attachment.eks_service_policy
  ]
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC and necessary subnets
3. Creates an IAM role named ""eks-cluster-demo"" with the specified assume role policy
4. Attaches the required EKS policies to the IAM role
5. Creates an EKS cluster named ""demo"" with the specified configuration
6. Ensures proper dependency handling using depends_on
7. Configures the cluster with both private and public subnet access
8. Uses the us-east-1 region as specified

The configuration follows AWS best practices and includes all necessary components for a functional EKS cluster setup.",,,,,
"aws_instance, aws_lb, aws_lb_listener, aws_lb_target_group, aws_lb_target_group_attachment, aws_subnet, aws_subnet, aws_vpc",generate a Load Balancer with ALB Target Group,"package terraform.validation

default is_valid_configuration = false

# has one asw_lb resource
is_valid_lb {
        is_valid_app_lb
        has_valid_subnet
}

is_valid_app_lb {
    resource := input.planned_values.root_module.resources[_]
    resource.type == ""aws_lb""
    resource.values.load_balancer_type == ""application""
}

has_valid_subnet {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.values.subnet_mapping != null
}

has_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb""
        resource.expressions.subnets
}

# has one vpc resource
is_valid_vpc {
        have_required_vpc_argument
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

have_required_vpc_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_vpc""
        resource.expressions.ipv4_ipam_pool_id.constant_value != null
}

# has valid subnet
is_valid_subnet {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.vpc_id != null
        have_required_subnet_argument
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.cidr_block != null
}

have_required_subnet_argument {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
        resource.expressions.ipv6_cider_block != null
}

# Validate aws_lb_listener with the required arguments
is_valid_lb_listener {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
    resource.expressions.load_balancer_arn
    has_valid_default_action
}
# if type is forward, must have target_group_arn
has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value == ""forward""
    resource.expressions.default_action[0].target_group_arn != null
}

has_valid_default_action {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_listener""
        resource.expressions.default_action[0].type.constant_value != ""forward""
}

# if target type is instance, ip, or alb, should have protocol, port, and vpc_id
is_valid_lb_target_group {
    resources := input.planned_values.root_module.resources[_]
    resources.type == ""aws_lb_target_group""
    resources.values.target_type == ""alb""
    
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group""
        resource.expressions.port != null
        resource.expressions.protocol != null
        resource.expressions.vpc_id != null
}

# Validate aws_lb_target_group_attachment with the required arguments
is_valid_lb_target_group_attachment {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_lb_target_group_attachment""
        resource.expressions.target_group_arn
        resource.expressions.target_id
}

# Validate at least one aws_instance with the required arguments
is_valid_instance {
        count(valid_instances) > 0
}

valid_instances[instance] {
        instance := input.configuration.root_module.resources[_]
        instance.type == ""aws_instance""
        requited_argument(instance)
}

requited_argument(instance) {
        instance.expressions.launch_template != null
}

requited_argument(instance) {
        instance.expressions.ami != null
        instance.expressions.instance_type != null
}

# Aggregate validation
is_valid_configuration {
    is_valid_vpc
    is_valid_subnet
        is_valid_lb
        is_valid_lb_listener
        is_valid_lb_target_group_attachment
        is_valid_lb_target_group
        is_valid_instance
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name = ""main-vpc""
  cidr = ""10.0.0.0/16""

  azs                  = data.aws_availability_zones.available.names
  public_subnets       = [""10.0.4.0/24"", ""10.0.5.0/24"", ""10.0.6.0/24""]
}

data ""aws_ami"" ""ubuntu"" {
  most_recent = true
  owners      = [""amazon""]

  filter {
    name   = ""name""
    values = [""*ubuntu-noble-24.04-amd64-server-*""]
  }
}

resource ""aws_security_group"" ""instance-sg"" {
  name = ""instance-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""instance-sg-ingress-rule"" {
  from_port       = 80
  to_port         = 80
  ip_protocol     = ""tcp""
  referenced_security_group_id = aws_security_group.lb-sg.id
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""instance-sg-egress-rule"" {
  from_port       = 0
  to_port         = 0
  ip_protocol     = ""-1""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.instance-sg.id
}

resource ""aws_security_group"" ""lb-sg"" {
  name = ""lb-sg""
  vpc_id = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""lb-sg-ingress-rule"" {
  from_port   = 80
  to_port     = 80
  ip_protocol = ""tcp""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""lb-sg-egress-rule"" {
  from_port   = 0
  to_port     = 0
  ip_protocol = ""-1""
  cidr_ipv4   = ""0.0.0.0/0""
  security_group_id = aws_security_group.lb-sg.id
}


resource ""aws_instance"" ""my_instance"" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = ""t2.micro""
  subnet_id     = module.vpc.public_subnets[0]
  security_groups = [ aws_security_group.instance-sg.id ]
}

resource ""aws_lb"" ""test"" {
  subnets            = module.vpc.public_subnets
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.lb-sg.id]
}

resource ""aws_lb_listener"" ""my_listener"" {
  load_balancer_arn = aws_lb.test.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}

resource ""aws_lb_target_group"" ""target_group"" {
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id
}

resource ""aws_lb_target_group_attachment"" ""ec2_attach"" {
  target_group_arn = aws_lb_target_group.target_group.arn
  target_id = aws_instance.my_instance.id
}","has one ""aws_lb"" resource
    with one of either ""subnets"" or ""subnet_mapping""
    with load_balancer_type = ""application""

has at least two aws_subnet
    has one of cidr_block, ipv6_cidr_block argument

has one aws_vpc
    has one of cider_block, ipv4_ipam_pool_id 

has one ""aws_lb_listener"" resource
    with one default_action argument
        with one type argument
            with one of following values: forward, redirect, fixed-response, authenticate-cognito and authenticate-oidc
    with one load_balancer_arn argument

has one ""aws_lb_target_group""
    with port
    with protocol
    with vpc_id
    with target_type = ""alb""

has one aws_lb_target_group_attachment
    with one target_group_arn
    with one target_id

has at least one ""aws_instance""
    with either launch_template or (ami and instance_type) argument","I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a target group. Based on the examples provided, I'll generate a complete and working configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Get availability zones for the region
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Create VPC
resource ""aws_vpc"" ""main"" {
  cidr_block = ""10.0.0.0/16""
  
  enable_dns_hostnames = true
  enable_dns_support   = true
  
  tags = {
    Name = ""main-vpc""
  }
}

# Create public subnets
resource ""aws_subnet"" ""public"" {
  count             = 2
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.${count.index + 1}.0/24""
  availability_zone = data.aws_availability_zones.available.names[count.index]
  
  map_public_ip_on_launch = true
  
  tags = {
    Name = ""public-subnet-${count.index + 1}""
  }
}

# Create security group for ALB
resource ""aws_security_group"" ""alb"" {
  name        = ""alb-sg""
  description = ""Security group for ALB""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Create Application Load Balancer
resource ""aws_lb"" ""alb"" {
  name               = ""my-alb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb.id]
  subnets            = aws_subnet.public[*].id

  enable_deletion_protection = false

  tags = {
    Name = ""my-alb""
  }
}

# Create ALB Target Group
resource ""aws_lb_target_group"" ""target_group"" {
  name     = ""my-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = aws_vpc.main.id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }

  tags = {
    Name = ""my-target-group""
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.alb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}
```

This configuration creates:
1. A VPC with two public subnets across different availability zones
2. A security group for the ALB allowing HTTP traffic
3. An Application Load Balancer (ALB)
4. A target group configured for HTTP traffic
5. A listener that forwards HTTP traffic to the target group

The ALB is set up as an internet-facing load balancer that can distribute traffic to targets in the specified target group. You can attach targets (like EC2 instances) to this target group separately using `aws_lb_target_group_attachment` resources.

Note that this is a",,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a target group. Based on the examples provided, I'll create a solution that includes the necessary components.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Create VPC (required for ALB)
module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""alb-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.available.names
  public_subnets  = [""10.0.1.0/24"", ""10.0.2.0/24""]
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Create Application Load Balancer
resource ""aws_lb"" ""alb"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = module.vpc.public_subnets

  enable_deletion_protection = false
}

# Create Target Group
resource ""aws_lb_target_group"" ""target_group"" {
  name     = ""my-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.alb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.target_group.arn
  }
}
```

This configuration creates:
1. A VPC with public subnets across multiple availability zones
2. A security group for the ALB allowing HTTP traffic
3. An Application Load Balancer
4. A target group with health check configuration
5. A listener that forwards HTTP traffic to the target group

Note that this creates the basic infrastructure. You would need to separately create and register targets (like EC2 instances) with the target group to start receiving traffic.",,,,,,"I'll help you create a Terraform configuration for an Application Load Balancer (ALB) with a target group. Based on the examples provided, I'll create a solution that includes the necessary components.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Get available AZs
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Create VPC and subnets
module ""vpc"" {
  source = ""terraform-aws-modules/vpc/aws""

  name = ""alb-vpc""
  cidr = ""10.0.0.0/16""

  azs             = data.aws_availability_zones.available.names
  public_subnets  = [""10.0.1.0/24"", ""10.0.2.0/24""]
}

# Security Group for ALB
resource ""aws_security_group"" ""alb_sg"" {
  name        = ""alb-security-group""
  description = ""Security group for ALB""
  vpc_id      = module.vpc.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# Create Application Load Balancer
resource ""aws_lb"" ""application_lb"" {
  name               = ""my-application-lb""
  internal           = false
  load_balancer_type = ""application""
  security_groups    = [aws_security_group.alb_sg.id]
  subnets            = module.vpc.public_subnets

  enable_deletion_protection = false

  tags = {
    Environment = ""production""
  }
}

# Create ALB Target Group
resource ""aws_lb_target_group"" ""alb_tg"" {
  name     = ""my-alb-target-group""
  port     = 80
  protocol = ""HTTP""
  vpc_id   = module.vpc.vpc_id

  health_check {
    enabled             = true
    healthy_threshold   = 2
    interval            = 30
    matcher            = ""200""
    path               = ""/""
    port               = ""traffic-port""
    protocol           = ""HTTP""
    timeout            = 5
    unhealthy_threshold = 2
  }
}

# Create ALB Listener
resource ""aws_lb_listener"" ""front_end"" {
  load_balancer_arn = aws_lb.application_lb.arn
  port              = ""80""
  protocol          = ""HTTP""

  default_action {
    type             = ""forward""
    target_group_arn = aws_lb_target_group.alb_tg.arn
  }
}
```

This configuration creates:
1. A VPC with public subnets across multiple availability zones
2. A security group for the ALB that allows HTTP traffic
3. An Application Load Balancer
4. A target group with health check configuration
5. A listener that forwards traffic to the target group

The ALB is set up as an internet-facing load balancer (internal = false) and will distribute HTTP traffic (port 80) to the targets in the target group. The target group is configured with basic health checks to monitor the health of the registered targets.

Note that you'll need to separately register targets (like EC2 instances) to the target group using either `aws_lb_target_group_attachment` or through auto-scaling groups.",,,,,
aws_elastic_beanstalk_application,Provision a resource to deploy and scale a web application that was developed with supported programming languages.,"package terraform.validation

default is_configuration_valid = false

default is_valid_eb_app = false


is_valid_eb_app {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_elastic_beanstalk_application""
    resource.expressions.name
}


is_configuration_valid {
    is_valid_eb_app
}",1,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_elastic_beanstalk_application"" ""tftest"" {
  name        = ""tf-test-name""
  description = ""tf-test-desc""
}","Has one ""aws_elastic_beanstalk_application"" resource
    with ""name""","I'll help you create a Terraform configuration to deploy and scale a web application using AWS Elastic Beanstalk, which is an ideal service for this purpose. Based on the provided examples, I'll create a solution that includes the necessary components.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM Role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policies to the IAM role
resource ""aws_iam_role_policy_attachment"" ""eb_web_tier"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Create an instance profile for the EC2 instances
resource ""aws_iam_instance_profile"" ""eb_instance_profile"" {
  name = ""eb_instance_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Create Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""web_app"" {
  name        = ""web-application""
  description = ""Web application deployment with Elastic Beanstalk""
}

# Create Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""web_app_env"" {
  name                = ""web-app-production""
  application         = aws_elastic_beanstalk_application.web_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_instance_profile.name
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:command""
    name      = ""DeploymentPolicy""
    value     = ""Rolling""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""MeasureName""
    value     = ""CPUUtilization""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""Statistic""
    value     = ""Average""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""Unit""
    value     = ""Percent""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""UpperThreshold""
    value     = ""80""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""LowerThreshold""
    value     = ""40""
  }
}
```

This Terraform configuration:

1. Sets up the required AWS provider
2. Creates an IAM role",,,,,,"I'll help you create a Terraform configuration to deploy and scale a web application using AWS Elastic Beanstalk, which is ideal for this use case. Based on the examples provided, I'll create a simplified but complete configuration that includes the necessary resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM Role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""ec2.amazonaws.com""
        }
      }
    ]
  })
}

# Attach necessary policy for Elastic Beanstalk
resource ""aws_iam_role_policy_attachment"" ""eb_web_tier"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Create instance profile for EC2 instances
resource ""aws_iam_instance_profile"" ""eb_instance_profile"" {
  name = ""elastic_beanstalk_instance_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Create Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""web_app"" {
  name        = ""web-application""
  description = ""Web application deployment environment""
}

# Create Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""web_app_env"" {
  name                = ""web-app-production""
  application         = aws_elastic_beanstalk_application.web_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_instance_profile.name
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""ENVIRONMENT""
    value     = ""production""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:healthreporting:system""
    name      = ""SystemType""
    value     = ""enhanced""
  }
}
```

This configuration:

1. Sets up the required AWS provider
2. Creates an IAM role and instance profile for the Elastic Beanstalk EC2 instances
3. Creates an Elastic Beanstalk application
4. Creates an Elastic Beanstalk environment with:
   - Auto-scaling configuration (min 1, max 4 instances)
   - Load balanced environment
   - Enhanced health monitoring
   - Python 3.9 platform (can be changed to other supported languages)
   - Production environment variable

The environment will automatically handle:
- Load balancing
- Auto-scaling
- Application health monitoring
- Platform updates
- Application deployment

You can modify the `solution_stack_name` to support other programming languages like Java, .NET, Node.js, PHP, Ruby, Go, or Docker based on your application's",,,,,,"I'll help you create a Terraform configuration to deploy and scale a web application using AWS Elastic Beanstalk, which is an ideal service for this purpose. Based on the examples provided, I'll create a solution that includes the necessary components.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
}

# IAM Role for Elastic Beanstalk EC2 instances
resource ""aws_iam_role"" ""eb_ec2_role"" {
  name = ""elastic_beanstalk_ec2_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17"",
    Statement = [
      {
        Action = ""sts:AssumeRole"",
        Effect = ""Allow"",
        Principal = {
          Service = ""ec2.amazonaws.com""
        },
      },
    ]
  })
}

# Attach the AWS managed policy for Elastic Beanstalk
resource ""aws_iam_role_policy_attachment"" ""eb_web_tier"" {
  role       = aws_iam_role.eb_ec2_role.name
  policy_arn = ""arn:aws:iam::aws:policy/AWSElasticBeanstalkWebTier""
}

# Create an instance profile
resource ""aws_iam_instance_profile"" ""eb_instance_profile"" {
  name = ""elastic_beanstalk_instance_profile""
  role = aws_iam_role.eb_ec2_role.name
}

# Create Elastic Beanstalk Application
resource ""aws_elastic_beanstalk_application"" ""web_app"" {
  name        = ""web-application""
  description = ""Web application deployment with Elastic Beanstalk""
}

# Create Elastic Beanstalk Environment
resource ""aws_elastic_beanstalk_environment"" ""web_app_env"" {
  name                = ""web-app-production""
  application         = aws_elastic_beanstalk_application.web_app.name
  solution_stack_name = ""64bit Amazon Linux 2023 v4.3.0 running Python 3.9""

  setting {
    namespace = ""aws:autoscaling:launchconfiguration""
    name      = ""IamInstanceProfile""
    value     = aws_iam_instance_profile.eb_instance_profile.name
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MinSize""
    value     = ""1""
  }

  setting {
    namespace = ""aws:autoscaling:asg""
    name      = ""MaxSize""
    value     = ""4""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:environment""
    name      = ""EnvironmentType""
    value     = ""LoadBalanced""
  }

  setting {
    namespace = ""aws:elasticbeanstalk:application:environment""
    name      = ""ENVIRONMENT""
    value     = ""production""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""MeasureName""
    value     = ""CPUUtilization""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""Statistic""
    value     = ""Average""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""Unit""
    value     = ""Percent""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""UpperThreshold""
    value     = ""75""
  }

  setting {
    namespace = ""aws:autoscaling:trigger""
    name      = ""LowerThreshold""
    value     = ""35""
  }
}
```

This Terraform configuration will:

1. Create an Elastic Beanstalk",,,,,
"aws_iam_role, aws_kinesisanalyticsv2_application, aws_iam_policy_document",Generate a basic Amazon Kinesis V2 Analytics Application to manage a SQL application,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
	some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kinesisanalytics.amazonaws.com"") 
}


has_valid_kinesisanalyticsv2_application {
	some i
	resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kinesisanalyticsv2_application""
    resource.values.name
    resource.values.runtime_environment == ""SQL-1_0""
   	role := input.configuration.root_module.resources[i]
    role.expressions.service_execution_role
}

has_valid_resources {
	has_valid_iam_role(input.resource_changes)
    has_valid_kinesisanalyticsv2_application
}",3,"provider ""aws"" {
  region = ""us-west-2""
}

data ""aws_iam_policy_document"" ""KinesisAssume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}


resource ""aws_iam_role"" ""kinesis_role"" {
  name               = ""kinesis_role""
  assume_role_policy = data.aws_iam_policy_document.KinesisAssume.json
}

resource ""aws_kinesisanalyticsv2_application"" ""flink_app"" {
  name                 = ""example-application1""
  runtime_environment  = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_role.arn
}","Has one ""aws_iam_role"" resource, and one ""aws_kinesisanalyticsv2_application"" with argument runtime_environment == ""SQL-1_0""","I'll help you create a Terraform configuration for a Kinesis Analytics V2 Application specifically for SQL processing. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM assume role policy for Kinesis Analytics
data ""aws_iam_policy_document"" ""kinesis_analytics_assume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}

# Create IAM role for Kinesis Analytics
resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name               = ""kinesis_analytics_sql_role""
  assume_role_policy = data.aws_iam_policy_document.kinesis_analytics_assume.json
}

# Create Kinesis Analytics V2 Application for SQL
resource ""aws_kinesisanalyticsv2_application"" ""sql_app"" {
  name                   = ""example-sql-application""
  runtime_environment    = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_analytics_role.arn

  application_configuration {
    sql_application_configuration {
      input {
        name_prefix = ""PREFIX_""
        
        input_schema {
          record_format {
            record_format_type = ""JSON""
            
            mapping_parameters {
              json_mapping_parameters {
                record_row_path = ""$""
              }
            }
          }
        }
      }
    }
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM assume role policy for Kinesis Analytics
3. Creates an IAM role that Kinesis Analytics can assume
4. Creates a Kinesis Analytics V2 application configured for SQL processing with:
   - SQL-1_0 runtime environment
   - Basic JSON input configuration
   - Service execution role attachment

The application is configured to process JSON-formatted input data. You can customize the input configuration based on your specific requirements.",,,,,,"I'll help you create a Terraform configuration for a Kinesis Analytics V2 Application specifically for SQL processing. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM role for Kinesis Analytics
data ""aws_iam_policy_document"" ""kinesis_analytics_assume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}

resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name               = ""kinesis_analytics_sql_role""
  assume_role_policy = data.aws_iam_policy_document.kinesis_analytics_assume.json
}

# Kinesis Analytics V2 Application for SQL
resource ""aws_kinesisanalyticsv2_application"" ""sql_app"" {
  name                   = ""example-sql-application""
  runtime_environment    = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_analytics_role.arn

  application_configuration {
    sql_application_configuration {
      input {
        name_prefix = ""PREFIX""
        
        # Schema version defaults to 1
        schema_version = ""1""

        # Empty input schema as placeholder
        input_schema {
          record_format {
            record_format_type = ""JSON""
          }
        }
      }
    }
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role with the necessary assume role policy for Kinesis Analytics
3. Creates a Kinesis Analytics V2 application configured for SQL processing with:
   - SQL-1_0 runtime environment
   - Basic input configuration with JSON record format
   - Service execution role for permissions

Note that you might want to add more specific input/output configurations and IAM permissions depending on your specific use case.",,,,,,"I'll help you generate a Terraform configuration for a Kinesis Analytics V2 Application specifically for SQL processing. Here's the code:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for Kinesis Analytics
data ""aws_iam_policy_document"" ""kinesis_analytics_assume"" {
  statement {
    effect  = ""Allow""
    actions = [""sts:AssumeRole""]
    principals {
      type        = ""Service""
      identifiers = [""kinesisanalytics.amazonaws.com""]
    }
  }
}

resource ""aws_iam_role"" ""kinesis_analytics_role"" {
  name               = ""kinesis_analytics_sql_role""
  assume_role_policy = data.aws_iam_policy_document.kinesis_analytics_assume.json
}

# Kinesis Analytics V2 Application for SQL
resource ""aws_kinesisanalyticsv2_application"" ""sql_app"" {
  name                   = ""example-sql-application""
  runtime_environment    = ""SQL-1_0""
  service_execution_role = aws_iam_role.kinesis_analytics_role.arn

  application_configuration {
    sql_application_configuration {
      input {
        name_prefix = ""PREFIX_""
        
        schema_version = ""1""

        input_schema {
          record_format {
            record_format_type = ""JSON""
            mapping_parameters {
              json_mapping_parameters {
                record_row_path = ""$""
              }
            }
          }
        }
      }
    }
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role with the necessary assume role policy for Kinesis Analytics
3. Creates a Kinesis Analytics V2 application configured for SQL processing with:
   - SQL-1_0 runtime environment
   - Basic SQL application configuration with JSON input format
   - A service execution role for permissions
   - Basic input schema configuration

The application is configured to process JSON-formatted data, but you can modify the input schema and configuration based on your specific requirements.",,,,,
"aws_dax_cluster, aws_iam_role",Create a DAX cluster with node type = dax.r4.large and replication factor = 1.,"package main

import future.keywords.in

default allow = false

# Check if the AWS DynamoDB table is being created in us-east-1
aws_dax_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_dax_cluster""
    resource.change.after.node_type == ""dax.r4.large""
    resource.change.after.replication_factor == 1
}

# Aggregate all checks
allow {
    aws_dax_cluster_valid(input.resource_changes)
}",3,"provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""example"" {
  name = ""test""

  assume_role_policy = jsonencode({
    Statement = [{
      Action = ""sts:AssumeRole""
      Effect = ""Allow""
      Principal = {
        Service = ""eks.amazonaws.com""
      }
    }]
    Version = ""2012-10-17""
  })
}


resource ""aws_dax_cluster"" ""bar"" {
  cluster_name       = ""cluster-example""
  iam_role_arn       = aws_iam_role.example.arn
  node_type          = ""dax.r4.large""
  replication_factor = 1
}","require a cluster name, iam_role_arn, node_type, replication factor","I'll help you create a DAX cluster with the specified node type and replication factor. This will require creating both the IAM role for DAX and the cluster itself.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
      }
    ]
  })
}

# Create the DAX cluster
resource ""aws_dax_cluster"" ""dax_cluster"" {
  cluster_name       = ""my-dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role that DAX can assume
3. Creates a DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - The IAM role created above

Note that you'll need to ensure you have:
- Appropriate VPC and subnet configuration
- Necessary permissions to create these resources
- The AWS region configured where you want to deploy these resources",,,,,,"I'll help you create a DAX cluster with the specified configurations. Based on the example provided, we'll need to create both the IAM role for DAX and the cluster itself.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
        Sid = """"
      }
    ]
  })
}

# Create DAX cluster
resource ""aws_dax_cluster"" ""dax_cluster"" {
  cluster_name       = ""my-dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role that DAX can assume
3. Creates a DAX cluster with:
   - Node type: dax.r4.large
   - Replication factor: 1
   - The IAM role created above
   - A cluster name ""my-dax-cluster""

The cluster will be created with the specified node type and replication factor as requested. Make sure you have appropriate VPC and subnet configurations in your AWS environment before deploying this DAX cluster.",,,,,,"I'll help you create a DAX cluster with the specified node type and replication factor. Based on the example provided, we'll need to create both the IAM role for DAX and the cluster itself.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

# Create IAM role for DAX
resource ""aws_iam_role"" ""dax_role"" {
  name = ""dax_service_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dax.amazonaws.com""
        }
        Sid = """"
      }
    ]
  })
}

# Create DAX cluster
resource ""aws_dax_cluster"" ""dax_cluster"" {
  cluster_name       = ""my-dax-cluster""
  iam_role_arn      = aws_iam_role.dax_role.arn
  node_type         = ""dax.r4.large""
  replication_factor = 1
}
```

This configuration will:
1. Set up the required AWS provider
2. Create an IAM role that DAX can assume
3. Create a DAX cluster with the specified node type (dax.r4.large) and replication factor (1)

The cluster will use the IAM role created in the same configuration for its permissions. Make sure you have the appropriate VPC and subnet configurations in your AWS account before applying this configuration.",,,,,
"aws_redshift_cluster, aws_redshift_usage_limit",Create a 2 node RedShift cluster and limit the concurrency scaling to 60 minutes,"package main

import future.keywords.in

default allow = false

aws_redshift_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_cluster""
    resource.change.after.number_of_nodes == 2
}

aws_redshift_usage_limit_valid(resources) {
    some resource in resources
    resource.type == ""aws_redshift_usage_limit""
    resource.change.after.feature_type == ""concurrency-scaling""
    resource.change.after.limit_type == ""time""
    resource.change.after.amount == 60
}

# Aggregate all checks
allow {
    aws_redshift_cluster_valid(input.resource_changes)
    aws_redshift_usage_limit_valid(input.resource_changes)
}
",2,"resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type          = ""dc2.large""
  number_of_nodes    = 2

  database_name = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  skip_final_snapshot = true
}

resource ""aws_redshift_usage_limit"" ""example"" {
  cluster_identifier = aws_redshift_cluster.example.id
  feature_type       = ""concurrency-scaling""
  limit_type         = ""time""
  amount             = 60
}","Has an aws_redshift_cluster resource and check cluster_type is ""single_node"" or number of nodes is 2, check there is a aws_redshift_usage_limit resouce where the feature type is concurrency_scaling, limit_type is ""time"", and amount is 60","I'll help you create a Terraform configuration for a 2-node RedShift cluster with concurrency scaling limited to 60 minutes. I'll use the provided examples as reference and add the necessary concurrency scaling configuration.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type         = ""dc2.large""
  cluster_type      = ""multi-node""
  number_of_nodes   = 2

  database_name   = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  # Concurrency scaling configuration
  concurrency_scaling = ""on""
  # Set maximum concurrency scaling time to 60 minutes (3600 seconds)
  max_concurrency_scaling_clusters = 1
  aqua_configuration_status       = ""auto""
  
  # Additional recommended settings
  skip_final_snapshot = true
  
  # Add tags for better resource management
  tags = {
    Name        = ""redshift-cluster-1""
    Environment = ""production""
  }
}

resource ""aws_redshift_parameter_group"" ""concurrency_scaling"" {
  name   = ""concurrency-scaling-param-group""
  family = ""redshift-1.0""

  parameter {
    name  = ""max_concurrency_scaling_time""
    value = ""3600""  # 60 minutes in seconds
  }
}
```

This configuration:
1. Creates a 2-node RedShift cluster
2. Enables concurrency scaling
3. Sets the maximum concurrency scaling time to 60 minutes (3600 seconds)
4. Uses a parameter group to enforce the concurrency scaling time limit
5. Includes the required AWS provider configuration
6. Adds basic security and management settings

The cluster is configured with:
- Multi-node setup (2 nodes)
- dc2.large node type
- Basic database configuration
- Concurrency scaling enabled with a 60-minute limit
- Skip final snapshot for easier cleanup (modify this for production use)",,,,,,"I'll help you create a Terraform configuration for a 2-node RedShift cluster with concurrency scaling limited to 60 minutes. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type         = ""dc2.large""
  cluster_type      = ""multi-node""
  number_of_nodes   = 2

  database_name   = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  # Configure concurrency scaling
  concurrency_scaling = ""enabled""
  # Set maximum concurrency scaling time to 60 minutes (3600 seconds)
  max_concurrency_scaling_minutes = 60

  # Additional recommended settings
  skip_final_snapshot     = true
  publicly_accessible     = false
  enhanced_vpc_routing    = true
  maintenance_track_name  = ""current""

  # Add tags for better resource management
  tags = {
    Name        = ""redshift-cluster-1""
    Environment = ""production""
  }
}
```

This configuration:
1. Creates a 2-node RedShift cluster
2. Enables concurrency scaling
3. Limits the concurrency scaling to 60 minutes
4. Includes basic security and maintenance settings
5. Adds relevant tags for resource management

The cluster uses the dc2.large node type and includes basic configuration parameters while specifically addressing your requirement for limiting concurrency scaling to 60 minutes. The `concurrency_scaling` parameter is set to ""enabled"" and `max_concurrency_scaling_minutes` is set to 60 to enforce the time limit you specified.",,,,,,"I'll help you create a Terraform configuration for a 2-node RedShift cluster with concurrency scaling limited to 60 minutes. I'll use the provided examples as reference and add the necessary configuration for concurrency scaling.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_redshift_cluster"" ""example"" {
  cluster_identifier = ""redshift-cluster-1""
  node_type         = ""dc2.large""
  cluster_type      = ""multi-node""
  number_of_nodes   = 2

  database_name   = ""mydb""
  master_username = ""foo""
  master_password = ""Mustbe8characters""

  # Concurrency scaling configuration
  concurrency_scaling = ""enabled""
  # Set maximum concurrency scaling time to 60 minutes (in seconds)
  max_concurrency_scaling_minutes = 60

  skip_final_snapshot = true

  # Additional recommended settings
  encrypted          = true
  publicly_accessible = false
  
  tags = {
    Name        = ""redshift-cluster-1""
    Environment = ""production""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a 2-node RedShift cluster
3. Enables concurrency scaling
4. Limits the concurrency scaling to 60 minutes
5. Includes some recommended security settings like encryption and private access
6. Adds basic tagging

The cluster will automatically scale up and down based on workload demands, but the total time spent in concurrency scaling mode will be limited to 60 minutes. This helps control costs while still providing flexibility for handling peak workloads.",,,,,
"aws_db_proxy, aws_db_subnet_group, aws_iam_role, aws_rds_cluster, aws_secretsmanager_secret, aws_secretsmanager_secret_version, aws_security_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc","Set up an AWS RDS Aurora MySQL cluster with a proxy for enhanced connection management, including VPC, subnets, security groups, IAM roles, and Secrets Manager for secure credential storage. The backup retention period for rds cluster should be 5, and the preffered backup window is 7 am to 9 am.","package terraform.validation

default is_valid_aws_db_proxy = false

default is_valid_aws_rds_cluster = false

default is_valid_aws_vpc = false

default is_valid_aws_subnet = false

default is_valid_aws_security_group = false

default is_valid_aws_db_subnet_group = false

default is_valid_aws_secretsmanager_secret = false

default is_valid_aws_iam_role = false

# Validate aws_db_proxy resource
is_valid_aws_db_proxy {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_proxy""
        resource.expressions.engine_family.constant_value == ""MYSQL""
        resource.expressions.require_tls.constant_value == true
        auth := resource.expressions.auth[_]
        auth.auth_scheme.constant_value == ""SECRETS""
}

# Validate aws_rds_cluster resource
is_valid_aws_rds_cluster {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_rds_cluster""
        resource.expressions.engine.constant_value == ""aurora-mysql""
        resource.expressions.master_username != null
        resource.expressions.master_password != null
    resource.expressions.preferred_backup_window.constant_value == ""07:00-09:00""
    resource.expressions.backup_retention_period.constant_value == 5
}

# Validate aws_vpc resource
is_valid_aws_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block != null
}

# Validate aws_subnet resource
is_valid_aws_subnet {
        count([x |
                resource := input.configuration.root_module.resources[x]
                resource.type == ""aws_subnet""
        ]) == 2 # Ensure there are exactly two subnet instances
}

# Validate aws_security_group resource
is_valid_aws_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""

        # Checks for at least one ingress and one egress rule, more specific validation can be added
        count(resource.expressions.ingress) > 0
        count(resource.expressions.egress) > 0
}

# Validate aws_db_subnet_group resource
is_valid_aws_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}

# Validate aws_secretsmanager_secret resource
is_valid_aws_secretsmanager_secret {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_secretsmanager_secret""
        resource.expressions.name != null
}

# Validate aws_iam_role resource
is_valid_aws_iam_role {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_iam_role""
        resource.expressions.assume_role_policy != null
}

",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port       = 3306
  to_port         = 3306
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port       = 3306
  to_port         = 3306
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

resource ""aws_rds_cluster"" ""example"" {
  cluster_identifier      = ""example-cluster""
  engine                  = ""aurora-mysql""
  engine_version          = ""8.0.mysql_aurora.3.08.0""
  master_username         = ""myusername""
  master_password         = ""password123""
  database_name           = ""exampledb""
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot     = true
  vpc_security_group_ids  = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name    = aws_db_subnet_group.main.name
}

resource ""aws_secretsmanager_secret"" ""db-credentials"" {
  name_prefix = ""db-credentials-""
}

resource ""aws_secretsmanager_secret_version"" ""db-credentials-version"" {
  secret_id     = aws_secretsmanager_secret.db-credentials.id
  secret_string = jsonencode({
    username = aws_rds_cluster.example.master_username
    password = aws_rds_cluster.example.master_password
  })
}

resource ""aws_iam_role"" ""rds-proxy-role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy"" ""rds-proxy-policy"" {
  name        = ""rds-proxy-policy""

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""secretsmanager:GetSecretValue"",
        ]
        Resource = [
          ""${aws_secretsmanager_secret_version.db-credentials-version.arn}"",
        ]
      }
    ]
  })
}

resource ""aws_iam_role_policy_attachment"" ""attach-proxy-policy"" {
  role       = aws_iam_role.rds-proxy-role.name
  policy_arn = aws_iam_policy.rds-proxy-policy.arn
}

resource ""aws_db_proxy"" ""example"" {
  name                   = ""example-proxy""
  debug_logging          = false
  engine_family          = ""MYSQL""
  idle_client_timeout    = 1800
  require_tls            = true
  role_arn               = aws_iam_role.rds-proxy-role.arn
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  vpc_subnet_ids         = module.vpc.private_subnets

  auth {
    auth_scheme  = ""SECRETS""
    secret_arn   = aws_secretsmanager_secret.db-credentials.arn
    iam_auth     = ""DISABLED""
  }
}

resource ""aws_db_proxy_default_target_group"" ""example"" {
  db_proxy_name = aws_db_proxy.example.name

  connection_pool_config {
    connection_borrow_timeout    = 120
    init_query                   = ""SET x=1, y=2""
    max_connections_percent      = 100
    max_idle_connections_percent = 50
    session_pinning_filters      = [""EXCLUDE_VARIABLE_SETS""]
  }
}

resource ""aws_db_proxy_target"" ""example"" {
  db_cluster_identifier = aws_rds_cluster.example.cluster_identifier
  db_proxy_name          = aws_db_proxy.example.name
  target_group_name      = aws_db_proxy_default_target_group.example.name
}
","has one ""aws_db_proxy"" resource
with ""engine_family"" = ""MYSQL""
with ""require_tls"" = true
with ""auth_scheme"" = ""SECRETS""

has one ""aws_rds_cluster"" resource
with ""engine"" = ""aurora-mysql""
with ""master_username"" and ""master_password"" from variables

has one ""aws_vpc"" resource
with a ""cidr_block""

has one ""aws_subnet"" resource
with two instances for different availability zones

has one ""aws_security_group"" resource
with ingress and egress rules for RDS and EC2 instances

has one ""aws_db_subnet_group"" resource
with subnet IDs

has one ""aws_secretsmanager_secret"" resource
with DB credentials

has one ""aws_iam_role"" resource
for the RDS proxy with an assume role policy","I'll help you create a Terraform configuration for an AWS RDS Aurora MySQL cluster with the specified backup settings and all necessary components. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port         = 3306
  to_port           = 3306
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port         = 3306
  to_port           = 3306
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

resource ""aws_rds_cluster"" ""example"" {
  cluster_identifier      = ""example-cluster""
  engine                  = ""aurora-mysql""
  engine_version          = ""8.0.mysql_aurora.3.08.0""
  master_username         = ""myusername""
  master_password         = ""password123""
  database_name          = ""exampledb""
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot     = true
  vpc_security_group_ids  = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name    = aws_db_subnet_group.main.name
}

resource ""aws_secretsmanager_secret"" ""db-credentials"" {
  name_prefix = ""db-credentials-""
}

resource ""aws_secretsmanager_secret_version"" ""db-credentials-version"" {
  secret_id     = aws_secretsmanager_secret.db-credentials.id
  secret_string = jsonencode({
    username = aws_rds_cluster.example.master_username
    password = aws_rds_cluster.example.master_password
  })
}

resource ""aws_iam_role"" ""rds-proxy-role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy"" ""rds-proxy-policy"" {
  name = ""rds-proxy-policy""

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ",,,,,,"I'll help you create a Terraform configuration for an AWS RDS Aurora MySQL cluster with the specified backup settings and all the required components. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port         = 3306
  to_port           = 3306
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port         = 3306
  to_port           = 3306
  ip_protocol       = ""tcp""
  cidr_ipv4         = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

resource ""aws_rds_cluster"" ""example"" {
  cluster_identifier      = ""example-cluster""
  engine                  = ""aurora-mysql""
  engine_version          = ""8.0.mysql_aurora.3.08.0""
  master_username         = ""myusername""
  master_password         = ""password123""
  database_name          = ""exampledb""
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot     = true
  vpc_security_group_ids  = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name    = aws_db_subnet_group.main.name
}

resource ""aws_secretsmanager_secret"" ""db-credentials"" {
  name_prefix = ""db-credentials-""
}

resource ""aws_secretsmanager_secret_version"" ""db-credentials-version"" {
  secret_id     = aws_secretsmanager_secret.db-credentials.id
  secret_string = jsonencode({
    username = aws_rds_cluster.example.master_username
    password = aws_rds_cluster.example.master_password
  })
}

resource ""aws_iam_role"" ""rds-proxy-role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy"" ""rds-proxy-policy"" {
  name = ""rds-proxy-policy""

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [",,,,,,"I'll help you create a Terraform configuration for an AWS RDS Aurora MySQL cluster with the specified backup settings and all necessary components. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port         = 3306
  to_port          = 3306
  ip_protocol      = ""tcp""
  cidr_ipv4        = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port         = 3306
  to_port          = 3306
  ip_protocol      = ""tcp""
  cidr_ipv4        = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}

resource ""aws_rds_cluster"" ""example"" {
  cluster_identifier      = ""example-cluster""
  engine                  = ""aurora-mysql""
  engine_version          = ""8.0.mysql_aurora.3.08.0""
  master_username         = ""myusername""
  master_password         = ""password123""
  database_name          = ""exampledb""
  backup_retention_period = 5
  preferred_backup_window = ""07:00-09:00""
  skip_final_snapshot     = true
  vpc_security_group_ids  = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name    = aws_db_subnet_group.main.name
}

resource ""aws_secretsmanager_secret"" ""db-credentials"" {
  name_prefix = ""db-credentials-""
}

resource ""aws_secretsmanager_secret_version"" ""db-credentials-version"" {
  secret_id     = aws_secretsmanager_secret.db-credentials.id
  secret_string = jsonencode({
    username = aws_rds_cluster.example.master_username
    password = aws_rds_cluster.example.master_password
  })
}

resource ""aws_iam_role"" ""rds-proxy-role"" {
  name = ""rds-proxy-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""rds.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_policy"" ""rds-proxy-policy"" {
  name = ""rds-proxy-policy""

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ",,,,,
"aws_dlm_lifecycle_policy, aws_glacier_vault",generate an S3 Glacier vault and use it with AWS Data Lifecycle Manager,"package terraform.validation

default is_valid_terraform_plan = false

# Rule to check for an AWS Glacier Vault resource with a 'name' attribute
is_valid_glacier_vault {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_glacier_vault""
        not is_null(resource.values.name)
}

# Rule to check for an AWS DLM Lifecycle Policy with required attributes
is_valid_dlm_lifecycle_policy {
        resource := input.planned_values.root_module.resources[_]
        resource.type == ""aws_dlm_lifecycle_policy""
        not is_null(resource.values.description)
#         not is_null(resource.values.execution_role_arn)
        not is_null(resource.values.policy_details)
    
    execution_role_arn_configured(resource)
}

execution_role_arn_configured(_) {
    role_resource := input.configuration.root_module.resources[_]
    role_resource.type == ""aws_dlm_lifecycle_policy""
#     role_resource.name == ""dlm_lifecycle_role""
    not is_null(role_resource.expressions.execution_role_arn)
}

# Combined rule to validate the entire Terraform plan
is_valid_terraform_plan {
        is_valid_glacier_vault
        is_valid_dlm_lifecycle_policy
}",3,"resource ""aws_glacier_vault"" ""example"" {
  name = ""my-glacier-vault""
}

resource ""aws_dlm_lifecycle_policy"" ""example"" {
  description = ""Automated archiving policy""

  execution_role_arn = ""arn:aws:iam::123456789012:role/DLMServiceRole""  # Replace with your DLM execution role ARN

  policy_details {
    resource_types = [""VOLUME""]
    target_tags = {
      ""archive"" = ""true""
    }
    schedule {
      name = ""2 weeks of daily snapshots""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times         = [""23:45""]
      }

      retain_rule {
        count = 14
      }

      tags_to_add = {
        SnapshotCreator = ""DLM""
      }

      copy_tags = false
    }
  }
}","has one ""aws_glacier_vault"" resource
     with ""name"" attribute

has one ""aws_dlm_lifecycle_policy"" resource
    with ""description"" attribute
    with ""execution_role_arn""
    with ""policy_details""","I'll help you create a Terraform configuration that sets up an S3 Glacier vault and integrates it with AWS Data Lifecycle Manager. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create the Glacier vault
resource ""aws_glacier_vault"" ""example"" {
  name = ""example-lifecycle-vault""
  
  tags = {
    Environment = ""Production""
    Purpose     = ""LongTermBackup""
  }
}

# Create IAM role for DLM
resource ""aws_iam_role"" ""dlm_lifecycle_role"" {
  name = ""dlm-lifecycle-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dlm.amazonaws.com""
        }
      }
    ]
  })
}

# Create IAM policy for DLM
resource ""aws_iam_role_policy"" ""dlm_lifecycle"" {
  name = ""dlm-lifecycle-policy""
  role = aws_iam_role.dlm_lifecycle_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""glacier:CreateVaultNotification"",
          ""glacier:DeleteVaultNotification"",
          ""glacier:GetVaultNotifications"",
          ""glacier:InitiateJob"",
          ""glacier:ListJobs"",
          ""glacier:ListVaults"",
          ""glacier:SetVaultNotifications""
        ]
        Resource = [
          aws_glacier_vault.example.arn
        ]
      }
    ]
  })
}

# Create DLM lifecycle policy
resource ""aws_dlm_lifecycle_policy"" ""example"" {
  description        = ""Example DLM lifecycle policy for Glacier vault""
  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn
  state              = ""ENABLED""

  policy_details {
    resource_types = [""VOLUME""]

    schedule {
      name = ""2 weeks of daily snapshots""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times        = [""23:45""]
      }

      retain_rule {
        count = 14
      }

      copy_tags = true
    }

    target_tags = {
      Snapshot = ""true""
    }
  }

  tags = {
    Name = ""example-dlm-lifecycle-policy""
  }
}
```

This configuration:

1. Creates a Glacier vault named ""example-lifecycle-vault""
2. Sets up an IAM role and policy that allows Data Lifecycle Manager to interact with the Glacier vault
3. Creates a DLM lifecycle policy that:
   - Runs daily at 23:45
   - Keeps 14 days of snapshots
   - Targets volumes tagged with ""Snapshot = true""
   - Copies tags from source to snapshot

The policy will automatically manage the lifecycle of your data, moving it to the Glacier vault according to the specified schedule. Make sure to tag the volumes you want to be managed with the appropriate tags (in this case, ""Snapshot = true"").

Remember to adjust the region, vault name, and policy details according to your specific requirements. Also, you might want to adjust the retention period and schedule based on your organization's backup and archival needs.",,,,,,"I'll help you create a Terraform configuration that sets up an S3 Glacier vault and integrates it with AWS Data Lifecycle Manager. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create the Glacier vault
resource ""aws_glacier_vault"" ""archive_vault"" {
  name = ""archive-vault""
  
  tags = {
    Environment = ""Production""
    Purpose     = ""Long-term-archive""
  }
}

# Create IAM role for DLM
resource ""aws_iam_role"" ""dlm_lifecycle_role"" {
  name = ""dlm-lifecycle-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dlm.amazonaws.com""
        }
      }
    ]
  })
}

# Attach required policy to the IAM role
resource ""aws_iam_role_policy"" ""dlm_lifecycle"" {
  name = ""dlm-lifecycle-policy""
  role = aws_iam_role.dlm_lifecycle_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""glacier:InitiateJob"",
          ""glacier:CreateVault"",
          ""glacier:UploadArchive"",
          ""glacier:DeleteArchive"",
          ""glacier:DeleteVault""
        ]
        Resource = [
          aws_glacier_vault.archive_vault.arn,
          ""${aws_glacier_vault.archive_vault.arn}/*""
        ]
      }
    ]
  })
}

# Create DLM lifecycle policy
resource ""aws_dlm_lifecycle_policy"" ""glacier_lifecycle"" {
  description        = ""DLM lifecycle policy for Glacier archival""
  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn
  state              = ""ENABLED""

  policy_details {
    resource_types = [""VOLUME""]

    schedule {
      name = ""Archive to Glacier""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times        = [""23:45""]
      }

      retain_rule {
        count = 30
      }

      tags_to_add = {
        SnapshotCreator = ""DLM""
        ArchiveStatus   = ""ToGlacier""
      }

      copy_tags = true
    }

    target_tags = {
      Archive = ""true""
    }
  }

  tags = {
    Name = ""glacier-archive-lifecycle""
  }
}
```

This configuration:

1. Creates a Glacier vault named ""archive-vault""
2. Sets up an IAM role with necessary permissions for Data Lifecycle Manager
3. Creates a DLM lifecycle policy that:
   - Runs daily at 23:45
   - Retains snapshots for 30 days
   - Targets volumes tagged with ""Archive = true""
   - Adds tracking tags to created snapshots
   - Copies existing tags from the source volume

To use this:
1. Tag your volumes with ""Archive = true""
2. The DLM policy will automatically manage the lifecycle of your data
3. Data will be archived to the Glacier vault according to the schedule

Remember to:
- Adjust the region in the provider block as needed
- Modify the retention periods and schedule according to your requirements
- Review and adjust the IAM permissions based on your security requirements

This configuration provides a foundation for automated data archival to Glacier using AWS Data Lifecycle Manager.",,,,,,"I'll help you create a Terraform configuration for an S3 Glacier vault integrated with AWS Data Lifecycle Manager. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# Create the Glacier vault
resource ""aws_glacier_vault"" ""example"" {
  name = ""example-vault""
  
  tags = {
    Environment = ""Production""
    Purpose     = ""LongTermBackup""
  }
}

# Create IAM role for DLM
resource ""aws_iam_role"" ""dlm_lifecycle_role"" {
  name = ""dlm-lifecycle-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""dlm.amazonaws.com""
        }
      }
    ]
  })
}

# Attach required policy to the IAM role
resource ""aws_iam_role_policy"" ""dlm_lifecycle"" {
  name = ""dlm-lifecycle-policy""
  role = aws_iam_role.dlm_lifecycle_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""glacier:InitiateJob"",
          ""glacier:PurchaseProvisionedCapacity"",
          ""glacier:DeleteArchive"",
          ""glacier:DeleteVault"",
          ""glacier:CreateVault"",
          ""glacier:UploadArchive"",
          ""glacier:GetVaultNotifications"",
          ""glacier:SetVaultNotifications""
        ]
        Resource = [
          aws_glacier_vault.example.arn
        ]
      }
    ]
  })
}

# Create DLM lifecycle policy
resource ""aws_dlm_lifecycle_policy"" ""example"" {
  description        = ""Example DLM lifecycle policy for Glacier vault""
  execution_role_arn = aws_iam_role.dlm_lifecycle_role.arn
  state             = ""ENABLED""

  policy_details {
    resource_types = [""VOLUME""]

    schedule {
      name = ""Move to Glacier after 90 days""

      create_rule {
        interval      = 24
        interval_unit = ""HOURS""
        times        = [""23:45""]
      }

      retain_rule {
        count = 90
      }

      copy_tags = true
    }

    target_tags = {
      Snapshot = ""true""
    }
  }

  tags = {
    Name = ""example-dlm-lifecycle-policy""
  }
}
```

This configuration:

1. Creates a Glacier vault named ""example-vault""
2. Sets up an IAM role with necessary permissions for Data Lifecycle Manager
3. Creates a DLM lifecycle policy that:
   - Runs daily at 23:45
   - Retains data for 90 days
   - Applies to volumes tagged with ""Snapshot = true""
   - Copies all tags from source to target

The DLM policy will manage the lifecycle of your data, automatically moving it to the Glacier vault based on the defined schedule and retention rules. Make sure to adjust the retention period, schedule, and tags according to your specific requirements.

Remember to:
- Adjust the region in the provider block as needed
- Modify the vault name and tags as per your naming conventions
- Adjust the DLM policy schedule and retention rules based on your requirements
- Review and modify the IAM permissions based on your security requirements",,,,,
"aws_connect_bot_association, aws_connect_instance, aws_lex_bot, aws_lex_intent, aws_region",create an aws_connect_bot_association resource with a SAML aws_connect_instance,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for multiple aws_lex_intent resources
has_valid_lex_intents {
    count([intent | intent := input.planned_values.root_module.resources[_]; intent.type == ""aws_lex_intent""; intent.values.name; intent.values.fulfillment_activity])
}

has_valid_connect_instance {
	count([instance | instance := input.planned_values.root_module.resources[_]; instance.type == ""aws_connect_instance""; instance.values.identity_management_type == ""SAML""; instance.values.inbound_calls_enabled; instance.values.outbound_calls_enabled])
}

# Rule for aws_lex_bot resource with specific arguments
has_valid_lex_bot_instance {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lex_bot""
    resource.values.child_directed == true
    resource.values.clarification_prompt
    resource.values.abort_statement
    resource.values.name
    # is_boolean(resource.values.create_version)
    # resource.values.description
    # is_boolean(resource.values.detect_sentiment)
    # resource.values.idle_session_ttl_in_seconds
    # resource.values.process_behavior
    # resource.values.voice_id
    # resource.values.locale
    # is_boolean(resource.values.enable_model_improvements)
    # resource.values.nlu_intent_confidence_threshold
}

has_valid_connect_bot_association {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_connect_bot_association""
    resource.values.lex_bot[_].name
    instance := input.configuration.root_module.resources[i]
    instance.expressions.instance_id
    
    
}



# Combined rule to ensure all conditions are met
has_valid_resources {
    has_valid_lex_intents
    has_valid_lex_bot_instance
    has_valid_connect_instance
    has_valid_connect_bot_association
}",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

data ""aws_region"" ""current"" {}

resource ""aws_connect_instance"" ""example"" {
  identity_management_type = ""CONNECT_MANAGED""
  inbound_calls_enabled    = true
  instance_alias           = ""friendly-name-connect-13""
  outbound_calls_enabled   = true
}

resource ""aws_lex_intent"" ""example"" {
  create_version = true
  name           = ""connect_lex_intent""
  
  sample_utterances = [
    ""I would like to pick up flowers."",
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

resource ""aws_lex_bot"" ""example13"" {
  name             = ""connect_lex_bot""
  process_behavior = ""BUILD""
  child_directed   = false

  abort_statement {
    message {
      content      = ""Sorry, I am not able to assist at this time.""
      content_type = ""PlainText""
    }
  }
  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, what would you like to do?""
      content_type = ""PlainText""
    }
  }
  intent {
    intent_name    = aws_lex_intent.example.name
    intent_version = ""1""
  }  
}

resource ""aws_connect_bot_association"" ""example"" {
  instance_id = aws_connect_instance.example.id
  lex_bot {
    lex_region = data.aws_region.current.name
    name       = aws_lex_bot.example13.name
  }
}

","Has one resource ""aws_connect_instance"" with argument ""identity_management_type"" = SAML, at least one ""aws_lex_intent"" resource, one ""aws_lex_bot"" resource, and one ""aws_connnect_bot_association"" resource","I'll help you create an AWS Connect bot association with a SAML-based Connect instance. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_region"" ""current"" {}

resource ""aws_connect_instance"" ""example"" {
  identity_management_type = ""SAML""
  inbound_calls_enabled   = true
  instance_alias          = ""saml-connect-instance""
  outbound_calls_enabled  = true
}

resource ""aws_lex_intent"" ""example"" {
  create_version = true
  name          = ""connect_lex_intent""
  
  sample_utterances = [
    ""I need assistance"",
    ""Can you help me""
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

resource ""aws_lex_bot"" ""example"" {
  name             = ""saml_connect_bot""
  process_behavior = ""BUILD""
  child_directed   = false

  abort_statement {
    message {
      content      = ""Sorry, I cannot assist you at this time.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand. Could you please repeat that?""
      content_type = ""PlainText""
    }
  }

  intent {
    intent_name    = aws_lex_intent.example.name
    intent_version = ""1""
  }
}

resource ""aws_connect_bot_association"" ""example"" {
  instance_id = aws_connect_instance.example.id
  lex_bot {
    lex_region = data.aws_region.current.name
    name       = aws_lex_bot.example.name
  }
}
```

This configuration:
1. Creates a Connect instance with SAML authentication
2. Sets up a basic Lex intent and bot
3. Associates the bot with the SAML-based Connect instance
4. Uses the current region for the Lex bot association

The main difference from the example is the `identity_management_type = ""SAML""` in the Connect instance configuration. The bot association itself follows the same pattern as the examples, but it's now linked to a SAML-authenticated instance.",,,,,,"I'll help you create an AWS Connect bot association with a SAML-based Connect instance. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

data ""aws_region"" ""current"" {}

# Create a SAML-based Connect instance
resource ""aws_connect_instance"" ""saml_instance"" {
  identity_management_type = ""SAML""
  inbound_calls_enabled   = true
  instance_alias          = ""saml-connect-instance""
  outbound_calls_enabled  = true
}

# Create a Lex intent
resource ""aws_lex_intent"" ""example"" {
  create_version = true
  name          = ""saml_connect_lex_intent""
  
  sample_utterances = [
    ""I need assistance"",
    ""Can you help me""
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

# Create a Lex bot
resource ""aws_lex_bot"" ""saml_bot"" {
  name             = ""saml_connect_lex_bot""
  process_behavior = ""BUILD""
  child_directed   = false

  abort_statement {
    message {
      content      = ""Sorry, I cannot assist at this time.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand. Could you please repeat that?""
      content_type = ""PlainText""
    }
  }

  intent {
    intent_name    = aws_lex_intent.example.name
    intent_version = ""1""
  }
}

# Create the bot association
resource ""aws_connect_bot_association"" ""saml_bot_association"" {
  instance_id = aws_connect_instance.saml_instance.id
  lex_bot {
    lex_region = data.aws_region.current.name
    name       = aws_lex_bot.saml_bot.name
  }
}
```

This configuration:
1. Creates a Connect instance with SAML authentication
2. Sets up a basic Lex intent and bot
3. Associates the Lex bot with the SAML-based Connect instance

The main difference from the example is the `identity_management_type` set to ""SAML"" in the Connect instance. Note that when using SAML, you'll need to configure your SAML provider separately and set up the necessary IAM roles and policies for SAML federation.",,,,,,"I'll help you create an AWS Connect bot association with a SAML-based Connect instance. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

data ""aws_region"" ""current"" {}

resource ""aws_connect_instance"" ""saml_instance"" {
  identity_management_type = ""SAML""
  inbound_calls_enabled   = true
  instance_alias         = ""saml-connect-instance""
  outbound_calls_enabled = true
}

resource ""aws_lex_intent"" ""example"" {
  create_version = true
  name          = ""connect_lex_intent""
  
  sample_utterances = [
    ""I would like to get help."",
  ]

  fulfillment_activity {
    type = ""ReturnIntent""
  }
}

resource ""aws_lex_bot"" ""example"" {
  name             = ""connect_lex_bot""
  process_behavior = ""BUILD""
  child_directed   = false

  abort_statement {
    message {
      content      = ""Sorry, I am not able to assist at this time.""
      content_type = ""PlainText""
    }
  }

  clarification_prompt {
    max_attempts = 2
    message {
      content      = ""I didn't understand you, what would you like to do?""
      content_type = ""PlainText""
    }
  }

  intent {
    intent_name    = aws_lex_intent.example.name
    intent_version = ""1""
  }  
}

resource ""aws_connect_bot_association"" ""saml_bot_association"" {
  instance_id = aws_connect_instance.saml_instance.id
  lex_bot {
    lex_region = data.aws_region.current.name
    name       = aws_lex_bot.example.name
  }
}
```

This configuration:
1. Sets up a SAML-based Connect instance using `identity_management_type = ""SAML""`
2. Creates a Lex intent and bot
3. Associates the Lex bot with the SAML Connect instance
4. Uses the current region for the Lex bot association

The main difference from the examples is the SAML configuration in the Connect instance. The bot association remains similar but references the SAML instance instead.",,,,,
aws_lightsail_database,"AWS Lightsail that creates a postgres database, which enable creating a final snapshot of your database on deletion","package terraform.validation

default has_valid_lightsail_database = false

# Rule for aws_lightsail_database resource with specific arguments
has_valid_lightsail_database {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_lightsail_database""
    resource.values.relational_database_name
    resource.values.master_database_name
    resource.values.master_password
    resource.values.master_username
    resource.values.blueprint_id == ""postgres_12""
    resource.values.bundle_id
    resource.values.final_snapshot_name
}",2,"resource ""aws_lightsail_database"" ""test"" {
  relational_database_name     = ""test""
  availability_zone            = ""us-east-1a""
  master_database_name         = ""testdatabasename""
  master_password              = ""testdatabasepassword""
  master_username              = ""test""
  blueprint_id                 = ""postgres_12""
  bundle_id                    = ""micro_1_0""
  final_snapshot_name          = ""MyFinalSnapshot""
}","""have one """"aws_lightsail_database"""" resource
    with relational_database_name argument
    with master_database_name 
    with master_password
    with master_username
    with blueprint_id = postgres_12
    with bundle_id
    with final_snapshot_name","I'll help you create a Terraform configuration for an AWS Lightsail Postgres database with final snapshot enabled on deletion. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_lightsail_database"" ""postgres_db"" {
  relational_database_name = ""postgres-db""
  availability_zone       = ""us-east-1a""
  master_database_name    = ""mydatabase""
  master_username        = ""dbadmin""
  master_password        = ""YourSecurePassword123!""  # Replace with secure password
  blueprint_id           = ""postgres_12""
  bundle_id             = ""micro_1_0""

  # Enable final snapshot on deletion
  skip_final_snapshot    = false
  final_snapshot_name    = ""final-snapshot-${timestamp()}""

  tags = {
    Environment = ""production""
    ManagedBy   = ""terraform""
  }
}
```

This configuration:
1. Declares the AWS provider requirement
2. Creates a Postgres database in Lightsail
3. Enables final snapshot creation on deletion by setting `skip_final_snapshot = false`
4. Specifies a name for the final snapshot using the `final_snapshot_name` parameter
5. Includes basic tags for resource management

Note: Remember to replace the master_password with a secure password value. In production environments, it's recommended to use variables or secrets management solutions for sensitive values.",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail Postgres database that creates a final snapshot when the database is deleted. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_lightsail_database"" ""postgres_db"" {
  relational_database_name = ""postgres-database""
  availability_zone       = ""us-east-1a""
  master_database_name    = ""mypostgresdb""
  master_password        = ""your-secure-password""
  master_username        = ""dbadmin""
  blueprint_id           = ""postgres_12""
  bundle_id             = ""micro_1_0""
  
  # Enable final snapshot on deletion
  skip_final_snapshot    = false
  final_snapshot_name    = ""final-snapshot-postgres-db""
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a Lightsail Postgres database
3. Enables final snapshot creation by setting `skip_final_snapshot = false`
4. Specifies the name for the final snapshot using `final_snapshot_name`

When you destroy this database using `terraform destroy`, it will automatically create a final snapshot with the specified name before deletion. This is useful for backup purposes and data preservation.

Note: Make sure to replace the password with a secure value and adjust the availability zone, database names, and other parameters according to your needs.",,,,,,"I'll help you create a Terraform configuration for an AWS Lightsail PostgreSQL database with final snapshot enabled on deletion. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_lightsail_database"" ""postgres_db"" {
  relational_database_name = ""postgres-db""
  availability_zone       = ""us-east-1a""
  master_database_name    = ""mypostgresdb""
  master_password        = ""your-secure-password""
  master_username        = ""dbadmin""
  blueprint_id           = ""postgres_12""
  bundle_id             = ""micro_1_0""
  
  # Enable final snapshot on deletion
  skip_final_snapshot    = false
  final_snapshot_name    = ""final-snapshot-postgres-db""

  tags = {
    Environment = ""production""
    Name        = ""postgres-database""
  }
}
```

This configuration:
1. Specifies the required AWS provider
2. Creates a Lightsail PostgreSQL database
3. Sets `skip_final_snapshot` to `false` to ensure a final snapshot is created when the database is deleted
4. Specifies a name for the final snapshot using `final_snapshot_name`
5. Includes basic database configuration like username, password, and database name
6. Uses PostgreSQL 12 as the database engine
7. Uses the micro bundle for the database instance size

Remember to replace sensitive values like `master_password` with secure values, preferably using variables or secrets management.",,,,,
"aws_s3_bucket, aws_s3_bucket_request_payment_configuration",Create a S3 bucket and an example payment configuration resource for the S3 bucket.,"package aws_s3_bucket_payment_config
import future.keywords.in

default valid := false

valid { # IF THE BUCKET IS REFERRED TO BY ID
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""

        bucket.address in payment_config.expressions.bucket.references
}

valid { # IF THE BUCKET IS REFFERED TO BY NAME
        some bucket in input.configuration.root_module.resources
        bucket.type == ""aws_s3_bucket""

        some payment_config in input.configuration.root_module.resources
        payment_config.type == ""aws_s3_bucket_request_payment_configuration""

        payment_config.expressions.bucket.constant_value == bucket.expressions.bucket.constant_value
}",2,"resource ""aws_s3_bucket"" ""example"" {
  bucket = ""mybucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""Requester""
}","Has an aws_s3_bucket

Has an aws_s3_bucket_request_payment_configuration
with bucket = bucket id OR name","I'll help you create a Terraform configuration for an S3 bucket with a payment configuration resource. Based on the provided example, I'll include the required provider block and create both resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-example-bucket""
}

resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""BucketOwner""
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an S3 bucket named ""my-example-bucket""
3. Sets up a request payment configuration where the bucket owner pays for all request fees

The `aws_s3_bucket_request_payment_configuration` resource references the bucket using the bucket's ID and sets the payer to ""BucketOwner"". This means the bucket owner will pay for all request costs associated with the bucket.

Note: Make sure to replace ""my-example-bucket"" with your desired unique bucket name, as S3 bucket names must be globally unique across all AWS accounts.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a payment configuration resource. Based on the provided examples, I'll include the required provider block and create both resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create the S3 bucket
resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-example-bucket""
}

# Configure the payment configuration for the bucket
resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""BucketOwner""  # Default configuration where bucket owner pays for all costs
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an S3 bucket named ""my-example-bucket""
3. Sets up a payment configuration where the bucket owner pays for all request costs

The `aws_s3_bucket_request_payment_configuration` resource is linked to the bucket using the bucket's ID reference (`aws_s3_bucket.example.id`). The `payer` is set to ""BucketOwner"", which means the bucket owner will pay for all storage and request costs associated with the bucket.",,,,,,"I'll help you create a Terraform configuration for an S3 bucket with a payment configuration resource. Based on the example provided, I'll generate a solution that includes the required provider and proper resource configurations.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

# Create the S3 bucket
resource ""aws_s3_bucket"" ""example"" {
  bucket = ""my-example-bucket""
}

# Configure the request payment configuration
resource ""aws_s3_bucket_request_payment_configuration"" ""example"" {
  bucket = aws_s3_bucket.example.id
  payer  = ""BucketOwner""  # Can be either ""BucketOwner"" or ""Requester""
}
```

This configuration:
1. Declares the required AWS provider
2. Creates an S3 bucket named ""my-example-bucket""
3. Sets up a payment configuration where the bucket owner pays for all request fees

The `payer` attribute in the payment configuration can be set to either ""BucketOwner"" (default) or ""Requester"" depending on who should pay for the request fees.

Make sure to replace ""my-example-bucket"" with your desired bucket name, as S3 bucket names must be globally unique across all AWS accounts.",,,,,
"aws_internet_gateway, aws_vpc","creates a VPC with a CIDR block, and an internet gateway in the AWS us-east-2 region, both tagged with names ""vpc"" and ""ig"" respectively.","package terraform.validation

# Set default validation states
default is_valid_vpc = false

default is_valid_internet_gateway = false

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
        resource.expressions.enable_dns_hostnames.constant_value == true
        resource.expressions.tags != null
}

# Validate aws_internet_gateway resource
is_valid_internet_gateway {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_internet_gateway""
        resource.expressions.vpc_id.references[0] == ""aws_vpc._.id""
        resource.expressions.tags != null
}
",3,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}
# Define the provider block for AWS
provider ""aws"" {
  region = ""us-east-2"" # Set your desired AWS region
}

resource ""aws_vpc"" ""_"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  tags = merge({
    ""Name"" = ""vpc""
  })
}

resource ""aws_internet_gateway"" ""_"" {
  vpc_id = aws_vpc._.id
  tags = merge({
    ""Name"" = ""ig""
  })
}","Has one ""aws_vpc"" resource:
with a specified ""cidr_block""
with ""enable_dns_hostnames"" set to true

Has one ""aws_internet_gateway"" resource:
with ""vpc_id"" referencing the ""aws_vpc"" resource by its ID
with a ""tags"" map that is not null, including at least a ""Name"" key with a value","Based on the examples provided and the current question, I'll help you create a Terraform configuration for a VPC with a CIDR block and an internet gateway in AWS us-east-2. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

resource ""aws_vpc"" ""_"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  tags = merge({
    ""Name"" = ""vpc""
  })
}

resource ""aws_internet_gateway"" ""_"" {
  vpc_id = aws_vpc._.id
  tags = merge({
    ""Name"" = ""ig""
  })
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC with CIDR block 10.0.0.0/16
3. Enables DNS hostnames for the VPC
4. Creates an Internet Gateway and attaches it to the VPC
5. Tags both resources with their respective names (""vpc"" and ""ig"")
6. Uses the us-east-2 region as specified

The VPC and Internet Gateway are linked using the `vpc_id` reference, ensuring proper association between the resources.",,,,,,"Based on the examples provided, I'll help you create a Terraform configuration for a VPC with a CIDR block and an internet gateway in AWS us-east-2 region. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

resource ""aws_vpc"" ""_"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  tags = merge({
    ""Name"" = ""vpc""
  })
}

resource ""aws_internet_gateway"" ""_"" {
  vpc_id = aws_vpc._.id
  tags = merge({
    ""Name"" = ""ig""
  })
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC with CIDR block 10.0.0.0/16
3. Enables DNS hostnames for the VPC
4. Creates an Internet Gateway and attaches it to the VPC
5. Tags both resources with their respective names (""vpc"" and ""ig"")
6. Uses the merge function for tags to allow for additional tags to be added if needed

The VPC and Internet Gateway are connected through the `vpc_id` reference, ensuring proper infrastructure deployment.",,,,,,"Based on the provided examples and your current question, I'll help you create a Terraform configuration for a VPC with an Internet Gateway in the us-east-2 region. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.16""
    }
  }

  required_version = "">= 1.2.0""
}

provider ""aws"" {
  region = ""us-east-2""
}

resource ""aws_vpc"" ""_"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  
  tags = {
    Name = ""vpc""
  }
}

resource ""aws_internet_gateway"" ""_"" {
  vpc_id = aws_vpc._.id
  
  tags = {
    Name = ""ig""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a VPC with CIDR block 10.0.0.0/16 and enables DNS hostnames
3. Creates an Internet Gateway and attaches it to the VPC
4. Tags both resources with their respective names (""vpc"" and ""ig"")

The configuration will deploy in the us-east-2 region as specified. The VPC and Internet Gateway are linked together using the `vpc_id` reference, and both resources are properly tagged.",,,,,
"aws_route53_health_check, aws_route53_health_check, aws_route53_record, aws_route53_record, aws_route53_zone","Configure Active-Active Failover policies using Route 53 resources. Name the health check resources ""primary_health_check"" and ""secondary_health_check"" and use a fqdn for both. Call the zone ""main""","package terraform.validation

default is_configuration_valid = false

default is_valid_r53_zone = false

default is_valid_r53_record_primary = false

default is_valid_r53_record_secondary = false

default is_valid_r53_health_check_primary = false

default is_valid_r53_health_check_secondary = false



# Validate aws_route53_zone resource
is_valid_r53_zone {
        some i
        resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_zone""

}

# Validate aws_route53_record
is_valid_r53_record_primary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.failover_routing_policy[0].type.constant_value == ""PRIMARY""
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.set_identifier
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.main.zone_id""
    resource.expressions.health_check_id.references[0] == ""aws_route53_health_check.primary_health_check.id""
}

is_valid_r53_record_secondary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_record""
    resource.expressions.name
    resource.expressions.failover_routing_policy[0].type.constant_value == ""SECONDARY""
    resource.expressions.type
    resource.expressions.ttl
    resource.expressions.records
    resource.expressions.set_identifier
    resource.expressions.zone_id.references[0] == ""aws_route53_zone.main.zone_id""
    resource.expressions.health_check_id.references[0] == ""aws_route53_health_check.secondary_health_check.id""

}

# Validate aws_route53_health_check
is_valid_r53_health_check_primary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_health_check""
    resource.name == ""primary_health_check""
    resource.expressions.fqdn
    resource.expressions.request_interval
    resource.expressions.type
}

is_valid_r53_health_check_secondary {
        some i
    resource := input.configuration.root_module.resources[i]
    resource.type == ""aws_route53_health_check""
    resource.name == ""secondary_health_check""
    resource.expressions.fqdn
    resource.expressions.request_interval
    resource.expressions.type
}


# Combine all checks into a final rule
is_configuration_valid {
    is_valid_r53_health_check_primary
    is_valid_r53_health_check_secondary
    is_valid_r53_zone
    is_valid_r53_record_primary
    is_valid_r53_record_secondary
}




",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example53.com""
}

resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example53.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
}

resource ""aws_route53_record"" ""primary_record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""60""
  records = [""192.0.2.101""]
  set_identifier = ""primary-endpoint""

  failover_routing_policy {
    type = ""PRIMARY""
  }

  health_check_id = aws_route53_health_check.primary_health_check.id
}

resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example53.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
}

resource ""aws_route53_record"" ""secondary_record"" {
  zone_id = aws_route53_zone.main.zone_id
  name    = ""example53.com""
  type    = ""A""
  ttl     = ""60""
  records = [""192.0.2.102""] 
  set_identifier = ""secondary-endpoint""
  
  failover_routing_policy {
    type = ""SECONDARY""
  }

  health_check_id = aws_route53_health_check.secondary_health_check.id
}","Has one ""aws_route53_zone"" resource
    with ""name""

Has two ""aws_route53_record"" resources
    The first ""aws_route53_record"" 
        with ""name""
        with ""type""
        with ""ttl""
        with ""records""
        with ""zone_id"" referencing the ""aws_route53_zone"" resource
        with ""health_check_id"" referencing one of the ""aws_health_check"" resource
        with ""set_identifier""
        with ""failover_routing_policy"" block that contains ""type"" = ""PRIMARY""
    The second ""aws_route53_record""
        with ""name""
        with ""type""
        with ""ttl""
        with ""records""
        with ""zone_id"" referencing the ""aws_route53_zone"" resource
        with ""health_check_id"" referencing the other ""aws_health_check"" resource
        with ""set_identifier""
        with ""failover_routing_policy"" block that contains ""type"" = ""SECONDARY""

Has two ""aws_health_check"" resources
    with ""fqdn"" 
    with ""request_interval""
    with ""type""

","I'll help you create a Terraform configuration for Active-Active Failover using Route 53 with two health checks. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Health check for the first endpoint
resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = ""3""
  request_interval  = ""30""
}

# Health check for the second endpoint
resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = ""3""
  request_interval  = ""30""
}

# First record with weight routing policy
resource ""aws_route53_record"" ""primary"" {
  zone_id         = aws_route53_zone.main.zone_id
  name            = ""service.example.com""
  type            = ""A""
  ttl             = ""60""
  records         = [""192.0.2.10""]
  set_identifier  = ""primary""
  health_check_id = aws_route53_health_check.primary_health_check.id

  weighted_routing_policy {
    weight = 50
  }
}

# Second record with weight routing policy
resource ""aws_route53_record"" ""secondary"" {
  zone_id         = aws_route53_zone.main.zone_id
  name            = ""service.example.com""
  type            = ""A""
  ttl             = ""60""
  records         = [""192.0.2.20""]
  set_identifier  = ""secondary""
  health_check_id = aws_route53_health_check.secondary_health_check.id

  weighted_routing_policy {
    weight = 50
  }
}
```

This configuration creates:
1. A Route 53 hosted zone named ""main""
2. Two health checks (primary_health_check and secondary_health_check) that monitor the respective endpoints
3. Two A records with weighted routing policy set to 50/50 distribution
4. Each record is associated with its corresponding health check

The Active-Active configuration ensures that traffic is distributed between both endpoints when they are healthy, and will automatically route around failures if one endpoint becomes unhealthy.",,,,,,"I'll help you create a Terraform configuration for Active-Active Failover using Route 53 with two health checks. Based on the examples provided, here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Health check for the first endpoint
resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
  request_interval  = 30
}

# Health check for the second endpoint
resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
  request_interval  = 30
}

# First record with weighted routing policy
resource ""aws_route53_record"" ""primary"" {
  zone_id         = aws_route53_zone.main.zone_id
  name            = ""www.example.com""
  type            = ""A""
  ttl             = ""60""
  records         = [""192.0.2.10""]
  set_identifier  = ""primary""
  health_check_id = aws_route53_health_check.primary_health_check.id

  weighted_routing_policy {
    weight = 50
  }
}

# Second record with weighted routing policy
resource ""aws_route53_record"" ""secondary"" {
  zone_id         = aws_route53_zone.main.zone_id
  name            = ""www.example.com""
  type            = ""A""
  ttl             = ""60""
  records         = [""192.0.2.20""]
  set_identifier  = ""secondary""
  health_check_id = aws_route53_health_check.secondary_health_check.id

  weighted_routing_policy {
    weight = 50
  }
}
```

This configuration creates:
1. A Route 53 hosted zone named ""main""
2. Two health checks (primary_health_check and secondary_health_check) that monitor the respective endpoints
3. Two A records with weighted routing policies, each with:
   - 50% weight distribution
   - Associated health checks
   - Unique set identifiers
   - Different IP addresses

The active-active configuration ensures traffic is distributed between both endpoints when they're healthy, and automatically routes traffic away from an unhealthy endpoint.",,,,,,"I'll help you create a Terraform configuration for Active-Active failover using Route 53 with two health checks. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_route53_zone"" ""main"" {
  name = ""example.com""
}

# Health check for the first endpoint
resource ""aws_route53_health_check"" ""primary_health_check"" {
  fqdn              = ""primary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
  request_interval  = 30

  tags = {
    Name = ""Primary Health Check""
  }
}

# Health check for the second endpoint
resource ""aws_route53_health_check"" ""secondary_health_check"" {
  fqdn              = ""secondary.example.com""
  port              = 80
  type              = ""HTTP""
  resource_path     = ""/""
  failure_threshold = 3
  request_interval  = 30

  tags = {
    Name = ""Secondary Health Check""
  }
}

# First record with weighted routing policy
resource ""aws_route53_record"" ""primary"" {
  zone_id         = aws_route53_zone.main.zone_id
  name            = ""www.example.com""
  type            = ""A""
  ttl             = ""60""
  records         = [""192.0.2.10""]
  set_identifier  = ""primary""
  health_check_id = aws_route53_health_check.primary_health_check.id

  weighted_routing_policy {
    weight = 50
  }
}

# Second record with weighted routing policy
resource ""aws_route53_record"" ""secondary"" {
  zone_id         = aws_route53_zone.main.zone_id
  name            = ""www.example.com""
  type            = ""A""
  ttl             = ""60""
  records         = [""192.0.2.20""]
  set_identifier  = ""secondary""
  health_check_id = aws_route53_health_check.secondary_health_check.id

  weighted_routing_policy {
    weight = 50
  }
}
```

This configuration creates:
1. A Route 53 hosted zone named ""main""
2. Two health checks (primary_health_check and secondary_health_check) monitoring different endpoints
3. Two A records with weighted routing policy, each with:
   - 50% weight distribution
   - Associated health checks
   - Different IP addresses
   - The same DNS name

The setup provides active-active failover where traffic is distributed between both endpoints when they're healthy, and automatically routed away from an endpoint if it fails its health check.",,,,,
"aws_iam_role, aws_iam_role_policy, aws_kendra_index",Generate a basic Kendra index resource With user group resolution configuration,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

has_valid_iam_role(resources) { 
    some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""kendra.amazonaws.com"") 
}

has_valid_kendra_index {
	some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_kendra_index""
    resource.values.name
    resource.values.user_group_resolution_configuration
    role := input.configuration.root_module.resources[i]
    role.expressions.role_arn
}


has_valid_resources {
    has_valid_iam_role(input.resource_changes)
    has_valid_kendra_index
}
",4,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""role"" {
  name = ""Kendra-Role""
  assume_role_policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Action"": ""sts:AssumeRole"",
      ""Principal"": {
        ""Service"": ""kendra.amazonaws.com""
      },
      ""Effect"": ""Allow"",
      ""Sid"": """"
    }
  ]
}
EOF
}

resource ""aws_iam_role_policy"" ""policy"" {
  name = ""Kendra-Policy""
  role = aws_iam_role.role.id

  policy = <<EOF
{
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:GetObject"",
        ""s3:ListBucket""
      ],
      ""Resource"": ""*""
    }
  ]
}
EOF
}

resource ""aws_kendra_index"" ""example"" {
  name     = ""example""
  role_arn = aws_iam_role.role.arn

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""
  }
}","Has one ""aws_iam_role"" resource and one ""aws_kendra_index"" resource with the ""user_group_resolution_configuration"" block","I'll help you create a Terraform configuration for an AWS Kendra index resource with user group resolution configuration. Based on the examples provided, I'll include the necessary provider configuration and IAM role setup as well.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra-index-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra-index-policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kendra:*"",
          ""iam:GetUser"",
          ""iam:GetGroup"",
          ""iam:GetRole""
        ]
        Resource = ""*""
      }
    ]
  })
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra index with user group resolution""
  role_arn    = aws_iam_role.kendra_role.arn
  edition     = ""ENTERPRISE_EDITION""

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""
  }

  tags = {
    Environment = ""production""
    Purpose     = ""search""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role and policy for Kendra with necessary permissions
3. Creates a Kendra index with user group resolution configuration set to use AWS SSO
4. Includes basic tagging for resource management

The index is set up with ENTERPRISE_EDITION as it's required for user group resolution configuration. The user_group_resolution_configuration block configures the index to use AWS SSO for resolving user groups, which is useful for implementing fine-grained access control for your Kendra index.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra index resource with user group resolution configuration. Based on the examples provided, I'll include the required provider, IAM role, and the Kendra index resource with user group resolution settings.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra-index-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra-index-policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kendra:*"",
          ""iam:GetUser"",
          ""iam:GetGroup"",
          ""iam:GetRole""
        ]
        Resource = ""*""
      }
    ]
  })
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra index with user group resolution""
  edition     = ""DEVELOPER_EDITION""
  role_arn    = aws_iam_role.kendra_role.arn

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""
  }

  tags = {
    Environment = ""production""
    Purpose     = ""search""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role and policy for Kendra with necessary permissions
3. Creates a Kendra index with user group resolution configuration set to use AWS SSO
4. Includes basic tags for resource management

The index is set up with DEVELOPER_EDITION and includes the necessary IAM permissions to interact with user and group information. The user_group_resolution_configuration block configures the index to use AWS SSO for resolving user groups.",,,,,,"I'll help you create a Terraform configuration for an AWS Kendra index resource with user group resolution configuration. Based on the examples provided, I'll include the necessary provider configuration and IAM role setup as well.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_iam_role"" ""kendra_role"" {
  name = ""kendra-index-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""kendra.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""kendra_policy"" {
  name = ""kendra-index-policy""
  role = aws_iam_role.kendra_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""kendra:*"",
          ""iam:GetGroup"",
          ""iam:GetUser""
        ]
        Resource = ""*""
      }
    ]
  })
}

resource ""aws_kendra_index"" ""example"" {
  name        = ""example-kendra-index""
  description = ""Example Kendra index with user group resolution""
  role_arn    = aws_iam_role.kendra_role.arn
  edition     = ""ENTERPRISE_EDITION""

  user_group_resolution_configuration {
    user_group_resolution_mode = ""AWS_SSO""
  }

  tags = {
    Environment = ""production""
    Purpose     = ""search""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an IAM role and policy for Kendra to use
3. Creates a Kendra index with user group resolution configuration set to use AWS SSO
4. Includes basic tagging for resource management

The index is set up with ENTERPRISE_EDITION as this is required for user group resolution features. The IAM role includes permissions necessary for Kendra to function and access user group information.

Make sure to adjust the region, role name, and other parameters according to your specific needs.",,,,,
aws_elasticache_cluster,Provide a Memcached Cluster,"package main

import future.keywords.in

default allow = false

aws_elasticache_cluster_valid(resources) {
    some resource in resources
    resource.type == ""aws_elasticache_cluster""
    resource.change.after.engine == ""memcached""
}

# Aggregate all checks
allow {
    aws_elasticache_cluster_valid(input.resource_changes)
}",2,"provider ""aws"" {
  region=""us-east-1""
}
resource ""aws_elasticache_cluster"" ""example"" {
  cluster_id           = ""cluster-example""
  engine               = ""memcached""
  node_type            = ""cache.m4.large""
  num_cache_nodes      = 2
  parameter_group_name = ""default.memcached1.4""
  port                 = 11211
}","requires cluster id, engine=memcached, node_type, num_cache_nodes, parameter_group_name","I'll help you create a Terraform configuration for an AWS ElastiCache Memcached cluster. This configuration will include the necessary networking components and a basic Memcached cluster setup.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC for the ElastiCache cluster
resource ""aws_vpc"" ""cache_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""cache-vpc""
  }
}

# Subnets for the ElastiCache cluster
resource ""aws_subnet"" ""cache_subnet_1"" {
  vpc_id            = aws_vpc.cache_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-east-1a""

  tags = {
    Name = ""cache-subnet-1""
  }
}

resource ""aws_subnet"" ""cache_subnet_2"" {
  vpc_id            = aws_vpc.cache_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-east-1b""

  tags = {
    Name = ""cache-subnet-2""
  }
}

# Subnet group for ElastiCache
resource ""aws_elasticache_subnet_group"" ""cache_subnet_group"" {
  name       = ""cache-subnet-group""
  subnet_ids = [aws_subnet.cache_subnet_1.id, aws_subnet.cache_subnet_2.id]
}

# Security group for ElastiCache
resource ""aws_security_group"" ""cache_security_group"" {
  name        = ""cache-security-group""
  description = ""Security group for Memcached cluster""
  vpc_id      = aws_vpc.cache_vpc.id

  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}

# Parameter group for Memcached
resource ""aws_elasticache_parameter_group"" ""cache_parameter_group"" {
  family = ""memcached1.6""
  name   = ""cache-params""

  parameter {
    name  = ""max_item_size""
    value = ""10485760""
  }
}

# Memcached cluster
resource ""aws_elasticache_cluster"" ""memcached_cluster"" {
  cluster_id           = ""memcached-cluster""
  engine              = ""memcached""
  node_type           = ""cache.t3.micro""
  num_cache_nodes     = 2
  port                = 11211
  
  parameter_group_name = aws_elasticache_parameter_group.cache_parameter_group.name
  subnet_group_name    = aws_elasticache_subnet_group.cache_subnet_group.name
  security_group_ids   = [aws_security_group.cache_security_group.id]

  az_mode             = ""cross-az""
  preferred_availability_zones = [""us-east-1a"", ""us-east-1b""]

  tags = {
    Name = ""memcached-cluster""
  }
}
```

This configuration creates:
1. A VPC with two subnets in different availability zones
2. A subnet group for the ElastiCache cluster
3. A security group allowing Memcached traffic (port 11211)
4. A parameter group with some basic Memcached settings
5. A Memcached cluster with two nodes spread across two availability zones

Key features of the Memcached cluster:
- Uses t3.micro instances (can be changed based on needs)
- Two nodes for high availability
- Cross-AZ deployment
- Default Memcached port (11211)
- Basic security group configuration
- Custom parameter group with",,,,,,"I'll help you create a Terraform configuration for an AWS ElastiCache Memcached cluster. I'll include necessary components like VPC, subnets, and the cluster itself.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC for the ElastiCache cluster
resource ""aws_vpc"" ""memcached_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""memcached-vpc""
  }
}

# Availability Zones data source
data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

# Subnets for the ElastiCache cluster
resource ""aws_subnet"" ""memcached_subnet_1"" {
  vpc_id            = aws_vpc.memcached_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = data.aws_availability_zones.available.names[0]

  tags = {
    Name = ""memcached-subnet-1""
  }
}

resource ""aws_subnet"" ""memcached_subnet_2"" {
  vpc_id            = aws_vpc.memcached_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = data.aws_availability_zones.available.names[1]

  tags = {
    Name = ""memcached-subnet-2""
  }
}

# Subnet group for ElastiCache
resource ""aws_elasticache_subnet_group"" ""memcached_subnet_group"" {
  name       = ""memcached-subnet-group""
  subnet_ids = [aws_subnet.memcached_subnet_1.id, aws_subnet.memcached_subnet_2.id]
}

# Security group for ElastiCache
resource ""aws_security_group"" ""memcached_sg"" {
  name        = ""memcached-security-group""
  description = ""Security group for Memcached cluster""
  vpc_id      = aws_vpc.memcached_vpc.id

  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}

# Parameter group for Memcached
resource ""aws_elasticache_parameter_group"" ""memcached_params"" {
  family = ""memcached1.6""
  name   = ""memcached-parameters""

  parameter {
    name  = ""max_item_size""
    value = ""10485760""
  }
}

# ElastiCache Memcached cluster
resource ""aws_elasticache_cluster"" ""memcached_cluster"" {
  cluster_id           = ""memcached-cluster""
  engine              = ""memcached""
  node_type           = ""cache.t3.micro""
  num_cache_nodes     = 2
  parameter_group_name = aws_elasticache_parameter_group.memcached_params.name
  port                = 11211
  security_group_ids  = [aws_security_group.memcached_sg.id]
  subnet_group_name   = aws_elasticache_subnet_group.memcached_subnet_group.name

  tags = {
    Name = ""memcached-cluster""
  }
}
```

This configuration creates:
1. A VPC with two subnets in different availability zones
2. A subnet group for the ElastiCache cluster
3. A security group allowing inbound traffic on port 11211 (default Memcached port)
4. A parameter group with some basic configurations
5. A Memcached cluster with 2 nodes spread across the subnets

Key features of the Memcached cluster:
- Uses t3.micro instances (",,,,,,"I'll help you create a Terraform configuration for an AWS ElastiCache Memcached cluster. This configuration will include the necessary networking components and a basic Memcached cluster setup.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# VPC for the ElastiCache cluster
resource ""aws_vpc"" ""cache_vpc"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""cache-vpc""
  }
}

# Subnets for the ElastiCache cluster
resource ""aws_subnet"" ""cache_subnet_1"" {
  vpc_id            = aws_vpc.cache_vpc.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-east-1a""

  tags = {
    Name = ""cache-subnet-1""
  }
}

resource ""aws_subnet"" ""cache_subnet_2"" {
  vpc_id            = aws_vpc.cache_vpc.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-east-1b""

  tags = {
    Name = ""cache-subnet-2""
  }
}

# Subnet group for ElastiCache
resource ""aws_elasticache_subnet_group"" ""cache_subnet_group"" {
  name       = ""cache-subnet-group""
  subnet_ids = [aws_subnet.cache_subnet_1.id, aws_subnet.cache_subnet_2.id]
}

# Security group for ElastiCache
resource ""aws_security_group"" ""cache_security_group"" {
  name        = ""cache-security-group""
  description = ""Security group for Memcached cluster""
  vpc_id      = aws_vpc.cache_vpc.id

  ingress {
    from_port   = 11211
    to_port     = 11211
    protocol    = ""tcp""
    cidr_blocks = [""10.0.0.0/16""]
  }
}

# Parameter group for Memcached
resource ""aws_elasticache_parameter_group"" ""cache_parameter_group"" {
  family = ""memcached1.6""
  name   = ""cache-params""

  parameter {
    name  = ""max_item_size""
    value = ""10485760""
  }
}

# Memcached cluster
resource ""aws_elasticache_cluster"" ""memcached_cluster"" {
  cluster_id           = ""memcached-cluster""
  engine              = ""memcached""
  node_type           = ""cache.t3.medium""
  num_cache_nodes     = 2
  port                = 11211
  
  parameter_group_name = aws_elasticache_parameter_group.cache_parameter_group.name
  subnet_group_name    = aws_elasticache_subnet_group.cache_subnet_group.name
  security_group_ids   = [aws_security_group.cache_security_group.id]

  az_mode             = ""cross-az""
  preferred_availability_zones = [""us-east-1a"", ""us-east-1b""]

  tags = {
    Environment = ""production""
  }
}
```

This configuration creates:
1. A VPC with two subnets in different availability zones
2. A subnet group for the ElastiCache cluster
3. A security group allowing inbound access on port 11211 (default Memcached port)
4. A parameter group with some basic Memcached configurations
5. A Memcached cluster with:
   - 2 nodes spread across availability zones
   - t3.medium instance type
   - Cross-AZ deployment for high availability
   - Basic security group and networking setup

The cluster will be accessible within the VPC on port 11211. You can modify the security group rules, node type, and number",,,,,
