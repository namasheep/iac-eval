Resource,Prompt,Rego intent,Difficulty,Reference output,Intent,LLM Output #0,LLM Plannable? #0,LLM Correct? #0,LLM Plan Phase Error #0,LLM OPA match phase Error #0,LLM Notes #0,LLM Output #1,LLM Plannable? #1,LLM Correct? #1,LLM Plan Phase Error #1,LLM OPA match phase Error #1,LLM Notes #1,LLM Output #2,LLM Plannable? #2,LLM Correct? #2,LLM Plan Phase Error #2,LLM OPA match phase Error #2,LLM Notes #2
"aws_api_gateway_integration, aws_api_gateway_integration, aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_dynamodb_table, aws_iam_role, aws_iam_role, aws_lambda_function, aws_lambda_function, aws_lambda_permission, aws_lambda_permission, aws_s3_bucket","An AWS service that holds a web server which allows you to upload cat pictures and provides random cat pictures on demand. Accomplish this using the following resources: AWS DynamoDB table, AWS S3 bucket, AWS Lambda function, AWS Lambda permission, AWS API Gateway rest API, AWS API Gateway resource, AWS API Gateway method. Add any necessary resources.","package caas_middle

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
	api.address in cat.expressions.parent_id.references
	api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
	method.expressions.http_method.constant_value == method_string
	resource.address in method.expressions.resource_id.references
	api.address in method.expressions.rest_api_id.references
}

table_valid(table) if {
	some attribute in table.expressions.attribute
	attribute.name
	attribute.type

	table.expressions.hash_key
}

lambda_valid(lambda, bucket) if {
	some env in lambda.expressions.environment
	bucket.address in env.variables.references
}

permission_valid(permission, lambda, api) if {
	permission.expressions.action.constant_value == ""lambda:InvokeFunction""
	lambda.address in permission.expressions.function_name.references
	permission.expressions.principal.constant_value == ""apigateway.amazonaws.com""
	api.address in permission.expressions.source_arn.references
}

integration_valid(integration, lambda, method, resource, api, integration_method) if {
	method.address in integration.expressions.http_method.references
	resource.address in integration.expressions.resource_id.references
	api.address in integration.expressions.rest_api_id.references
	integration.expressions.integration_http_method.constant_value == integration_method
	integration.expressions.type.constant_value == ""AWS_PROXY""
	lambda.address in integration.expressions.uri.references
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some api in resources
	api.type == ""aws_api_gateway_rest_api""

	some cat in resources
	cat.type == ""aws_api_gateway_resource""

	some method_get in resources
	method_get.type == ""aws_api_gateway_method""

	some method_put in resources
	method_put.type == ""aws_api_gateway_method""

	some table in resources
	table.type == ""aws_dynamodb_table""

	some lambda_get in resources
	lambda_get.type == ""aws_lambda_function""

	some lambda_put in resources
	lambda_put.type == ""aws_lambda_function""

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	some permission_get in resources
	permission_get.type == ""aws_lambda_permission""

	some permission_put in resources
	permission_put.type == ""aws_lambda_permission""

	some integration_get in resources
	integration_get.type == ""aws_api_gateway_integration""

	some integration_put in resources
	integration_put.type == ""aws_api_gateway_integration""

	api_valid(api)
	cat_valid(cat, api)
	method_valid(method_get, ""GET"", cat, api)
	method_valid(method_put, ""PUT"", cat, api)
	lambda_valid(lambda_get, bucket, archive_get)
	lambda_valid(lambda_put, bucket, archive_put)
	permission_valid(permission_get, lambda_get, api)
	permission_valid(permission_put, lambda_put, api)
	integration_valid(integration_get, lambda_get, method_get, cat, api, ""GET"")
	integration_valid(integration_put, lambda_put, method_put, cat, api, ""PUT"")
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_api resource id
    with ""resource_id"" linking to aws_api_gateway_resource resource
    with ""authorization"" argument
    with ""http_method"" argument with values ""GET"" and ""PUT""
have one ""aws_s3_bucket"" resource
have one ""aws_dynamodb_table"" resource
    with ""name"" argument
    with ""hash_key"" argument
    with ""attribute"" argument block
        with ""name"" argument
        with ""type"" argument
with two ""aws_lambda_function"" resources
    with ""function_name"" argument
    with ""role"" argument
    with ""environment"" argument block
        with ""variables"" argument block linking to the aws_s3_bucket
with two ""aws_lambda_permission"" resources
    with ""action"" argument with value ""lambda:InvokeFunction""
    with ""principal"" argument with value ""apigateway.amazonaws.com""
    with ""function_name"" argument linking to aws_lambda_function resource
    with ""source_arn"" argument linking to aws_api_gateway_rest_api (two different end points)
with two ""aws_api_gateway_integration"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource resource
    with ""http_method"" argument linking to aws_api_gateway_method resource
    with ""type"" argument with value ""AWS_PROXY""
    with ""integration_http_method"" argument with values ""GET"" and ""PUT""","I'll help you create a Terraform configuration for a cat picture service using AWS services. This solution will include all the requested components and necessary additional resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-storage-${random_string.suffix.result}""
}

# Random string for unique naming
resource ""random_string"" ""suffix"" {
  length  = 8
  special = false
  upper   = false
}

# DynamoDB table for metadata
resource ""aws_dynamodb_table"" ""cat_metadata"" {
  name           = ""cat-pictures-metadata""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  attribute {
    name = ""id""
    type = ""S""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket"",
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*"",
          aws_dynamodb_table.cat_metadata.arn
        ]
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_pictures_lambda"" {
  filename         = ""lambda_function.zip""
  function_name    = ""cat_pictures_handler""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME = aws_s3_bucket.cat_pictures.id
      TABLE_NAME  = aws_dynamodb_table.cat_metadata.name
    }
  }
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name        = ""cat-pictures-api""
  description = ""API for cat pictures service""
}

# API Gateway resource for /cats
resource ""aws_api_gateway_resource"" ""cats"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway GET method
resource ""aws_api_gateway_method"" ""get_cats"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cats.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# API Gateway POST method
resource ""aws_api_gateway_method"" ""post_cats"" {
  rest_api_id   = aws_api_gateway_rest_api.",,,,,,"I'll help you create a Terraform configuration for a cat picture service using AWS services. This solution will include all the requested components and necessary additional resources.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-storage-${random_string.suffix.result}""
}

# Random string for unique bucket naming
resource ""random_string"" ""suffix"" {
  length  = 8
  special = false
  upper   = false
}

# S3 bucket configuration
resource ""aws_s3_bucket_public_access_block"" ""cat_pictures"" {
  bucket = aws_s3_bucket.cat_pictures.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# DynamoDB table for metadata
resource ""aws_dynamodb_table"" ""cat_metadata"" {
  name           = ""cat-pictures-metadata""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  attribute {
    name = ""id""
    type = ""S""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket"",
          ""dynamodb:GetItem"",
          ""dynamodb:PutItem"",
          ""dynamodb:Scan""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*"",
          aws_dynamodb_table.cat_metadata.arn
        ]
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_service"" {
  filename      = ""lambda_function.zip""  # You need to provide the actual Lambda code
  function_name = ""cat-pictures-service""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME = aws_s3_bucket.cat_pictures.id
      TABLE_NAME  = aws_dynamodb_table.cat_metadata.name
    }
  }
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name        = ""cat-pictures-api""
  description = ""API for cat pictures service""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway methods
resource ""aws_api_gateway_method"" ""get_cats"" {
  rest_api_id   = aws_api_gateway_rest_api.cat",,,,,,"I'll help create a Terraform configuration for a cat picture service using AWS services. Here's a solution that includes all the requested components:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-storage-${random_string.suffix.result}""
}

# Random string for unique naming
resource ""random_string"" ""suffix"" {
  length  = 8
  special = false
}

# DynamoDB table for metadata
resource ""aws_dynamodb_table"" ""cat_metadata"" {
  name           = ""cat-pictures-metadata""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""picture_id""
  attribute {
    name = ""picture_id""
    type = ""S""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket"",
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*"",
          aws_dynamodb_table.cat_metadata.arn
        ]
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_pictures_handler"" {
  filename      = ""lambda_function.zip""
  function_name = ""cat_pictures_handler""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""

  environment {
    variables = {
      S3_BUCKET     = aws_s3_bucket.cat_pictures.id
      DYNAMODB_TABLE = aws_dynamodb_table.cat_metadata.name
    }
  }
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name        = ""cat-pictures-api""
  description = ""API for cat pictures service""
}

# API Gateway resource for /cats
resource ""aws_api_gateway_resource"" ""cats"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway GET method
resource ""aws_api_gateway_method"" ""get_cats"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cats.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# API Gateway POST method
resource ""aws_api_gateway_method"" ""post_cats"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api",,,,,
"aws_codebuild_project, aws_iam_role, aws_s3_bucket, aws_s3_bucket",create a AWS codebuild project resource with example iam role and secondary artifiacts,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for aws_iam_role resource
has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""codebuild.amazonaws.com"") 
} 

has_valid_codebuild {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_codebuild_project""
        resource.values.artifacts
    resource.values.environment
    resource.values.name
    resource.values.source[_].type
    resource.values.secondary_artifacts[_].type
    role := input.configuration.root_module.resources[i]
    role.expressions.service_role
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    
}


has_valid_resources {
        has_valid_iam_role(input.resource_changes)
    has_valid_codebuild
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""test_role8"" {
  name = ""test_role8""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Sid    = """"
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      },
    ]
  })
}

resource ""aws_s3_bucket"" ""apriltwentyninth"" {
  bucket = ""apriltwentyninth""
}

resource ""aws_s3_bucket"" ""apriltwentyninth2"" {
  bucket = ""apriltwentyninth2""
}


resource ""aws_codebuild_project"" ""example7"" {
  name          = ""test-project8""
  service_role  = aws_iam_role.test_role8.arn

  artifacts {
    location  = aws_s3_bucket.apriltwentyninth.bucket
    type      = ""S3""
    name     = ""results.zip""
    path      = ""/""
    packaging = ""ZIP""
  }

  secondary_artifacts {
    artifact_identifier =  ""SecondaryArtifact""
    type = ""S3""
    location  = aws_s3_bucket.apriltwentyninth2.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:7.0-24.10.29""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  source_version = ""master""

  secondary_sources {
    source_identifier = ""source2""
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  secondary_source_version {
    source_identifier = ""source2""
    source_version = ""master""
  }
}","Has one ""aws_iam_role"" resource, at least one ""aws_s3_bucket"" resource, and one ""aws_codebuild_project"" resource with a ""secondary_artifacts"" block","I'll help you create an AWS CodeBuild project with an IAM role and secondary artifacts. Here's a complete Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-project-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""codebuild-project-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:PutObject"",
          ""ecr:GetAuthorizationToken"",
          ""ecr:BatchCheckLayerAvailability"",
          ""ecr:GetDownloadUrlForLayer"",
          ""ecr:BatchGetImage""
        ]
      }
    ]
  })
}

# S3 bucket for artifacts
resource ""aws_s3_bucket"" ""artifacts"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""project"" {
  name          = ""my-codebuild-project""
  description   = ""My CodeBuild project""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  secondary_artifacts {
    type                = ""S3""
    artifact_identifier = ""secondary_artifact""
    location           = aws_s3_bucket.artifacts.id
    path               = ""secondary/""
    namespace_type     = ""BUILD_ID""
    packaging          = ""ZIP""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/myuser/myrepo.git""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  cache {
    type  = ""NO_CACHE""
  }

  logs_config {
    cloudwatch_logs {
      status = ""ENABLED""
    }

    s3_logs {
      status = ""DISABLED""
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration includes:

1. The required AWS provider configuration
2. An IAM role and policy for CodeBuild with common permissions
3. An S3 bucket for artifacts
4. A CodeBuild project with:
   - Primary artifacts set to NO_ARTIFACTS
   - Secondary artifacts configured to store in S3
   - Basic environment configuration
   - GitHub source configuration
   - Logging configuration
   - Tags

The CodeBuild project uses a secondary artifact that will be stored in the specified S3 bucket. The IAM role includes permissions for CloudWatch Logs, S3, and ECR operations commonly needed by CodeBuild projects.

Remember to:
- Replace the S3 bucket name",,,,,,"I'll help you create an AWS CodeBuild project with an IAM role and secondary artifacts. Here's a complete Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-service-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""codebuild-service-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:*"",
          ""codecommit:*""
        ]
      }
    ]
  })
}

# S3 Bucket for artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""example"" {
  name           = ""example-project""
  description    = ""Example CodeBuild project""
  build_timeout  = ""60""
  queued_timeout = ""480""
  service_role   = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  secondary_artifacts {
    type                = ""S3""
    artifact_identifier = ""secondary_artifact""
    location           = aws_s3_bucket.artifact_bucket.id
    path               = ""secondary/""
    packaging          = ""ZIP""
    namespace_type     = ""BUILD_ID""
  }

  secondary_artifacts {
    type                = ""S3""
    artifact_identifier = ""another_artifact""
    location           = aws_s3_bucket.artifact_bucket.id
    path               = ""another/""
    packaging          = ""ZIP""
    namespace_type     = ""BUILD_ID""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""ENVIRONMENT""
      value = ""development""
    }
  }

  source {
    type            = ""CODECOMMIT""
    location        = ""https://git-codecommit.region.amazonaws.com/v1/repos/example""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""example-codebuild-log-group""
      stream_name = ""example-codebuild-log-stream""
      status      = ""ENABLED""
    }

    s3_logs {
      status   = ""ENABLED""
      location = ""${aws_s3_bucket.artifact_bucket.id}/build-logs/""
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration includes:

1. An IAM role and policy for CodeBuild with basic permissions
2. An S3 bucket for artifacts
3. A CodeBuild project with:
   - Two secondary artifacts configurations
   - Environment configuration
   -",,,,,,"I'll help you create an AWS CodeBuild project with an IAM role and secondary artifacts. Here's a comprehensive Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-project-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""codebuild-project-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:*"",
          ""codecommit:*""
        ]
      }
    ]
  })
}

# S3 Bucket for artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""project"" {
  name           = ""my-codebuild-project""
  description    = ""My CodeBuild project""
  build_timeout  = ""60""
  service_role   = aws_iam_role.codebuild_role.arn
  queued_timeout = ""480""

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  secondary_artifacts {
    type                = ""S3""
    artifact_identifier = ""secondary_artifact_1""
    location           = aws_s3_bucket.artifact_bucket.bucket
    path               = ""secondary/""
    packaging          = ""ZIP""
    namespace_type     = ""BUILD_ID""
  }

  secondary_artifacts {
    type                = ""S3""
    artifact_identifier = ""secondary_artifact_2""
    location           = aws_s3_bucket.artifact_bucket.bucket
    path               = ""another-secondary/""
    packaging          = ""ZIP""
    namespace_type     = ""BUILD_ID""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""ENVIRONMENT""
      value = ""development""
    }
  }

  source {
    type            = ""CODECOMMIT""
    location        = ""https://git-codecommit.region.amazonaws.com/v1/repos/my-repo""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""codebuild-log-group""
      stream_name = ""codebuild-log-stream""
      status      = ""ENABLED""
    }

    s3_logs {
      status   = ""ENABLED""
      location = ""${aws_s3_bucket.artifact_bucket.bucket}/build-logs/""
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration includes:

1. The required AWS provider configuration
2. An IAM role and policy for CodeBuild with basic permissions
3. An S3 bucket for storing artifacts
4. A CodeBuild project",,,,,
"aws_db_instance, aws_db_subnet_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc","sets up an AWS environment with a main VPC, two subnets across different availability zones, and a security group to allow database access. It also configures a 20 GB 15.3 PostgresSQL RDS instance with public accessibility, linked to the defined subnets and security group, outputting the database endpoint for connectivity.","package terraform.validation

default is_valid_db_instance = false

default is_valid_vpc = false

default is_valid_subnet = false

default is_valid_security_group = false

default is_valid_db_subnet_group = false

# Validate aws_db_instance resource
is_valid_db_instance {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_instance""
        resource.expressions.allocated_storage.constant_value == 20
        resource.expressions.engine.constant_value == ""postgres""
        resource.expressions.engine_version.constant_value == ""15.3""
        resource.expressions.instance_class.constant_value != null
        resource.expressions.password.constant_value != null
        resource.expressions.username.constant_value != null
        resource.expressions.publicly_accessible.constant_value == true
        resource.expressions.db_subnet_group_name.references != null
        resource.expressions.vpc_security_group_ids.references != null
}

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# Validate aws_subnet resource (more than one)
is_valid_subnet {
        count(subnets) > 1
}

subnets[resource] {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
}

# Validate aws_security_group resource
is_valid_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""
        # Additional checks for specific ingress/egress rules can be added here
}

# Validate aws_db_subnet_group resource
is_valid_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}
",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type           = ""gp2""
  engine                 = ""postgres""
  engine_version         = ""17.2""
  instance_class         = ""db.t4g.micro""
  skip_final_snapshot    = true
  db_name                = ""mydbname""
  username               = ""username""
  password               = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name   = aws_db_subnet_group.main.id
  publicly_accessible    = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""
    with one publicly_accessible = true

has one aws_vpc

has more than one aws_subnet

has one security_group

has one db_subnet_group","I'll help you create a Terraform configuration that sets up an AWS environment with VPC, subnets, security group, and a PostgreSQL RDS instance. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""subnet-2""
  }
}

# Security Group
resource ""aws_security_group"" ""db_sg"" {
  name        = ""database-sg""
  description = ""Security group for PostgreSQL database""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""main-db-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgresql-db""
  engine              = ""postgres""
  engine_version      = ""15.3""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""your-secure-password""
  
  publicly_accessible = true
  
  db_subnet_group_name   = aws_db_subnet_group.db_subnet_group.name
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  
  skip_final_snapshot = true

  tags = {
    Name = ""PostgreSQL-DB""
  }
}

# Output
output ""db_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with CIDR block 10.0.0.0/16
2. Creates two subnets in different availability zones
3. Configures a security group allowing PostgreSQL access (port 5432)
4. Creates a DB subnet group with both subnets
5. Provisions a PostgreSQL 15.3 RDS instance with:
   - 20GB storage
   - Public accessibility
   - Connected to the created subnets and security group
6. Outputs the database endpoint for connection

Remember to:
- Replace ""your-secure-password"" with a strong password
- Consider your security requirements before allowing public access (0.0.0.0/0)
- Adjust the region and availability zones as needed
- Modify the instance class based on your performance requirements",,,,,,"I'll help you create a Terraform configuration that sets up an AWS environment with VPC, subnets, security group, and a PostgreSQL RDS instance. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""subnet-2""
  }
}

# Security Group
resource ""aws_security_group"" ""db_sg"" {
  name        = ""database-sg""
  description = ""Security group for PostgreSQL database""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""database-sg""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""main-db-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""Main DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgresql-db""
  engine              = ""postgres""
  engine_version      = ""15.3""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""your-secure-password""
  
  publicly_accessible = true
  
  db_subnet_group_name   = aws_db_subnet_group.db_subnet_group.name
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  
  skip_final_snapshot = true

  tags = {
    Name = ""postgresql-db""
  }
}

# Output
output ""db_endpoint"" {
  description = ""The endpoint of the database""
  value       = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with CIDR block 10.0.0.0/16
2. Creates two subnets in different availability zones
3. Creates a security group allowing PostgreSQL access (port 5432)
4. Sets up a DB subnet group with both subnets
5. Creates a PostgreSQL 15.3 RDS instance with:
   - 20GB storage
   - Public accessibility
   - Connected to the created VPC, subnets, and security group
6. Outputs the database endpoint

Note: Remember to replace ""your-secure-password"" with a strong password and consider storing it in a secure manner (e.g., using AWS Secrets Manager or SSM Parameter",,,,,,"I'll help you create a Terraform configuration that sets up an AWS environment with VPC, subnets, security group, and a PostgreSQL RDS instance according to your requirements. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""subnet-2""
  }
}

# Security Group
resource ""aws_security_group"" ""db_sg"" {
  name        = ""database-sg""
  description = ""Security group for PostgreSQL database""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""database-sg""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""main-db-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""Main DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgres-db""
  engine              = ""postgres""
  engine_version      = ""15.3""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""your-secure-password""
  
  publicly_accessible = true
  
  db_subnet_group_name   = aws_db_subnet_group.db_subnet_group.name
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  
  skip_final_snapshot = true

  tags = {
    Name = ""postgres-db""
  }
}

# Output
output ""db_endpoint"" {
  description = ""The connection endpoint for the database""
  value       = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with CIDR block 10.0.0.0/16
2. Creates two subnets in different availability zones
3. Creates a security group allowing PostgreSQL access (port 5432)
4. Sets up a DB subnet group with both subnets
5. Creates a PostgreSQL 15.3 RDS instance with:
   - 20GB storage
   - Public accessibility
   - Linked to the created subnets and security group
6. Outputs the database endpoint

Remember to:
- Replace ""your-secure-password"" with a strong password
- Consider your security requirements before allowing public access (0.0.0.0",,,,,
"aws_rds_cluster, aws_vpc","Create an mySQL cluster with 100 GB allocated to us-east-1a, us-east-1b, us-east-1c.","package main

import future.keywords.in

default allow = false

# Required AZs
required_azs := [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]

# Check for RDS cluster creation
rds_cluster_created(resources) {
some resource in resources
resource.type == ""aws_rds_cluster""
resource.change.actions[_] == ""create""
}

# Check if engine is MySQL
rds_engine_mysql(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.engine == ""mysql""
}

# Check if AZs are correct and storage is 100GB
azs_and_storage_valid(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.availability_zones == required_azs
resource.change.after.allocated_storage == 100
}

# Aggregate checks for RDS cluster
allow {
rds_cluster_created(input.resource_changes)
some resource in input.resource_changes
rds_engine_mysql(resource)
azs_and_storage_valid(resource)
}",3,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

# Configure the AWS Provider
provider ""aws"" {
region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
cidr_block = ""192.168.0.0/22""
}

resource ""aws_rds_cluster"" ""example"" {
cluster_identifier = ""example""
availability_zones = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
engine = ""mysql""
db_cluster_instance_class = ""db.r6gd.large""
storage_type = ""io1""
allocated_storage = 100
iops = 1000
master_username = ""test""
master_password = ""mustbeeightcharaters""
skip_final_snapshot = true
}
","Has one ""aws_rds_cluster"" resource
with one ""allocated_storage"" = 100
with one ""availability_zones"" = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
with one ""engine"" = ""mysql""","I'll help you create a Terraform configuration for an Amazon RDS MySQL cluster with multi-AZ deployment across three availability zones in us-east-1. Here's the code:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_subnet_group"" ""mysql_subnet_group"" {
  name       = ""mysql-subnet-group""
  subnet_ids = [""subnet-12345678"", ""subnet-87654321"", ""subnet-11223344""] # Replace with your subnet IDs

  tags = {
    Name = ""MySQL DB subnet group""
  }
}

resource ""aws_db_instance"" ""mysql_cluster"" {
  identifier           = ""mysql-cluster""
  engine              = ""mysql""
  engine_version      = ""8.0.28""
  instance_class      = ""db.t3.medium""
  allocated_storage   = 100
  storage_type        = ""gp2""
  
  multi_az            = true
  availability_zone   = ""us-east-1a""
  
  db_name             = ""mydb""
  username           = ""admin""
  password           = ""your-password-here""  # Replace with secure password
  
  db_subnet_group_name   = aws_db_subnet_group.mysql_subnet_group.name
  vpc_security_group_ids = [""sg-12345678""]  # Replace with your security group ID
  
  backup_retention_period = 7
  skip_final_snapshot    = true

  tags = {
    Name = ""MySQL Cluster""
    Environment = ""Production""
  }
}

resource ""aws_db_instance"" ""mysql_replicas"" {
  count               = 2
  identifier          = ""mysql-replica-${count.index + 1}""
  instance_class      = ""db.t3.medium""
  replicate_source_db = aws_db_instance.mysql_cluster.identifier
  
  availability_zone   = count.index == 0 ? ""us-east-1b"" : ""us-east-1c""
  
  auto_minor_version_upgrade = true
  multi_az                  = false
  
  tags = {
    Name = ""MySQL Replica ${count.index + 1}""
    Environment = ""Production""
  }
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a DB subnet group spanning multiple AZs
3. Creates a primary MySQL instance in us-east-1a with 100GB storage
4. Creates two read replicas in us-east-1b and us-east-1c
5. Includes basic security and backup configurations

Note: You'll need to replace the following placeholder values:
- subnet IDs in the subnet group
- security group ID
- database password
- Adjust the instance class and other parameters based on your specific requirements

The cluster will have 100GB storage allocated across all three availability zones as requested, with automatic replication between the primary and replica instances.",,,,,,"I'll help you create a Terraform configuration for an AWS RDS MySQL cluster with multi-AZ deployment across three availability zones. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier     = ""mysql-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.10.2""
  database_name         = ""mydb""
  master_username       = ""admin""
  master_password       = ""your-secure-password""
  storage_encrypted     = true
  skip_final_snapshot   = true
  
  availability_zones = [
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ]
}

resource ""aws_rds_cluster_instance"" ""cluster_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r5.large""
  engine              = ""aurora-mysql""
  engine_version      = ""5.7.mysql_aurora.2.10.2""
  
  availability_zone   = element([
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ], count.index)
  
  allocated_storage   = 100
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates an Aurora MySQL cluster
3. Creates three cluster instances, one in each specified availability zone
4. Allocates 100GB of storage for each instance
5. Uses the Aurora MySQL engine which is AWS's optimized version of MySQL
6. Enables encryption at rest
7. Distributes the instances across us-east-1a, us-east-1b, and us-east-1c

Remember to:
- Replace ""your-secure-password"" with a strong password
- Adjust the instance class if needed (db.r5.large is just an example)
- Consider adding additional security configurations like VPC settings and security groups

Note: The actual storage allocation in Aurora is managed automatically, but we've specified the allocated_storage parameter as requested. Aurora storage automatically grows up to 128TB as needed.",,,,,,"I'll help you create a Terraform configuration for an Amazon RDS MySQL cluster with storage allocated across three availability zones. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_subnet_group"" ""mysql_subnet_group"" {
  name        = ""mysql-subnet-group""
  description = ""Subnet group for MySQL cluster""
  subnet_ids  = [""subnet-12345678"", ""subnet-87654321"", ""subnet-11223344""] # Replace with your subnet IDs
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier     = ""mysql-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.10.2""
  database_name         = ""mydb""
  master_username       = ""admin""
  master_password       = ""your-secure-password""
  skip_final_snapshot   = true
  db_subnet_group_name  = aws_db_subnet_group.mysql_subnet_group.name
  
  availability_zones = [
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ]
}

resource ""aws_rds_cluster_instance"" ""mysql_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r5.large""
  engine              = ""aurora-mysql""
  engine_version      = ""5.7.mysql_aurora.2.10.2""
  
  allocated_storage   = 100
  availability_zone   = element([""us-east-1a"", ""us-east-1b"", ""us-east-1c""], count.index)
}

```

This configuration:
1. Sets up the required AWS provider
2. Creates a DB subnet group spanning the three AZs
3. Creates an Aurora MySQL cluster
4. Creates three DB instances, each with 100GB storage, distributed across us-east-1a, us-east-1b, and us-east-1c

Make sure to:
- Replace the subnet IDs with your actual subnet IDs
- Choose a secure master password
- Adjust the instance class based on your performance needs
- Consider adding security groups and other security measures
- Review and adjust the engine version based on your requirements

Also note that Aurora MySQL handles storage automatically, so the actual storage will scale automatically based on your needs, with a minimum of 100GB as specified.",,,,,
