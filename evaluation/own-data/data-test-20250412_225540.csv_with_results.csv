Resource,Prompt,Rego intent,Difficulty,Reference output,Intent,LLM Output #0,LLM Plannable? #0,LLM Correct? #0,LLM Plan Phase Error #0,LLM OPA match phase Error #0,LLM Notes #0,LLM Output #1,LLM Plannable? #1,LLM Correct? #1,LLM Plan Phase Error #1,LLM OPA match phase Error #1,LLM Notes #1,LLM Output #2,LLM Plannable? #2,LLM Correct? #2,LLM Plan Phase Error #2,LLM OPA match phase Error #2,LLM Notes #2
"aws_api_gateway_integration, aws_api_gateway_integration, aws_api_gateway_method, aws_api_gateway_method, aws_api_gateway_resource, aws_api_gateway_rest_api, aws_dynamodb_table, aws_iam_role, aws_iam_role, aws_lambda_function, aws_lambda_function, aws_lambda_permission, aws_lambda_permission, aws_s3_bucket","An AWS service that holds a web server which allows you to upload cat pictures and provides random cat pictures on demand. Accomplish this using the following resources: AWS DynamoDB table, AWS S3 bucket, AWS Lambda function, AWS Lambda permission, AWS API Gateway rest API, AWS API Gateway resource, AWS API Gateway method. Add any necessary resources.","package caas_middle

import rego.v1

api_valid(api) := true

cat_valid(cat, api) if {
	api.address in cat.expressions.parent_id.references
	api.address in cat.expressions.rest_api_id.references
}

method_valid(method, method_string, resource, api) if {
	method.expressions.http_method.constant_value == method_string
	resource.address in method.expressions.resource_id.references
	api.address in method.expressions.rest_api_id.references
}

table_valid(table) if {
	some attribute in table.expressions.attribute
	attribute.name
	attribute.type

	table.expressions.hash_key
}

lambda_valid(lambda, bucket) if {
	some env in lambda.expressions.environment
	bucket.address in env.variables.references
}

permission_valid(permission, lambda, api) if {
	permission.expressions.action.constant_value == ""lambda:InvokeFunction""
	lambda.address in permission.expressions.function_name.references
	permission.expressions.principal.constant_value == ""apigateway.amazonaws.com""
	api.address in permission.expressions.source_arn.references
}

integration_valid(integration, lambda, method, resource, api, integration_method) if {
	method.address in integration.expressions.http_method.references
	resource.address in integration.expressions.resource_id.references
	api.address in integration.expressions.rest_api_id.references
	integration.expressions.integration_http_method.constant_value == integration_method
	integration.expressions.type.constant_value == ""AWS_PROXY""
	lambda.address in integration.expressions.uri.references
}

default valid := false

valid if {
	resources := input.configuration.root_module.resources

	some api in resources
	api.type == ""aws_api_gateway_rest_api""

	some cat in resources
	cat.type == ""aws_api_gateway_resource""

	some method_get in resources
	method_get.type == ""aws_api_gateway_method""

	some method_put in resources
	method_put.type == ""aws_api_gateway_method""

	some table in resources
	table.type == ""aws_dynamodb_table""

	some lambda_get in resources
	lambda_get.type == ""aws_lambda_function""

	some lambda_put in resources
	lambda_put.type == ""aws_lambda_function""

	some bucket in resources
	bucket.type == ""aws_s3_bucket""

	some permission_get in resources
	permission_get.type == ""aws_lambda_permission""

	some permission_put in resources
	permission_put.type == ""aws_lambda_permission""

	some integration_get in resources
	integration_get.type == ""aws_api_gateway_integration""

	some integration_put in resources
	integration_put.type == ""aws_api_gateway_integration""

	api_valid(api)
	cat_valid(cat, api)
	method_valid(method_get, ""GET"", cat, api)
	method_valid(method_put, ""PUT"", cat, api)
	lambda_valid(lambda_get, bucket, archive_get)
	lambda_valid(lambda_put, bucket, archive_put)
	permission_valid(permission_get, lambda_get, api)
	permission_valid(permission_put, lambda_put, api)
	integration_valid(integration_get, lambda_get, method_get, cat, api, ""GET"")
	integration_valid(integration_put, lambda_put, method_put, cat, api, ""PUT"")
}",6,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}


provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_dynamodb_table"" ""caas"" {
  name           = ""cat_names""
  hash_key       = ""name""
  billing_mode   = ""PAY_PER_REQUEST""

  attribute {
    name = ""name""
    type = ""S""
  }
}

resource ""aws_s3_bucket"" ""caas"" {
  bucket_prefix = ""cat-image""
}

resource ""aws_iam_role"" ""lambda_role"" {
  name = ""lambda_api_gateway_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name   = ""lambda_policy""
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject""
        ]
        Effect   = ""Allow""
        Resource = ""${aws_s3_bucket.caas.arn}/*""
      },
      {
        Action = [
          ""dynamodb:PutItem""
        ]
        Effect   = ""Allow""
        Resource = aws_dynamodb_table.caas.arn
      }
    ]
  })
}

resource ""aws_api_gateway_rest_api"" ""caas"" {
  name = ""caas""
}

resource ""aws_api_gateway_resource"" ""caas_cat"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  parent_id   = aws_api_gateway_rest_api.caas.root_resource_id
  path_part   = ""cat""
}

resource ""aws_api_gateway_method"" ""caas_cat_get"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""GET""
  authorization = ""NONE""
}

resource ""aws_api_gateway_method"" ""caas_cat_put"" {
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  resource_id   = aws_api_gateway_resource.caas_cat.id
  http_method   = ""PUT""
  authorization = ""NONE""
}

data ""archive_file"" ""caas_cat"" {
  type        = ""zip""
  source_file = ""./supplement/caas_cat.py""
  output_path = ""./supplement/caas_cat.zip""
}

resource ""aws_lambda_function"" ""caas_cat"" {
  function_name = ""caas_cat""
  role          = aws_iam_role.lambda_role.arn
  filename      = data.archive_file.caas_cat.output_path
  source_code_hash = data.archive_file.caas_cat.output_base64sha256
  handler       = ""caas_cat.handler""
  runtime       = ""python3.12""

  environment {
    variables = {
      CAAS_S3_BUCKET = ""${aws_s3_bucket.caas.id}""
      DYNAMODB_TABLE_NAME = ""${aws_dynamodb_table.caas.id}""
    }
  }
}

resource ""aws_api_gateway_integration"" ""caas_cat_get"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_get.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""GET""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_api_gateway_integration"" ""caas_cat_put"" {
  rest_api_id             = aws_api_gateway_rest_api.caas.id
  resource_id             = aws_api_gateway_resource.caas_cat.id
  http_method             = aws_api_gateway_method.caas_cat_put.http_method
  type                    = ""AWS_PROXY""
  integration_http_method = ""PUT""
  uri                     = aws_lambda_function.caas_cat.invoke_arn
}

resource ""aws_lambda_permission"" ""caas_cat"" {
  action        = ""lambda:InvokeFunction""
  principal     = ""apigateway.amazonaws.com""
  function_name = aws_lambda_function.caas_cat.function_name

  source_arn = ""${aws_api_gateway_rest_api.caas.execution_arn}/*/*""
}

resource ""aws_api_gateway_deployment"" ""api_deployment"" {
  rest_api_id = aws_api_gateway_rest_api.caas.id
  depends_on  = [aws_api_gateway_integration.caas_cat_get, 
                 aws_api_gateway_integration.caas_cat_put]
}

resource ""aws_api_gateway_stage"" ""api_stage"" {
  deployment_id = aws_api_gateway_deployment.api_deployment.id
  rest_api_id   = aws_api_gateway_rest_api.caas.id
  stage_name    = ""dev""
}

output ""api_id"" {
  value = aws_api_gateway_rest_api.caas.id
  description = ""The API Gateway ID""
}","have one ""aws_api_gateway_rest_api"" resource
    with ""name"" argument
have one ""aws_api_gateway_resource"" resource
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource id
    with ""parent_id"" argument linking to aws_api_gateway_rest_api resource root_resource_id
    with ""path_part"" argument
have two ""aws_api_gateway_method"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_api resource id
    with ""resource_id"" linking to aws_api_gateway_resource resource
    with ""authorization"" argument
    with ""http_method"" argument with values ""GET"" and ""PUT""
have one ""aws_s3_bucket"" resource
have one ""aws_dynamodb_table"" resource
    with ""name"" argument
    with ""hash_key"" argument
    with ""attribute"" argument block
        with ""name"" argument
        with ""type"" argument
with two ""aws_lambda_function"" resources
    with ""function_name"" argument
    with ""role"" argument
    with ""environment"" argument block
        with ""variables"" argument block linking to the aws_s3_bucket
with two ""aws_lambda_permission"" resources
    with ""action"" argument with value ""lambda:InvokeFunction""
    with ""principal"" argument with value ""apigateway.amazonaws.com""
    with ""function_name"" argument linking to aws_lambda_function resource
    with ""source_arn"" argument linking to aws_api_gateway_rest_api (two different end points)
with two ""aws_api_gateway_integration"" resources
    with ""rest_api_id"" argument linking to aws_api_gateway_rest_api resource
    with ""resource_id"" argument linking to aws_api_gateway_resource resource
    with ""http_method"" argument linking to aws_api_gateway_method resource
    with ""type"" argument with value ""AWS_PROXY""
    with ""integration_http_method"" argument with values ""GET"" and ""PUT""","I'll help you create a Terraform configuration for a cat picture service using AWS services. Here's a complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-storage-${random_string.suffix.result}""
}

# Random string for unique naming
resource ""random_string"" ""suffix"" {
  length  = 8
  special = false
  upper   = false
}

# DynamoDB table for metadata
resource ""aws_dynamodb_table"" ""cat_metadata"" {
  name           = ""cat-pictures-metadata""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""

  attribute {
    name = ""id""
    type = ""S""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket"",
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*"",
          aws_dynamodb_table.cat_metadata.arn
        ]
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_pictures_lambda"" {
  filename         = ""lambda_function.zip""
  function_name    = ""cat_pictures_handler""
  role            = aws_iam_role.lambda_role.arn
  handler         = ""index.handler""
  runtime         = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME = aws_s3_bucket.cat_pictures.id
      TABLE_NAME  = aws_dynamodb_table.cat_metadata.name
    }
  }
}

# API Gateway REST API
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name        = ""cat-pictures-api""
  description = ""API for cat pictures service""
}

# API Gateway resource for /cats
resource ""aws_api_gateway_resource"" ""cats"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway GET method
resource ""aws_api_gateway_method"" ""get_cats"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  resource_id   = aws_api_gateway_resource.cats.id
  http_method   = ""GET""
  authorization = ""NONE""
}

# API Gateway POST method
resource ""aws_api_gateway_method"" ""post_cats"" {
  rest_api_id   = aws_api_gateway_rest_api.cat_api.id
  ",,,,,,"I'll help you create a Terraform configuration for a cat picture service using the specified AWS resources. This will include an S3 bucket for storage, DynamoDB for metadata, Lambda for processing, and API Gateway for the interface.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-storage-${random_pet.suffix.id}""
}

resource ""aws_s3_bucket_public_access_block"" ""cat_pictures"" {
  bucket = aws_s3_bucket.cat_pictures.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# Random suffix for unique naming
resource ""random_pet"" ""suffix"" {
  length = 2
}

# DynamoDB table for metadata
resource ""aws_dynamodb_table"" ""cat_metadata"" {
  name           = ""cat-pictures-metadata-${random_pet.suffix.id}""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  attribute {
    name = ""id""
    type = ""S""
  }
}

# IAM role for Lambda
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket"",
          ""dynamodb:GetItem"",
          ""dynamodb:PutItem"",
          ""dynamodb:Query"",
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*"",
          aws_dynamodb_table.cat_metadata.arn
        ]
      }
    ]
  })
}

# Lambda function
resource ""aws_lambda_function"" ""cat_processor"" {
  filename         = ""lambda_function.zip""  # You'll need to create this
  function_name    = ""cat-picture-processor""
  role             = aws_iam_role.lambda_role.arn
  handler          = ""index.handler""
  runtime          = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME = aws_s3_bucket.cat_pictures.id
      TABLE_NAME  = aws_dynamodb_table.cat_metadata.name
    }
  }
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat-pictures-api""
}

# API Gateway resource
resource ""aws_api_gateway_resource"" ""cat_resource"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat_api.root_resource_id
  path_part   = ""cats""
}

# API Gateway method
resource ""aws_api_gateway",,,,,,"I'll help you create a Terraform configuration for a cat picture service using AWS services. This will include all the necessary resources to store, upload, and retrieve cat pictures randomly.

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# S3 bucket for storing cat pictures
resource ""aws_s3_bucket"" ""cat_pictures"" {
  bucket = ""cat-pictures-storage-${random_id.unique.hex}""
}

resource ""random_id"" ""unique"" {
  byte_length = 8
}

# DynamoDB table to store picture metadata
resource ""aws_dynamodb_table"" ""cat_pictures_table"" {
  name           = ""cat-pictures-metadata""
  billing_mode   = ""PAY_PER_REQUEST""
  hash_key       = ""id""
  attribute {
    name = ""id""
    type = ""S""
  }
}

# IAM role for Lambda functions
resource ""aws_iam_role"" ""lambda_role"" {
  name = ""cat_pictures_lambda_role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""lambda.amazonaws.com""
        }
      }
    ]
  })
}

# IAM policy for Lambda to access S3 and DynamoDB
resource ""aws_iam_role_policy"" ""lambda_policy"" {
  name = ""cat_pictures_lambda_policy""
  role = aws_iam_role.lambda_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Action = [
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:ListBucket"",
          ""dynamodb:PutItem"",
          ""dynamodb:GetItem"",
          ""dynamodb:Scan""
        ]
        Resource = [
          aws_s3_bucket.cat_pictures.arn,
          ""${aws_s3_bucket.cat_pictures.arn}/*"",
          aws_dynamodb_table.cat_pictures_table.arn
        ]
      }
    ]
  })
}

# Lambda function for getting random cat picture
resource ""aws_lambda_function"" ""get_random_cat"" {
  filename      = ""get_random_cat.zip""  # You'll need to create this
  function_name = ""get-random-cat""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME = aws_s3_bucket.cat_pictures.id
      TABLE_NAME  = aws_dynamodb_table.cat_pictures_table.name
    }
  }
}

# Lambda function for uploading cat picture
resource ""aws_lambda_function"" ""upload_cat"" {
  filename      = ""upload_cat.zip""  # You'll need to create this
  function_name = ""upload-cat""
  role          = aws_iam_role.lambda_role.arn
  handler       = ""index.handler""
  runtime       = ""nodejs14.x""

  environment {
    variables = {
      BUCKET_NAME = aws_s3_bucket.cat_pictures.id
      TABLE_NAME  = aws_dynamodb_table.cat_pictures_table.name
    }
  }
}

# API Gateway
resource ""aws_api_gateway_rest_api"" ""cat_api"" {
  name = ""cat-pictures-api""
}

# API Gateway resources
resource ""aws_api_gateway_resource"" ""get_cat"" {
  rest_api_id = aws_api_gateway_rest_api.cat_api.id
  parent_id   = aws_api_gateway_rest_api.cat",,,,,
"aws_codebuild_project, aws_iam_role, aws_s3_bucket, aws_s3_bucket",create a AWS codebuild project resource with example iam role and secondary artifiacts,"package terraform.validation

import future.keywords.in

default has_valid_resources = false

# Rule for aws_iam_role resource
has_valid_iam_role(resources) { 
        some resource in resources 
    resource.type == ""aws_iam_role"" 
    contains(resource.change.after.assume_role_policy,""codebuild.amazonaws.com"") 
} 

has_valid_codebuild {
        some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_codebuild_project""
        resource.values.artifacts
    resource.values.environment
    resource.values.name
    resource.values.source[_].type
    resource.values.secondary_artifacts[_].type
    role := input.configuration.root_module.resources[i]
    role.expressions.service_role
}

has_valid_bucket {
    some i
    resource := input.planned_values.root_module.resources[i]
    resource.type == ""aws_s3_bucket""
    
}


has_valid_resources {
        has_valid_iam_role(input.resource_changes)
    has_valid_codebuild
}",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region  = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_iam_role"" ""test_role8"" {
  name = ""test_role8""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Sid    = """"
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      },
    ]
  })
}

resource ""aws_s3_bucket"" ""apriltwentyninth"" {
  bucket = ""apriltwentyninth""
}

resource ""aws_s3_bucket"" ""apriltwentyninth2"" {
  bucket = ""apriltwentyninth2""
}


resource ""aws_codebuild_project"" ""example7"" {
  name          = ""test-project8""
  service_role  = aws_iam_role.test_role8.arn

  artifacts {
    location  = aws_s3_bucket.apriltwentyninth.bucket
    type      = ""S3""
    name     = ""results.zip""
    path      = ""/""
    packaging = ""ZIP""
  }

  secondary_artifacts {
    artifact_identifier =  ""SecondaryArtifact""
    type = ""S3""
    location  = aws_s3_bucket.apriltwentyninth2.bucket
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:7.0-24.10.29""
    type                        = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  source_version = ""master""

  secondary_sources {
    source_identifier = ""source2""
    type            = ""GITHUB""
    location        = ""https://github.com/mitchellh/packer.git""
    git_clone_depth = 1
  }

  secondary_source_version {
    source_identifier = ""source2""
    source_version = ""master""
  }
}","Has one ""aws_iam_role"" resource, at least one ""aws_s3_bucket"" resource, and one ""aws_codebuild_project"" resource with a ""secondary_artifacts"" block","I'll help you create a AWS CodeBuild project resource with an IAM role and secondary artifacts. Here's a complete Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-service-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""codebuild-service-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:GetObject"",
          ""s3:GetObjectVersion"",
          ""s3:PutObject"",
          ""s3:ListBucket""
        ]
      }
    ]
  })
}

# S3 Bucket for artifacts
resource ""aws_s3_bucket"" ""artifacts"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""example"" {
  name          = ""example-project""
  description   = ""Example CodeBuild project""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  secondary_artifacts {
    type                = ""S3""
    artifact_identifier = ""secondary_artifact_1""
    location           = aws_s3_bucket.artifacts.bucket
    path               = ""secondary1/""
    packaging          = ""ZIP""
    namespace_type     = ""BUILD_ID""
  }

  secondary_artifacts {
    type                = ""S3""
    artifact_identifier = ""secondary_artifact_2""
    location           = aws_s3_bucket.artifacts.bucket
    path               = ""secondary2/""
    packaging          = ""ZIP""
    namespace_type     = ""BUILD_ID""
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""example-log-group""
      stream_name = ""example-log-stream""
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration includes:

1. Required AWS provider configuration
2. An IAM role and policy for CodeBuild with basic permissions
3. An S3 bucket for artifacts
4. A CodeBuild project with:
   - Two secondary artifacts configured to store in S3
   - Basic environment configuration
   - GitHub source configuration
   - CloudWatch logs configuration
   - Tags

The CodeBuild project uses the created IAM role and includes two secondary artifacts that will be stored in the S3 bucket. Each secondary artifact has its own path and configuration.

Remember to:
- Replace the S3 bucket name with a unique name",,,,,,"I'll help you create an AWS CodeBuild project with an IAM role and secondary artifacts using Terraform. Here's a complete example:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-service-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""codebuild-service-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:*"",
          ""codecommit:*""
        ]
      }
    ]
  })
}

# S3 Bucket for artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""example"" {
  name          = ""example-project""
  description   = ""Example CodeBuild project""
  build_timeout = ""60""
  service_role  = aws_iam_role.codebuild_role.arn

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  secondary_artifacts {
    type                   = ""S3""
    artifact_identifier    = ""secondary_artifact_1""
    location              = aws_s3_bucket.artifact_bucket.id
    packaging             = ""ZIP""
    encryption_disabled   = false
    name                  = ""secondary_artifact_name""
    namespace_type        = ""BUILD_ID""
    override_artifact_name = true
  }

  secondary_artifacts {
    type                   = ""S3""
    artifact_identifier    = ""secondary_artifact_2""
    location              = aws_s3_bucket.artifact_bucket.id
    packaging             = ""ZIP""
    encryption_disabled   = false
    name                  = ""another_secondary_artifact""
    namespace_type        = ""BUILD_ID""
    override_artifact_name = true
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                      = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""

    environment_variable {
      name  = ""ENVIRONMENT""
      value = ""production""
    }
  }

  source {
    type            = ""CODECOMMIT""
    location        = ""https://git-codecommit.region.amazonaws.com/v1/repos/example""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""example-codebuild-log-group""
      stream_name = ""example-codebuild-log-stream""
      status      = ""ENABLED""
    }

    s3_logs {
      status   = ""ENABLED""
      location = ""${aws_s3_bucket.artifact_bucket.id}/build-logs""
    }
  }

  tags = {
    Environment = ""Production""
  }
}
```

This configuration includes:

1. An IAM role and policy",,,,,,"I'll help you create an AWS CodeBuild project with an IAM role and secondary artifacts. Here's a complete Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# IAM Role for CodeBuild
resource ""aws_iam_role"" ""codebuild_role"" {
  name = ""codebuild-project-role""

  assume_role_policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Action = ""sts:AssumeRole""
        Effect = ""Allow""
        Principal = {
          Service = ""codebuild.amazonaws.com""
        }
      }
    ]
  })
}

# IAM Role Policy for CodeBuild
resource ""aws_iam_role_policy"" ""codebuild_policy"" {
  name = ""codebuild-project-policy""
  role = aws_iam_role.codebuild_role.id

  policy = jsonencode({
    Version = ""2012-10-17""
    Statement = [
      {
        Effect = ""Allow""
        Resource = [""*""]
        Action = [
          ""logs:CreateLogGroup"",
          ""logs:CreateLogStream"",
          ""logs:PutLogEvents"",
          ""s3:GetObject"",
          ""s3:PutObject"",
          ""s3:GetObjectVersion""
        ]
      }
    ]
  })
}

# S3 Bucket for artifacts
resource ""aws_s3_bucket"" ""artifact_bucket"" {
  bucket = ""my-codebuild-artifacts-bucket""
}

# CodeBuild Project
resource ""aws_codebuild_project"" ""project"" {
  name           = ""example-project""
  description    = ""Example CodeBuild project""
  build_timeout  = ""60""
  service_role   = aws_iam_role.codebuild_role.arn
  queued_timeout = 480

  artifacts {
    type = ""NO_ARTIFACTS""
  }

  secondary_artifacts {
    type                   = ""S3""
    artifact_identifier    = ""secondary_artifact""
    location              = aws_s3_bucket.artifact_bucket.bucket
    name                  = ""secondary_output""
    namespace_type        = ""BUILD_ID""
    packaging            = ""ZIP""
    encryption_disabled  = false
  }

  secondary_artifacts {
    type                   = ""S3""
    artifact_identifier    = ""another_artifact""
    location              = aws_s3_bucket.artifact_bucket.bucket
    name                  = ""another_output""
    namespace_type        = ""BUILD_ID""
    packaging            = ""ZIP""
    encryption_disabled  = false
  }

  environment {
    compute_type                = ""BUILD_GENERAL1_SMALL""
    image                       = ""aws/codebuild/standard:5.0""
    type                       = ""LINUX_CONTAINER""
    image_pull_credentials_type = ""CODEBUILD""
  }

  source {
    type            = ""GITHUB""
    location        = ""https://github.com/example/repository.git""
    git_clone_depth = 1
    buildspec       = ""buildspec.yml""
  }

  logs_config {
    cloudwatch_logs {
      group_name  = ""example-project-log-group""
      stream_name = ""example-project-log-stream""
    }

    s3_logs {
      status   = ""ENABLED""
      location = ""${aws_s3_bucket.artifact_bucket.bucket}/build-logs""
    }
  }

  tags = {
    Environment = ""Development""
  }
}
```

This configuration includes:

1. Required AWS provider configuration
2. An IAM role and policy for CodeBuild with basic permissions
3. An S3 bucket for artifacts
4. A CodeBuild project with:
   - Multiple secondary artifacts configurations
   - Environment settings
   - Source configuration (GitHub in",,,,,
"aws_db_instance, aws_db_subnet_group, aws_security_group, aws_subnet, aws_subnet, aws_vpc","sets up an AWS environment with a main VPC, two subnets across different availability zones, and a security group to allow database access. It also configures a 20 GB 15.3 PostgresSQL RDS instance with public accessibility, linked to the defined subnets and security group, outputting the database endpoint for connectivity.","package terraform.validation

default is_valid_db_instance = false

default is_valid_vpc = false

default is_valid_subnet = false

default is_valid_security_group = false

default is_valid_db_subnet_group = false

# Validate aws_db_instance resource
is_valid_db_instance {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_instance""
        resource.expressions.allocated_storage.constant_value == 20
        resource.expressions.engine.constant_value == ""postgres""
        resource.expressions.engine_version.constant_value == ""15.3""
        resource.expressions.instance_class.constant_value != null
        resource.expressions.password.constant_value != null
        resource.expressions.username.constant_value != null
        resource.expressions.publicly_accessible.constant_value == true
        resource.expressions.db_subnet_group_name.references != null
        resource.expressions.vpc_security_group_ids.references != null
}

# Validate aws_vpc resource
is_valid_vpc {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_vpc""
        resource.expressions.cidr_block.constant_value != null
}

# Validate aws_subnet resource (more than one)
is_valid_subnet {
        count(subnets) > 1
}

subnets[resource] {
        resource := input.configuration.root_module.resources[_]
        resource.type == ""aws_subnet""
}

# Validate aws_security_group resource
is_valid_security_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_security_group""
        # Additional checks for specific ingress/egress rules can be added here
}

# Validate aws_db_subnet_group resource
is_valid_db_subnet_group {
        some i
        resource := input.configuration.root_module.resources[i]
        resource.type == ""aws_db_subnet_group""
        count(resource.expressions.subnet_ids) > 0
}
",5,"terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 5.75""
    }
  }

  required_version = ""~> 1.9.8""
}

provider ""aws"" {
  region = ""us-east-1""
  profile = ""admin-1""

  assume_role {
    role_arn = ""arn:aws:iam::590184057477:role/yicun-iac""
  }
}

resource ""aws_db_instance"" ""postgres"" {
  allocated_storage      = 20
  storage_type           = ""gp2""
  engine                 = ""postgres""
  engine_version         = ""17.2""
  instance_class         = ""db.t4g.micro""
  skip_final_snapshot    = true
  db_name                = ""mydbname""
  username               = ""username""
  password               = ""password""
  vpc_security_group_ids = [aws_security_group.rds-proxy-sg.id]
  db_subnet_group_name   = aws_db_subnet_group.main.id
  publicly_accessible    = true
}

data ""aws_availability_zones"" ""available"" {
  state = ""available""
}

module ""vpc"" {
  source  = ""terraform-aws-modules/vpc/aws""

  name                 = ""main-vpc""
  cidr                 = ""10.0.0.0/16""
  azs                  = data.aws_availability_zones.available.names
  private_subnets      = [""10.0.1.0/24"", ""10.0.2.0/24""]
  public_subnets       = [""10.0.3.0/24"", ""10.0.4.0/24""]
}

resource ""aws_security_group"" ""rds-proxy-sg"" {
  name        = ""rds-proxy-sg""
  vpc_id      = module.vpc.vpc_id
}

resource ""aws_vpc_security_group_ingress_rule"" ""db-proxy-sg-ingress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_vpc_security_group_egress_rule"" ""db-proxy-sg-egress-rule"" {
  from_port       = 5432
  to_port         = 5432
  ip_protocol     = ""tcp""
  cidr_ipv4       = ""0.0.0.0/0""
  security_group_id = aws_security_group.rds-proxy-sg.id
}

resource ""aws_db_subnet_group"" ""main"" {
  name       = ""main""
  subnet_ids = module.vpc.private_subnets
}","Has one ""aws_db_instance"" resource
    with one ""allocated_storage""
    with one ""engine"" and a valid engine value
    with one ""instance_class"" and a valid instance class type
    with one ""password""
    with one ""username""
    with one publicly_accessible = true

has one aws_vpc

has more than one aws_subnet

has one security_group

has one db_subnet_group","I'll help you create a Terraform configuration that sets up an AWS environment with VPC, subnets, security group, and a PostgreSQL RDS instance. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""subnet-2""
  }
}

# Security Group
resource ""aws_security_group"" ""db_sg"" {
  name        = ""database-sg""
  description = ""Security group for PostgreSQL database""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""database-sg""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""main-db-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""Main DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgresql"" {
  identifier           = ""postgresql-db""
  engine              = ""postgres""
  engine_version      = ""15.3""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""your-secure-password""
  
  publicly_accessible = true
  
  db_subnet_group_name   = aws_db_subnet_group.db_subnet_group.name
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  
  skip_final_snapshot = true

  tags = {
    Name = ""postgresql-db""
  }
}

# Output
output ""db_endpoint"" {
  description = ""The endpoint of the database""
  value       = aws_db_instance.postgresql.endpoint
}
```

This configuration:
1. Sets up a VPC with CIDR block 10.0.0.0/16
2. Creates two subnets in different availability zones
3. Configures a security group allowing PostgreSQL access (port 5432)
4. Creates a DB subnet group using both subnets
5. Provisions a PostgreSQL 15.3 RDS instance with:
   - 20GB GP2 storage
   - Public accessibility
   - t3.micro instance class
   - Connected to the created VPC, subnets, and security group
6. Outputs the database endpoint for connection

Remember to:
- Replace ""your-secure-password"" with a strong password
- Consider your security",,,,,,"I'll help you create a Terraform configuration that sets up an AWS environment with VPC, subnets, security group, and a PostgreSQL RDS instance. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC Configuration
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Subnet Configurations
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""subnet-2""
  }
}

# Security Group
resource ""aws_security_group"" ""db_sg"" {
  name        = ""database-sg""
  description = ""Security group for PostgreSQL database""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""database-sg""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""main-db-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgresql-db""
  engine              = ""postgres""
  engine_version      = ""15.3""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""your-secure-password""
  
  publicly_accessible = true
  
  db_subnet_group_name   = aws_db_subnet_group.db_subnet_group.name
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  
  skip_final_snapshot = true

  tags = {
    Name = ""postgresql-db""
  }
}

# Output the database endpoint
output ""db_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with CIDR block 10.0.0.0/16
2. Creates two subnets in different availability zones
3. Configures a security group allowing PostgreSQL access (port 5432)
4. Creates a DB subnet group with both subnets
5. Deploys a PostgreSQL 15.3 RDS instance with:
   - 20GB GP2 storage
   - Public accessibility
   - DB instance class: db.t3.micro
   - Connected to the created VPC, subnets, and security group
6. Outputs the database endpoint for connection

Remember to:
- Replace ""your-secure-password"" with a strong password
- Consider your security requirements before allowing public",,,,,,"I'll help you create a Terraform configuration that sets up an AWS VPC, subnets, security group, and a PostgreSQL RDS instance. Here's the complete solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-west-2""
}

# VPC
resource ""aws_vpc"" ""main"" {
  cidr_block           = ""10.0.0.0/16""
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name = ""main-vpc""
  }
}

# Subnets
resource ""aws_subnet"" ""subnet_1"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.1.0/24""
  availability_zone = ""us-west-2a""

  tags = {
    Name = ""subnet-1""
  }
}

resource ""aws_subnet"" ""subnet_2"" {
  vpc_id            = aws_vpc.main.id
  cidr_block        = ""10.0.2.0/24""
  availability_zone = ""us-west-2b""

  tags = {
    Name = ""subnet-2""
  }
}

# Security Group
resource ""aws_security_group"" ""db_sg"" {
  name        = ""database-sg""
  description = ""Security group for PostgreSQL database""
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 5432
    to_port     = 5432
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }

  tags = {
    Name = ""database-sg""
  }
}

# DB Subnet Group
resource ""aws_db_subnet_group"" ""db_subnet_group"" {
  name       = ""main-db-subnet-group""
  subnet_ids = [aws_subnet.subnet_1.id, aws_subnet.subnet_2.id]

  tags = {
    Name = ""Main DB subnet group""
  }
}

# RDS Instance
resource ""aws_db_instance"" ""postgres"" {
  identifier           = ""postgresql-db""
  engine              = ""postgres""
  engine_version      = ""15.3""
  instance_class      = ""db.t3.micro""
  allocated_storage   = 20
  storage_type        = ""gp2""
  
  db_name             = ""mydb""
  username            = ""dbadmin""
  password            = ""yourpasswordhere""
  
  publicly_accessible = true
  
  db_subnet_group_name   = aws_db_subnet_group.db_subnet_group.name
  vpc_security_group_ids = [aws_security_group.db_sg.id]
  
  skip_final_snapshot = true

  tags = {
    Name = ""postgresql-db""
  }
}

# Output the database endpoint
output ""db_endpoint"" {
  value = aws_db_instance.postgres.endpoint
}
```

This configuration:
1. Sets up a VPC with CIDR block 10.0.0.0/16
2. Creates two subnets in different availability zones
3. Creates a security group allowing PostgreSQL access (port 5432)
4. Sets up a DB subnet group with both subnets
5. Creates a PostgreSQL 15.3 RDS instance with:
   - 20GB storage
   - Public accessibility
   - Proper subnet and security group associations
6. Outputs the database endpoint for connection

Note: Remember to:
- Replace ""yourpasswordhere"" with a secure password
- Consider your security requirements before making the database publicly accessible
- Adjust the region and availability zones if needed
- Consider using variables for sensitive information like passwords
- Review",,,,,
"aws_rds_cluster, aws_vpc","Create an mySQL cluster with 100 GB allocated to us-east-1a, us-east-1b, us-east-1c.","package main

import future.keywords.in

default allow = false

# Required AZs
required_azs := [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]

# Check for RDS cluster creation
rds_cluster_created(resources) {
some resource in resources
resource.type == ""aws_rds_cluster""
resource.change.actions[_] == ""create""
}

# Check if engine is MySQL
rds_engine_mysql(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.engine == ""mysql""
}

# Check if AZs are correct and storage is 100GB
azs_and_storage_valid(resource) {
resource.type == ""aws_rds_cluster""
resource.change.after.availability_zones == required_azs
resource.change.after.allocated_storage == 100
}

# Aggregate checks for RDS cluster
allow {
rds_cluster_created(input.resource_changes)
some resource in input.resource_changes
rds_engine_mysql(resource)
azs_and_storage_valid(resource)
}",3,"terraform {
required_providers {
aws = {
source = ""hashicorp/aws""
version = ""~> 5.0""
}
}
}

# Configure the AWS Provider
provider ""aws"" {
region = ""us-east-1""
}

resource ""aws_vpc"" ""vpc"" {
cidr_block = ""192.168.0.0/22""
}

resource ""aws_rds_cluster"" ""example"" {
cluster_identifier = ""example""
availability_zones = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
engine = ""mysql""
db_cluster_instance_class = ""db.r6gd.large""
storage_type = ""io1""
allocated_storage = 100
iops = 1000
master_username = ""test""
master_password = ""mustbeeightcharaters""
skip_final_snapshot = true
}
","Has one ""aws_rds_cluster"" resource
with one ""allocated_storage"" = 100
with one ""availability_zones"" = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
with one ""engine"" = ""mysql""","I'll help you create a Terraform configuration for an Amazon RDS MySQL cluster with multi-AZ deployment across three availability zones. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_subnet_group"" ""mysql_subnet_group"" {
  name        = ""mysql-subnet-group""
  description = ""Subnet group for MySQL cluster""
  subnet_ids  = [""subnet-12345678"", ""subnet-87654321"", ""subnet-11223344""] # Replace with your subnet IDs
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier     = ""mysql-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.10.2""
  database_name         = ""mydb""
  master_username       = ""admin""
  master_password       = ""your-secure-password""
  storage_encrypted     = true
  skip_final_snapshot   = true
  db_subnet_group_name  = aws_db_subnet_group.mysql_subnet_group.name
  
  availability_zones = [
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ]
}

resource ""aws_rds_cluster_instance"" ""mysql_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r5.large""
  engine              = ""aurora-mysql""
  engine_version      = ""5.7.mysql_aurora.2.10.2""
  
  allocated_storage   = 100
  
  availability_zone   = element([
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ], count.index)
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a DB subnet group across multiple AZs
3. Creates an Aurora MySQL cluster
4. Creates three DB instances (one in each AZ) with 100GB storage each
5. Distributes the instances across us-east-1a, us-east-1b, and us-east-1c

Note: 
- Replace the subnet IDs with your actual subnet IDs
- Choose a secure password for the master_password
- You may want to adjust the instance_class based on your performance needs
- The storage allocation is handled automatically by Aurora, but set to 100GB as requested

Remember to properly secure your database by setting up appropriate security groups and network access controls, which aren't included in this basic example.",,,,,,"I'll help you create a Terraform configuration for an AWS RDS MySQL cluster with multi-AZ deployment across three availability zones, each with 100 GB storage. Here's the solution:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier     = ""mysql-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.10.2""
  database_name         = ""mydb""
  master_username       = ""admin""
  master_password       = ""your-secure-password""
  skip_final_snapshot   = true
  availability_zones    = [""us-east-1a"", ""us-east-1b"", ""us-east-1c""]
  db_subnet_group_name  = aws_db_subnet_group.mysql_subnet_group.name
}

resource ""aws_rds_cluster_instance"" ""mysql_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r5.large""
  engine              = ""aurora-mysql""
  engine_version      = ""5.7.mysql_aurora.2.10.2""
  allocated_storage   = 100
  availability_zone   = element([""us-east-1a"", ""us-east-1b"", ""us-east-1c""], count.index)
}

resource ""aws_db_subnet_group"" ""mysql_subnet_group"" {
  name       = ""mysql-subnet-group""
  subnet_ids = [""subnet-xxxxx"", ""subnet-yyyyy"", ""subnet-zzzzz""] # Replace with your subnet IDs
}

resource ""aws_security_group"" ""mysql_sg"" {
  name        = ""mysql-security-group""
  description = ""Security group for MySQL cluster""
  vpc_id      = ""vpc-xxxxx"" # Replace with your VPC ID

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = ""tcp""
    cidr_blocks = [""0.0.0.0/0""] # Modify as per your security requirements
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = ""-1""
    cidr_blocks = [""0.0.0.0/0""]
  }
}
```

This configuration creates:
1. An Aurora MySQL cluster across three availability zones
2. Three DB instances, one in each AZ, each with 100 GB storage
3. A DB subnet group for the cluster
4. A security group for MySQL access

Note: 
- Replace the subnet IDs and VPC ID with your actual values
- Update the security group ingress rules according to your security requirements
- Choose an appropriate instance class based on your workload requirements
- Update the master password with a secure value
- The engine version might need to be adjusted based on your requirements",,,,,,"I'll help you create a MySQL cluster using AWS RDS with multi-AZ deployment across the specified availability zones. Here's the Terraform configuration:

```hcl
terraform {
  required_providers {
    aws = {
      source  = ""hashicorp/aws""
      version = ""~> 4.0""
    }
  }
}

provider ""aws"" {
  region = ""us-east-1""
}

resource ""aws_db_subnet_group"" ""mysql_subnet_group"" {
  name       = ""mysql-subnet-group""
  subnet_ids = [""subnet-12345678"", ""subnet-87654321"", ""subnet-11223344""] # Replace with your subnet IDs

  tags = {
    Name = ""MySQL DB subnet group""
  }
}

resource ""aws_rds_cluster"" ""mysql_cluster"" {
  cluster_identifier     = ""mysql-cluster""
  engine                = ""aurora-mysql""
  engine_version        = ""5.7.mysql_aurora.2.10.2""
  database_name         = ""mydb""
  master_username       = ""admin""
  master_password       = ""your-secure-password""
  db_subnet_group_name  = aws_db_subnet_group.mysql_subnet_group.name
  skip_final_snapshot   = true

  availability_zones = [
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ]
}

resource ""aws_rds_cluster_instance"" ""cluster_instances"" {
  count               = 3
  identifier          = ""mysql-cluster-${count.index}""
  cluster_identifier  = aws_rds_cluster.mysql_cluster.id
  instance_class      = ""db.r5.large""
  engine              = ""aurora-mysql""
  engine_version      = ""5.7.mysql_aurora.2.10.2""
  allocated_storage   = 100

  availability_zone = element([
    ""us-east-1a"",
    ""us-east-1b"",
    ""us-east-1c""
  ], count.index)
}
```

This configuration:
1. Sets up the required AWS provider
2. Creates a DB subnet group for the cluster
3. Creates an Aurora MySQL cluster
4. Creates 3 cluster instances with 100GB storage each, distributed across us-east-1a, us-east-1b, and us-east-1c

Note: 
- Replace the subnet IDs with your actual subnet IDs
- Choose a secure password for the master_password
- Adjust the instance_class based on your performance requirements
- You may want to add security groups and other security measures
- The allocated_storage is set to 100GB as requested

Make sure you have the appropriate VPC, subnets, and network configuration in place before applying this configuration.",,,,,
